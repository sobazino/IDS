{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import time\n",
    "import cpuinfo\n",
    "import torch\n",
    "import psutil\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import platform\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score, accuracy_score, roc_auc_score, precision_recall_curve, auc, f1_score, recall_score, precision_score, classification_report\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info().rss / (1024 ** 2)\n",
    "    return mem\n",
    "\n",
    "# print(\"=\" * 40, \"SYSTEM INFORMATION\", \"=\" * 40)\n",
    "# print(f\"Operating System : {platform.system()} {platform.release()} ({platform.version()})\")\n",
    "# print(f\"Architecture     : {platform.architecture()[0]}\")\n",
    "# print(f\"Machine Type     : {platform.machine()}\")\n",
    "# cpu = cpuinfo.get_cpu_info()\n",
    "# print(f\"CPU Brand        : {cpu['brand_raw']}\")\n",
    "# print(f\"Logical Cores    : {psutil.cpu_count(logical=True)}\")\n",
    "# print(f\"Physical Cores   : {psutil.cpu_count(logical=False)}\")\n",
    "# print(f\"CPU Frequency    : {psutil.cpu_freq().current:.2f} MHz\")\n",
    "# ram = psutil.virtual_memory()\n",
    "# print(f\"Total RAM        : {ram.total / 1e9:.2f} GB\")\n",
    "# print(f\"Python Version   : {platform.python_version()}\")\n",
    "# print(\"=\" * 40, \"PYTORCH & GPU\", \"=\" * 40)\n",
    "# print(f\"PyTorch Version  : {torch.__version__}\")\n",
    "# print(f\"CUDA Available   : {torch.cuda.is_available()}\")\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"CUDA Version     : {torch.version.cuda}\")\n",
    "#     print(f\"cuDNN Version    : {torch.backends.cudnn.version()}\")\n",
    "#     print(f\"GPU Count        : {torch.cuda.device_count()}\")\n",
    "#     for i in range(torch.cuda.device_count()):\n",
    "#         print(f\"GPU {i} Name      : {torch.cuda.get_device_name(i)}\")\n",
    "#         print(f\"  Capability     : {torch.cuda.get_device_capability(i)}\")\n",
    "#         print(f\"  Allocated Mem  : {torch.cuda.memory_allocated(i)/1e6:.2f} MB\")\n",
    "#         print(f\"  Reserved Mem   : {torch.cuda.memory_reserved(i)/1e6:.2f} MB\")\n",
    "# else:\n",
    "#     print(\"GPU              : Not Available\")\n",
    "# print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0eae2c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "76a04f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MODEL, self).__init__()\n",
    "        self.C1 = nn.Conv1d(2, 16, 4)\n",
    "        self.C2 = nn.Conv1d(16, 16, 4)\n",
    "        self.C3 = nn.Conv1d(16, 16, 4)\n",
    "        self.pool = nn.MaxPool1d(3)\n",
    "        \n",
    "        self.F1 = nn.Linear(16, 32)\n",
    "        self.F2 = nn.Linear(32, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.attn_weights = nn.Parameter(torch.randn(78))\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = torch.matmul(x, self.attn_weights)\n",
    "        attention = torch.sigmoid(attention).unsqueeze(1)\n",
    "        y = (x * attention).unsqueeze(1)\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        \n",
    "        x = self.tanh(self.C1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.tanh(self.C2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.tanh(self.C3(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x).squeeze(2)\n",
    "        x = self.tanh(self.F1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.F2(x)).squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6947db8",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4a0c5b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    start_time = time.time()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch).cpu().numpy()\n",
    "            preds = (outputs >= 0.5).astype(int)\n",
    "            all_predictions.extend(preds if preds.ndim == 1 else preds.tolist())\n",
    "            all_labels.extend(y_batch.cpu().numpy().tolist())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    M = \"OK-\"\n",
    "    if len(set(all_predictions)) == 1:\n",
    "        M = \"ER-\"\n",
    "        \n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    \n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "    aupr = average_precision_score(all_labels, all_predictions)\n",
    "    Far = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "    \n",
    "    print(classification_report(all_labels, all_predictions, digits=5))\n",
    "    errors = [(i, p, l) for i, (p, l) in enumerate(zip(all_predictions, all_labels)) if p != l]\n",
    "    print(f\"Total Errors: {len(errors)}\")\n",
    "    for i, pred, label in errors[:5]:\n",
    "        print(f\"Index: {i}, Predicted: {pred}, Actual: {label}\")\n",
    "    memory_usage = get_memory_usage()\n",
    "    return f\"{M} Accuracy: {accuracy:.5f}, Precision: {precision:.5f}, Recall: {recall:.5f}, F1: {f1:.5f}, ROC AUC: {roc_auc:.5f}, AUPR (PR-AUC): {aupr:.5f}, Sensitivity: {sensitivity:.5f}, Specificity: {specificity:.5f}, Far: {Far}, False Positive Rate (FPR): {false_positive_rate:.5f}, False Negative Rate (FNR): {false_negative_rate:.5f}, Runtime: {elapsed_time:.3f} sec , Memory Usage: {memory_usage:.2f} MB\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665976f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cc26c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, epochs, train_loader, val_loader, test_loader):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs_train = model(X_train_batch)\n",
    "            loss_train = criterion(outputs_train, y_train_batch.float())\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss_batch = 0\n",
    "                    for X_val_batch, y_val_batch in val_loader:\n",
    "                        outputs_val = model(X_val_batch)\n",
    "                        loss_val = criterion(outputs_val, y_val_batch.float())\n",
    "                        val_loss_batch += loss_val.item()\n",
    "                    train_loss_batch = 0\n",
    "                    for X_train_batch, y_train_batch in train_loader:\n",
    "                        outputs_train = model(X_train_batch)\n",
    "                        loss_train = criterion(outputs_train, y_train_batch.float())\n",
    "                        train_loss_batch += loss_train.item()\n",
    "                        \n",
    "                    val_loss_avg = val_loss_batch / len(val_loader)\n",
    "                    train_loss_avg = train_loss_batch / len(train_loader)\n",
    "                \n",
    "                for param_group in optimizer.param_groups:\n",
    "                    lrnum = param_group['lr']\n",
    "                train_losses.append(train_loss_avg)\n",
    "                val_losses.append(val_loss_avg)\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Train Loss: {loss_train.item():.4f}, Val Loss: {val_loss_avg:.4f}, LR: {lrnum:.10f}')\n",
    "                model.train()\n",
    "        \n",
    "        if test_loader and epoch % 10 == 0:\n",
    "            RES = test(model, test_loader)\n",
    "            print(f\"Epoch {epoch+1}: {RES}\")\n",
    "\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('loss.pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49384efa",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fabedcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CONVERT(df):\n",
    "    X = df.drop(' Label', axis=1)\n",
    "    y = df[' Label']\n",
    "    X.columns = X.columns.str.strip()\n",
    "    important_features = ['Bwd Packet Length Std', 'Average Packet Size', 'Flow Duration', 'Flow IAT Std']\n",
    "    important_df = X[important_features] * 2.0\n",
    "    remaining_df = X.drop(columns=important_features)\n",
    "    X = pd.concat([remaining_df, important_df], axis=1)\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a6db8a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== D: cpu ===========\n",
      "\n",
      " Label\n",
      "1    5675\n",
      "0    4325\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/EAAAEDCAYAAAB9BISuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgJ0lEQVR4nO3dfWxV930/8I+via8HlIeUxQTKgggtBGXBmSkIVJZOc8akqglJutEoK8gpRF3nKOKqycqWmlDaOiOUsKUkjKdS0kTQVWgLVUWbuqVpGzSGGWnXmlRbHiisNlCasjiKITa/P1j9i8tDz7HvHZzL6yUdCX/v537P54C5vm9/zzm34vTp06cDAAAAuOTlLnYDAAAAQDJCPAAAAGSEEA8AAAAZIcQDAABARgjxAAAAkBFCPAAAAGSEEA8AAAAZIcQDAABARgjxAAAAkBGDLnYDvR54IHltc3Py2tOnk9e+9Vby2nw+ee0Pf5i8dty45LURET//efLawYOT1151VfLan/0see3Ysclr3/GO5LWnTiWvjYh4883ktYcPJ6+dPDl5bWtr8topU5LXHjmSvPbkyeS1Eycmr42I2Ls3ee0NNySvffXVdH0k9e53J6996aXktddem76XUujuvvjzVlUlr03zffzyy8lrKyuT10ake928+urktcOHJ69Nc3xpvt/Wr09e++STyWtnzEheGxHxyCPp6pN66qnktbW1yWt/7/eS16b5/kn7vVnO0ryupPn5Pyjl2940/yYHDyavHT06eW2antPUpnnPm0ux5pf2Z01FRfLaNO83R45MXtvRkbw2zXvTd74zee1PfpK8Ns172Ih0799GjUo3d1ak+R6OiOjpKU0fRXTphHgAAAAopjS/LMoIIR4AAIDyJMQDAABARgjxAAAAkBFCPAAAAGSEEA8AAAAZIcQDAABARgjxAAAAkBFCPAAAAGRELnexOyg6IR4AAIDyZCUeAAAAMkKIBwAAgIwQ4gEAACAjhHgAAADICCEeAAAAMkKIBwAAgIwQ4gEAACAjhHgAAADIiFzuYndQdEI8AAAA5clKPAAAAGSEEA8AAAAZIcQDAABARgjxAAAAkBFCPAAAAGSEEA8AAAAZIcQDAABARgjxAAAAkBG53MXuoOiEeAAAAMqTlXgAAADIiDIM8eV3bgEAAABEnAnxabaU1qxZE+PHj4/q6uqYMWNG7Nmz57y1mzdvjoqKij5bdXV16n0K8QAAAJSnEob4bdu2RaFQiKVLl8a+ffti6tSpMWfOnDhy5Mh5nzNs2LD4+c9/3ru9+uqrqQ9JiAcAAKA8lTDEr1q1KhYtWhQNDQ0xZcqUWLt2bQwePDg2bdp0gXYqYvTo0b1bTU1N6kMS4gEAAChPKUN8V1dXnDhxos/W1dV11rQnT56M1tbWqK+v7x3L5XJRX18fu3fvPm87r7/+elxzzTUxbty4uPXWW+PHP/5x6kNKFeKPHTsWK1asiNtuuy1mzpwZM2fOjNtuuy0eeeSROHr0aOqdAwAAQMmkDPHNzc0xfPjwPltzc/NZ0x47diy6u7vPWkmvqamJ9vb2c7YyadKk2LRpU/zLv/xLfPnLX46enp6YNWtWHDp0KNUhJb47/b/927/FnDlzYvDgwVFfXx/vec97IiKio6Mj/uEf/iEefvjh+MY3vhHTpk1L1QAAAACURMpT5JcsWRKFQqHPWD6fL0orv14I/7VZs2bFddddF//4j/8Yy5cvTzxP4hB/7733xp/92Z/F2rVro+I3/iJOnz4dH/vYx+Lee++94KkDAAAA8H8ml+4K8nw+nyi0jxo1KiorK6Ojo6PPeEdHR4wePTrRvq644oq48cYb4z//8z9T9Zj4iF544YVYvHjxWQE+4szF+YsXL479+/en2jkAAACUTIlubFdVVRV1dXXR0tLSO9bT0xMtLS19VtsvpLu7O370ox/F1VdfneqQEof40aNHX/Az7/bs2dOvO+sBAABASZTw7vSFQiHWr18fX/rSl6KtrS3+8i//Mjo7O6OhoSEiIubPnx9Llizprf/0pz8d3/zmN+Oll16Kffv2xV/8xV/Eq6++GgsXLky138Sn03/iE5+Ie+65J1pbW+OP//iPewN7R0dHtLS0xPr162PlypWpdg4AAAAlkzKYpzFv3rw4evRoNDU1RXt7e9TW1sbOnTt7s/LBgwcj97bT+X/5y1/GokWLor29PUaOHBl1dXXx/PPPx5QpU1LtN3GI/6u/+qsYNWpUPProo/H4449Hd3d3RERUVlZGXV1dbN68Of78z/881c4BAACgZEoY4iMiGhsbo7Gx8ZyP7dq1q8/Xjz76aDz66KMD3mfiEB9x5jcN8+bNi1OnTsWxY8ci4swF/VdcccWAGwEAAICiKnGIvxhShfhfu+KKK1JffA8AAAD/p4R4AAAAyAghHgAAADJCiAcAAICMEOIBAAAgI972EW/lIlGIf+aZZxJPeMstt/S7GQAAACiay3Ulfu7cuYkmq6io6P38eAAAALioLtcQ39PTU+o+AAAAoLjKMMQP6AKBN998s1h9AAAAQHFVVKTbMiB1iO/u7o7ly5fH2LFjY+jQofHSSy9FRMSnPvWp2LhxY9EbBAAAgH4R4iM++9nPxubNm2PFihVRVVXVO3799dfHhg0bitocAAAA9JsQH7Fly5ZYt25d3HXXXVFZWdk7PnXq1Dhw4EBRmwMAAIB+K8MQn/pz4g8fPhwTJ048a7ynpydOnTpVlKYAAABgwDISzNNIvRI/ZcqU+N73vnfW+Fe/+tW48cYbi9IUAAAADFgul27LgNQr8U1NTbFgwYI4fPhw9PT0xPbt2+PFF1+MLVu2xNe+9rVEc3R1dUVXV1efsfxbb0V+UOp2AAAA4NysxEfceuutsWPHjvjWt74VQ4YMiaampmhra4sdO3bEzTffnGiO5ubmGD58eJ+t+V//NXXzAAAAcF6uiT9j9uzZ8eyzz/Z7p0uWLIlCodBnLP/QQ/2eDwAAAM6SkWCeRr/PX9+7d2+0tbVFxJnr5Ovq6hI/N5/PRz6f/41OnEoPAABAEQnxEYcOHYo777wzfvCDH8SIESMiIuK1116LWbNmxdatW+Nd73pXsXsEAACA9MowxKe+Jn7hwoVx6tSpaGtri+PHj8fx48ejra0tenp6YuHChaXoEQAAANJzTXzEd7/73Xj++edj0qRJvWOTJk2Kxx57LGbPnl3U5gAAAKDfMhLM00gd4seNGxenTp06a7y7uzvGjBlTlKYAAABgwMowxKc+nf6RRx6Je++9N/bu3ds7tnfv3rjvvvti5cqVRW0OAAAA+i2XS7dlQKKV+JEjR0bF236D0dnZGTNmzIhB/3tH+bfeeisGDRoUd999d8ydO7ckjQIAAEAqZbgSnyjEr169usRtAAAAQJFdriF+wYIFpe4DAAAAiutyDfHn8+abb8bJkyf7jA0bNmxADQEAAEBRlGGIT33lfmdnZzQ2NsZVV10VQ4YMiZEjR/bZAAAA4JJQ4s+JX7NmTYwfPz6qq6tjxowZsWfPnkTP27p1a1RUVPTrnnKpQ/wDDzwQ3/72t+OJJ56IfD4fGzZsiGXLlsWYMWNiy5YtqRsAAACAkihhiN+2bVsUCoVYunRp7Nu3L6ZOnRpz5syJI0eOXPB5r7zySnziE5+I2bNn9+uQUof4HTt2xOOPPx533HFHDBo0KGbPnh0PPvhgfO5zn4unnnqqX00AAABA0ZUwxK9atSoWLVoUDQ0NMWXKlFi7dm0MHjw4Nm3adN7ndHd3x1133RXLli2LCRMm9OuQUof448eP9+5s2LBhcfz48YiIeN/73hfPPfdcv5oAAACAoksZ4ru6uuLEiRN9tq6urrOmPXnyZLS2tkZ9fX3vWC6Xi/r6+ti9e/d52/n0pz8dV111VXz0ox/t9yGlDvETJkyIl19+OSIiJk+eHF/5ylci4swK/YgRI/rdCAAAABRVLpdqa25ujuHDh/fZmpubz5r22LFj0d3dHTU1NX3Ga2pqor29/ZytfP/734+NGzfG+vXrB3RIqe9O39DQEC+88ELcdNNN8clPfjI++MEPxhe+8IU4depUrFq1akDNAAAAQNGkPEV+yZIlUSgU+ozl8/kBt/E///M/8ZGPfCTWr18fo0aNGtBcqUP84sWLe/9cX18fBw4ciNbW1pg4cWLccMMNA2oGAAAAiiZliM/n84lC+6hRo6KysjI6Ojr6jHd0dMTo0aPPqv+v//qveOWVV+KDH/xg71hPT09ERAwaNChefPHFuPbaaxP1mPp0+t90zTXXxO233x5XXnll3HPPPQOdDgAAAIqjRDe2q6qqirq6umhpaekd6+npiZaWlpg5c+ZZ9ZMnT44f/ehHsX///t7tlltuiT/6oz+K/fv3x7hx4xLvO/VK/Pn84he/iI0bN8a6deuKNSUAAAD0Xz8++z2pQqEQCxYsiGnTpsX06dNj9erV0dnZGQ0NDRERMX/+/Bg7dmw0NzdHdXV1XH/99X2e/+t7yv3m+G9TtBAPAAAAl5QShvh58+bF0aNHo6mpKdrb26O2tjZ27tzZe7O7gwcPRi434JPfzyLEAwAAUJ5KGOIjIhobG6OxsfGcj+3ateuCz928eXO/9inEAwAAUJ5KHOIvhsQh/vbbb7/g46+99tpAewEAAIDiuZxD/PDhw3/r4/Pnzx9wQwAAAFAUl3OI/+IXv1jKPgAAAKC4SnBjuYvNNfEAAACUp8t5JR4AAAAyRYgHAACAjBDiAQAAICOEeAAAAMgIIR4AAAAyQogHAACAjBDiAQAAICOEeAAAAMiIXO5id1B0QjwAAADlyUo8AAAAZIQQDwAAABkhxAMAAEBGCPEAAACQEUI8AAAAZIQQDwAAABkhxAMAAEBGCPEAAACQEbncxe6g6IR4AAAAypOVeAAAAMgIIR4AAAAyQogHAACAjBDiAQAAICOEeAAAAMiIMgzx5Xe/fQAAAIg4E+LTbCmtWbMmxo8fH9XV1TFjxozYs2fPeWu3b98e06ZNixEjRsSQIUOitrY2nnzyydT7FOIBAAAoTyUM8du2bYtCoRBLly6Nffv2xdSpU2POnDlx5MiRc9ZfeeWV8bd/+7exe/fu+OEPfxgNDQ3R0NAQ3/jGN1LtV4gHAACgPOVy6bYUVq1aFYsWLYqGhoaYMmVKrF27NgYPHhybNm06Z/373//+uO222+K6666La6+9Nu6777644YYb4vvf/366Q0pVDQAAAFmRciW+q6srTpw40Wfr6uo6a9qTJ09Ga2tr1NfX947lcrmor6+P3bt3/9a2Tp8+HS0tLfHiiy/GH/7hH6Y6JCEeAACA8pQyxDc3N8fw4cP7bM3NzWdNe+zYseju7o6ampo+4zU1NdHe3n7edn71q1/F0KFDo6qqKj7wgQ/EY489FjfffHOqQ3J3egAAAMpTyuvclyxZEoVCoc9YPp8vWjvveMc7Yv/+/fH6669HS0tLFAqFmDBhQrz//e9PPIcQDwAAQHlKGeLz+Xyi0D5q1KiorKyMjo6OPuMdHR0xevTo8z4vl8vFxIkTIyKitrY22traorm5OaMhfsWK5LU9Pclr0/yjVVeXpoepU5PXpnXllaWZ9/bbk9du35689sMfTl67dWvy2iuuSF4bEfHLXyavPc/dJc9p8uTktW+8kbz29OnktWmO7fXXk9e++93JayMipk1LXpvm/+n/vugVfd40tddem7z2UlGqv4vKyvS9JPHOdyav/djHktem/U367/5u8to017N96EPJa0v1/Xaem+6c009/mrz2uefS91IK731v8tr3vCd5bZqfY08/nbyW/+9SeO8Wke5nb1VV8to0r0NvvZW8Ns3P6TTHVqqfH2n7+NWvStNHmvd5g1JEpwcfTF6b5r3bRz6SvDYi4sSJ5LWjRqWbOytK9DnxVVVVUVdXFy0tLTF37tyIiOjp6YmWlpZobGxMPE9PT885r7m/kEsnxAMAAEAxlSjER0QUCoVYsGBBTJs2LaZPnx6rV6+Ozs7OaGhoiIiI+fPnx9ixY3uvqW9ubo5p06bFtddeG11dXfH1r389nnzyyXjiiSdS7VeIBwAAoDyVMMTPmzcvjh49Gk1NTdHe3h61tbWxc+fO3pvdHTx4MHJv+9i6zs7O+PjHPx6HDh2K3/md34nJkyfHl7/85Zg3b16q/QrxAAAAlKcShviIiMbGxvOePr9r164+X3/mM5+Jz3zmMwPepxAPAABAeSpxiL8YhHgAAADK09tOZy8XQjwAAADlyUo8AAAAZIQQDwAAABkhxAMAAEBGCPEAAACQEUI8AAAAZIQQDwAAABkhxAMAAEBGCPEAAACQEbncxe6g6IR4AAAAypOVeAAAAMgIIR4AAAAyQogHAACAjBDiAQAAICOEeAAAAMgIIR4AAAAyQogHAACAjBDiAQAAICNyuYvdQdEJ8QAAAJQnK/EAAACQEUI8AAAAZIQQDwAAABkhxAMAAEBGCPEAAACQEUI8AAAAZIQQDwAAABkhxAMAAEBG5HIXu4OiK78jAgAAgIgzK/FptpTWrFkT48ePj+rq6pgxY0bs2bPnvLXr16+P2bNnx8iRI2PkyJFRX19/wfrzEeIBAAAoTyUM8du2bYtCoRBLly6Nffv2xdSpU2POnDlx5MiRc9bv2rUr7rzzzvjOd74Tu3fvjnHjxsWf/MmfxOHDh1Ptt2gh/mc/+1ncfffdxZoOAAAABqaEIX7VqlWxaNGiaGhoiClTpsTatWtj8ODBsWnTpnPWP/XUU/Hxj388amtrY/LkybFhw4bo6emJlpaWVPstWog/fvx4fOlLXyrWdAAAADAwKUN8V1dXnDhxos/W1dV11rQnT56M1tbWqK+v7x3L5XJRX18fu3fvTtTaG2+8EadOnYorr7wy1SElvrHdM888c8HHX3rppVQ7BgAAgJJKubre3Nwcy5Yt6zO2dOnSeOihh/qMHTt2LLq7u6OmpqbPeE1NTRw4cCDRvv76r/86xowZ0+cXAUkkDvFz586NioqKOH369HlrKsrw9v0AAABkVMqMumTJkigUCn3G8vl8MTuKiIiHH344tm7dGrt27Yrq6upUz018Ov3VV18d27dvj56ennNu+/btS904AAAAlEzK0+nz+XwMGzasz3auED9q1KiorKyMjo6OPuMdHR0xevToC7a0cuXKePjhh+Ob3/xm3HDDDakPKXGIr6uri9bW1vM+/ttW6QEAAOD/VIlubFdVVRV1dXV9bkr365vUzZw587zPW7FiRSxfvjx27twZ06ZN69chJT6d/v7774/Ozs7zPj5x4sT4zne+068mAAAAoOhKeMl3oVCIBQsWxLRp02L69OmxevXq6OzsjIaGhoiImD9/fowdOzaam5sjIuLv/u7voqmpKZ5++ukYP358tLe3R0TE0KFDY+jQoYn3mzjEz549+4KPDxkyJG666abEOwYAAICSyhXtA9nOMm/evDh69Gg0NTVFe3t71NbWxs6dO3tvdnfw4MHIvW3/TzzxRJw8eTI+9KEP9ZnnXDfOu5DEIR4AAAAypcQ3X29sbIzGxsZzPrZr164+X7/yyitF2acQDwAAQHkqw09QE+IBAAAoT0I8AAAAZIQQDwAAABlxuYb4Z555JvGEt9xyS7+bAQAAgKK5XEP83LlzE01WUVER3d3dA+kHAAAAiuNyDfE9PT2l7gMAAACK63IN8efz5ptvRnV1dbF6AQAAgOLJ5S52B0WX+oi6u7tj+fLlMXbs2Bg6dGi89NJLERHxqU99KjZu3Fj0BgEAAKBfKirSbRmQOsR/9rOfjc2bN8eKFSuiqqqqd/z666+PDRs2FLU5AAAA6DchPmLLli2xbt26uOuuu6KysrJ3fOrUqXHgwIGiNgcAAAD9VoYhPvU18YcPH46JEyeeNd7T0xOnTp0qSlMAAAAwYBkJ5mmkXomfMmVKfO973ztr/Ktf/WrceOONRWkKAAAABsxKfERTU1MsWLAgDh8+HD09PbF9+/Z48cUXY8uWLfG1r30t0RxdXV3R1dXVZyyfz0c+n0/bDgAAAJxbRoJ5GqlX4m+99dbYsWNHfOtb34ohQ4ZEU1NTtLW1xY4dO+Lmm29ONEdzc3MMHz68z9bc3Jy6eQAAADgvK/FnzJ49O5599tl+73TJkiVRKBT6jFmFBwAAoKgyEszT6FeIj4jYu3dvtLW1RcSZ6+Tr6uoSP9ep8wAAAJRcLvXJ55e81CH+0KFDceedd8YPfvCDGDFiREREvPbaazFr1qzYunVrvOtd7yp2jwAAAJBeGa7Ep/61xMKFC+PUqVPR1tYWx48fj+PHj0dbW1v09PTEwoULS9EjAAAApOea+Ijvfve78fzzz8ekSZN6xyZNmhSPPfZYzJ49u6jNAQAAQL9lJJinkTrEjxs3Lk6dOnXWeHd3d4wZM6YoTQEAAMCAlWGIT306/SOPPBL33ntv7N27t3ds7969cd9998XKlSuL2hwAAAD02+V6Ov3IkSOj4m0H1NnZGTNmzIhBg848/a233opBgwbF3XffHXPnzi1JowAAAJBKRoJ5GolC/OrVq0vcBgAAABTZ5RriFyxYUOo+AAAAoLgu1xB/Pm+++WacPHmyz9iwYcMG1BAAAAAURS71beAueamPqLOzMxobG+Oqq66KIUOGxMiRI/tsAAAAcEkowxvbpQ7xDzzwQHz729+OJ554IvL5fGzYsCGWLVsWY8aMiS1btpSiRwAAAEhPiI/YsWNHPP7443HHHXfEoEGDYvbs2fHggw/G5z73uXjqqadK0SMAAACkV+IQv2bNmhg/fnxUV1fHjBkzYs+ePeet/fGPfxx33HFHjB8/PioqKvp9A/nUIf748eMxYcKEiDhz/fvx48cjIuJ973tfPPfcc/1qAgAAAIquhCF+27ZtUSgUYunSpbFv376YOnVqzJkzJ44cOXLO+jfeeCMmTJgQDz/8cIwePbrfh5Q6xE+YMCFefvnliIiYPHlyfOUrX4mIMyv0I0aM6HcjAAAAUFQlDPGrVq2KRYsWRUNDQ0yZMiXWrl0bgwcPjk2bNp2z/r3vfW888sgj8eEPfzjy+Xy/Dyl1iG9oaIgXXnghIiI++clPxpo1a6K6ujoWL14c999/f78bAQAAgKJKGeK7urrixIkTfbaurq6zpj158mS0trZGfX1971gul4v6+vrYvXt3SQ8p9UfMLV68uPfP9fX1ceDAgWhtbY2JEyfGDTfcUNTmAAAAoN9Srq43NzfHsmXL+owtXbo0HnrooT5jx44di+7u7qipqekzXlNTEwcOHOhXq0kN6HPiIyKuueaauOaaa+LQoUNxzz33xLp164rRFwAAAAxMyhC/ZMmSKBQKfcYGcup7KaQ+nf58fvGLX8TGjRuLNR0AAAAMTMrT6fP5fAwbNqzPdq4QP2rUqKisrIyOjo4+4x0dHQO6aV0SRQvxAAAAcEnJ5dJtCVVVVUVdXV20tLT0jvX09ERLS0vMnDmzFEfSa8Cn0wMAAMAlqR+f/Z5UoVCIBQsWxLRp02L69OmxevXq6OzsjIaGhoiImD9/fowdOzaam5sj4szN8H7yk5/0/vnw4cOxf//+GDp0aEycODHxfoV4AAAAylMJQ/y8efPi6NGj0dTUFO3t7VFbWxs7d+7svdndwYMHI/e21f3//u//jhtvvLH365UrV8bKlSvjpptuil27diXeb+IQf/vtt1/w8ddeey3xTgEAAKDkShjiIyIaGxujsbHxnI/9ZjAfP358nD59esD7TBzihw8f/lsfnz9//oAbAgAAgKIocYi/GBKH+C9+8Yul7AMAAACK63IO8QAAAJApQjwAAABkhBAPAAAAGSHEAwAAQEa87SPeyoUQDwAAQHmyEg8AAAAZIcQDAABARgjxAAAAkBFCPAAAAGSEEA8AAAAZIcQDAABARgjxAAAAkBFCPAAAAGRELnexOyg6IR4AAIDyZCUeAAAAMkKIBwAAgIwQ4gEAACAjhHgAAADICCEeAAAAMkKIBwAAgIwQ4gEAACAjhHgAAADIiFzuYndQdEI8AAAA5clKPAAAAGSEEA8AAAAZIcQDAABARpRhiC+/q/wBAAAg4kyIT7OltGbNmhg/fnxUV1fHjBkzYs+ePRes/6d/+qeYPHlyVFdXx+///u/H17/+9dT7FOIBAAAoTyUM8du2bYtCoRBLly6Nffv2xdSpU2POnDlx5MiRc9Y///zzceedd8ZHP/rR+Pd///eYO3duzJ07N/7jP/4j1X6FeAAAAMpTCUP8qlWrYtGiRdHQ0BBTpkyJtWvXxuDBg2PTpk3nrP/7v//7+NM//dO4//7747rrrovly5fHH/zBH8QXvvCFVPsV4gEAAChPJQrxJ0+ejNbW1qivr+8dy+VyUV9fH7t37z7nc3bv3t2nPiJizpw5560/Hze2AwAAoDylXF3v6uqKrq6uPmP5fD7y+XyfsWPHjkV3d3fU1NT0Ga+pqYkDBw6cc+729vZz1re3t6fqMZshPncJnEBwKfRQStu3l2berVtLM29aV11Vmto0Zs8uzby1taWZN61S3Qm03P/vlUrW/t4qK5PXlur1qtyl/K1/5rznPaWZ91L5OVbOSvV6VcrXwauvLs28g1K8Vf/pT0vTQxppXrvTmjq1NPNec01p5l2xojTzUnLNzc2xbNmyPmNLly6Nhx566OI0dA7ZDPEAAABQZEuWLIlCodBn7DdX4SMiRo0aFZWVldHR0dFnvKOjI0aPHn3OuUePHp2q/nwytjQDAAAApZHP52PYsGF9tnOF+Kqqqqirq4uWlpbesZ6enmhpaYmZM2eec+6ZM2f2qY+IePbZZ89bfz5W4gEAACClQqEQCxYsiGnTpsX06dNj9erV0dnZGQ0NDRERMX/+/Bg7dmw0NzdHRMR9990XN910U3z+85+PD3zgA7F169bYu3dvrFu3LtV+hXgAAABIad68eXH06NFoamqK9vb2qK2tjZ07d/bevO7gwYORe9v9OGbNmhVPP/10PPjgg/E3f/M38e53vzv++Z//Oa6//vpU+604ffr06aIeCQAAAFASrokHAACAjBDiAQAAICOEeAAAAMgIIR4AAAAyQogHAACAjBDiAQAAICOEeAAAAMgIIR4AAAAyQogHAACAjBDiAQAAICOEeAAAAMgIIR4AAAAy4v8BNJZamliWIXYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1400x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = None, None, None\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'=========== D: {device} ===========\\n')\n",
    "current_dir = \"Data/\"\n",
    "df = pd.read_csv(os.path.join(current_dir, 'DDos.csv'))\n",
    "encoder = LabelEncoder()\n",
    "df[' Label'] = encoder.fit_transform(df[' Label'])\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna()\n",
    "df = df.astype(int)\n",
    "df = df.sample(n=10000, random_state=28, replace=False)\n",
    "X, y = CONVERT(df)\n",
    "print(y.value_counts())\n",
    "X_array = np.array(X)\n",
    "y_array = np.array(y)\n",
    "X_class1 = X_array[y_array == 1]\n",
    "X_class0 = X_array[y_array == 0]\n",
    "mean1 = X_class1.mean(axis=0)\n",
    "mean0 = X_class0.mean(axis=0)\n",
    "mean_features = np.vstack([X_class0.mean(axis=0), X_class1.mean(axis=0)])\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_array)\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_array)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(mean1, color='black', label='Label 1')\n",
    "plt.plot(mean0, color='red', label='Label 0')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[y_array==0, 0], X_pca[y_array==0, 1], color='red', alpha=0.5, label='Label 0')\n",
    "plt.scatter(X_pca[y_array==1, 0], X_pca[y_array==1, 1], color='black', alpha=0.5, label='Label 1')\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_tsne[y_array==0, 0], X_tsne[y_array==0, 1], color='red', alpha=0.5, label='Label 0')\n",
    "plt.scatter(X_tsne[y_array==1, 0], X_tsne[y_array==1, 1], color='black', alpha=0.5, label='Label 1')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "cmap = LinearSegmentedColormap.from_list(\"c\", [\"white\", \"red\"])\n",
    "plt.figure(figsize=(14, 3))\n",
    "sns.heatmap(mean_features, cmap=cmap, annot=False, xticklabels=False, yticklabels=[\"Label 0\", \"Label 1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4a21a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523309d1",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9fa6c1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== TP: 2,879 ===========\n",
      "Epoch [1/500], Batch [10/110], Train Loss: 0.6807, Val Loss: 0.6846, LR: 0.0009999999\n",
      "Epoch [1/500], Batch [20/110], Train Loss: 0.6962, Val Loss: 0.6697, LR: 0.0009999997\n",
      "Epoch [1/500], Batch [30/110], Train Loss: 0.6599, Val Loss: 0.6580, LR: 0.0009999993\n",
      "Epoch [1/500], Batch [40/110], Train Loss: 0.6330, Val Loss: 0.6360, LR: 0.0009999987\n",
      "Epoch [1/500], Batch [50/110], Train Loss: 0.5584, Val Loss: 0.5982, LR: 0.0009999980\n",
      "Epoch [1/500], Batch [60/110], Train Loss: 0.4633, Val Loss: 0.5441, LR: 0.0009999971\n",
      "Epoch [1/500], Batch [70/110], Train Loss: 0.5050, Val Loss: 0.4870, LR: 0.0009999960\n",
      "Epoch [1/500], Batch [80/110], Train Loss: 0.4498, Val Loss: 0.4191, LR: 0.0009999948\n",
      "Epoch [1/500], Batch [90/110], Train Loss: 0.3367, Val Loss: 0.3353, LR: 0.0009999935\n",
      "Epoch [1/500], Batch [100/110], Train Loss: 0.2124, Val Loss: 0.2483, LR: 0.0009999919\n",
      "Epoch [1/500], Batch [110/110], Train Loss: 0.2884, Val Loss: 0.1754, LR: 0.0009999902\n",
      "Confusion Matrix:\n",
      "[[593  44]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99832   0.93093   0.96344       637\n",
      "           1    0.95143   0.99884   0.97456       863\n",
      "\n",
      "    accuracy                        0.97000      1500\n",
      "   macro avg    0.97488   0.96488   0.96900      1500\n",
      "weighted avg    0.97134   0.97000   0.96984      1500\n",
      "\n",
      "Total Errors: 45\n",
      "Index: 41, Predicted: 1, Actual: 0\n",
      "Index: 44, Predicted: 1, Actual: 0\n",
      "Index: 168, Predicted: 1, Actual: 0\n",
      "Index: 176, Predicted: 1, Actual: 0\n",
      "Index: 200, Predicted: 1, Actual: 0\n",
      "Epoch 1: OK- Accuracy: 0.97000, Precision: 0.95143, Recall: 0.99884, F1: 0.97456, ROC AUC: 0.96488, AUPR (PR-AUC): 0.95100, Sensitivity: 0.99884, Specificity: 0.93093, Far: 0.06907378335949764, False Positive Rate (FPR): 0.06907, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 415.54 MB\n",
      "Epoch [2/500], Batch [10/110], Train Loss: 0.0637, Val Loss: 0.1336, LR: 0.0009999884\n",
      "Epoch [2/500], Batch [20/110], Train Loss: 0.1472, Val Loss: 0.1141, LR: 0.0009999864\n",
      "Epoch [2/500], Batch [30/110], Train Loss: 0.1572, Val Loss: 0.1032, LR: 0.0009999842\n",
      "Epoch [2/500], Batch [40/110], Train Loss: 0.0310, Val Loss: 0.0979, LR: 0.0009999818\n",
      "Epoch [2/500], Batch [50/110], Train Loss: 0.0501, Val Loss: 0.0856, LR: 0.0009999793\n",
      "Epoch [2/500], Batch [60/110], Train Loss: 0.0507, Val Loss: 0.1076, LR: 0.0009999767\n",
      "Epoch [2/500], Batch [70/110], Train Loss: 0.1099, Val Loss: 0.0803, LR: 0.0009999738\n",
      "Epoch [2/500], Batch [80/110], Train Loss: 0.0395, Val Loss: 0.0760, LR: 0.0009999708\n",
      "Epoch [2/500], Batch [90/110], Train Loss: 0.0935, Val Loss: 0.0992, LR: 0.0009999677\n",
      "Epoch [2/500], Batch [100/110], Train Loss: 0.1292, Val Loss: 0.0726, LR: 0.0009999644\n",
      "Epoch [2/500], Batch [110/110], Train Loss: 0.0232, Val Loss: 0.0718, LR: 0.0009999609\n",
      "Epoch [3/500], Batch [10/110], Train Loss: 0.0350, Val Loss: 0.0729, LR: 0.0009999573\n",
      "Epoch [3/500], Batch [20/110], Train Loss: 0.0449, Val Loss: 0.0702, LR: 0.0009999535\n",
      "Epoch [3/500], Batch [30/110], Train Loss: 0.0184, Val Loss: 0.0715, LR: 0.0009999495\n",
      "Epoch [3/500], Batch [40/110], Train Loss: 0.0387, Val Loss: 0.0699, LR: 0.0009999454\n",
      "Epoch [3/500], Batch [50/110], Train Loss: 0.1030, Val Loss: 0.0728, LR: 0.0009999411\n",
      "Epoch [3/500], Batch [60/110], Train Loss: 0.0290, Val Loss: 0.0728, LR: 0.0009999367\n",
      "Epoch [3/500], Batch [70/110], Train Loss: 0.1323, Val Loss: 0.0622, LR: 0.0009999321\n",
      "Epoch [3/500], Batch [80/110], Train Loss: 0.0176, Val Loss: 0.0602, LR: 0.0009999273\n",
      "Epoch [3/500], Batch [90/110], Train Loss: 0.1317, Val Loss: 0.0605, LR: 0.0009999224\n",
      "Epoch [3/500], Batch [100/110], Train Loss: 0.1451, Val Loss: 0.0861, LR: 0.0009999173\n",
      "Epoch [3/500], Batch [110/110], Train Loss: 0.0217, Val Loss: 0.0580, LR: 0.0009999121\n",
      "Epoch [4/500], Batch [10/110], Train Loss: 0.1637, Val Loss: 0.0760, LR: 0.0009999067\n",
      "Epoch [4/500], Batch [20/110], Train Loss: 0.0259, Val Loss: 0.0592, LR: 0.0009999011\n",
      "Epoch [4/500], Batch [30/110], Train Loss: 0.1209, Val Loss: 0.0638, LR: 0.0009998953\n",
      "Epoch [4/500], Batch [40/110], Train Loss: 0.0231, Val Loss: 0.0710, LR: 0.0009998895\n",
      "Epoch [4/500], Batch [50/110], Train Loss: 0.0135, Val Loss: 0.0590, LR: 0.0009998834\n",
      "Epoch [4/500], Batch [60/110], Train Loss: 0.1129, Val Loss: 0.0602, LR: 0.0009998772\n",
      "Epoch [4/500], Batch [70/110], Train Loss: 0.2222, Val Loss: 0.0636, LR: 0.0009998708\n",
      "Epoch [4/500], Batch [80/110], Train Loss: 0.3941, Val Loss: 0.0604, LR: 0.0009998643\n",
      "Epoch [4/500], Batch [90/110], Train Loss: 0.1523, Val Loss: 0.0630, LR: 0.0009998576\n",
      "Epoch [4/500], Batch [100/110], Train Loss: 0.0246, Val Loss: 0.0554, LR: 0.0009998507\n",
      "Epoch [4/500], Batch [110/110], Train Loss: 0.0114, Val Loss: 0.0699, LR: 0.0009998437\n",
      "Epoch [5/500], Batch [10/110], Train Loss: 0.0180, Val Loss: 0.0635, LR: 0.0009998365\n",
      "Epoch [5/500], Batch [20/110], Train Loss: 0.0056, Val Loss: 0.0555, LR: 0.0009998291\n",
      "Epoch [5/500], Batch [30/110], Train Loss: 0.0104, Val Loss: 0.0519, LR: 0.0009998216\n",
      "Epoch [5/500], Batch [40/110], Train Loss: 0.0138, Val Loss: 0.0542, LR: 0.0009998140\n",
      "Epoch [5/500], Batch [50/110], Train Loss: 0.0296, Val Loss: 0.0507, LR: 0.0009998061\n",
      "Epoch [5/500], Batch [60/110], Train Loss: 0.0144, Val Loss: 0.0567, LR: 0.0009997981\n",
      "Epoch [5/500], Batch [70/110], Train Loss: 0.0178, Val Loss: 0.0519, LR: 0.0009997900\n",
      "Epoch [5/500], Batch [80/110], Train Loss: 0.1174, Val Loss: 0.0592, LR: 0.0009997817\n",
      "Epoch [5/500], Batch [90/110], Train Loss: 0.0062, Val Loss: 0.0686, LR: 0.0009997732\n",
      "Epoch [5/500], Batch [100/110], Train Loss: 0.1014, Val Loss: 0.0513, LR: 0.0009997645\n",
      "Epoch [5/500], Batch [110/110], Train Loss: 0.1510, Val Loss: 0.0588, LR: 0.0009997557\n",
      "Epoch [6/500], Batch [10/110], Train Loss: 0.0113, Val Loss: 0.0489, LR: 0.0009997468\n",
      "Epoch [6/500], Batch [20/110], Train Loss: 0.1574, Val Loss: 0.0785, LR: 0.0009997377\n",
      "Epoch [6/500], Batch [30/110], Train Loss: 0.2960, Val Loss: 0.0648, LR: 0.0009997284\n",
      "Epoch [6/500], Batch [40/110], Train Loss: 0.0354, Val Loss: 0.0472, LR: 0.0009997189\n",
      "Epoch [6/500], Batch [50/110], Train Loss: 0.0031, Val Loss: 0.0623, LR: 0.0009997093\n",
      "Epoch [6/500], Batch [60/110], Train Loss: 0.0129, Val Loss: 0.0496, LR: 0.0009996996\n",
      "Epoch [6/500], Batch [70/110], Train Loss: 0.1232, Val Loss: 0.0471, LR: 0.0009996896\n",
      "Epoch [6/500], Batch [80/110], Train Loss: 0.0079, Val Loss: 0.0495, LR: 0.0009996795\n",
      "Epoch [6/500], Batch [90/110], Train Loss: 0.1223, Val Loss: 0.0534, LR: 0.0009996693\n",
      "Epoch [6/500], Batch [100/110], Train Loss: 0.2082, Val Loss: 0.0684, LR: 0.0009996589\n",
      "Epoch [6/500], Batch [110/110], Train Loss: 0.0884, Val Loss: 0.0455, LR: 0.0009996483\n",
      "Epoch [7/500], Batch [10/110], Train Loss: 0.1054, Val Loss: 0.0487, LR: 0.0009996376\n",
      "Epoch [7/500], Batch [20/110], Train Loss: 0.0143, Val Loss: 0.0615, LR: 0.0009996267\n",
      "Epoch [7/500], Batch [30/110], Train Loss: 0.0060, Val Loss: 0.0457, LR: 0.0009996156\n",
      "Epoch [7/500], Batch [40/110], Train Loss: 0.0109, Val Loss: 0.0554, LR: 0.0009996044\n",
      "Epoch [7/500], Batch [50/110], Train Loss: 0.0988, Val Loss: 0.0451, LR: 0.0009995930\n",
      "Epoch [7/500], Batch [60/110], Train Loss: 0.0040, Val Loss: 0.0506, LR: 0.0009995814\n",
      "Epoch [7/500], Batch [70/110], Train Loss: 0.0065, Val Loss: 0.0586, LR: 0.0009995697\n",
      "Epoch [7/500], Batch [80/110], Train Loss: 0.0080, Val Loss: 0.0461, LR: 0.0009995579\n",
      "Epoch [7/500], Batch [90/110], Train Loss: 0.0956, Val Loss: 0.0458, LR: 0.0009995458\n",
      "Epoch [7/500], Batch [100/110], Train Loss: 0.0874, Val Loss: 0.0435, LR: 0.0009995337\n",
      "Epoch [7/500], Batch [110/110], Train Loss: 0.0040, Val Loss: 0.0483, LR: 0.0009995213\n",
      "Epoch [8/500], Batch [10/110], Train Loss: 0.0640, Val Loss: 0.0480, LR: 0.0009995088\n",
      "Epoch [8/500], Batch [20/110], Train Loss: 0.0154, Val Loss: 0.0463, LR: 0.0009994961\n",
      "Epoch [8/500], Batch [30/110], Train Loss: 0.0128, Val Loss: 0.0463, LR: 0.0009994833\n",
      "Epoch [8/500], Batch [40/110], Train Loss: 0.0090, Val Loss: 0.0475, LR: 0.0009994703\n",
      "Epoch [8/500], Batch [50/110], Train Loss: 0.0063, Val Loss: 0.0443, LR: 0.0009994571\n",
      "Epoch [8/500], Batch [60/110], Train Loss: 0.0163, Val Loss: 0.0438, LR: 0.0009994438\n",
      "Epoch [8/500], Batch [70/110], Train Loss: 0.0089, Val Loss: 0.0448, LR: 0.0009994303\n",
      "Epoch [8/500], Batch [80/110], Train Loss: 0.2208, Val Loss: 0.0508, LR: 0.0009994167\n",
      "Epoch [8/500], Batch [90/110], Train Loss: 0.0105, Val Loss: 0.0493, LR: 0.0009994029\n",
      "Epoch [8/500], Batch [100/110], Train Loss: 0.1865, Val Loss: 0.0435, LR: 0.0009993889\n",
      "Epoch [8/500], Batch [110/110], Train Loss: 0.0146, Val Loss: 0.0414, LR: 0.0009993748\n",
      "Epoch [9/500], Batch [10/110], Train Loss: 0.3399, Val Loss: 0.0445, LR: 0.0009993605\n",
      "Epoch [9/500], Batch [20/110], Train Loss: 0.0045, Val Loss: 0.0494, LR: 0.0009993461\n",
      "Epoch [9/500], Batch [30/110], Train Loss: 0.0042, Val Loss: 0.0406, LR: 0.0009993314\n",
      "Epoch [9/500], Batch [40/110], Train Loss: 0.0042, Val Loss: 0.0442, LR: 0.0009993167\n",
      "Epoch [9/500], Batch [50/110], Train Loss: 0.0025, Val Loss: 0.0472, LR: 0.0009993017\n",
      "Epoch [9/500], Batch [60/110], Train Loss: 0.0102, Val Loss: 0.0419, LR: 0.0009992867\n",
      "Epoch [9/500], Batch [70/110], Train Loss: 0.0035, Val Loss: 0.0673, LR: 0.0009992714\n",
      "Epoch [9/500], Batch [80/110], Train Loss: 0.0217, Val Loss: 0.0382, LR: 0.0009992560\n",
      "Epoch [9/500], Batch [90/110], Train Loss: 0.0076, Val Loss: 0.0492, LR: 0.0009992404\n",
      "Epoch [9/500], Batch [100/110], Train Loss: 0.0057, Val Loss: 0.0443, LR: 0.0009992247\n",
      "Epoch [9/500], Batch [110/110], Train Loss: 0.0174, Val Loss: 0.0383, LR: 0.0009992088\n",
      "Epoch [10/500], Batch [10/110], Train Loss: 0.0022, Val Loss: 0.0765, LR: 0.0009991927\n",
      "Epoch [10/500], Batch [20/110], Train Loss: 0.0071, Val Loss: 0.0468, LR: 0.0009991765\n",
      "Epoch [10/500], Batch [30/110], Train Loss: 0.0195, Val Loss: 0.0376, LR: 0.0009991601\n",
      "Epoch [10/500], Batch [40/110], Train Loss: 0.0049, Val Loss: 0.0509, LR: 0.0009991436\n",
      "Epoch [10/500], Batch [50/110], Train Loss: 0.0058, Val Loss: 0.0456, LR: 0.0009991269\n",
      "Epoch [10/500], Batch [60/110], Train Loss: 0.0622, Val Loss: 0.0371, LR: 0.0009991100\n",
      "Epoch [10/500], Batch [70/110], Train Loss: 0.0082, Val Loss: 0.0461, LR: 0.0009990930\n",
      "Epoch [10/500], Batch [80/110], Train Loss: 0.0733, Val Loss: 0.0375, LR: 0.0009990758\n",
      "Epoch [10/500], Batch [90/110], Train Loss: 0.0097, Val Loss: 0.0485, LR: 0.0009990584\n",
      "Epoch [10/500], Batch [100/110], Train Loss: 0.0034, Val Loss: 0.0385, LR: 0.0009990409\n",
      "Epoch [10/500], Batch [110/110], Train Loss: 0.0165, Val Loss: 0.0361, LR: 0.0009990232\n",
      "Epoch [11/500], Batch [10/110], Train Loss: 0.0934, Val Loss: 0.0429, LR: 0.0009990054\n",
      "Epoch [11/500], Batch [20/110], Train Loss: 0.0127, Val Loss: 0.0384, LR: 0.0009989874\n",
      "Epoch [11/500], Batch [30/110], Train Loss: 0.0735, Val Loss: 0.0374, LR: 0.0009989692\n",
      "Epoch [11/500], Batch [40/110], Train Loss: 0.0831, Val Loss: 0.0402, LR: 0.0009989509\n",
      "Epoch [11/500], Batch [50/110], Train Loss: 0.0087, Val Loss: 0.0360, LR: 0.0009989324\n",
      "Epoch [11/500], Batch [60/110], Train Loss: 0.0054, Val Loss: 0.0448, LR: 0.0009989138\n",
      "Epoch [11/500], Batch [70/110], Train Loss: 0.0025, Val Loss: 0.0410, LR: 0.0009988950\n",
      "Epoch [11/500], Batch [80/110], Train Loss: 0.0041, Val Loss: 0.0380, LR: 0.0009988760\n",
      "Epoch [11/500], Batch [90/110], Train Loss: 0.0160, Val Loss: 0.0643, LR: 0.0009988569\n",
      "Epoch [11/500], Batch [100/110], Train Loss: 0.0998, Val Loss: 0.0349, LR: 0.0009988376\n",
      "Epoch [11/500], Batch [110/110], Train Loss: 0.0041, Val Loss: 0.0570, LR: 0.0009988182\n",
      "Confusion Matrix:\n",
      "[[612  25]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99837   0.96075   0.97920       637\n",
      "           1    0.97182   0.99884   0.98514       863\n",
      "\n",
      "    accuracy                        0.98267      1500\n",
      "   macro avg    0.98509   0.97980   0.98217      1500\n",
      "weighted avg    0.98309   0.98267   0.98262      1500\n",
      "\n",
      "Total Errors: 26\n",
      "Index: 41, Predicted: 1, Actual: 0\n",
      "Index: 248, Predicted: 1, Actual: 0\n",
      "Index: 275, Predicted: 1, Actual: 0\n",
      "Index: 313, Predicted: 1, Actual: 0\n",
      "Index: 460, Predicted: 1, Actual: 0\n",
      "Epoch 11: OK- Accuracy: 0.98267, Precision: 0.97182, Recall: 0.99884, F1: 0.98514, ROC AUC: 0.97980, AUPR (PR-AUC): 0.97136, Sensitivity: 0.99884, Specificity: 0.96075, Far: 0.03924646781789639, False Positive Rate (FPR): 0.03925, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 415.61 MB\n",
      "Epoch [12/500], Batch [10/110], Train Loss: 0.0019, Val Loss: 0.0618, LR: 0.0009987986\n",
      "Epoch [12/500], Batch [20/110], Train Loss: 0.1163, Val Loss: 0.0390, LR: 0.0009987788\n",
      "Epoch [12/500], Batch [30/110], Train Loss: 0.0044, Val Loss: 0.0467, LR: 0.0009987589\n",
      "Epoch [12/500], Batch [40/110], Train Loss: 0.0059, Val Loss: 0.0340, LR: 0.0009987388\n",
      "Epoch [12/500], Batch [50/110], Train Loss: 0.0030, Val Loss: 0.0486, LR: 0.0009987185\n",
      "Epoch [12/500], Batch [60/110], Train Loss: 0.0017, Val Loss: 0.0601, LR: 0.0009986981\n",
      "Epoch [12/500], Batch [70/110], Train Loss: 0.0108, Val Loss: 0.0359, LR: 0.0009986776\n",
      "Epoch [12/500], Batch [80/110], Train Loss: 0.0070, Val Loss: 0.0487, LR: 0.0009986568\n",
      "Epoch [12/500], Batch [90/110], Train Loss: 0.0232, Val Loss: 0.0356, LR: 0.0009986359\n",
      "Epoch [12/500], Batch [100/110], Train Loss: 0.0686, Val Loss: 0.0441, LR: 0.0009986149\n",
      "Epoch [12/500], Batch [110/110], Train Loss: 0.0714, Val Loss: 0.0357, LR: 0.0009985937\n",
      "Epoch [13/500], Batch [10/110], Train Loss: 0.0110, Val Loss: 0.0382, LR: 0.0009985723\n",
      "Epoch [13/500], Batch [20/110], Train Loss: 0.0027, Val Loss: 0.0413, LR: 0.0009985507\n",
      "Epoch [13/500], Batch [30/110], Train Loss: 0.0114, Val Loss: 0.0342, LR: 0.0009985290\n",
      "Epoch [13/500], Batch [40/110], Train Loss: 0.0093, Val Loss: 0.0410, LR: 0.0009985072\n",
      "Epoch [13/500], Batch [50/110], Train Loss: 0.0041, Val Loss: 0.0431, LR: 0.0009984852\n",
      "Epoch [13/500], Batch [60/110], Train Loss: 0.0053, Val Loss: 0.0367, LR: 0.0009984630\n",
      "Epoch [13/500], Batch [70/110], Train Loss: 0.0096, Val Loss: 0.0358, LR: 0.0009984406\n",
      "Epoch [13/500], Batch [80/110], Train Loss: 0.0165, Val Loss: 0.0331, LR: 0.0009984181\n",
      "Epoch [13/500], Batch [90/110], Train Loss: 0.0092, Val Loss: 0.0373, LR: 0.0009983955\n",
      "Epoch [13/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0338, LR: 0.0009983726\n",
      "Epoch [13/500], Batch [110/110], Train Loss: 0.0119, Val Loss: 0.0329, LR: 0.0009983496\n",
      "Epoch [14/500], Batch [10/110], Train Loss: 0.0039, Val Loss: 0.0389, LR: 0.0009983265\n",
      "Epoch [14/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0364, LR: 0.0009983032\n",
      "Epoch [14/500], Batch [30/110], Train Loss: 0.0982, Val Loss: 0.0320, LR: 0.0009982797\n",
      "Epoch [14/500], Batch [40/110], Train Loss: 0.0037, Val Loss: 0.0354, LR: 0.0009982561\n",
      "Epoch [14/500], Batch [50/110], Train Loss: 0.0087, Val Loss: 0.0330, LR: 0.0009982323\n",
      "Epoch [14/500], Batch [60/110], Train Loss: 0.0036, Val Loss: 0.0412, LR: 0.0009982083\n",
      "Epoch [14/500], Batch [70/110], Train Loss: 0.0038, Val Loss: 0.0333, LR: 0.0009981842\n",
      "Epoch [14/500], Batch [80/110], Train Loss: 0.0126, Val Loss: 0.0370, LR: 0.0009981599\n",
      "Epoch [14/500], Batch [90/110], Train Loss: 0.0055, Val Loss: 0.0495, LR: 0.0009981355\n",
      "Epoch [14/500], Batch [100/110], Train Loss: 0.0228, Val Loss: 0.0416, LR: 0.0009981109\n",
      "Epoch [14/500], Batch [110/110], Train Loss: 0.2645, Val Loss: 0.0500, LR: 0.0009980861\n",
      "Epoch [15/500], Batch [10/110], Train Loss: 0.0029, Val Loss: 0.0376, LR: 0.0009980612\n",
      "Epoch [15/500], Batch [20/110], Train Loss: 0.0210, Val Loss: 0.0324, LR: 0.0009980361\n",
      "Epoch [15/500], Batch [30/110], Train Loss: 0.0703, Val Loss: 0.0350, LR: 0.0009980109\n",
      "Epoch [15/500], Batch [40/110], Train Loss: 0.1175, Val Loss: 0.0360, LR: 0.0009979855\n",
      "Epoch [15/500], Batch [50/110], Train Loss: 0.0050, Val Loss: 0.0327, LR: 0.0009979599\n",
      "Epoch [15/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0402, LR: 0.0009979342\n",
      "Epoch [15/500], Batch [70/110], Train Loss: 0.1302, Val Loss: 0.0385, LR: 0.0009979083\n",
      "Epoch [15/500], Batch [80/110], Train Loss: 0.0094, Val Loss: 0.0310, LR: 0.0009978823\n",
      "Epoch [15/500], Batch [90/110], Train Loss: 0.0080, Val Loss: 0.0553, LR: 0.0009978561\n",
      "Epoch [15/500], Batch [100/110], Train Loss: 0.0808, Val Loss: 0.0330, LR: 0.0009978297\n",
      "Epoch [15/500], Batch [110/110], Train Loss: 0.0162, Val Loss: 0.0333, LR: 0.0009978032\n",
      "Epoch [16/500], Batch [10/110], Train Loss: 0.0040, Val Loss: 0.0579, LR: 0.0009977765\n",
      "Epoch [16/500], Batch [20/110], Train Loss: 0.0429, Val Loss: 0.0314, LR: 0.0009977496\n",
      "Epoch [16/500], Batch [30/110], Train Loss: 0.0019, Val Loss: 0.0313, LR: 0.0009977226\n",
      "Epoch [16/500], Batch [40/110], Train Loss: 0.0054, Val Loss: 0.0489, LR: 0.0009976955\n",
      "Epoch [16/500], Batch [50/110], Train Loss: 0.0216, Val Loss: 0.0310, LR: 0.0009976681\n",
      "Epoch [16/500], Batch [60/110], Train Loss: 0.0015, Val Loss: 0.0349, LR: 0.0009976406\n",
      "Epoch [16/500], Batch [70/110], Train Loss: 0.0023, Val Loss: 0.0313, LR: 0.0009976130\n",
      "Epoch [16/500], Batch [80/110], Train Loss: 0.0014, Val Loss: 0.0312, LR: 0.0009975852\n",
      "Epoch [16/500], Batch [90/110], Train Loss: 0.0732, Val Loss: 0.0331, LR: 0.0009975572\n",
      "Epoch [16/500], Batch [100/110], Train Loss: 0.0295, Val Loss: 0.0306, LR: 0.0009975290\n",
      "Epoch [16/500], Batch [110/110], Train Loss: 0.0036, Val Loss: 0.0351, LR: 0.0009975008\n",
      "Epoch [17/500], Batch [10/110], Train Loss: 0.0455, Val Loss: 0.0294, LR: 0.0009974723\n",
      "Epoch [17/500], Batch [20/110], Train Loss: 0.0112, Val Loss: 0.0310, LR: 0.0009974437\n",
      "Epoch [17/500], Batch [30/110], Train Loss: 0.0512, Val Loss: 0.0288, LR: 0.0009974149\n",
      "Epoch [17/500], Batch [40/110], Train Loss: 0.0271, Val Loss: 0.0324, LR: 0.0009973860\n",
      "Epoch [17/500], Batch [50/110], Train Loss: 0.0129, Val Loss: 0.0327, LR: 0.0009973569\n",
      "Epoch [17/500], Batch [60/110], Train Loss: 0.0015, Val Loss: 0.0356, LR: 0.0009973276\n",
      "Epoch [17/500], Batch [70/110], Train Loss: 0.0037, Val Loss: 0.0377, LR: 0.0009972982\n",
      "Epoch [17/500], Batch [80/110], Train Loss: 0.0073, Val Loss: 0.0319, LR: 0.0009972686\n",
      "Epoch [17/500], Batch [90/110], Train Loss: 0.0015, Val Loss: 0.0303, LR: 0.0009972389\n",
      "Epoch [17/500], Batch [100/110], Train Loss: 0.0149, Val Loss: 0.0368, LR: 0.0009972090\n",
      "Epoch [17/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0354, LR: 0.0009971789\n",
      "Epoch [18/500], Batch [10/110], Train Loss: 0.0016, Val Loss: 0.0322, LR: 0.0009971487\n",
      "Epoch [18/500], Batch [20/110], Train Loss: 0.0178, Val Loss: 0.0289, LR: 0.0009971183\n",
      "Epoch [18/500], Batch [30/110], Train Loss: 0.0186, Val Loss: 0.0295, LR: 0.0009970877\n",
      "Epoch [18/500], Batch [40/110], Train Loss: 0.0198, Val Loss: 0.0325, LR: 0.0009970570\n",
      "Epoch [18/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0328, LR: 0.0009970262\n",
      "Epoch [18/500], Batch [60/110], Train Loss: 0.0054, Val Loss: 0.0290, LR: 0.0009969951\n",
      "Epoch [18/500], Batch [70/110], Train Loss: 0.0579, Val Loss: 0.0284, LR: 0.0009969640\n",
      "Epoch [18/500], Batch [80/110], Train Loss: 0.0636, Val Loss: 0.0272, LR: 0.0009969326\n",
      "Epoch [18/500], Batch [90/110], Train Loss: 0.0016, Val Loss: 0.0360, LR: 0.0009969011\n",
      "Epoch [18/500], Batch [100/110], Train Loss: 0.0144, Val Loss: 0.0272, LR: 0.0009968694\n",
      "Epoch [18/500], Batch [110/110], Train Loss: 0.0886, Val Loss: 0.0316, LR: 0.0009968376\n",
      "Epoch [19/500], Batch [10/110], Train Loss: 0.0786, Val Loss: 0.0302, LR: 0.0009968056\n",
      "Epoch [19/500], Batch [20/110], Train Loss: 0.0057, Val Loss: 0.0283, LR: 0.0009967735\n",
      "Epoch [19/500], Batch [30/110], Train Loss: 0.0061, Val Loss: 0.0332, LR: 0.0009967411\n",
      "Epoch [19/500], Batch [40/110], Train Loss: 0.0030, Val Loss: 0.0279, LR: 0.0009967087\n",
      "Epoch [19/500], Batch [50/110], Train Loss: 0.0148, Val Loss: 0.0323, LR: 0.0009966760\n",
      "Epoch [19/500], Batch [60/110], Train Loss: 0.0014, Val Loss: 0.0329, LR: 0.0009966433\n",
      "Epoch [19/500], Batch [70/110], Train Loss: 0.0037, Val Loss: 0.0285, LR: 0.0009966103\n",
      "Epoch [19/500], Batch [80/110], Train Loss: 0.0958, Val Loss: 0.0418, LR: 0.0009965772\n",
      "Epoch [19/500], Batch [90/110], Train Loss: 0.0022, Val Loss: 0.0271, LR: 0.0009965439\n",
      "Epoch [19/500], Batch [100/110], Train Loss: 0.0097, Val Loss: 0.0341, LR: 0.0009965105\n",
      "Epoch [19/500], Batch [110/110], Train Loss: 0.0072, Val Loss: 0.0289, LR: 0.0009964769\n",
      "Epoch [20/500], Batch [10/110], Train Loss: 0.0086, Val Loss: 0.0291, LR: 0.0009964431\n",
      "Epoch [20/500], Batch [20/110], Train Loss: 0.1424, Val Loss: 0.0314, LR: 0.0009964092\n",
      "Epoch [20/500], Batch [30/110], Train Loss: 0.0013, Val Loss: 0.0271, LR: 0.0009963751\n",
      "Epoch [20/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0460, LR: 0.0009963409\n",
      "Epoch [20/500], Batch [50/110], Train Loss: 0.1097, Val Loss: 0.0309, LR: 0.0009963065\n",
      "Epoch [20/500], Batch [60/110], Train Loss: 0.0346, Val Loss: 0.0264, LR: 0.0009962720\n",
      "Epoch [20/500], Batch [70/110], Train Loss: 0.0032, Val Loss: 0.0393, LR: 0.0009962372\n",
      "Epoch [20/500], Batch [80/110], Train Loss: 0.0029, Val Loss: 0.0276, LR: 0.0009962024\n",
      "Epoch [20/500], Batch [90/110], Train Loss: 0.2047, Val Loss: 0.0410, LR: 0.0009961673\n",
      "Epoch [20/500], Batch [100/110], Train Loss: 0.1937, Val Loss: 0.0413, LR: 0.0009961321\n",
      "Epoch [20/500], Batch [110/110], Train Loss: 0.0043, Val Loss: 0.0337, LR: 0.0009960968\n",
      "Epoch [21/500], Batch [10/110], Train Loss: 0.0028, Val Loss: 0.0331, LR: 0.0009960613\n",
      "Epoch [21/500], Batch [20/110], Train Loss: 0.0321, Val Loss: 0.0268, LR: 0.0009960256\n",
      "Epoch [21/500], Batch [30/110], Train Loss: 0.0055, Val Loss: 0.0401, LR: 0.0009959897\n",
      "Epoch [21/500], Batch [40/110], Train Loss: 0.0017, Val Loss: 0.0337, LR: 0.0009959537\n",
      "Epoch [21/500], Batch [50/110], Train Loss: 0.0718, Val Loss: 0.0293, LR: 0.0009959176\n",
      "Epoch [21/500], Batch [60/110], Train Loss: 0.0927, Val Loss: 0.0264, LR: 0.0009958813\n",
      "Epoch [21/500], Batch [70/110], Train Loss: 0.0626, Val Loss: 0.0286, LR: 0.0009958448\n",
      "Epoch [21/500], Batch [80/110], Train Loss: 0.0118, Val Loss: 0.0256, LR: 0.0009958082\n",
      "Epoch [21/500], Batch [90/110], Train Loss: 0.2266, Val Loss: 0.0402, LR: 0.0009957714\n",
      "Epoch [21/500], Batch [100/110], Train Loss: 0.0156, Val Loss: 0.0259, LR: 0.0009957344\n",
      "Epoch [21/500], Batch [110/110], Train Loss: 0.1623, Val Loss: 0.0252, LR: 0.0009956973\n",
      "Confusion Matrix:\n",
      "[[622  15]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99839   0.97645   0.98730       637\n",
      "           1    0.98290   0.99884   0.99080       863\n",
      "\n",
      "    accuracy                        0.98933      1500\n",
      "   macro avg    0.99065   0.98765   0.98905      1500\n",
      "weighted avg    0.98948   0.98933   0.98932      1500\n",
      "\n",
      "Total Errors: 16\n",
      "Index: 41, Predicted: 1, Actual: 0\n",
      "Index: 248, Predicted: 1, Actual: 0\n",
      "Index: 575, Predicted: 1, Actual: 0\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Index: 715, Predicted: 1, Actual: 0\n",
      "Epoch 21: OK- Accuracy: 0.98933, Precision: 0.98290, Recall: 0.99884, F1: 0.99080, ROC AUC: 0.98765, AUPR (PR-AUC): 0.98242, Sensitivity: 0.99884, Specificity: 0.97645, Far: 0.023547880690737835, False Positive Rate (FPR): 0.02355, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 415.62 MB\n",
      "Epoch [22/500], Batch [10/110], Train Loss: 0.0030, Val Loss: 0.0294, LR: 0.0009956600\n",
      "Epoch [22/500], Batch [20/110], Train Loss: 0.1535, Val Loss: 0.0273, LR: 0.0009956226\n",
      "Epoch [22/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0360, LR: 0.0009955850\n",
      "Epoch [22/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0293, LR: 0.0009955472\n",
      "Epoch [22/500], Batch [50/110], Train Loss: 0.0513, Val Loss: 0.0255, LR: 0.0009955093\n",
      "Epoch [22/500], Batch [60/110], Train Loss: 0.0127, Val Loss: 0.0265, LR: 0.0009954712\n",
      "Epoch [22/500], Batch [70/110], Train Loss: 0.0594, Val Loss: 0.0271, LR: 0.0009954330\n",
      "Epoch [22/500], Batch [80/110], Train Loss: 0.1078, Val Loss: 0.0410, LR: 0.0009953946\n",
      "Epoch [22/500], Batch [90/110], Train Loss: 0.0019, Val Loss: 0.0453, LR: 0.0009953560\n",
      "Epoch [22/500], Batch [100/110], Train Loss: 0.0496, Val Loss: 0.0308, LR: 0.0009953173\n",
      "Epoch [22/500], Batch [110/110], Train Loss: 0.0017, Val Loss: 0.0364, LR: 0.0009952784\n",
      "Epoch [23/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0317, LR: 0.0009952394\n",
      "Epoch [23/500], Batch [20/110], Train Loss: 0.2507, Val Loss: 0.0289, LR: 0.0009952002\n",
      "Epoch [23/500], Batch [30/110], Train Loss: 0.0026, Val Loss: 0.0288, LR: 0.0009951608\n",
      "Epoch [23/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0251, LR: 0.0009951213\n",
      "Epoch [23/500], Batch [50/110], Train Loss: 0.0042, Val Loss: 0.0291, LR: 0.0009950816\n",
      "Epoch [23/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0274, LR: 0.0009950418\n",
      "Epoch [23/500], Batch [70/110], Train Loss: 0.0668, Val Loss: 0.0274, LR: 0.0009950018\n",
      "Epoch [23/500], Batch [80/110], Train Loss: 0.0863, Val Loss: 0.0244, LR: 0.0009949616\n",
      "Epoch [23/500], Batch [90/110], Train Loss: 0.1072, Val Loss: 0.0262, LR: 0.0009949213\n",
      "Epoch [23/500], Batch [100/110], Train Loss: 0.0017, Val Loss: 0.0304, LR: 0.0009948808\n",
      "Epoch [23/500], Batch [110/110], Train Loss: 0.0261, Val Loss: 0.0263, LR: 0.0009948402\n",
      "Epoch [24/500], Batch [10/110], Train Loss: 0.0062, Val Loss: 0.0337, LR: 0.0009947994\n",
      "Epoch [24/500], Batch [20/110], Train Loss: 0.0551, Val Loss: 0.0257, LR: 0.0009947584\n",
      "Epoch [24/500], Batch [30/110], Train Loss: 0.0636, Val Loss: 0.0242, LR: 0.0009947173\n",
      "Epoch [24/500], Batch [40/110], Train Loss: 0.1260, Val Loss: 0.0336, LR: 0.0009946760\n",
      "Epoch [24/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0280, LR: 0.0009946346\n",
      "Epoch [24/500], Batch [60/110], Train Loss: 0.0029, Val Loss: 0.0329, LR: 0.0009945930\n",
      "Epoch [24/500], Batch [70/110], Train Loss: 0.0048, Val Loss: 0.0283, LR: 0.0009945512\n",
      "Epoch [24/500], Batch [80/110], Train Loss: 0.0010, Val Loss: 0.0254, LR: 0.0009945093\n",
      "Epoch [24/500], Batch [90/110], Train Loss: 0.0448, Val Loss: 0.0232, LR: 0.0009944672\n",
      "Epoch [24/500], Batch [100/110], Train Loss: 0.0034, Val Loss: 0.0266, LR: 0.0009944250\n",
      "Epoch [24/500], Batch [110/110], Train Loss: 0.0046, Val Loss: 0.0230, LR: 0.0009943826\n",
      "Epoch [25/500], Batch [10/110], Train Loss: 0.0088, Val Loss: 0.0225, LR: 0.0009943401\n",
      "Epoch [25/500], Batch [20/110], Train Loss: 0.0673, Val Loss: 0.0261, LR: 0.0009942973\n",
      "Epoch [25/500], Batch [30/110], Train Loss: 0.0825, Val Loss: 0.0252, LR: 0.0009942545\n",
      "Epoch [25/500], Batch [40/110], Train Loss: 0.0118, Val Loss: 0.0237, LR: 0.0009942114\n",
      "Epoch [25/500], Batch [50/110], Train Loss: 0.0781, Val Loss: 0.0231, LR: 0.0009941682\n",
      "Epoch [25/500], Batch [60/110], Train Loss: 0.0120, Val Loss: 0.0272, LR: 0.0009941249\n",
      "Epoch [25/500], Batch [70/110], Train Loss: 0.0130, Val Loss: 0.0225, LR: 0.0009940814\n",
      "Epoch [25/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0238, LR: 0.0009940377\n",
      "Epoch [25/500], Batch [90/110], Train Loss: 0.1533, Val Loss: 0.0619, LR: 0.0009939939\n",
      "Epoch [25/500], Batch [100/110], Train Loss: 0.0531, Val Loss: 0.0256, LR: 0.0009939499\n",
      "Epoch [25/500], Batch [110/110], Train Loss: 0.0038, Val Loss: 0.0230, LR: 0.0009939057\n",
      "Epoch [26/500], Batch [10/110], Train Loss: 0.1327, Val Loss: 0.0436, LR: 0.0009938614\n",
      "Epoch [26/500], Batch [20/110], Train Loss: 0.0485, Val Loss: 0.0233, LR: 0.0009938169\n",
      "Epoch [26/500], Batch [30/110], Train Loss: 0.0326, Val Loss: 0.0237, LR: 0.0009937723\n",
      "Epoch [26/500], Batch [40/110], Train Loss: 0.0044, Val Loss: 0.0330, LR: 0.0009937275\n",
      "Epoch [26/500], Batch [50/110], Train Loss: 0.0041, Val Loss: 0.0226, LR: 0.0009936826\n",
      "Epoch [26/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0243, LR: 0.0009936375\n",
      "Epoch [26/500], Batch [70/110], Train Loss: 0.0136, Val Loss: 0.0237, LR: 0.0009935922\n",
      "Epoch [26/500], Batch [80/110], Train Loss: 0.0044, Val Loss: 0.0309, LR: 0.0009935468\n",
      "Epoch [26/500], Batch [90/110], Train Loss: 0.0559, Val Loss: 0.0265, LR: 0.0009935012\n",
      "Epoch [26/500], Batch [100/110], Train Loss: 0.0426, Val Loss: 0.0269, LR: 0.0009934554\n",
      "Epoch [26/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0240, LR: 0.0009934095\n",
      "Epoch [27/500], Batch [10/110], Train Loss: 0.0034, Val Loss: 0.0343, LR: 0.0009933635\n",
      "Epoch [27/500], Batch [20/110], Train Loss: 0.0159, Val Loss: 0.0219, LR: 0.0009933173\n",
      "Epoch [27/500], Batch [30/110], Train Loss: 0.0169, Val Loss: 0.0224, LR: 0.0009932709\n",
      "Epoch [27/500], Batch [40/110], Train Loss: 0.0360, Val Loss: 0.0227, LR: 0.0009932243\n",
      "Epoch [27/500], Batch [50/110], Train Loss: 0.0081, Val Loss: 0.0217, LR: 0.0009931776\n",
      "Epoch [27/500], Batch [60/110], Train Loss: 0.0242, Val Loss: 0.0217, LR: 0.0009931308\n",
      "Epoch [27/500], Batch [70/110], Train Loss: 0.0017, Val Loss: 0.0350, LR: 0.0009930837\n",
      "Epoch [27/500], Batch [80/110], Train Loss: 0.0235, Val Loss: 0.0240, LR: 0.0009930366\n",
      "Epoch [27/500], Batch [90/110], Train Loss: 0.0088, Val Loss: 0.0386, LR: 0.0009929892\n",
      "Epoch [27/500], Batch [100/110], Train Loss: 0.1977, Val Loss: 0.0375, LR: 0.0009929417\n",
      "Epoch [27/500], Batch [110/110], Train Loss: 0.0231, Val Loss: 0.0223, LR: 0.0009928941\n",
      "Epoch [28/500], Batch [10/110], Train Loss: 0.0138, Val Loss: 0.0295, LR: 0.0009928463\n",
      "Epoch [28/500], Batch [20/110], Train Loss: 0.0602, Val Loss: 0.0235, LR: 0.0009927983\n",
      "Epoch [28/500], Batch [30/110], Train Loss: 0.0035, Val Loss: 0.0240, LR: 0.0009927501\n",
      "Epoch [28/500], Batch [40/110], Train Loss: 0.0027, Val Loss: 0.0235, LR: 0.0009927019\n",
      "Epoch [28/500], Batch [50/110], Train Loss: 0.0012, Val Loss: 0.0240, LR: 0.0009926534\n",
      "Epoch [28/500], Batch [60/110], Train Loss: 0.0582, Val Loss: 0.0211, LR: 0.0009926048\n",
      "Epoch [28/500], Batch [70/110], Train Loss: 0.0702, Val Loss: 0.0227, LR: 0.0009925560\n",
      "Epoch [28/500], Batch [80/110], Train Loss: 0.0022, Val Loss: 0.0208, LR: 0.0009925071\n",
      "Epoch [28/500], Batch [90/110], Train Loss: 0.0032, Val Loss: 0.0236, LR: 0.0009924580\n",
      "Epoch [28/500], Batch [100/110], Train Loss: 0.0384, Val Loss: 0.0273, LR: 0.0009924088\n",
      "Epoch [28/500], Batch [110/110], Train Loss: 0.0015, Val Loss: 0.0285, LR: 0.0009923593\n",
      "Epoch [29/500], Batch [10/110], Train Loss: 0.0903, Val Loss: 0.0322, LR: 0.0009923098\n",
      "Epoch [29/500], Batch [20/110], Train Loss: 0.0092, Val Loss: 0.0256, LR: 0.0009922601\n",
      "Epoch [29/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0231, LR: 0.0009922102\n",
      "Epoch [29/500], Batch [40/110], Train Loss: 0.0234, Val Loss: 0.0207, LR: 0.0009921601\n",
      "Epoch [29/500], Batch [50/110], Train Loss: 0.0653, Val Loss: 0.0216, LR: 0.0009921099\n",
      "Epoch [29/500], Batch [60/110], Train Loss: 0.0972, Val Loss: 0.0314, LR: 0.0009920596\n",
      "Epoch [29/500], Batch [70/110], Train Loss: 0.0480, Val Loss: 0.0212, LR: 0.0009920090\n",
      "Epoch [29/500], Batch [80/110], Train Loss: 0.0091, Val Loss: 0.0211, LR: 0.0009919584\n",
      "Epoch [29/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0256, LR: 0.0009919075\n",
      "Epoch [29/500], Batch [100/110], Train Loss: 0.0088, Val Loss: 0.0201, LR: 0.0009918565\n",
      "Epoch [29/500], Batch [110/110], Train Loss: 0.0281, Val Loss: 0.0202, LR: 0.0009918054\n",
      "Epoch [30/500], Batch [10/110], Train Loss: 0.0250, Val Loss: 0.0243, LR: 0.0009917541\n",
      "Epoch [30/500], Batch [20/110], Train Loss: 0.0226, Val Loss: 0.0213, LR: 0.0009917026\n",
      "Epoch [30/500], Batch [30/110], Train Loss: 0.0150, Val Loss: 0.0254, LR: 0.0009916510\n",
      "Epoch [30/500], Batch [40/110], Train Loss: 0.0310, Val Loss: 0.0205, LR: 0.0009915992\n",
      "Epoch [30/500], Batch [50/110], Train Loss: 0.0224, Val Loss: 0.0198, LR: 0.0009915472\n",
      "Epoch [30/500], Batch [60/110], Train Loss: 0.0027, Val Loss: 0.0196, LR: 0.0009914951\n",
      "Epoch [30/500], Batch [70/110], Train Loss: 0.0983, Val Loss: 0.0290, LR: 0.0009914428\n",
      "Epoch [30/500], Batch [80/110], Train Loss: 0.0196, Val Loss: 0.0197, LR: 0.0009913904\n",
      "Epoch [30/500], Batch [90/110], Train Loss: 0.0014, Val Loss: 0.0203, LR: 0.0009913378\n",
      "Epoch [30/500], Batch [100/110], Train Loss: 0.0957, Val Loss: 0.0354, LR: 0.0009912851\n",
      "Epoch [30/500], Batch [110/110], Train Loss: 0.0328, Val Loss: 0.0199, LR: 0.0009912322\n",
      "Epoch [31/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0227, LR: 0.0009911791\n",
      "Epoch [31/500], Batch [20/110], Train Loss: 0.0862, Val Loss: 0.0212, LR: 0.0009911259\n",
      "Epoch [31/500], Batch [30/110], Train Loss: 0.0358, Val Loss: 0.0198, LR: 0.0009910725\n",
      "Epoch [31/500], Batch [40/110], Train Loss: 0.0603, Val Loss: 0.0250, LR: 0.0009910190\n",
      "Epoch [31/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0202, LR: 0.0009909653\n",
      "Epoch [31/500], Batch [60/110], Train Loss: 0.0224, Val Loss: 0.0200, LR: 0.0009909114\n",
      "Epoch [31/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0326, LR: 0.0009908574\n",
      "Epoch [31/500], Batch [80/110], Train Loss: 0.0518, Val Loss: 0.0197, LR: 0.0009908033\n",
      "Epoch [31/500], Batch [90/110], Train Loss: 0.0097, Val Loss: 0.0277, LR: 0.0009907489\n",
      "Epoch [31/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0229, LR: 0.0009906945\n",
      "Epoch [31/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0191, LR: 0.0009906398\n",
      "Confusion Matrix:\n",
      "[[622  15]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99839   0.97645   0.98730       637\n",
      "           1    0.98290   0.99884   0.99080       863\n",
      "\n",
      "    accuracy                        0.98933      1500\n",
      "   macro avg    0.99065   0.98765   0.98905      1500\n",
      "weighted avg    0.98948   0.98933   0.98932      1500\n",
      "\n",
      "Total Errors: 16\n",
      "Index: 41, Predicted: 1, Actual: 0\n",
      "Index: 248, Predicted: 1, Actual: 0\n",
      "Index: 575, Predicted: 1, Actual: 0\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Index: 715, Predicted: 1, Actual: 0\n",
      "Epoch 31: OK- Accuracy: 0.98933, Precision: 0.98290, Recall: 0.99884, F1: 0.99080, ROC AUC: 0.98765, AUPR (PR-AUC): 0.98242, Sensitivity: 0.99884, Specificity: 0.97645, Far: 0.023547880690737835, False Positive Rate (FPR): 0.02355, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 415.64 MB\n",
      "Epoch [32/500], Batch [10/110], Train Loss: 0.0045, Val Loss: 0.0262, LR: 0.0009905850\n",
      "Epoch [32/500], Batch [20/110], Train Loss: 0.0096, Val Loss: 0.0252, LR: 0.0009905300\n",
      "Epoch [32/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0243, LR: 0.0009904749\n",
      "Epoch [32/500], Batch [40/110], Train Loss: 0.0044, Val Loss: 0.0189, LR: 0.0009904196\n",
      "Epoch [32/500], Batch [50/110], Train Loss: 0.0033, Val Loss: 0.0215, LR: 0.0009903642\n",
      "Epoch [32/500], Batch [60/110], Train Loss: 0.0126, Val Loss: 0.0206, LR: 0.0009903086\n",
      "Epoch [32/500], Batch [70/110], Train Loss: 0.0511, Val Loss: 0.0187, LR: 0.0009902529\n",
      "Epoch [32/500], Batch [80/110], Train Loss: 0.0069, Val Loss: 0.0189, LR: 0.0009901969\n",
      "Epoch [32/500], Batch [90/110], Train Loss: 0.0857, Val Loss: 0.0210, LR: 0.0009901409\n",
      "Epoch [32/500], Batch [100/110], Train Loss: 0.0250, Val Loss: 0.0203, LR: 0.0009900846\n",
      "Epoch [32/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0268, LR: 0.0009900283\n",
      "Epoch [33/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0204, LR: 0.0009899717\n",
      "Epoch [33/500], Batch [20/110], Train Loss: 0.0389, Val Loss: 0.0200, LR: 0.0009899150\n",
      "Epoch [33/500], Batch [30/110], Train Loss: 0.0511, Val Loss: 0.0190, LR: 0.0009898581\n",
      "Epoch [33/500], Batch [40/110], Train Loss: 0.0060, Val Loss: 0.0189, LR: 0.0009898011\n",
      "Epoch [33/500], Batch [50/110], Train Loss: 0.1829, Val Loss: 0.0229, LR: 0.0009897439\n",
      "Epoch [33/500], Batch [60/110], Train Loss: 0.0123, Val Loss: 0.0229, LR: 0.0009896866\n",
      "Epoch [33/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0188, LR: 0.0009896291\n",
      "Epoch [33/500], Batch [80/110], Train Loss: 0.0569, Val Loss: 0.0239, LR: 0.0009895715\n",
      "Epoch [33/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0246, LR: 0.0009895136\n",
      "Epoch [33/500], Batch [100/110], Train Loss: 0.0548, Val Loss: 0.0237, LR: 0.0009894557\n",
      "Epoch [33/500], Batch [110/110], Train Loss: 0.0653, Val Loss: 0.0212, LR: 0.0009893975\n",
      "Epoch [34/500], Batch [10/110], Train Loss: 0.1111, Val Loss: 0.0191, LR: 0.0009893393\n",
      "Epoch [34/500], Batch [20/110], Train Loss: 0.0035, Val Loss: 0.0212, LR: 0.0009892808\n",
      "Epoch [34/500], Batch [30/110], Train Loss: 0.0238, Val Loss: 0.0188, LR: 0.0009892222\n",
      "Epoch [34/500], Batch [40/110], Train Loss: 0.0014, Val Loss: 0.0215, LR: 0.0009891635\n",
      "Epoch [34/500], Batch [50/110], Train Loss: 0.0179, Val Loss: 0.0190, LR: 0.0009891045\n",
      "Epoch [34/500], Batch [60/110], Train Loss: 0.0342, Val Loss: 0.0177, LR: 0.0009890455\n",
      "Epoch [34/500], Batch [70/110], Train Loss: 0.0019, Val Loss: 0.0188, LR: 0.0009889862\n",
      "Epoch [34/500], Batch [80/110], Train Loss: 0.1180, Val Loss: 0.0181, LR: 0.0009889268\n",
      "Epoch [34/500], Batch [90/110], Train Loss: 0.0246, Val Loss: 0.0176, LR: 0.0009888673\n",
      "Epoch [34/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0174, LR: 0.0009888076\n",
      "Epoch [34/500], Batch [110/110], Train Loss: 0.0297, Val Loss: 0.0176, LR: 0.0009887477\n",
      "Epoch [35/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0361, LR: 0.0009886877\n",
      "Epoch [35/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0281, LR: 0.0009886275\n",
      "Epoch [35/500], Batch [30/110], Train Loss: 0.0342, Val Loss: 0.0190, LR: 0.0009885672\n",
      "Epoch [35/500], Batch [40/110], Train Loss: 0.0418, Val Loss: 0.0198, LR: 0.0009885067\n",
      "Epoch [35/500], Batch [50/110], Train Loss: 0.0854, Val Loss: 0.0192, LR: 0.0009884460\n",
      "Epoch [35/500], Batch [60/110], Train Loss: 0.0143, Val Loss: 0.0198, LR: 0.0009883852\n",
      "Epoch [35/500], Batch [70/110], Train Loss: 0.0027, Val Loss: 0.0193, LR: 0.0009883243\n",
      "Epoch [35/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0196, LR: 0.0009882631\n",
      "Epoch [35/500], Batch [90/110], Train Loss: 0.0237, Val Loss: 0.0169, LR: 0.0009882018\n",
      "Epoch [35/500], Batch [100/110], Train Loss: 0.0123, Val Loss: 0.0196, LR: 0.0009881404\n",
      "Epoch [35/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0174, LR: 0.0009880788\n",
      "Epoch [36/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0232, LR: 0.0009880170\n",
      "Epoch [36/500], Batch [20/110], Train Loss: 0.0347, Val Loss: 0.0172, LR: 0.0009879551\n",
      "Epoch [36/500], Batch [30/110], Train Loss: 0.0027, Val Loss: 0.0174, LR: 0.0009878931\n",
      "Epoch [36/500], Batch [40/110], Train Loss: 0.0617, Val Loss: 0.0245, LR: 0.0009878308\n",
      "Epoch [36/500], Batch [50/110], Train Loss: 0.0036, Val Loss: 0.0176, LR: 0.0009877684\n",
      "Epoch [36/500], Batch [60/110], Train Loss: 0.0514, Val Loss: 0.0251, LR: 0.0009877059\n",
      "Epoch [36/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0193, LR: 0.0009876432\n",
      "Epoch [36/500], Batch [80/110], Train Loss: 0.0069, Val Loss: 0.0192, LR: 0.0009875803\n",
      "Epoch [36/500], Batch [90/110], Train Loss: 0.0439, Val Loss: 0.0177, LR: 0.0009875173\n",
      "Epoch [36/500], Batch [100/110], Train Loss: 0.0224, Val Loss: 0.0176, LR: 0.0009874541\n",
      "Epoch [36/500], Batch [110/110], Train Loss: 0.0330, Val Loss: 0.0174, LR: 0.0009873908\n",
      "Epoch [37/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0303, LR: 0.0009873273\n",
      "Epoch [37/500], Batch [20/110], Train Loss: 0.0099, Val Loss: 0.0225, LR: 0.0009872637\n",
      "Epoch [37/500], Batch [30/110], Train Loss: 0.0059, Val Loss: 0.0176, LR: 0.0009871999\n",
      "Epoch [37/500], Batch [40/110], Train Loss: 0.0436, Val Loss: 0.0183, LR: 0.0009871359\n",
      "Epoch [37/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0224, LR: 0.0009870718\n",
      "Epoch [37/500], Batch [60/110], Train Loss: 0.0111, Val Loss: 0.0186, LR: 0.0009870075\n",
      "Epoch [37/500], Batch [70/110], Train Loss: 0.0580, Val Loss: 0.0199, LR: 0.0009869431\n",
      "Epoch [37/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0194, LR: 0.0009868785\n",
      "Epoch [37/500], Batch [90/110], Train Loss: 0.0087, Val Loss: 0.0182, LR: 0.0009868137\n",
      "Epoch [37/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0181, LR: 0.0009867488\n",
      "Epoch [37/500], Batch [110/110], Train Loss: 0.0025, Val Loss: 0.0169, LR: 0.0009866838\n",
      "Epoch [38/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0184, LR: 0.0009866185\n",
      "Epoch [38/500], Batch [20/110], Train Loss: 0.0038, Val Loss: 0.0174, LR: 0.0009865532\n",
      "Epoch [38/500], Batch [30/110], Train Loss: 0.0092, Val Loss: 0.0164, LR: 0.0009864876\n",
      "Epoch [38/500], Batch [40/110], Train Loss: 0.0066, Val Loss: 0.0176, LR: 0.0009864219\n",
      "Epoch [38/500], Batch [50/110], Train Loss: 0.0247, Val Loss: 0.0164, LR: 0.0009863561\n",
      "Epoch [38/500], Batch [60/110], Train Loss: 0.0295, Val Loss: 0.0161, LR: 0.0009862901\n",
      "Epoch [38/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0220, LR: 0.0009862239\n",
      "Epoch [38/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0177, LR: 0.0009861576\n",
      "Epoch [38/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0184, LR: 0.0009860911\n",
      "Epoch [38/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0164, LR: 0.0009860245\n",
      "Epoch [38/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0166, LR: 0.0009859577\n",
      "Epoch [39/500], Batch [10/110], Train Loss: 0.0420, Val Loss: 0.0166, LR: 0.0009858908\n",
      "Epoch [39/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0162, LR: 0.0009858237\n",
      "Epoch [39/500], Batch [30/110], Train Loss: 0.0065, Val Loss: 0.0182, LR: 0.0009857564\n",
      "Epoch [39/500], Batch [40/110], Train Loss: 0.0144, Val Loss: 0.0175, LR: 0.0009856890\n",
      "Epoch [39/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0257, LR: 0.0009856214\n",
      "Epoch [39/500], Batch [60/110], Train Loss: 0.0069, Val Loss: 0.0217, LR: 0.0009855537\n",
      "Epoch [39/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0217, LR: 0.0009854858\n",
      "Epoch [39/500], Batch [80/110], Train Loss: 0.0168, Val Loss: 0.0174, LR: 0.0009854177\n",
      "Epoch [39/500], Batch [90/110], Train Loss: 0.0817, Val Loss: 0.0254, LR: 0.0009853495\n",
      "Epoch [39/500], Batch [100/110], Train Loss: 0.0465, Val Loss: 0.0202, LR: 0.0009852812\n",
      "Epoch [39/500], Batch [110/110], Train Loss: 0.0247, Val Loss: 0.0187, LR: 0.0009852127\n",
      "Epoch [40/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0177, LR: 0.0009851440\n",
      "Epoch [40/500], Batch [20/110], Train Loss: 0.0289, Val Loss: 0.0167, LR: 0.0009850752\n",
      "Epoch [40/500], Batch [30/110], Train Loss: 0.1019, Val Loss: 0.0164, LR: 0.0009850062\n",
      "Epoch [40/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0162, LR: 0.0009849370\n",
      "Epoch [40/500], Batch [50/110], Train Loss: 0.0304, Val Loss: 0.0163, LR: 0.0009848677\n",
      "Epoch [40/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0223, LR: 0.0009847983\n",
      "Epoch [40/500], Batch [70/110], Train Loss: 0.2970, Val Loss: 0.0162, LR: 0.0009847287\n",
      "Epoch [40/500], Batch [80/110], Train Loss: 0.0053, Val Loss: 0.0159, LR: 0.0009846589\n",
      "Epoch [40/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0159, LR: 0.0009845890\n",
      "Epoch [40/500], Batch [100/110], Train Loss: 0.0181, Val Loss: 0.0157, LR: 0.0009845189\n",
      "Epoch [40/500], Batch [110/110], Train Loss: 0.0382, Val Loss: 0.0162, LR: 0.0009844487\n",
      "Epoch [41/500], Batch [10/110], Train Loss: 0.0290, Val Loss: 0.0176, LR: 0.0009843783\n",
      "Epoch [41/500], Batch [20/110], Train Loss: 0.1712, Val Loss: 0.0201, LR: 0.0009843077\n",
      "Epoch [41/500], Batch [30/110], Train Loss: 0.0515, Val Loss: 0.0154, LR: 0.0009842370\n",
      "Epoch [41/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0151, LR: 0.0009841662\n",
      "Epoch [41/500], Batch [50/110], Train Loss: 0.0382, Val Loss: 0.0163, LR: 0.0009840951\n",
      "Epoch [41/500], Batch [60/110], Train Loss: 0.0014, Val Loss: 0.0154, LR: 0.0009840240\n",
      "Epoch [41/500], Batch [70/110], Train Loss: 0.0376, Val Loss: 0.0158, LR: 0.0009839526\n",
      "Epoch [41/500], Batch [80/110], Train Loss: 0.0490, Val Loss: 0.0206, LR: 0.0009838811\n",
      "Epoch [41/500], Batch [90/110], Train Loss: 0.0227, Val Loss: 0.0189, LR: 0.0009838095\n",
      "Epoch [41/500], Batch [100/110], Train Loss: 0.0012, Val Loss: 0.0149, LR: 0.0009837377\n",
      "Epoch [41/500], Batch [110/110], Train Loss: 0.0035, Val Loss: 0.0153, LR: 0.0009836657\n",
      "Confusion Matrix:\n",
      "[[631   6]\n",
      " [  3 860]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99527   0.99058   0.99292       637\n",
      "           1    0.99307   0.99652   0.99479       863\n",
      "\n",
      "    accuracy                        0.99400      1500\n",
      "   macro avg    0.99417   0.99355   0.99386      1500\n",
      "weighted avg    0.99400   0.99400   0.99400      1500\n",
      "\n",
      "Total Errors: 9\n",
      "Index: 248, Predicted: 1, Actual: 0\n",
      "Index: 407, Predicted: 0, Actual: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Index: 719, Predicted: 1, Actual: 0\n",
      "Index: 721, Predicted: 1, Actual: 0\n",
      "Epoch 41: OK- Accuracy: 0.99400, Precision: 0.99307, Recall: 0.99652, F1: 0.99479, ROC AUC: 0.99355, AUPR (PR-AUC): 0.99162, Sensitivity: 0.99652, Specificity: 0.99058, Far: 0.009419152276295133, False Positive Rate (FPR): 0.00942, False Negative Rate (FNR): 0.00348, Runtime: 0.035 sec , Memory Usage: 415.66 MB\n",
      "Epoch [42/500], Batch [10/110], Train Loss: 0.0163, Val Loss: 0.0160, LR: 0.0009835936\n",
      "Epoch [42/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0209, LR: 0.0009835214\n",
      "Epoch [42/500], Batch [30/110], Train Loss: 0.0026, Val Loss: 0.0162, LR: 0.0009834489\n",
      "Epoch [42/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0162, LR: 0.0009833763\n",
      "Epoch [42/500], Batch [50/110], Train Loss: 0.0199, Val Loss: 0.0153, LR: 0.0009833036\n",
      "Epoch [42/500], Batch [60/110], Train Loss: 0.0413, Val Loss: 0.0169, LR: 0.0009832307\n",
      "Epoch [42/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0225, LR: 0.0009831577\n",
      "Epoch [42/500], Batch [80/110], Train Loss: 0.0328, Val Loss: 0.0150, LR: 0.0009830845\n",
      "Epoch [42/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0191, LR: 0.0009830111\n",
      "Epoch [42/500], Batch [100/110], Train Loss: 0.0440, Val Loss: 0.0148, LR: 0.0009829376\n",
      "Epoch [42/500], Batch [110/110], Train Loss: 0.0018, Val Loss: 0.0150, LR: 0.0009828639\n",
      "Epoch [43/500], Batch [10/110], Train Loss: 0.0089, Val Loss: 0.0218, LR: 0.0009827901\n",
      "Epoch [43/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0209, LR: 0.0009827161\n",
      "Epoch [43/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0152, LR: 0.0009826420\n",
      "Epoch [43/500], Batch [40/110], Train Loss: 0.0174, Val Loss: 0.0194, LR: 0.0009825677\n",
      "Epoch [43/500], Batch [50/110], Train Loss: 0.0092, Val Loss: 0.0148, LR: 0.0009824932\n",
      "Epoch [43/500], Batch [60/110], Train Loss: 0.0114, Val Loss: 0.0156, LR: 0.0009824186\n",
      "Epoch [43/500], Batch [70/110], Train Loss: 0.0384, Val Loss: 0.0153, LR: 0.0009823438\n",
      "Epoch [43/500], Batch [80/110], Train Loss: 0.0598, Val Loss: 0.0157, LR: 0.0009822689\n",
      "Epoch [43/500], Batch [90/110], Train Loss: 0.2184, Val Loss: 0.0208, LR: 0.0009821938\n",
      "Epoch [43/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0139, LR: 0.0009821186\n",
      "Epoch [43/500], Batch [110/110], Train Loss: 0.0382, Val Loss: 0.0141, LR: 0.0009820432\n",
      "Epoch [44/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0158, LR: 0.0009819677\n",
      "Epoch [44/500], Batch [20/110], Train Loss: 0.0119, Val Loss: 0.0207, LR: 0.0009818920\n",
      "Epoch [44/500], Batch [30/110], Train Loss: 0.0470, Val Loss: 0.0148, LR: 0.0009818161\n",
      "Epoch [44/500], Batch [40/110], Train Loss: 0.0391, Val Loss: 0.0148, LR: 0.0009817401\n",
      "Epoch [44/500], Batch [50/110], Train Loss: 0.0026, Val Loss: 0.0146, LR: 0.0009816640\n",
      "Epoch [44/500], Batch [60/110], Train Loss: 0.0018, Val Loss: 0.0198, LR: 0.0009815876\n",
      "Epoch [44/500], Batch [70/110], Train Loss: 0.0029, Val Loss: 0.0138, LR: 0.0009815112\n",
      "Epoch [44/500], Batch [80/110], Train Loss: 0.0571, Val Loss: 0.0207, LR: 0.0009814345\n",
      "Epoch [44/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0141, LR: 0.0009813577\n",
      "Epoch [44/500], Batch [100/110], Train Loss: 0.0016, Val Loss: 0.0145, LR: 0.0009812808\n",
      "Epoch [44/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0271, LR: 0.0009812037\n",
      "Epoch [45/500], Batch [10/110], Train Loss: 0.0194, Val Loss: 0.0190, LR: 0.0009811264\n",
      "Epoch [45/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0216, LR: 0.0009810490\n",
      "Epoch [45/500], Batch [30/110], Train Loss: 0.0021, Val Loss: 0.0142, LR: 0.0009809715\n",
      "Epoch [45/500], Batch [40/110], Train Loss: 0.0049, Val Loss: 0.0137, LR: 0.0009808938\n",
      "Epoch [45/500], Batch [50/110], Train Loss: 0.0146, Val Loss: 0.0164, LR: 0.0009808159\n",
      "Epoch [45/500], Batch [60/110], Train Loss: 0.0091, Val Loss: 0.0168, LR: 0.0009807378\n",
      "Epoch [45/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0160, LR: 0.0009806597\n",
      "Epoch [45/500], Batch [80/110], Train Loss: 0.1115, Val Loss: 0.0151, LR: 0.0009805813\n",
      "Epoch [45/500], Batch [90/110], Train Loss: 0.0441, Val Loss: 0.0145, LR: 0.0009805028\n",
      "Epoch [45/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0317, LR: 0.0009804242\n",
      "Epoch [45/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0153, LR: 0.0009803454\n",
      "Epoch [46/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0138, LR: 0.0009802664\n",
      "Epoch [46/500], Batch [20/110], Train Loss: 0.0579, Val Loss: 0.0185, LR: 0.0009801873\n",
      "Epoch [46/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0206, LR: 0.0009801080\n",
      "Epoch [46/500], Batch [40/110], Train Loss: 0.0058, Val Loss: 0.0150, LR: 0.0009800286\n",
      "Epoch [46/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0176, LR: 0.0009799490\n",
      "Epoch [46/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0165, LR: 0.0009798693\n",
      "Epoch [46/500], Batch [70/110], Train Loss: 0.0018, Val Loss: 0.0153, LR: 0.0009797894\n",
      "Epoch [46/500], Batch [80/110], Train Loss: 0.0351, Val Loss: 0.0206, LR: 0.0009797094\n",
      "Epoch [46/500], Batch [90/110], Train Loss: 0.0164, Val Loss: 0.0139, LR: 0.0009796292\n",
      "Epoch [46/500], Batch [100/110], Train Loss: 0.0087, Val Loss: 0.0146, LR: 0.0009795488\n",
      "Epoch [46/500], Batch [110/110], Train Loss: 0.0089, Val Loss: 0.0157, LR: 0.0009794683\n",
      "Epoch [47/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0139, LR: 0.0009793876\n",
      "Epoch [47/500], Batch [20/110], Train Loss: 0.0085, Val Loss: 0.0138, LR: 0.0009793068\n",
      "Epoch [47/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0135, LR: 0.0009792258\n",
      "Epoch [47/500], Batch [40/110], Train Loss: 0.0030, Val Loss: 0.0155, LR: 0.0009791447\n",
      "Epoch [47/500], Batch [50/110], Train Loss: 0.0602, Val Loss: 0.0212, LR: 0.0009790634\n",
      "Epoch [47/500], Batch [60/110], Train Loss: 0.0220, Val Loss: 0.0144, LR: 0.0009789820\n",
      "Epoch [47/500], Batch [70/110], Train Loss: 0.0088, Val Loss: 0.0170, LR: 0.0009789004\n",
      "Epoch [47/500], Batch [80/110], Train Loss: 0.0291, Val Loss: 0.0281, LR: 0.0009788186\n",
      "Epoch [47/500], Batch [90/110], Train Loss: 0.0029, Val Loss: 0.0137, LR: 0.0009787367\n",
      "Epoch [47/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0206, LR: 0.0009786547\n",
      "Epoch [47/500], Batch [110/110], Train Loss: 0.0555, Val Loss: 0.0141, LR: 0.0009785725\n",
      "Epoch [48/500], Batch [10/110], Train Loss: 0.0430, Val Loss: 0.0142, LR: 0.0009784901\n",
      "Epoch [48/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0148, LR: 0.0009784076\n",
      "Epoch [48/500], Batch [30/110], Train Loss: 0.0053, Val Loss: 0.0131, LR: 0.0009783249\n",
      "Epoch [48/500], Batch [40/110], Train Loss: 0.0169, Val Loss: 0.0134, LR: 0.0009782421\n",
      "Epoch [48/500], Batch [50/110], Train Loss: 0.0031, Val Loss: 0.0195, LR: 0.0009781591\n",
      "Epoch [48/500], Batch [60/110], Train Loss: 0.0524, Val Loss: 0.0141, LR: 0.0009780760\n",
      "Epoch [48/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0123, LR: 0.0009779927\n",
      "Epoch [48/500], Batch [80/110], Train Loss: 0.0116, Val Loss: 0.0233, LR: 0.0009779092\n",
      "Epoch [48/500], Batch [90/110], Train Loss: 0.0019, Val Loss: 0.0127, LR: 0.0009778256\n",
      "Epoch [48/500], Batch [100/110], Train Loss: 0.0391, Val Loss: 0.0126, LR: 0.0009777419\n",
      "Epoch [48/500], Batch [110/110], Train Loss: 0.0159, Val Loss: 0.0125, LR: 0.0009776579\n",
      "Epoch [49/500], Batch [10/110], Train Loss: 0.0189, Val Loss: 0.0137, LR: 0.0009775739\n",
      "Epoch [49/500], Batch [20/110], Train Loss: 0.0632, Val Loss: 0.0169, LR: 0.0009774897\n",
      "Epoch [49/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0164, LR: 0.0009774053\n",
      "Epoch [49/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0173, LR: 0.0009773208\n",
      "Epoch [49/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0232, LR: 0.0009772361\n",
      "Epoch [49/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0230, LR: 0.0009771513\n",
      "Epoch [49/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0212, LR: 0.0009770663\n",
      "Epoch [49/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0134, LR: 0.0009769811\n",
      "Epoch [49/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0126, LR: 0.0009768958\n",
      "Epoch [49/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0136, LR: 0.0009768104\n",
      "Epoch [49/500], Batch [110/110], Train Loss: 0.0009, Val Loss: 0.0129, LR: 0.0009767248\n",
      "Epoch [50/500], Batch [10/110], Train Loss: 0.0010, Val Loss: 0.0127, LR: 0.0009766390\n",
      "Epoch [50/500], Batch [20/110], Train Loss: 0.0553, Val Loss: 0.0129, LR: 0.0009765531\n",
      "Epoch [50/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0141, LR: 0.0009764670\n",
      "Epoch [50/500], Batch [40/110], Train Loss: 0.0036, Val Loss: 0.0128, LR: 0.0009763808\n",
      "Epoch [50/500], Batch [50/110], Train Loss: 0.0188, Val Loss: 0.0124, LR: 0.0009762944\n",
      "Epoch [50/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0131, LR: 0.0009762079\n",
      "Epoch [50/500], Batch [70/110], Train Loss: 0.0247, Val Loss: 0.0123, LR: 0.0009761212\n",
      "Epoch [50/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0143, LR: 0.0009760344\n",
      "Epoch [50/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0120, LR: 0.0009759474\n",
      "Epoch [50/500], Batch [100/110], Train Loss: 0.0012, Val Loss: 0.0146, LR: 0.0009758603\n",
      "Epoch [50/500], Batch [110/110], Train Loss: 0.0167, Val Loss: 0.0139, LR: 0.0009757730\n",
      "Epoch [51/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0116, LR: 0.0009756855\n",
      "Epoch [51/500], Batch [20/110], Train Loss: 0.0009, Val Loss: 0.0117, LR: 0.0009755979\n",
      "Epoch [51/500], Batch [30/110], Train Loss: 0.0253, Val Loss: 0.0114, LR: 0.0009755102\n",
      "Epoch [51/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0113, LR: 0.0009754223\n",
      "Epoch [51/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0130, LR: 0.0009753342\n",
      "Epoch [51/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0123, LR: 0.0009752460\n",
      "Epoch [51/500], Batch [70/110], Train Loss: 0.1508, Val Loss: 0.0152, LR: 0.0009751576\n",
      "Epoch [51/500], Batch [80/110], Train Loss: 0.0082, Val Loss: 0.0235, LR: 0.0009750691\n",
      "Epoch [51/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0180, LR: 0.0009749804\n",
      "Epoch [51/500], Batch [100/110], Train Loss: 0.0058, Val Loss: 0.0126, LR: 0.0009748916\n",
      "Epoch [51/500], Batch [110/110], Train Loss: 0.0156, Val Loss: 0.0112, LR: 0.0009748026\n",
      "Confusion Matrix:\n",
      "[[631   6]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99842   0.99058   0.99448       637\n",
      "           1    0.99309   0.99884   0.99596       863\n",
      "\n",
      "    accuracy                        0.99533      1500\n",
      "   macro avg    0.99575   0.99471   0.99522      1500\n",
      "weighted avg    0.99535   0.99533   0.99533      1500\n",
      "\n",
      "Total Errors: 7\n",
      "Index: 248, Predicted: 1, Actual: 0\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Index: 719, Predicted: 1, Actual: 0\n",
      "Index: 721, Predicted: 1, Actual: 0\n",
      "Index: 750, Predicted: 1, Actual: 0\n",
      "Epoch 51: OK- Accuracy: 0.99533, Precision: 0.99309, Recall: 0.99884, F1: 0.99596, ROC AUC: 0.99471, AUPR (PR-AUC): 0.99260, Sensitivity: 0.99884, Specificity: 0.99058, Far: 0.009419152276295133, False Positive Rate (FPR): 0.00942, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 415.66 MB\n",
      "Epoch [52/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0137, LR: 0.0009747135\n",
      "Epoch [52/500], Batch [20/110], Train Loss: 0.0037, Val Loss: 0.0114, LR: 0.0009746242\n",
      "Epoch [52/500], Batch [30/110], Train Loss: 0.0014, Val Loss: 0.0123, LR: 0.0009745347\n",
      "Epoch [52/500], Batch [40/110], Train Loss: 0.0246, Val Loss: 0.0109, LR: 0.0009744451\n",
      "Epoch [52/500], Batch [50/110], Train Loss: 0.0428, Val Loss: 0.0129, LR: 0.0009743554\n",
      "Epoch [52/500], Batch [60/110], Train Loss: 0.0365, Val Loss: 0.0118, LR: 0.0009742655\n",
      "Epoch [52/500], Batch [70/110], Train Loss: 0.0107, Val Loss: 0.0121, LR: 0.0009741754\n",
      "Epoch [52/500], Batch [80/110], Train Loss: 0.0032, Val Loss: 0.0249, LR: 0.0009740852\n",
      "Epoch [52/500], Batch [90/110], Train Loss: 0.0263, Val Loss: 0.0111, LR: 0.0009739948\n",
      "Epoch [52/500], Batch [100/110], Train Loss: 0.0288, Val Loss: 0.0113, LR: 0.0009739043\n",
      "Epoch [52/500], Batch [110/110], Train Loss: 0.0766, Val Loss: 0.0210, LR: 0.0009738137\n",
      "Epoch [53/500], Batch [10/110], Train Loss: 0.0021, Val Loss: 0.0131, LR: 0.0009737228\n",
      "Epoch [53/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0114, LR: 0.0009736319\n",
      "Epoch [53/500], Batch [30/110], Train Loss: 0.0720, Val Loss: 0.0116, LR: 0.0009735407\n",
      "Epoch [53/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0114, LR: 0.0009734495\n",
      "Epoch [53/500], Batch [50/110], Train Loss: 0.0010, Val Loss: 0.0227, LR: 0.0009733580\n",
      "Epoch [53/500], Batch [60/110], Train Loss: 0.0312, Val Loss: 0.0127, LR: 0.0009732664\n",
      "Epoch [53/500], Batch [70/110], Train Loss: 0.0133, Val Loss: 0.0165, LR: 0.0009731747\n",
      "Epoch [53/500], Batch [80/110], Train Loss: 0.0677, Val Loss: 0.0206, LR: 0.0009730828\n",
      "Epoch [53/500], Batch [90/110], Train Loss: 0.0107, Val Loss: 0.0152, LR: 0.0009729908\n",
      "Epoch [53/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0121, LR: 0.0009728986\n",
      "Epoch [53/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0120, LR: 0.0009728062\n",
      "Epoch [54/500], Batch [10/110], Train Loss: 0.1811, Val Loss: 0.0117, LR: 0.0009727137\n",
      "Epoch [54/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0116, LR: 0.0009726211\n",
      "Epoch [54/500], Batch [30/110], Train Loss: 0.0056, Val Loss: 0.0147, LR: 0.0009725283\n",
      "Epoch [54/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0362, LR: 0.0009724353\n",
      "Epoch [54/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0129, LR: 0.0009723422\n",
      "Epoch [54/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0175, LR: 0.0009722489\n",
      "Epoch [54/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0195, LR: 0.0009721555\n",
      "Epoch [54/500], Batch [80/110], Train Loss: 0.0836, Val Loss: 0.0163, LR: 0.0009720619\n",
      "Epoch [54/500], Batch [90/110], Train Loss: 0.0461, Val Loss: 0.0116, LR: 0.0009719682\n",
      "Epoch [54/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0119, LR: 0.0009718743\n",
      "Epoch [54/500], Batch [110/110], Train Loss: 0.0154, Val Loss: 0.0119, LR: 0.0009717803\n",
      "Epoch [55/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0113, LR: 0.0009716861\n",
      "Epoch [55/500], Batch [20/110], Train Loss: 0.0497, Val Loss: 0.0145, LR: 0.0009715918\n",
      "Epoch [55/500], Batch [30/110], Train Loss: 0.0025, Val Loss: 0.0150, LR: 0.0009714973\n",
      "Epoch [55/500], Batch [40/110], Train Loss: 0.0125, Val Loss: 0.0127, LR: 0.0009714027\n",
      "Epoch [55/500], Batch [50/110], Train Loss: 0.0105, Val Loss: 0.0148, LR: 0.0009713079\n",
      "Epoch [55/500], Batch [60/110], Train Loss: 0.0168, Val Loss: 0.0117, LR: 0.0009712130\n",
      "Epoch [55/500], Batch [70/110], Train Loss: 0.0104, Val Loss: 0.0141, LR: 0.0009711179\n",
      "Epoch [55/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0112, LR: 0.0009710226\n",
      "Epoch [55/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0110, LR: 0.0009709272\n",
      "Epoch [55/500], Batch [100/110], Train Loss: 0.0435, Val Loss: 0.0139, LR: 0.0009708317\n",
      "Epoch [55/500], Batch [110/110], Train Loss: 0.0056, Val Loss: 0.0111, LR: 0.0009707360\n",
      "Epoch [56/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0109, LR: 0.0009706401\n",
      "Epoch [56/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0199, LR: 0.0009705441\n",
      "Epoch [56/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0123, LR: 0.0009704480\n",
      "Epoch [56/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0119, LR: 0.0009703517\n",
      "Epoch [56/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0141, LR: 0.0009702552\n",
      "Epoch [56/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0174, LR: 0.0009701586\n",
      "Epoch [56/500], Batch [70/110], Train Loss: 0.0184, Val Loss: 0.0104, LR: 0.0009700618\n",
      "Epoch [56/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0108, LR: 0.0009699649\n",
      "Epoch [56/500], Batch [90/110], Train Loss: 0.0674, Val Loss: 0.0209, LR: 0.0009698678\n",
      "Epoch [56/500], Batch [100/110], Train Loss: 0.0042, Val Loss: 0.0142, LR: 0.0009697706\n",
      "Epoch [56/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0202, LR: 0.0009696733\n",
      "Epoch [57/500], Batch [10/110], Train Loss: 0.0581, Val Loss: 0.0148, LR: 0.0009695757\n",
      "Epoch [57/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0127, LR: 0.0009694781\n",
      "Epoch [57/500], Batch [30/110], Train Loss: 0.0243, Val Loss: 0.0191, LR: 0.0009693802\n",
      "Epoch [57/500], Batch [40/110], Train Loss: 0.1131, Val Loss: 0.0164, LR: 0.0009692823\n",
      "Epoch [57/500], Batch [50/110], Train Loss: 0.0299, Val Loss: 0.0111, LR: 0.0009691841\n",
      "Epoch [57/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0176, LR: 0.0009690859\n",
      "Epoch [57/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0124, LR: 0.0009689874\n",
      "Epoch [57/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0118, LR: 0.0009688888\n",
      "Epoch [57/500], Batch [90/110], Train Loss: 0.0156, Val Loss: 0.0137, LR: 0.0009687901\n",
      "Epoch [57/500], Batch [100/110], Train Loss: 0.0171, Val Loss: 0.0106, LR: 0.0009686912\n",
      "Epoch [57/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0138, LR: 0.0009685922\n",
      "Epoch [58/500], Batch [10/110], Train Loss: 0.0711, Val Loss: 0.0155, LR: 0.0009684930\n",
      "Epoch [58/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0112, LR: 0.0009683937\n",
      "Epoch [58/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0101, LR: 0.0009682942\n",
      "Epoch [58/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0106, LR: 0.0009681945\n",
      "Epoch [58/500], Batch [50/110], Train Loss: 0.0020, Val Loss: 0.0182, LR: 0.0009680947\n",
      "Epoch [58/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0118, LR: 0.0009679948\n",
      "Epoch [58/500], Batch [70/110], Train Loss: 0.0240, Val Loss: 0.0104, LR: 0.0009678947\n",
      "Epoch [58/500], Batch [80/110], Train Loss: 0.0031, Val Loss: 0.0098, LR: 0.0009677945\n",
      "Epoch [58/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0108, LR: 0.0009676941\n",
      "Epoch [58/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0098, LR: 0.0009675935\n",
      "Epoch [58/500], Batch [110/110], Train Loss: 0.0295, Val Loss: 0.0117, LR: 0.0009674928\n",
      "Epoch [59/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0120, LR: 0.0009673920\n",
      "Epoch [59/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0219, LR: 0.0009672910\n",
      "Epoch [59/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0009671898\n",
      "Epoch [59/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0107, LR: 0.0009670885\n",
      "Epoch [59/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0152, LR: 0.0009669871\n",
      "Epoch [59/500], Batch [60/110], Train Loss: 0.0021, Val Loss: 0.0095, LR: 0.0009668855\n",
      "Epoch [59/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0099, LR: 0.0009667837\n",
      "Epoch [59/500], Batch [80/110], Train Loss: 0.0133, Val Loss: 0.0097, LR: 0.0009666818\n",
      "Epoch [59/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0100, LR: 0.0009665798\n",
      "Epoch [59/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0106, LR: 0.0009664776\n",
      "Epoch [59/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0158, LR: 0.0009663752\n",
      "Epoch [60/500], Batch [10/110], Train Loss: 0.0348, Val Loss: 0.0248, LR: 0.0009662727\n",
      "Epoch [60/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0181, LR: 0.0009661700\n",
      "Epoch [60/500], Batch [30/110], Train Loss: 0.2850, Val Loss: 0.0176, LR: 0.0009660672\n",
      "Epoch [60/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0101, LR: 0.0009659643\n",
      "Epoch [60/500], Batch [50/110], Train Loss: 0.0177, Val Loss: 0.0102, LR: 0.0009658612\n",
      "Epoch [60/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0094, LR: 0.0009657579\n",
      "Epoch [60/500], Batch [70/110], Train Loss: 0.0017, Val Loss: 0.0093, LR: 0.0009656545\n",
      "Epoch [60/500], Batch [80/110], Train Loss: 0.0565, Val Loss: 0.0150, LR: 0.0009655509\n",
      "Epoch [60/500], Batch [90/110], Train Loss: 0.0141, Val Loss: 0.0182, LR: 0.0009654472\n",
      "Epoch [60/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0130, LR: 0.0009653434\n",
      "Epoch [60/500], Batch [110/110], Train Loss: 0.0148, Val Loss: 0.0131, LR: 0.0009652394\n",
      "Epoch [61/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0094, LR: 0.0009651352\n",
      "Epoch [61/500], Batch [20/110], Train Loss: 0.0190, Val Loss: 0.0090, LR: 0.0009650309\n",
      "Epoch [61/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0169, LR: 0.0009649264\n",
      "Epoch [61/500], Batch [40/110], Train Loss: 0.0278, Val Loss: 0.0099, LR: 0.0009648218\n",
      "Epoch [61/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0101, LR: 0.0009647171\n",
      "Epoch [61/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0176, LR: 0.0009646122\n",
      "Epoch [61/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0111, LR: 0.0009645071\n",
      "Epoch [61/500], Batch [80/110], Train Loss: 0.0015, Val Loss: 0.0094, LR: 0.0009644019\n",
      "Epoch [61/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0124, LR: 0.0009642965\n",
      "Epoch [61/500], Batch [100/110], Train Loss: 0.0046, Val Loss: 0.0160, LR: 0.0009641910\n",
      "Epoch [61/500], Batch [110/110], Train Loss: 0.0168, Val Loss: 0.0099, LR: 0.0009640854\n",
      "Confusion Matrix:\n",
      "[[634   3]\n",
      " [  2 861]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99686   0.99529   0.99607       637\n",
      "           1    0.99653   0.99768   0.99710       863\n",
      "\n",
      "    accuracy                        0.99667      1500\n",
      "   macro avg    0.99669   0.99649   0.99659      1500\n",
      "weighted avg    0.99667   0.99667   0.99667      1500\n",
      "\n",
      "Total Errors: 5\n",
      "Index: 248, Predicted: 1, Actual: 0\n",
      "Index: 407, Predicted: 0, Actual: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Index: 750, Predicted: 1, Actual: 0\n",
      "Index: 1197, Predicted: 1, Actual: 0\n",
      "Epoch 61: OK- Accuracy: 0.99667, Precision: 0.99653, Recall: 0.99768, F1: 0.99710, ROC AUC: 0.99649, AUPR (PR-AUC): 0.99555, Sensitivity: 0.99768, Specificity: 0.99529, Far: 0.004709576138147566, False Positive Rate (FPR): 0.00471, False Negative Rate (FNR): 0.00232, Runtime: 0.035 sec , Memory Usage: 415.66 MB\n",
      "Epoch [62/500], Batch [10/110], Train Loss: 0.0090, Val Loss: 0.0153, LR: 0.0009639795\n",
      "Epoch [62/500], Batch [20/110], Train Loss: 0.0668, Val Loss: 0.0143, LR: 0.0009638736\n",
      "Epoch [62/500], Batch [30/110], Train Loss: 0.0208, Val Loss: 0.0105, LR: 0.0009637675\n",
      "Epoch [62/500], Batch [40/110], Train Loss: 0.0216, Val Loss: 0.0090, LR: 0.0009636612\n",
      "Epoch [62/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0209, LR: 0.0009635548\n",
      "Epoch [62/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0109, LR: 0.0009634482\n",
      "Epoch [62/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0110, LR: 0.0009633415\n",
      "Epoch [62/500], Batch [80/110], Train Loss: 0.0227, Val Loss: 0.0090, LR: 0.0009632347\n",
      "Epoch [62/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0097, LR: 0.0009631277\n",
      "Epoch [62/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0094, LR: 0.0009630205\n",
      "Epoch [62/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0101, LR: 0.0009629132\n",
      "Epoch [63/500], Batch [10/110], Train Loss: 0.0561, Val Loss: 0.0131, LR: 0.0009628058\n",
      "Epoch [63/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0009626982\n",
      "Epoch [63/500], Batch [30/110], Train Loss: 0.0158, Val Loss: 0.0085, LR: 0.0009625904\n",
      "Epoch [63/500], Batch [40/110], Train Loss: 0.0443, Val Loss: 0.0089, LR: 0.0009624825\n",
      "Epoch [63/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0116, LR: 0.0009623745\n",
      "Epoch [63/500], Batch [60/110], Train Loss: 0.0362, Val Loss: 0.0098, LR: 0.0009622662\n",
      "Epoch [63/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0009621579\n",
      "Epoch [63/500], Batch [80/110], Train Loss: 0.0202, Val Loss: 0.0096, LR: 0.0009620494\n",
      "Epoch [63/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0112, LR: 0.0009619408\n",
      "Epoch [63/500], Batch [100/110], Train Loss: 0.0021, Val Loss: 0.0085, LR: 0.0009618320\n",
      "Epoch [63/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0122, LR: 0.0009617230\n",
      "Epoch [64/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0009616139\n",
      "Epoch [64/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0097, LR: 0.0009615047\n",
      "Epoch [64/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0160, LR: 0.0009613953\n",
      "Epoch [64/500], Batch [40/110], Train Loss: 0.0087, Val Loss: 0.0149, LR: 0.0009612857\n",
      "Epoch [64/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0009611760\n",
      "Epoch [64/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0009610662\n",
      "Epoch [64/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0125, LR: 0.0009609562\n",
      "Epoch [64/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0009608461\n",
      "Epoch [64/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0009607358\n",
      "Epoch [64/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0009606253\n",
      "Epoch [64/500], Batch [110/110], Train Loss: 0.0099, Val Loss: 0.0091, LR: 0.0009605148\n",
      "Epoch [65/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0146, LR: 0.0009604040\n",
      "Epoch [65/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0204, LR: 0.0009602932\n",
      "Epoch [65/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0118, LR: 0.0009601821\n",
      "Epoch [65/500], Batch [40/110], Train Loss: 0.0053, Val Loss: 0.0133, LR: 0.0009600709\n",
      "Epoch [65/500], Batch [50/110], Train Loss: 0.0047, Val Loss: 0.0080, LR: 0.0009599596\n",
      "Epoch [65/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0088, LR: 0.0009598481\n",
      "Epoch [65/500], Batch [70/110], Train Loss: 0.0380, Val Loss: 0.0077, LR: 0.0009597365\n",
      "Epoch [65/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0009596247\n",
      "Epoch [65/500], Batch [90/110], Train Loss: 0.0110, Val Loss: 0.0077, LR: 0.0009595128\n",
      "Epoch [65/500], Batch [100/110], Train Loss: 0.0064, Val Loss: 0.0078, LR: 0.0009594008\n",
      "Epoch [65/500], Batch [110/110], Train Loss: 0.0179, Val Loss: 0.0093, LR: 0.0009592885\n",
      "Epoch [66/500], Batch [10/110], Train Loss: 0.0038, Val Loss: 0.0079, LR: 0.0009591762\n",
      "Epoch [66/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0075, LR: 0.0009590637\n",
      "Epoch [66/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0009589510\n",
      "Epoch [66/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0009588382\n",
      "Epoch [66/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0077, LR: 0.0009587252\n",
      "Epoch [66/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0009586121\n",
      "Epoch [66/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0155, LR: 0.0009584989\n",
      "Epoch [66/500], Batch [80/110], Train Loss: 0.0329, Val Loss: 0.0078, LR: 0.0009583855\n",
      "Epoch [66/500], Batch [90/110], Train Loss: 0.0255, Val Loss: 0.0104, LR: 0.0009582719\n",
      "Epoch [66/500], Batch [100/110], Train Loss: 0.0058, Val Loss: 0.0075, LR: 0.0009581582\n",
      "Epoch [66/500], Batch [110/110], Train Loss: 0.0018, Val Loss: 0.0076, LR: 0.0009580444\n",
      "Epoch [67/500], Batch [10/110], Train Loss: 0.0405, Val Loss: 0.0077, LR: 0.0009579304\n",
      "Epoch [67/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0009578162\n",
      "Epoch [67/500], Batch [30/110], Train Loss: 0.0057, Val Loss: 0.0077, LR: 0.0009577020\n",
      "Epoch [67/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0082, LR: 0.0009575875\n",
      "Epoch [67/500], Batch [50/110], Train Loss: 0.0044, Val Loss: 0.0070, LR: 0.0009574729\n",
      "Epoch [67/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0075, LR: 0.0009573582\n",
      "Epoch [67/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0009572433\n",
      "Epoch [67/500], Batch [80/110], Train Loss: 0.0026, Val Loss: 0.0069, LR: 0.0009571283\n",
      "Epoch [67/500], Batch [90/110], Train Loss: 0.0190, Val Loss: 0.0070, LR: 0.0009570131\n",
      "Epoch [67/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0182, LR: 0.0009568978\n",
      "Epoch [67/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0143, LR: 0.0009567823\n",
      "Epoch [68/500], Batch [10/110], Train Loss: 0.0319, Val Loss: 0.0091, LR: 0.0009566667\n",
      "Epoch [68/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0088, LR: 0.0009565510\n",
      "Epoch [68/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0009564350\n",
      "Epoch [68/500], Batch [40/110], Train Loss: 0.0062, Val Loss: 0.0086, LR: 0.0009563190\n",
      "Epoch [68/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0077, LR: 0.0009562028\n",
      "Epoch [68/500], Batch [60/110], Train Loss: 0.0172, Val Loss: 0.0075, LR: 0.0009560864\n",
      "Epoch [68/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0114, LR: 0.0009559699\n",
      "Epoch [68/500], Batch [80/110], Train Loss: 0.0112, Val Loss: 0.0075, LR: 0.0009558533\n",
      "Epoch [68/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0009557365\n",
      "Epoch [68/500], Batch [100/110], Train Loss: 0.0021, Val Loss: 0.0079, LR: 0.0009556195\n",
      "Epoch [68/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0009555025\n",
      "Epoch [69/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0009553852\n",
      "Epoch [69/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0092, LR: 0.0009552678\n",
      "Epoch [69/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0137, LR: 0.0009551503\n",
      "Epoch [69/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0077, LR: 0.0009550326\n",
      "Epoch [69/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0098, LR: 0.0009549148\n",
      "Epoch [69/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0009547968\n",
      "Epoch [69/500], Batch [70/110], Train Loss: 0.0129, Val Loss: 0.0087, LR: 0.0009546787\n",
      "Epoch [69/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0070, LR: 0.0009545605\n",
      "Epoch [69/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0009544420\n",
      "Epoch [69/500], Batch [100/110], Train Loss: 0.0028, Val Loss: 0.0121, LR: 0.0009543235\n",
      "Epoch [69/500], Batch [110/110], Train Loss: 0.0057, Val Loss: 0.0107, LR: 0.0009542048\n",
      "Epoch [70/500], Batch [10/110], Train Loss: 0.0061, Val Loss: 0.0071, LR: 0.0009540859\n",
      "Epoch [70/500], Batch [20/110], Train Loss: 0.0065, Val Loss: 0.0089, LR: 0.0009539669\n",
      "Epoch [70/500], Batch [30/110], Train Loss: 0.0038, Val Loss: 0.0084, LR: 0.0009538478\n",
      "Epoch [70/500], Batch [40/110], Train Loss: 0.0155, Val Loss: 0.0069, LR: 0.0009537285\n",
      "Epoch [70/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0070, LR: 0.0009536091\n",
      "Epoch [70/500], Batch [60/110], Train Loss: 0.0238, Val Loss: 0.0067, LR: 0.0009534895\n",
      "Epoch [70/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0009533698\n",
      "Epoch [70/500], Batch [80/110], Train Loss: 0.0135, Val Loss: 0.0211, LR: 0.0009532499\n",
      "Epoch [70/500], Batch [90/110], Train Loss: 0.0041, Val Loss: 0.0072, LR: 0.0009531299\n",
      "Epoch [70/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0067, LR: 0.0009530097\n",
      "Epoch [70/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0070, LR: 0.0009528894\n",
      "Epoch [71/500], Batch [10/110], Train Loss: 0.0021, Val Loss: 0.0073, LR: 0.0009527689\n",
      "Epoch [71/500], Batch [20/110], Train Loss: 0.0090, Val Loss: 0.0077, LR: 0.0009526483\n",
      "Epoch [71/500], Batch [30/110], Train Loss: 0.0030, Val Loss: 0.0097, LR: 0.0009525276\n",
      "Epoch [71/500], Batch [40/110], Train Loss: 0.0020, Val Loss: 0.0130, LR: 0.0009524067\n",
      "Epoch [71/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0077, LR: 0.0009522856\n",
      "Epoch [71/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0009521644\n",
      "Epoch [71/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0133, LR: 0.0009520431\n",
      "Epoch [71/500], Batch [80/110], Train Loss: 0.0582, Val Loss: 0.0147, LR: 0.0009519216\n",
      "Epoch [71/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0124, LR: 0.0009518000\n",
      "Epoch [71/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0009516782\n",
      "Epoch [71/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0065, LR: 0.0009515563\n",
      "Confusion Matrix:\n",
      "[[634   3]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   0.99529   0.99686       637\n",
      "           1    0.99653   0.99884   0.99769       863\n",
      "\n",
      "    accuracy                        0.99733      1500\n",
      "   macro avg    0.99748   0.99707   0.99727      1500\n",
      "weighted avg    0.99734   0.99733   0.99733      1500\n",
      "\n",
      "Total Errors: 4\n",
      "Index: 248, Predicted: 1, Actual: 0\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Index: 750, Predicted: 1, Actual: 0\n",
      "Index: 1197, Predicted: 1, Actual: 0\n",
      "Epoch 71: OK- Accuracy: 0.99733, Precision: 0.99653, Recall: 0.99884, F1: 0.99769, ROC AUC: 0.99707, AUPR (PR-AUC): 0.99604, Sensitivity: 0.99884, Specificity: 0.99529, Far: 0.004709576138147566, False Positive Rate (FPR): 0.00471, False Negative Rate (FNR): 0.00116, Runtime: 0.036 sec , Memory Usage: 415.66 MB\n",
      "Epoch [72/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0169, LR: 0.0009514342\n",
      "Epoch [72/500], Batch [20/110], Train Loss: 0.0102, Val Loss: 0.0075, LR: 0.0009513120\n",
      "Epoch [72/500], Batch [30/110], Train Loss: 0.0412, Val Loss: 0.0076, LR: 0.0009511897\n",
      "Epoch [72/500], Batch [40/110], Train Loss: 0.0055, Val Loss: 0.0073, LR: 0.0009510672\n",
      "Epoch [72/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0067, LR: 0.0009509445\n",
      "Epoch [72/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0009508217\n",
      "Epoch [72/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0146, LR: 0.0009506988\n",
      "Epoch [72/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0009505757\n",
      "Epoch [72/500], Batch [90/110], Train Loss: 0.0152, Val Loss: 0.0085, LR: 0.0009504525\n",
      "Epoch [72/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0009503291\n",
      "Epoch [72/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0009502056\n",
      "Epoch [73/500], Batch [10/110], Train Loss: 0.0259, Val Loss: 0.0122, LR: 0.0009500819\n",
      "Epoch [73/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0009499581\n",
      "Epoch [73/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0066, LR: 0.0009498342\n",
      "Epoch [73/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0009497101\n",
      "Epoch [73/500], Batch [50/110], Train Loss: 0.0450, Val Loss: 0.0100, LR: 0.0009495858\n",
      "Epoch [73/500], Batch [60/110], Train Loss: 0.0030, Val Loss: 0.0067, LR: 0.0009494614\n",
      "Epoch [73/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0072, LR: 0.0009493369\n",
      "Epoch [73/500], Batch [80/110], Train Loss: 0.0048, Val Loss: 0.0084, LR: 0.0009492122\n",
      "Epoch [73/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0009490874\n",
      "Epoch [73/500], Batch [100/110], Train Loss: 0.0193, Val Loss: 0.0073, LR: 0.0009489624\n",
      "Epoch [73/500], Batch [110/110], Train Loss: 0.0019, Val Loss: 0.0072, LR: 0.0009488373\n",
      "Epoch [74/500], Batch [10/110], Train Loss: 0.0024, Val Loss: 0.0100, LR: 0.0009487121\n",
      "Epoch [74/500], Batch [20/110], Train Loss: 0.0250, Val Loss: 0.0067, LR: 0.0009485866\n",
      "Epoch [74/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0009484611\n",
      "Epoch [74/500], Batch [40/110], Train Loss: 0.0015, Val Loss: 0.0065, LR: 0.0009483354\n",
      "Epoch [74/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0102, LR: 0.0009482096\n",
      "Epoch [74/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0009480836\n",
      "Epoch [74/500], Batch [70/110], Train Loss: 0.0116, Val Loss: 0.0080, LR: 0.0009479575\n",
      "Epoch [74/500], Batch [80/110], Train Loss: 0.0214, Val Loss: 0.0066, LR: 0.0009478312\n",
      "Epoch [74/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0009477048\n",
      "Epoch [74/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0140, LR: 0.0009475782\n",
      "Epoch [74/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0073, LR: 0.0009474515\n",
      "Epoch [75/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0009473247\n",
      "Epoch [75/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0009471977\n",
      "Epoch [75/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0065, LR: 0.0009470705\n",
      "Epoch [75/500], Batch [40/110], Train Loss: 0.0138, Val Loss: 0.0064, LR: 0.0009469432\n",
      "Epoch [75/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0009468158\n",
      "Epoch [75/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0065, LR: 0.0009466882\n",
      "Epoch [75/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0009465605\n",
      "Epoch [75/500], Batch [80/110], Train Loss: 0.0028, Val Loss: 0.0077, LR: 0.0009464327\n",
      "Epoch [75/500], Batch [90/110], Train Loss: 0.0015, Val Loss: 0.0085, LR: 0.0009463047\n",
      "Epoch [75/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0068, LR: 0.0009461765\n",
      "Epoch [75/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0073, LR: 0.0009460482\n",
      "Epoch [76/500], Batch [10/110], Train Loss: 0.0034, Val Loss: 0.0100, LR: 0.0009459198\n",
      "Epoch [76/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0009457912\n",
      "Epoch [76/500], Batch [30/110], Train Loss: 0.0607, Val Loss: 0.0073, LR: 0.0009456625\n",
      "Epoch [76/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0009455336\n",
      "Epoch [76/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0067, LR: 0.0009454046\n",
      "Epoch [76/500], Batch [60/110], Train Loss: 0.0107, Val Loss: 0.0064, LR: 0.0009452755\n",
      "Epoch [76/500], Batch [70/110], Train Loss: 0.0083, Val Loss: 0.0061, LR: 0.0009451462\n",
      "Epoch [76/500], Batch [80/110], Train Loss: 0.0008, Val Loss: 0.0063, LR: 0.0009450167\n",
      "Epoch [76/500], Batch [90/110], Train Loss: 0.0048, Val Loss: 0.0122, LR: 0.0009448871\n",
      "Epoch [76/500], Batch [100/110], Train Loss: 0.0014, Val Loss: 0.0108, LR: 0.0009447574\n",
      "Epoch [76/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0137, LR: 0.0009446275\n",
      "Epoch [77/500], Batch [10/110], Train Loss: 0.0063, Val Loss: 0.0077, LR: 0.0009444975\n",
      "Epoch [77/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0068, LR: 0.0009443674\n",
      "Epoch [77/500], Batch [30/110], Train Loss: 0.0121, Val Loss: 0.0062, LR: 0.0009442371\n",
      "Epoch [77/500], Batch [40/110], Train Loss: 0.0090, Val Loss: 0.0062, LR: 0.0009441066\n",
      "Epoch [77/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0009439760\n",
      "Epoch [77/500], Batch [60/110], Train Loss: 0.0025, Val Loss: 0.0067, LR: 0.0009438453\n",
      "Epoch [77/500], Batch [70/110], Train Loss: 0.0010, Val Loss: 0.0074, LR: 0.0009437144\n",
      "Epoch [77/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0009435834\n",
      "Epoch [77/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0009434522\n",
      "Epoch [77/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0009433209\n",
      "Epoch [77/500], Batch [110/110], Train Loss: 0.0500, Val Loss: 0.0096, LR: 0.0009431895\n",
      "Epoch [78/500], Batch [10/110], Train Loss: 0.0067, Val Loss: 0.0079, LR: 0.0009430579\n",
      "Epoch [78/500], Batch [20/110], Train Loss: 0.0071, Val Loss: 0.0082, LR: 0.0009429262\n",
      "Epoch [78/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0009427943\n",
      "Epoch [78/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0066, LR: 0.0009426623\n",
      "Epoch [78/500], Batch [50/110], Train Loss: 0.0021, Val Loss: 0.0066, LR: 0.0009425301\n",
      "Epoch [78/500], Batch [60/110], Train Loss: 0.0028, Val Loss: 0.0082, LR: 0.0009423978\n",
      "Epoch [78/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0009422654\n",
      "Epoch [78/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0348, LR: 0.0009421328\n",
      "Epoch [78/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0065, LR: 0.0009420000\n",
      "Epoch [78/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0067, LR: 0.0009418672\n",
      "Epoch [78/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0080, LR: 0.0009417342\n",
      "Epoch [79/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0073, LR: 0.0009416010\n",
      "Epoch [79/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0070, LR: 0.0009414677\n",
      "Epoch [79/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0009413343\n",
      "Epoch [79/500], Batch [40/110], Train Loss: 0.0016, Val Loss: 0.0079, LR: 0.0009412007\n",
      "Epoch [79/500], Batch [50/110], Train Loss: 0.0016, Val Loss: 0.0098, LR: 0.0009410669\n",
      "Epoch [79/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0077, LR: 0.0009409331\n",
      "Epoch [79/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0009407990\n",
      "Epoch [79/500], Batch [80/110], Train Loss: 0.0127, Val Loss: 0.0078, LR: 0.0009406649\n",
      "Epoch [79/500], Batch [90/110], Train Loss: 0.0039, Val Loss: 0.0069, LR: 0.0009405306\n",
      "Epoch [79/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0009403962\n",
      "Epoch [79/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0067, LR: 0.0009402616\n",
      "Epoch [80/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0009401268\n",
      "Epoch [80/500], Batch [20/110], Train Loss: 0.0059, Val Loss: 0.0067, LR: 0.0009399920\n",
      "Epoch [80/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0009398570\n",
      "Epoch [80/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0064, LR: 0.0009397218\n",
      "Epoch [80/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0066, LR: 0.0009395865\n",
      "Epoch [80/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0009394511\n",
      "Epoch [80/500], Batch [70/110], Train Loss: 0.0672, Val Loss: 0.0123, LR: 0.0009393155\n",
      "Epoch [80/500], Batch [80/110], Train Loss: 0.0034, Val Loss: 0.0074, LR: 0.0009391798\n",
      "Epoch [80/500], Batch [90/110], Train Loss: 0.0111, Val Loss: 0.0089, LR: 0.0009390439\n",
      "Epoch [80/500], Batch [100/110], Train Loss: 0.0018, Val Loss: 0.0079, LR: 0.0009389079\n",
      "Epoch [80/500], Batch [110/110], Train Loss: 0.0024, Val Loss: 0.0077, LR: 0.0009387718\n",
      "Epoch [81/500], Batch [10/110], Train Loss: 0.0206, Val Loss: 0.0073, LR: 0.0009386355\n",
      "Epoch [81/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0072, LR: 0.0009384991\n",
      "Epoch [81/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0009383625\n",
      "Epoch [81/500], Batch [40/110], Train Loss: 0.0025, Val Loss: 0.0083, LR: 0.0009382258\n",
      "Epoch [81/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0009380890\n",
      "Epoch [81/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0009379520\n",
      "Epoch [81/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0009378149\n",
      "Epoch [81/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0082, LR: 0.0009376776\n",
      "Epoch [81/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0090, LR: 0.0009375402\n",
      "Epoch [81/500], Batch [100/110], Train Loss: 0.0023, Val Loss: 0.0097, LR: 0.0009374026\n",
      "Epoch [81/500], Batch [110/110], Train Loss: 0.0045, Val Loss: 0.0081, LR: 0.0009372649\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 81: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 415.66 MB\n",
      "Epoch [82/500], Batch [10/110], Train Loss: 0.0123, Val Loss: 0.0096, LR: 0.0009371271\n",
      "Epoch [82/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0122, LR: 0.0009369891\n",
      "Epoch [82/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0123, LR: 0.0009368510\n",
      "Epoch [82/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0009367127\n",
      "Epoch [82/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0009365743\n",
      "Epoch [82/500], Batch [60/110], Train Loss: 0.0067, Val Loss: 0.0088, LR: 0.0009364358\n",
      "Epoch [82/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0068, LR: 0.0009362971\n",
      "Epoch [82/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0070, LR: 0.0009361583\n",
      "Epoch [82/500], Batch [90/110], Train Loss: 0.0038, Val Loss: 0.0071, LR: 0.0009360193\n",
      "Epoch [82/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0009358802\n",
      "Epoch [82/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0100, LR: 0.0009357410\n",
      "Epoch [83/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0100, LR: 0.0009356016\n",
      "Epoch [83/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0009354620\n",
      "Epoch [83/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0111, LR: 0.0009353224\n",
      "Epoch [83/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0112, LR: 0.0009351826\n",
      "Epoch [83/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0111, LR: 0.0009350426\n",
      "Epoch [83/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0102, LR: 0.0009349025\n",
      "Epoch [83/500], Batch [70/110], Train Loss: 0.0030, Val Loss: 0.0086, LR: 0.0009347623\n",
      "Epoch [83/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0082, LR: 0.0009346219\n",
      "Epoch [83/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0083, LR: 0.0009344814\n",
      "Epoch [83/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0009343408\n",
      "Epoch [83/500], Batch [110/110], Train Loss: 0.0108, Val Loss: 0.0093, LR: 0.0009342000\n",
      "Epoch [84/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0095, LR: 0.0009340591\n",
      "Epoch [84/500], Batch [20/110], Train Loss: 0.0371, Val Loss: 0.0092, LR: 0.0009339180\n",
      "Epoch [84/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0009337768\n",
      "Epoch [84/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0009336354\n",
      "Epoch [84/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0127, LR: 0.0009334940\n",
      "Epoch [84/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0146, LR: 0.0009333523\n",
      "Epoch [84/500], Batch [70/110], Train Loss: 0.0468, Val Loss: 0.0098, LR: 0.0009332106\n",
      "Epoch [84/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0117, LR: 0.0009330687\n",
      "Epoch [84/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0103, LR: 0.0009329266\n",
      "Epoch [84/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0009327844\n",
      "Epoch [84/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0009326421\n",
      "Epoch [85/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0009324996\n",
      "Epoch [85/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0116, LR: 0.0009323570\n",
      "Epoch [85/500], Batch [30/110], Train Loss: 0.0499, Val Loss: 0.0148, LR: 0.0009322143\n",
      "Epoch [85/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0079, LR: 0.0009320714\n",
      "Epoch [85/500], Batch [50/110], Train Loss: 0.0010, Val Loss: 0.0165, LR: 0.0009319284\n",
      "Epoch [85/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0108, LR: 0.0009317852\n",
      "Epoch [85/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0009316419\n",
      "Epoch [85/500], Batch [80/110], Train Loss: 0.0103, Val Loss: 0.0090, LR: 0.0009314985\n",
      "Epoch [85/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0120, LR: 0.0009313549\n",
      "Epoch [85/500], Batch [100/110], Train Loss: 0.0347, Val Loss: 0.0137, LR: 0.0009312112\n",
      "Epoch [85/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0009310673\n",
      "Epoch [86/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0085, LR: 0.0009309233\n",
      "Epoch [86/500], Batch [20/110], Train Loss: 0.0141, Val Loss: 0.0079, LR: 0.0009307792\n",
      "Epoch [86/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0075, LR: 0.0009306349\n",
      "Epoch [86/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0088, LR: 0.0009304905\n",
      "Epoch [86/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0009303459\n",
      "Epoch [86/500], Batch [60/110], Train Loss: 0.0225, Val Loss: 0.0104, LR: 0.0009302012\n",
      "Epoch [86/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0091, LR: 0.0009300564\n",
      "Epoch [86/500], Batch [80/110], Train Loss: 0.0041, Val Loss: 0.0081, LR: 0.0009299114\n",
      "Epoch [86/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0140, LR: 0.0009297663\n",
      "Epoch [86/500], Batch [100/110], Train Loss: 0.0020, Val Loss: 0.0088, LR: 0.0009296211\n",
      "Epoch [86/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0009294757\n",
      "Epoch [87/500], Batch [10/110], Train Loss: 0.0075, Val Loss: 0.0085, LR: 0.0009293302\n",
      "Epoch [87/500], Batch [20/110], Train Loss: 0.0062, Val Loss: 0.0088, LR: 0.0009291845\n",
      "Epoch [87/500], Batch [30/110], Train Loss: 0.0088, Val Loss: 0.0092, LR: 0.0009290387\n",
      "Epoch [87/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0009288928\n",
      "Epoch [87/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0009287467\n",
      "Epoch [87/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0009286005\n",
      "Epoch [87/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0080, LR: 0.0009284541\n",
      "Epoch [87/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0086, LR: 0.0009283076\n",
      "Epoch [87/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0009281610\n",
      "Epoch [87/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0009280142\n",
      "Epoch [87/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0080, LR: 0.0009278673\n",
      "Epoch [88/500], Batch [10/110], Train Loss: 0.0124, Val Loss: 0.0098, LR: 0.0009277203\n",
      "Epoch [88/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0009275731\n",
      "Epoch [88/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0082, LR: 0.0009274258\n",
      "Epoch [88/500], Batch [40/110], Train Loss: 0.0027, Val Loss: 0.0097, LR: 0.0009272783\n",
      "Epoch [88/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0009271307\n",
      "Epoch [88/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0009269830\n",
      "Epoch [88/500], Batch [70/110], Train Loss: 0.0012, Val Loss: 0.0092, LR: 0.0009268351\n",
      "Epoch [88/500], Batch [80/110], Train Loss: 0.0100, Val Loss: 0.0102, LR: 0.0009266871\n",
      "Epoch [88/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0009265390\n",
      "Epoch [88/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0009263907\n",
      "Epoch [88/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0009262423\n",
      "Epoch [89/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0009260937\n",
      "Epoch [89/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0092, LR: 0.0009259450\n",
      "Epoch [89/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0009257962\n",
      "Epoch [89/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0132, LR: 0.0009256472\n",
      "Epoch [89/500], Batch [50/110], Train Loss: 0.0045, Val Loss: 0.0091, LR: 0.0009254981\n",
      "Epoch [89/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0009253489\n",
      "Epoch [89/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0009251995\n",
      "Epoch [89/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0111, LR: 0.0009250500\n",
      "Epoch [89/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0009249003\n",
      "Epoch [89/500], Batch [100/110], Train Loss: 0.2014, Val Loss: 0.0084, LR: 0.0009247505\n",
      "Epoch [89/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0009246006\n",
      "Epoch [90/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0009244505\n",
      "Epoch [90/500], Batch [20/110], Train Loss: 0.0176, Val Loss: 0.0085, LR: 0.0009243003\n",
      "Epoch [90/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0009241500\n",
      "Epoch [90/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0009239995\n",
      "Epoch [90/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0087, LR: 0.0009238489\n",
      "Epoch [90/500], Batch [60/110], Train Loss: 0.0023, Val Loss: 0.0088, LR: 0.0009236981\n",
      "Epoch [90/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0107, LR: 0.0009235472\n",
      "Epoch [90/500], Batch [80/110], Train Loss: 0.0035, Val Loss: 0.0100, LR: 0.0009233962\n",
      "Epoch [90/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0104, LR: 0.0009232451\n",
      "Epoch [90/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0188, LR: 0.0009230938\n",
      "Epoch [90/500], Batch [110/110], Train Loss: 0.0076, Val Loss: 0.0087, LR: 0.0009229423\n",
      "Epoch [91/500], Batch [10/110], Train Loss: 0.0031, Val Loss: 0.0092, LR: 0.0009227908\n",
      "Epoch [91/500], Batch [20/110], Train Loss: 0.0054, Val Loss: 0.0104, LR: 0.0009226390\n",
      "Epoch [91/500], Batch [30/110], Train Loss: 0.0098, Val Loss: 0.0091, LR: 0.0009224872\n",
      "Epoch [91/500], Batch [40/110], Train Loss: 0.0252, Val Loss: 0.0101, LR: 0.0009223352\n",
      "Epoch [91/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0009221831\n",
      "Epoch [91/500], Batch [60/110], Train Loss: 0.0049, Val Loss: 0.0098, LR: 0.0009220309\n",
      "Epoch [91/500], Batch [70/110], Train Loss: 0.0099, Val Loss: 0.0100, LR: 0.0009218785\n",
      "Epoch [91/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0009217260\n",
      "Epoch [91/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0009215733\n",
      "Epoch [91/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0009214205\n",
      "Epoch [91/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0009212676\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 91: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 415.66 MB\n",
      "Epoch [92/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0117, LR: 0.0009211145\n",
      "Epoch [92/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0112, LR: 0.0009209613\n",
      "Epoch [92/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0144, LR: 0.0009208080\n",
      "Epoch [92/500], Batch [40/110], Train Loss: 0.0047, Val Loss: 0.0084, LR: 0.0009206545\n",
      "Epoch [92/500], Batch [50/110], Train Loss: 0.0193, Val Loss: 0.0094, LR: 0.0009205009\n",
      "Epoch [92/500], Batch [60/110], Train Loss: 0.0065, Val Loss: 0.0085, LR: 0.0009203471\n",
      "Epoch [92/500], Batch [70/110], Train Loss: 0.0169, Val Loss: 0.0084, LR: 0.0009201933\n",
      "Epoch [92/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0009200392\n",
      "Epoch [92/500], Batch [90/110], Train Loss: 0.0053, Val Loss: 0.0103, LR: 0.0009198851\n",
      "Epoch [92/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0009197308\n",
      "Epoch [92/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0009195764\n",
      "Epoch [93/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0009194218\n",
      "Epoch [93/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0087, LR: 0.0009192671\n",
      "Epoch [93/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0118, LR: 0.0009191123\n",
      "Epoch [93/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0009189573\n",
      "Epoch [93/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0009188022\n",
      "Epoch [93/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0009186470\n",
      "Epoch [93/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0009184916\n",
      "Epoch [93/500], Batch [80/110], Train Loss: 0.0367, Val Loss: 0.0131, LR: 0.0009183361\n",
      "Epoch [93/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0009181805\n",
      "Epoch [93/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0009180247\n",
      "Epoch [93/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0105, LR: 0.0009178688\n",
      "Epoch [94/500], Batch [10/110], Train Loss: 0.0055, Val Loss: 0.0088, LR: 0.0009177128\n",
      "Epoch [94/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0009175566\n",
      "Epoch [94/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0009174003\n",
      "Epoch [94/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0009172439\n",
      "Epoch [94/500], Batch [50/110], Train Loss: 0.0034, Val Loss: 0.0089, LR: 0.0009170873\n",
      "Epoch [94/500], Batch [60/110], Train Loss: 0.0014, Val Loss: 0.0086, LR: 0.0009169306\n",
      "Epoch [94/500], Batch [70/110], Train Loss: 0.0210, Val Loss: 0.0094, LR: 0.0009167737\n",
      "Epoch [94/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0009166167\n",
      "Epoch [94/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0091, LR: 0.0009164596\n",
      "Epoch [94/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0107, LR: 0.0009163024\n",
      "Epoch [94/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0009161450\n",
      "Epoch [95/500], Batch [10/110], Train Loss: 0.0131, Val Loss: 0.0087, LR: 0.0009159875\n",
      "Epoch [95/500], Batch [20/110], Train Loss: 0.2537, Val Loss: 0.0087, LR: 0.0009158298\n",
      "Epoch [95/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0094, LR: 0.0009156720\n",
      "Epoch [95/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0009155141\n",
      "Epoch [95/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0009153560\n",
      "Epoch [95/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0100, LR: 0.0009151978\n",
      "Epoch [95/500], Batch [70/110], Train Loss: 0.1524, Val Loss: 0.0089, LR: 0.0009150395\n",
      "Epoch [95/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0098, LR: 0.0009148811\n",
      "Epoch [95/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0009147225\n",
      "Epoch [95/500], Batch [100/110], Train Loss: 0.0021, Val Loss: 0.0124, LR: 0.0009145637\n",
      "Epoch [95/500], Batch [110/110], Train Loss: 0.0107, Val Loss: 0.0084, LR: 0.0009144049\n",
      "Epoch [96/500], Batch [10/110], Train Loss: 0.0061, Val Loss: 0.0086, LR: 0.0009142459\n",
      "Epoch [96/500], Batch [20/110], Train Loss: 0.0014, Val Loss: 0.0091, LR: 0.0009140868\n",
      "Epoch [96/500], Batch [30/110], Train Loss: 0.0024, Val Loss: 0.0099, LR: 0.0009139275\n",
      "Epoch [96/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0009137681\n",
      "Epoch [96/500], Batch [50/110], Train Loss: 0.0075, Val Loss: 0.0087, LR: 0.0009136086\n",
      "Epoch [96/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0009134489\n",
      "Epoch [96/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0111, LR: 0.0009132891\n",
      "Epoch [96/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0009131292\n",
      "Epoch [96/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0088, LR: 0.0009129692\n",
      "Epoch [96/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0088, LR: 0.0009128090\n",
      "Epoch [96/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0009126486\n",
      "Epoch [97/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0009124882\n",
      "Epoch [97/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0009123276\n",
      "Epoch [97/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0094, LR: 0.0009121669\n",
      "Epoch [97/500], Batch [40/110], Train Loss: 0.0051, Val Loss: 0.0127, LR: 0.0009120060\n",
      "Epoch [97/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0174, LR: 0.0009118450\n",
      "Epoch [97/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0009116839\n",
      "Epoch [97/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0133, LR: 0.0009115226\n",
      "Epoch [97/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0176, LR: 0.0009113613\n",
      "Epoch [97/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0009111997\n",
      "Epoch [97/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0106, LR: 0.0009110381\n",
      "Epoch [97/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0009108763\n",
      "Epoch [98/500], Batch [10/110], Train Loss: 0.0232, Val Loss: 0.0092, LR: 0.0009107144\n",
      "Epoch [98/500], Batch [20/110], Train Loss: 0.0013, Val Loss: 0.0094, LR: 0.0009105523\n",
      "Epoch [98/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0009103901\n",
      "Epoch [98/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0009102278\n",
      "Epoch [98/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0140, LR: 0.0009100654\n",
      "Epoch [98/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0009099028\n",
      "Epoch [98/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0009097401\n",
      "Epoch [98/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0106, LR: 0.0009095773\n",
      "Epoch [98/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0120, LR: 0.0009094143\n",
      "Epoch [98/500], Batch [100/110], Train Loss: 0.0117, Val Loss: 0.0091, LR: 0.0009092512\n",
      "Epoch [98/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0087, LR: 0.0009090879\n",
      "Epoch [99/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0009089246\n",
      "Epoch [99/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0125, LR: 0.0009087611\n",
      "Epoch [99/500], Batch [30/110], Train Loss: 0.0059, Val Loss: 0.0089, LR: 0.0009085974\n",
      "Epoch [99/500], Batch [40/110], Train Loss: 0.0098, Val Loss: 0.0093, LR: 0.0009084337\n",
      "Epoch [99/500], Batch [50/110], Train Loss: 0.0019, Val Loss: 0.0090, LR: 0.0009082698\n",
      "Epoch [99/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0009081057\n",
      "Epoch [99/500], Batch [70/110], Train Loss: 0.0537, Val Loss: 0.0128, LR: 0.0009079416\n",
      "Epoch [99/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0086, LR: 0.0009077773\n",
      "Epoch [99/500], Batch [90/110], Train Loss: 0.0088, Val Loss: 0.0110, LR: 0.0009076129\n",
      "Epoch [99/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0009074483\n",
      "Epoch [99/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0116, LR: 0.0009072836\n",
      "Epoch [100/500], Batch [10/110], Train Loss: 0.0046, Val Loss: 0.0093, LR: 0.0009071188\n",
      "Epoch [100/500], Batch [20/110], Train Loss: 0.0116, Val Loss: 0.0090, LR: 0.0009069538\n",
      "Epoch [100/500], Batch [30/110], Train Loss: 0.0029, Val Loss: 0.0093, LR: 0.0009067888\n",
      "Epoch [100/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0009066236\n",
      "Epoch [100/500], Batch [50/110], Train Loss: 0.0019, Val Loss: 0.0091, LR: 0.0009064582\n",
      "Epoch [100/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0181, LR: 0.0009062927\n",
      "Epoch [100/500], Batch [70/110], Train Loss: 0.2804, Val Loss: 0.0106, LR: 0.0009061271\n",
      "Epoch [100/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0009059614\n",
      "Epoch [100/500], Batch [90/110], Train Loss: 0.0033, Val Loss: 0.0093, LR: 0.0009057955\n",
      "Epoch [100/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0009056295\n",
      "Epoch [100/500], Batch [110/110], Train Loss: 0.0031, Val Loss: 0.0086, LR: 0.0009054634\n",
      "Epoch [101/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0085, LR: 0.0009052972\n",
      "Epoch [101/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0009051308\n",
      "Epoch [101/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0009049642\n",
      "Epoch [101/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0086, LR: 0.0009047976\n",
      "Epoch [101/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0095, LR: 0.0009046308\n",
      "Epoch [101/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0009044639\n",
      "Epoch [101/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0009042969\n",
      "Epoch [101/500], Batch [80/110], Train Loss: 0.0032, Val Loss: 0.0097, LR: 0.0009041297\n",
      "Epoch [101/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0009039624\n",
      "Epoch [101/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0193, LR: 0.0009037950\n",
      "Epoch [101/500], Batch [110/110], Train Loss: 0.0052, Val Loss: 0.0087, LR: 0.0009036274\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 101: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.041 sec , Memory Usage: 415.66 MB\n",
      "Epoch [102/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0089, LR: 0.0009034597\n",
      "Epoch [102/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0009032919\n",
      "Epoch [102/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0009031239\n",
      "Epoch [102/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0186, LR: 0.0009029559\n",
      "Epoch [102/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0009027876\n",
      "Epoch [102/500], Batch [60/110], Train Loss: 0.0014, Val Loss: 0.0092, LR: 0.0009026193\n",
      "Epoch [102/500], Batch [70/110], Train Loss: 0.0023, Val Loss: 0.0089, LR: 0.0009024508\n",
      "Epoch [102/500], Batch [80/110], Train Loss: 0.0042, Val Loss: 0.0090, LR: 0.0009022822\n",
      "Epoch [102/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0094, LR: 0.0009021135\n",
      "Epoch [102/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0093, LR: 0.0009019446\n",
      "Epoch [102/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0092, LR: 0.0009017757\n",
      "Epoch [103/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0087, LR: 0.0009016065\n",
      "Epoch [103/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0096, LR: 0.0009014373\n",
      "Epoch [103/500], Batch [30/110], Train Loss: 0.0360, Val Loss: 0.0100, LR: 0.0009012679\n",
      "Epoch [103/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0009010984\n",
      "Epoch [103/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0099, LR: 0.0009009288\n",
      "Epoch [103/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0009007590\n",
      "Epoch [103/500], Batch [70/110], Train Loss: 0.0288, Val Loss: 0.0104, LR: 0.0009005891\n",
      "Epoch [103/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0230, LR: 0.0009004191\n",
      "Epoch [103/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0183, LR: 0.0009002489\n",
      "Epoch [103/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0110, LR: 0.0009000787\n",
      "Epoch [103/500], Batch [110/110], Train Loss: 0.0038, Val Loss: 0.0138, LR: 0.0008999082\n",
      "Epoch [104/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0142, LR: 0.0008997377\n",
      "Epoch [104/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0008995670\n",
      "Epoch [104/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0141, LR: 0.0008993962\n",
      "Epoch [104/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0008992253\n",
      "Epoch [104/500], Batch [50/110], Train Loss: 0.3606, Val Loss: 0.0090, LR: 0.0008990543\n",
      "Epoch [104/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0108, LR: 0.0008988831\n",
      "Epoch [104/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0008987118\n",
      "Epoch [104/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0111, LR: 0.0008985403\n",
      "Epoch [104/500], Batch [90/110], Train Loss: 0.0045, Val Loss: 0.0093, LR: 0.0008983688\n",
      "Epoch [104/500], Batch [100/110], Train Loss: 0.0168, Val Loss: 0.0167, LR: 0.0008981971\n",
      "Epoch [104/500], Batch [110/110], Train Loss: 0.0024, Val Loss: 0.0088, LR: 0.0008980252\n",
      "Epoch [105/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0008978533\n",
      "Epoch [105/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0008976812\n",
      "Epoch [105/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008975090\n",
      "Epoch [105/500], Batch [40/110], Train Loss: 0.0009, Val Loss: 0.0122, LR: 0.0008973367\n",
      "Epoch [105/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008971642\n",
      "Epoch [105/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0008969916\n",
      "Epoch [105/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0008968189\n",
      "Epoch [105/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008966460\n",
      "Epoch [105/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0113, LR: 0.0008964731\n",
      "Epoch [105/500], Batch [100/110], Train Loss: 0.0020, Val Loss: 0.0107, LR: 0.0008963000\n",
      "Epoch [105/500], Batch [110/110], Train Loss: 0.0009, Val Loss: 0.0088, LR: 0.0008961267\n",
      "Epoch [106/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008959534\n",
      "Epoch [106/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0121, LR: 0.0008957799\n",
      "Epoch [106/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0008956063\n",
      "Epoch [106/500], Batch [40/110], Train Loss: 0.0164, Val Loss: 0.0091, LR: 0.0008954325\n",
      "Epoch [106/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0008952587\n",
      "Epoch [106/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0089, LR: 0.0008950847\n",
      "Epoch [106/500], Batch [70/110], Train Loss: 0.0093, Val Loss: 0.0086, LR: 0.0008949105\n",
      "Epoch [106/500], Batch [80/110], Train Loss: 0.0259, Val Loss: 0.0099, LR: 0.0008947363\n",
      "Epoch [106/500], Batch [90/110], Train Loss: 0.0075, Val Loss: 0.0085, LR: 0.0008945619\n",
      "Epoch [106/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0121, LR: 0.0008943874\n",
      "Epoch [106/500], Batch [110/110], Train Loss: 0.0090, Val Loss: 0.0110, LR: 0.0008942128\n",
      "Epoch [107/500], Batch [10/110], Train Loss: 0.0047, Val Loss: 0.0086, LR: 0.0008940380\n",
      "Epoch [107/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008938631\n",
      "Epoch [107/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008936881\n",
      "Epoch [107/500], Batch [40/110], Train Loss: 0.0065, Val Loss: 0.0114, LR: 0.0008935130\n",
      "Epoch [107/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0008933377\n",
      "Epoch [107/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0008931623\n",
      "Epoch [107/500], Batch [70/110], Train Loss: 0.0021, Val Loss: 0.0090, LR: 0.0008929868\n",
      "Epoch [107/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0008928111\n",
      "Epoch [107/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008926354\n",
      "Epoch [107/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008924595\n",
      "Epoch [107/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0008922835\n",
      "Epoch [108/500], Batch [10/110], Train Loss: 0.0227, Val Loss: 0.0165, LR: 0.0008921073\n",
      "Epoch [108/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0270, LR: 0.0008919310\n",
      "Epoch [108/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0123, LR: 0.0008917546\n",
      "Epoch [108/500], Batch [40/110], Train Loss: 0.0432, Val Loss: 0.0126, LR: 0.0008915781\n",
      "Epoch [108/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0230, LR: 0.0008914014\n",
      "Epoch [108/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0008912246\n",
      "Epoch [108/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0008910477\n",
      "Epoch [108/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008908707\n",
      "Epoch [108/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0097, LR: 0.0008906935\n",
      "Epoch [108/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0008905163\n",
      "Epoch [108/500], Batch [110/110], Train Loss: 0.0024, Val Loss: 0.0088, LR: 0.0008903388\n",
      "Epoch [109/500], Batch [10/110], Train Loss: 0.0016, Val Loss: 0.0083, LR: 0.0008901613\n",
      "Epoch [109/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0008899836\n",
      "Epoch [109/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0095, LR: 0.0008898058\n",
      "Epoch [109/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0098, LR: 0.0008896279\n",
      "Epoch [109/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0008894499\n",
      "Epoch [109/500], Batch [60/110], Train Loss: 0.0023, Val Loss: 0.0093, LR: 0.0008892717\n",
      "Epoch [109/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0081, LR: 0.0008890934\n",
      "Epoch [109/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0081, LR: 0.0008889150\n",
      "Epoch [109/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0082, LR: 0.0008887365\n",
      "Epoch [109/500], Batch [100/110], Train Loss: 0.0018, Val Loss: 0.0113, LR: 0.0008885578\n",
      "Epoch [109/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0098, LR: 0.0008883790\n",
      "Epoch [110/500], Batch [10/110], Train Loss: 0.0015, Val Loss: 0.0100, LR: 0.0008882001\n",
      "Epoch [110/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0088, LR: 0.0008880211\n",
      "Epoch [110/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0008878419\n",
      "Epoch [110/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0008876626\n",
      "Epoch [110/500], Batch [50/110], Train Loss: 0.0084, Val Loss: 0.0080, LR: 0.0008874832\n",
      "Epoch [110/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008873036\n",
      "Epoch [110/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0114, LR: 0.0008871240\n",
      "Epoch [110/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008869442\n",
      "Epoch [110/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008867643\n",
      "Epoch [110/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0095, LR: 0.0008865842\n",
      "Epoch [110/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0160, LR: 0.0008864041\n",
      "Epoch [111/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0098, LR: 0.0008862238\n",
      "Epoch [111/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0118, LR: 0.0008860434\n",
      "Epoch [111/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008858628\n",
      "Epoch [111/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0082, LR: 0.0008856822\n",
      "Epoch [111/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0102, LR: 0.0008855014\n",
      "Epoch [111/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0093, LR: 0.0008853205\n",
      "Epoch [111/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0008851394\n",
      "Epoch [111/500], Batch [80/110], Train Loss: 0.0187, Val Loss: 0.0088, LR: 0.0008849583\n",
      "Epoch [111/500], Batch [90/110], Train Loss: 0.0025, Val Loss: 0.0082, LR: 0.0008847770\n",
      "Epoch [111/500], Batch [100/110], Train Loss: 0.0020, Val Loss: 0.0083, LR: 0.0008845956\n",
      "Epoch [111/500], Batch [110/110], Train Loss: 0.0134, Val Loss: 0.0131, LR: 0.0008844140\n",
      "Confusion Matrix:\n",
      "[[631   6]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99842   0.99058   0.99448       637\n",
      "           1    0.99309   0.99884   0.99596       863\n",
      "\n",
      "    accuracy                        0.99533      1500\n",
      "   macro avg    0.99575   0.99471   0.99522      1500\n",
      "weighted avg    0.99535   0.99533   0.99533      1500\n",
      "\n",
      "Total Errors: 7\n",
      "Index: 41, Predicted: 1, Actual: 0\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Index: 715, Predicted: 1, Actual: 0\n",
      "Index: 719, Predicted: 1, Actual: 0\n",
      "Index: 750, Predicted: 1, Actual: 0\n",
      "Epoch 111: OK- Accuracy: 0.99533, Precision: 0.99309, Recall: 0.99884, F1: 0.99596, ROC AUC: 0.99471, AUPR (PR-AUC): 0.99260, Sensitivity: 0.99884, Specificity: 0.99058, Far: 0.009419152276295133, False Positive Rate (FPR): 0.00942, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 415.66 MB\n",
      "Epoch [112/500], Batch [10/110], Train Loss: 0.0010, Val Loss: 0.0080, LR: 0.0008842324\n",
      "Epoch [112/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0112, LR: 0.0008840506\n",
      "Epoch [112/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0008838687\n",
      "Epoch [112/500], Batch [40/110], Train Loss: 0.0072, Val Loss: 0.0084, LR: 0.0008836867\n",
      "Epoch [112/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008835045\n",
      "Epoch [112/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0085, LR: 0.0008833223\n",
      "Epoch [112/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0082, LR: 0.0008831399\n",
      "Epoch [112/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008829573\n",
      "Epoch [112/500], Batch [90/110], Train Loss: 0.0216, Val Loss: 0.0090, LR: 0.0008827747\n",
      "Epoch [112/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0107, LR: 0.0008825919\n",
      "Epoch [112/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0008824090\n",
      "Epoch [113/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0008822260\n",
      "Epoch [113/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0098, LR: 0.0008820429\n",
      "Epoch [113/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0101, LR: 0.0008818596\n",
      "Epoch [113/500], Batch [40/110], Train Loss: 0.0047, Val Loss: 0.0084, LR: 0.0008816763\n",
      "Epoch [113/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0089, LR: 0.0008814928\n",
      "Epoch [113/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008813091\n",
      "Epoch [113/500], Batch [70/110], Train Loss: 0.0030, Val Loss: 0.0107, LR: 0.0008811254\n",
      "Epoch [113/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008809415\n",
      "Epoch [113/500], Batch [90/110], Train Loss: 0.0120, Val Loss: 0.0077, LR: 0.0008807575\n",
      "Epoch [113/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0008805734\n",
      "Epoch [113/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0008803891\n",
      "Epoch [114/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008802048\n",
      "Epoch [114/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0089, LR: 0.0008800203\n",
      "Epoch [114/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0081, LR: 0.0008798357\n",
      "Epoch [114/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0008796510\n",
      "Epoch [114/500], Batch [50/110], Train Loss: 0.0552, Val Loss: 0.0124, LR: 0.0008794661\n",
      "Epoch [114/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0124, LR: 0.0008792811\n",
      "Epoch [114/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0194, LR: 0.0008790960\n",
      "Epoch [114/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0109, LR: 0.0008789108\n",
      "Epoch [114/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008787255\n",
      "Epoch [114/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0008785400\n",
      "Epoch [114/500], Batch [110/110], Train Loss: 0.0015, Val Loss: 0.0178, LR: 0.0008783544\n",
      "Epoch [115/500], Batch [10/110], Train Loss: 0.0092, Val Loss: 0.0084, LR: 0.0008781687\n",
      "Epoch [115/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0008779829\n",
      "Epoch [115/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0119, LR: 0.0008777969\n",
      "Epoch [115/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0081, LR: 0.0008776109\n",
      "Epoch [115/500], Batch [50/110], Train Loss: 0.1073, Val Loss: 0.0087, LR: 0.0008774247\n",
      "Epoch [115/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0102, LR: 0.0008772384\n",
      "Epoch [115/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008770519\n",
      "Epoch [115/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0102, LR: 0.0008768654\n",
      "Epoch [115/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0084, LR: 0.0008766787\n",
      "Epoch [115/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0081, LR: 0.0008764919\n",
      "Epoch [115/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0008763050\n",
      "Epoch [116/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0008761179\n",
      "Epoch [116/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0008759308\n",
      "Epoch [116/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0008757435\n",
      "Epoch [116/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008755561\n",
      "Epoch [116/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0008753686\n",
      "Epoch [116/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008751809\n",
      "Epoch [116/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0008749931\n",
      "Epoch [116/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0083, LR: 0.0008748053\n",
      "Epoch [116/500], Batch [90/110], Train Loss: 0.0231, Val Loss: 0.0096, LR: 0.0008746172\n",
      "Epoch [116/500], Batch [100/110], Train Loss: 0.0013, Val Loss: 0.0079, LR: 0.0008744291\n",
      "Epoch [116/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0079, LR: 0.0008742409\n",
      "Epoch [117/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0008740525\n",
      "Epoch [117/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0008738640\n",
      "Epoch [117/500], Batch [30/110], Train Loss: 0.0019, Val Loss: 0.0085, LR: 0.0008736754\n",
      "Epoch [117/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0008734867\n",
      "Epoch [117/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0008732978\n",
      "Epoch [117/500], Batch [60/110], Train Loss: 0.0013, Val Loss: 0.0079, LR: 0.0008731088\n",
      "Epoch [117/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008729197\n",
      "Epoch [117/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0008727305\n",
      "Epoch [117/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0008725412\n",
      "Epoch [117/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0008723517\n",
      "Epoch [117/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0081, LR: 0.0008721622\n",
      "Epoch [118/500], Batch [10/110], Train Loss: 0.0278, Val Loss: 0.0098, LR: 0.0008719725\n",
      "Epoch [118/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0008717827\n",
      "Epoch [118/500], Batch [30/110], Train Loss: 0.0553, Val Loss: 0.0112, LR: 0.0008715927\n",
      "Epoch [118/500], Batch [40/110], Train Loss: 0.0085, Val Loss: 0.0089, LR: 0.0008714027\n",
      "Epoch [118/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0084, LR: 0.0008712125\n",
      "Epoch [118/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008710222\n",
      "Epoch [118/500], Batch [70/110], Train Loss: 0.0092, Val Loss: 0.0085, LR: 0.0008708318\n",
      "Epoch [118/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0086, LR: 0.0008706413\n",
      "Epoch [118/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008704506\n",
      "Epoch [118/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008702599\n",
      "Epoch [118/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0096, LR: 0.0008700690\n",
      "Epoch [119/500], Batch [10/110], Train Loss: 0.0022, Val Loss: 0.0084, LR: 0.0008698780\n",
      "Epoch [119/500], Batch [20/110], Train Loss: 0.0069, Val Loss: 0.0080, LR: 0.0008696869\n",
      "Epoch [119/500], Batch [30/110], Train Loss: 0.0013, Val Loss: 0.0085, LR: 0.0008694956\n",
      "Epoch [119/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0008693043\n",
      "Epoch [119/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0081, LR: 0.0008691128\n",
      "Epoch [119/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0082, LR: 0.0008689212\n",
      "Epoch [119/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0124, LR: 0.0008687295\n",
      "Epoch [119/500], Batch [80/110], Train Loss: 0.0026, Val Loss: 0.0081, LR: 0.0008685376\n",
      "Epoch [119/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0083, LR: 0.0008683457\n",
      "Epoch [119/500], Batch [100/110], Train Loss: 0.0020, Val Loss: 0.0078, LR: 0.0008681536\n",
      "Epoch [119/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0008679614\n",
      "Epoch [120/500], Batch [10/110], Train Loss: 0.0419, Val Loss: 0.0129, LR: 0.0008677691\n",
      "Epoch [120/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0008675767\n",
      "Epoch [120/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0008673841\n",
      "Epoch [120/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0086, LR: 0.0008671914\n",
      "Epoch [120/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008669987\n",
      "Epoch [120/500], Batch [60/110], Train Loss: 0.0018, Val Loss: 0.0101, LR: 0.0008668058\n",
      "Epoch [120/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0078, LR: 0.0008666127\n",
      "Epoch [120/500], Batch [80/110], Train Loss: 0.0597, Val Loss: 0.0135, LR: 0.0008664196\n",
      "Epoch [120/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0099, LR: 0.0008662263\n",
      "Epoch [120/500], Batch [100/110], Train Loss: 0.0053, Val Loss: 0.0085, LR: 0.0008660330\n",
      "Epoch [120/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0008658395\n",
      "Epoch [121/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0190, LR: 0.0008656459\n",
      "Epoch [121/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0078, LR: 0.0008654521\n",
      "Epoch [121/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0008652583\n",
      "Epoch [121/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0008650643\n",
      "Epoch [121/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0008648702\n",
      "Epoch [121/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0008646760\n",
      "Epoch [121/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0117, LR: 0.0008644817\n",
      "Epoch [121/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0008642873\n",
      "Epoch [121/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0111, LR: 0.0008640927\n",
      "Epoch [121/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0081, LR: 0.0008638981\n",
      "Epoch [121/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0079, LR: 0.0008637033\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 121: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 414.57 MB\n",
      "Epoch [122/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0109, LR: 0.0008635084\n",
      "Epoch [122/500], Batch [20/110], Train Loss: 0.0177, Val Loss: 0.0099, LR: 0.0008633134\n",
      "Epoch [122/500], Batch [30/110], Train Loss: 0.0077, Val Loss: 0.0081, LR: 0.0008631182\n",
      "Epoch [122/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0008629230\n",
      "Epoch [122/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0082, LR: 0.0008627276\n",
      "Epoch [122/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0008625321\n",
      "Epoch [122/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008623365\n",
      "Epoch [122/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0008621408\n",
      "Epoch [122/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0008619450\n",
      "Epoch [122/500], Batch [100/110], Train Loss: 0.0022, Val Loss: 0.0081, LR: 0.0008617490\n",
      "Epoch [122/500], Batch [110/110], Train Loss: 0.0035, Val Loss: 0.0082, LR: 0.0008615530\n",
      "Epoch [123/500], Batch [10/110], Train Loss: 0.0016, Val Loss: 0.0082, LR: 0.0008613568\n",
      "Epoch [123/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008611605\n",
      "Epoch [123/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008609641\n",
      "Epoch [123/500], Batch [40/110], Train Loss: 0.0044, Val Loss: 0.0079, LR: 0.0008607675\n",
      "Epoch [123/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0008605709\n",
      "Epoch [123/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0008603741\n",
      "Epoch [123/500], Batch [70/110], Train Loss: 0.0046, Val Loss: 0.0080, LR: 0.0008601772\n",
      "Epoch [123/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0082, LR: 0.0008599802\n",
      "Epoch [123/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0008597831\n",
      "Epoch [123/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0170, LR: 0.0008595859\n",
      "Epoch [123/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0008593886\n",
      "Epoch [124/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0008591911\n",
      "Epoch [124/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0147, LR: 0.0008589935\n",
      "Epoch [124/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008587958\n",
      "Epoch [124/500], Batch [40/110], Train Loss: 0.0121, Val Loss: 0.0085, LR: 0.0008585980\n",
      "Epoch [124/500], Batch [50/110], Train Loss: 0.0089, Val Loss: 0.0090, LR: 0.0008584001\n",
      "Epoch [124/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0076, LR: 0.0008582021\n",
      "Epoch [124/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0080, LR: 0.0008580039\n",
      "Epoch [124/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0008578057\n",
      "Epoch [124/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008576073\n",
      "Epoch [124/500], Batch [100/110], Train Loss: 0.0222, Val Loss: 0.0094, LR: 0.0008574088\n",
      "Epoch [124/500], Batch [110/110], Train Loss: 0.0024, Val Loss: 0.0084, LR: 0.0008572102\n",
      "Epoch [125/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0117, LR: 0.0008570114\n",
      "Epoch [125/500], Batch [20/110], Train Loss: 0.0118, Val Loss: 0.0084, LR: 0.0008568126\n",
      "Epoch [125/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0008566136\n",
      "Epoch [125/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0008564146\n",
      "Epoch [125/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0117, LR: 0.0008562154\n",
      "Epoch [125/500], Batch [60/110], Train Loss: 0.0060, Val Loss: 0.0096, LR: 0.0008560161\n",
      "Epoch [125/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0078, LR: 0.0008558167\n",
      "Epoch [125/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0008556171\n",
      "Epoch [125/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0008554175\n",
      "Epoch [125/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0008552177\n",
      "Epoch [125/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0081, LR: 0.0008550179\n",
      "Epoch [126/500], Batch [10/110], Train Loss: 0.0028, Val Loss: 0.0109, LR: 0.0008548179\n",
      "Epoch [126/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0008546178\n",
      "Epoch [126/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0083, LR: 0.0008544176\n",
      "Epoch [126/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0086, LR: 0.0008542172\n",
      "Epoch [126/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008540168\n",
      "Epoch [126/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0008538162\n",
      "Epoch [126/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0008536156\n",
      "Epoch [126/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0118, LR: 0.0008534148\n",
      "Epoch [126/500], Batch [90/110], Train Loss: 0.2920, Val Loss: 0.0103, LR: 0.0008532139\n",
      "Epoch [126/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008530129\n",
      "Epoch [126/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0008528117\n",
      "Epoch [127/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0008526105\n",
      "Epoch [127/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0008524091\n",
      "Epoch [127/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0008522077\n",
      "Epoch [127/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0008520061\n",
      "Epoch [127/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008518044\n",
      "Epoch [127/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0008516026\n",
      "Epoch [127/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0008514007\n",
      "Epoch [127/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0008511987\n",
      "Epoch [127/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0093, LR: 0.0008509965\n",
      "Epoch [127/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0008507943\n",
      "Epoch [127/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008505919\n",
      "Epoch [128/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008503894\n",
      "Epoch [128/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0088, LR: 0.0008501868\n",
      "Epoch [128/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0164, LR: 0.0008499841\n",
      "Epoch [128/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0083, LR: 0.0008497813\n",
      "Epoch [128/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0008495783\n",
      "Epoch [128/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0098, LR: 0.0008493753\n",
      "Epoch [128/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0008491721\n",
      "Epoch [128/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0008489689\n",
      "Epoch [128/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0008487655\n",
      "Epoch [128/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0008485620\n",
      "Epoch [128/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0127, LR: 0.0008483584\n",
      "Epoch [129/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0130, LR: 0.0008481547\n",
      "Epoch [129/500], Batch [20/110], Train Loss: 0.2728, Val Loss: 0.0147, LR: 0.0008479508\n",
      "Epoch [129/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008477469\n",
      "Epoch [129/500], Batch [40/110], Train Loss: 0.0027, Val Loss: 0.0080, LR: 0.0008475428\n",
      "Epoch [129/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0131, LR: 0.0008473387\n",
      "Epoch [129/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0181, LR: 0.0008471344\n",
      "Epoch [129/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008469300\n",
      "Epoch [129/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008467255\n",
      "Epoch [129/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0008465209\n",
      "Epoch [129/500], Batch [100/110], Train Loss: 0.0022, Val Loss: 0.0081, LR: 0.0008463162\n",
      "Epoch [129/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0081, LR: 0.0008461113\n",
      "Epoch [130/500], Batch [10/110], Train Loss: 0.0009, Val Loss: 0.0093, LR: 0.0008459064\n",
      "Epoch [130/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0082, LR: 0.0008457013\n",
      "Epoch [130/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0008454962\n",
      "Epoch [130/500], Batch [40/110], Train Loss: 0.0018, Val Loss: 0.0113, LR: 0.0008452909\n",
      "Epoch [130/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0008450855\n",
      "Epoch [130/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0008448800\n",
      "Epoch [130/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0008446744\n",
      "Epoch [130/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008444687\n",
      "Epoch [130/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0008442628\n",
      "Epoch [130/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008440569\n",
      "Epoch [130/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0129, LR: 0.0008438508\n",
      "Epoch [131/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0008436447\n",
      "Epoch [131/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0008434384\n",
      "Epoch [131/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008432320\n",
      "Epoch [131/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0008430255\n",
      "Epoch [131/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0081, LR: 0.0008428189\n",
      "Epoch [131/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0119, LR: 0.0008426122\n",
      "Epoch [131/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0148, LR: 0.0008424053\n",
      "Epoch [131/500], Batch [80/110], Train Loss: 0.0029, Val Loss: 0.0090, LR: 0.0008421984\n",
      "Epoch [131/500], Batch [90/110], Train Loss: 0.0028, Val Loss: 0.0137, LR: 0.0008419913\n",
      "Epoch [131/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0008417842\n",
      "Epoch [131/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008415769\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 131: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 414.59 MB\n",
      "Epoch [132/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0008413695\n",
      "Epoch [132/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0008411621\n",
      "Epoch [132/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008409545\n",
      "Epoch [132/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0008407468\n",
      "Epoch [132/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0008405389\n",
      "Epoch [132/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0078, LR: 0.0008403310\n",
      "Epoch [132/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0105, LR: 0.0008401230\n",
      "Epoch [132/500], Batch [80/110], Train Loss: 0.0021, Val Loss: 0.0077, LR: 0.0008399148\n",
      "Epoch [132/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0078, LR: 0.0008397066\n",
      "Epoch [132/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0086, LR: 0.0008394982\n",
      "Epoch [132/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0008392897\n",
      "Epoch [133/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0008390812\n",
      "Epoch [133/500], Batch [20/110], Train Loss: 0.0013, Val Loss: 0.0076, LR: 0.0008388725\n",
      "Epoch [133/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0008386637\n",
      "Epoch [133/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0118, LR: 0.0008384548\n",
      "Epoch [133/500], Batch [50/110], Train Loss: 0.0046, Val Loss: 0.0077, LR: 0.0008382457\n",
      "Epoch [133/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0120, LR: 0.0008380366\n",
      "Epoch [133/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008378274\n",
      "Epoch [133/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0008376180\n",
      "Epoch [133/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0111, LR: 0.0008374086\n",
      "Epoch [133/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0008371990\n",
      "Epoch [133/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0008369894\n",
      "Epoch [134/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0115, LR: 0.0008367796\n",
      "Epoch [134/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008365697\n",
      "Epoch [134/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0008363597\n",
      "Epoch [134/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0008361496\n",
      "Epoch [134/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008359394\n",
      "Epoch [134/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0008357291\n",
      "Epoch [134/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0079, LR: 0.0008355187\n",
      "Epoch [134/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0080, LR: 0.0008353081\n",
      "Epoch [134/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0008350975\n",
      "Epoch [134/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0008348867\n",
      "Epoch [134/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0092, LR: 0.0008346759\n",
      "Epoch [135/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0008344649\n",
      "Epoch [135/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0081, LR: 0.0008342538\n",
      "Epoch [135/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0096, LR: 0.0008340427\n",
      "Epoch [135/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008338314\n",
      "Epoch [135/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0008336200\n",
      "Epoch [135/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008334085\n",
      "Epoch [135/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008331969\n",
      "Epoch [135/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008329852\n",
      "Epoch [135/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0008327733\n",
      "Epoch [135/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0008325614\n",
      "Epoch [135/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0211, LR: 0.0008323494\n",
      "Epoch [136/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0008321372\n",
      "Epoch [136/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008319250\n",
      "Epoch [136/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0008317126\n",
      "Epoch [136/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0112, LR: 0.0008315002\n",
      "Epoch [136/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0082, LR: 0.0008312876\n",
      "Epoch [136/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0084, LR: 0.0008310749\n",
      "Epoch [136/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0008308621\n",
      "Epoch [136/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0008306493\n",
      "Epoch [136/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008304363\n",
      "Epoch [136/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0101, LR: 0.0008302232\n",
      "Epoch [136/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0084, LR: 0.0008300099\n",
      "Epoch [137/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008297966\n",
      "Epoch [137/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0008295832\n",
      "Epoch [137/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008293697\n",
      "Epoch [137/500], Batch [40/110], Train Loss: 0.0077, Val Loss: 0.0078, LR: 0.0008291561\n",
      "Epoch [137/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0075, LR: 0.0008289423\n",
      "Epoch [137/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0008287285\n",
      "Epoch [137/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0008285145\n",
      "Epoch [137/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0008283005\n",
      "Epoch [137/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0008280863\n",
      "Epoch [137/500], Batch [100/110], Train Loss: 0.0155, Val Loss: 0.0082, LR: 0.0008278721\n",
      "Epoch [137/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0008276577\n",
      "Epoch [138/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008274432\n",
      "Epoch [138/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0008272286\n",
      "Epoch [138/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0008270140\n",
      "Epoch [138/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0008267992\n",
      "Epoch [138/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0008265843\n",
      "Epoch [138/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0008263693\n",
      "Epoch [138/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0176, LR: 0.0008261542\n",
      "Epoch [138/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0008259390\n",
      "Epoch [138/500], Batch [90/110], Train Loss: 0.0014, Val Loss: 0.0081, LR: 0.0008257236\n",
      "Epoch [138/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0008255082\n",
      "Epoch [138/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0008252927\n",
      "Epoch [139/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0008250771\n",
      "Epoch [139/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0085, LR: 0.0008248613\n",
      "Epoch [139/500], Batch [30/110], Train Loss: 0.0061, Val Loss: 0.0086, LR: 0.0008246455\n",
      "Epoch [139/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0008244296\n",
      "Epoch [139/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0008242135\n",
      "Epoch [139/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0105, LR: 0.0008239974\n",
      "Epoch [139/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0129, LR: 0.0008237811\n",
      "Epoch [139/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0008235648\n",
      "Epoch [139/500], Batch [90/110], Train Loss: 0.0090, Val Loss: 0.0129, LR: 0.0008233483\n",
      "Epoch [139/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0008231317\n",
      "Epoch [139/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0080, LR: 0.0008229151\n",
      "Epoch [140/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0080, LR: 0.0008226983\n",
      "Epoch [140/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0008224814\n",
      "Epoch [140/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008222644\n",
      "Epoch [140/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008220474\n",
      "Epoch [140/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0155, LR: 0.0008218302\n",
      "Epoch [140/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008216129\n",
      "Epoch [140/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0079, LR: 0.0008213955\n",
      "Epoch [140/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0008211780\n",
      "Epoch [140/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008209604\n",
      "Epoch [140/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008207427\n",
      "Epoch [140/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0008205249\n",
      "Epoch [141/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008203070\n",
      "Epoch [141/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0008200890\n",
      "Epoch [141/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0080, LR: 0.0008198708\n",
      "Epoch [141/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0098, LR: 0.0008196526\n",
      "Epoch [141/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0125, LR: 0.0008194343\n",
      "Epoch [141/500], Batch [60/110], Train Loss: 0.0066, Val Loss: 0.0082, LR: 0.0008192159\n",
      "Epoch [141/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0008189974\n",
      "Epoch [141/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008187787\n",
      "Epoch [141/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0146, LR: 0.0008185600\n",
      "Epoch [141/500], Batch [100/110], Train Loss: 0.0337, Val Loss: 0.0095, LR: 0.0008183412\n",
      "Epoch [141/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0008181222\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 141: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.037 sec , Memory Usage: 414.59 MB\n",
      "Epoch [142/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008179032\n",
      "Epoch [142/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0079, LR: 0.0008176841\n",
      "Epoch [142/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0136, LR: 0.0008174648\n",
      "Epoch [142/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0079, LR: 0.0008172455\n",
      "Epoch [142/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0008170260\n",
      "Epoch [142/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0008168065\n",
      "Epoch [142/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0078, LR: 0.0008165868\n",
      "Epoch [142/500], Batch [80/110], Train Loss: 0.0091, Val Loss: 0.0085, LR: 0.0008163671\n",
      "Epoch [142/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008161472\n",
      "Epoch [142/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008159273\n",
      "Epoch [142/500], Batch [110/110], Train Loss: 0.0207, Val Loss: 0.0100, LR: 0.0008157072\n",
      "Epoch [143/500], Batch [10/110], Train Loss: 0.0792, Val Loss: 0.0166, LR: 0.0008154871\n",
      "Epoch [143/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008152668\n",
      "Epoch [143/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0077, LR: 0.0008150465\n",
      "Epoch [143/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0008148260\n",
      "Epoch [143/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0080, LR: 0.0008146054\n",
      "Epoch [143/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0008143848\n",
      "Epoch [143/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0080, LR: 0.0008141640\n",
      "Epoch [143/500], Batch [80/110], Train Loss: 0.0046, Val Loss: 0.0077, LR: 0.0008139431\n",
      "Epoch [143/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0081, LR: 0.0008137222\n",
      "Epoch [143/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0008135011\n",
      "Epoch [143/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0008132800\n",
      "Epoch [144/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008130587\n",
      "Epoch [144/500], Batch [20/110], Train Loss: 0.0009, Val Loss: 0.0080, LR: 0.0008128373\n",
      "Epoch [144/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0008126159\n",
      "Epoch [144/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0108, LR: 0.0008123943\n",
      "Epoch [144/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0008121726\n",
      "Epoch [144/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0008119508\n",
      "Epoch [144/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0110, LR: 0.0008117290\n",
      "Epoch [144/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0081, LR: 0.0008115070\n",
      "Epoch [144/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0008112849\n",
      "Epoch [144/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0008110628\n",
      "Epoch [144/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008108405\n",
      "Epoch [145/500], Batch [10/110], Train Loss: 0.0339, Val Loss: 0.0193, LR: 0.0008106181\n",
      "Epoch [145/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0008103957\n",
      "Epoch [145/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0183, LR: 0.0008101731\n",
      "Epoch [145/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008099504\n",
      "Epoch [145/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0106, LR: 0.0008097277\n",
      "Epoch [145/500], Batch [60/110], Train Loss: 0.0099, Val Loss: 0.0084, LR: 0.0008095048\n",
      "Epoch [145/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008092818\n",
      "Epoch [145/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0008090588\n",
      "Epoch [145/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0008088356\n",
      "Epoch [145/500], Batch [100/110], Train Loss: 0.0120, Val Loss: 0.0086, LR: 0.0008086124\n",
      "Epoch [145/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0008083890\n",
      "Epoch [146/500], Batch [10/110], Train Loss: 0.0270, Val Loss: 0.0140, LR: 0.0008081655\n",
      "Epoch [146/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0111, LR: 0.0008079420\n",
      "Epoch [146/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0008077183\n",
      "Epoch [146/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0085, LR: 0.0008074946\n",
      "Epoch [146/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0117, LR: 0.0008072707\n",
      "Epoch [146/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0084, LR: 0.0008070467\n",
      "Epoch [146/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0008068227\n",
      "Epoch [146/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0097, LR: 0.0008065985\n",
      "Epoch [146/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0008063743\n",
      "Epoch [146/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0008061499\n",
      "Epoch [146/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0138, LR: 0.0008059255\n",
      "Epoch [147/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0008057010\n",
      "Epoch [147/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0084, LR: 0.0008054763\n",
      "Epoch [147/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0008052516\n",
      "Epoch [147/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0008050267\n",
      "Epoch [147/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0082, LR: 0.0008048018\n",
      "Epoch [147/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0008045768\n",
      "Epoch [147/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0113, LR: 0.0008043516\n",
      "Epoch [147/500], Batch [80/110], Train Loss: 0.0543, Val Loss: 0.0169, LR: 0.0008041264\n",
      "Epoch [147/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0090, LR: 0.0008039011\n",
      "Epoch [147/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0008036756\n",
      "Epoch [147/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0008034501\n",
      "Epoch [148/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0085, LR: 0.0008032245\n",
      "Epoch [148/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0008029988\n",
      "Epoch [148/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008027730\n",
      "Epoch [148/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008025471\n",
      "Epoch [148/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0008023211\n",
      "Epoch [148/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008020949\n",
      "Epoch [148/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008018687\n",
      "Epoch [148/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0008016424\n",
      "Epoch [148/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0008014160\n",
      "Epoch [148/500], Batch [100/110], Train Loss: 0.0012, Val Loss: 0.0086, LR: 0.0008011896\n",
      "Epoch [148/500], Batch [110/110], Train Loss: 0.0066, Val Loss: 0.0097, LR: 0.0008009630\n",
      "Epoch [149/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0008007363\n",
      "Epoch [149/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0008005095\n",
      "Epoch [149/500], Batch [30/110], Train Loss: 0.0018, Val Loss: 0.0088, LR: 0.0008002826\n",
      "Epoch [149/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0008000556\n",
      "Epoch [149/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0119, LR: 0.0007998286\n",
      "Epoch [149/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0007996014\n",
      "Epoch [149/500], Batch [70/110], Train Loss: 0.0012, Val Loss: 0.0084, LR: 0.0007993741\n",
      "Epoch [149/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0007991468\n",
      "Epoch [149/500], Batch [90/110], Train Loss: 0.0051, Val Loss: 0.0086, LR: 0.0007989193\n",
      "Epoch [149/500], Batch [100/110], Train Loss: 0.0026, Val Loss: 0.0103, LR: 0.0007986918\n",
      "Epoch [149/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0169, LR: 0.0007984641\n",
      "Epoch [150/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0007982364\n",
      "Epoch [150/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0084, LR: 0.0007980085\n",
      "Epoch [150/500], Batch [30/110], Train Loss: 0.0094, Val Loss: 0.0140, LR: 0.0007977806\n",
      "Epoch [150/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0160, LR: 0.0007975526\n",
      "Epoch [150/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0007973245\n",
      "Epoch [150/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0007970962\n",
      "Epoch [150/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0083, LR: 0.0007968679\n",
      "Epoch [150/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0007966395\n",
      "Epoch [150/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007964110\n",
      "Epoch [150/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007961824\n",
      "Epoch [150/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0007959537\n",
      "Epoch [151/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007957249\n",
      "Epoch [151/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0136, LR: 0.0007954960\n",
      "Epoch [151/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0133, LR: 0.0007952670\n",
      "Epoch [151/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0007950380\n",
      "Epoch [151/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0091, LR: 0.0007948088\n",
      "Epoch [151/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0090, LR: 0.0007945795\n",
      "Epoch [151/500], Batch [70/110], Train Loss: 0.0289, Val Loss: 0.0098, LR: 0.0007943502\n",
      "Epoch [151/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007941207\n",
      "Epoch [151/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007938912\n",
      "Epoch [151/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007936615\n",
      "Epoch [151/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0007934318\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 151: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.033 sec , Memory Usage: 414.58 MB\n",
      "Epoch [152/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0126, LR: 0.0007932020\n",
      "Epoch [152/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0007929720\n",
      "Epoch [152/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0086, LR: 0.0007927420\n",
      "Epoch [152/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0007925119\n",
      "Epoch [152/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0007922817\n",
      "Epoch [152/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007920514\n",
      "Epoch [152/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0102, LR: 0.0007918210\n",
      "Epoch [152/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0090, LR: 0.0007915905\n",
      "Epoch [152/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007913599\n",
      "Epoch [152/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0084, LR: 0.0007911293\n",
      "Epoch [152/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0100, LR: 0.0007908985\n",
      "Epoch [153/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007906676\n",
      "Epoch [153/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007904367\n",
      "Epoch [153/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0007902056\n",
      "Epoch [153/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0007899745\n",
      "Epoch [153/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0007897433\n",
      "Epoch [153/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0007895119\n",
      "Epoch [153/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007892805\n",
      "Epoch [153/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0132, LR: 0.0007890490\n",
      "Epoch [153/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007888174\n",
      "Epoch [153/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007885857\n",
      "Epoch [153/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0007883539\n",
      "Epoch [154/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0099, LR: 0.0007881220\n",
      "Epoch [154/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007878900\n",
      "Epoch [154/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007876580\n",
      "Epoch [154/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007874258\n",
      "Epoch [154/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0134, LR: 0.0007871936\n",
      "Epoch [154/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0095, LR: 0.0007869612\n",
      "Epoch [154/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007867288\n",
      "Epoch [154/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0084, LR: 0.0007864963\n",
      "Epoch [154/500], Batch [90/110], Train Loss: 0.0094, Val Loss: 0.0088, LR: 0.0007862636\n",
      "Epoch [154/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0007860309\n",
      "Epoch [154/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0007857981\n",
      "Epoch [155/500], Batch [10/110], Train Loss: 0.0014, Val Loss: 0.0087, LR: 0.0007855652\n",
      "Epoch [155/500], Batch [20/110], Train Loss: 0.0060, Val Loss: 0.0097, LR: 0.0007853322\n",
      "Epoch [155/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0088, LR: 0.0007850992\n",
      "Epoch [155/500], Batch [40/110], Train Loss: 0.0009, Val Loss: 0.0098, LR: 0.0007848660\n",
      "Epoch [155/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0007846327\n",
      "Epoch [155/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0128, LR: 0.0007843994\n",
      "Epoch [155/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0007841660\n",
      "Epoch [155/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0007839324\n",
      "Epoch [155/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007836988\n",
      "Epoch [155/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0007834651\n",
      "Epoch [155/500], Batch [110/110], Train Loss: 0.0043, Val Loss: 0.0104, LR: 0.0007832313\n",
      "Epoch [156/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007829974\n",
      "Epoch [156/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007827634\n",
      "Epoch [156/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0007825293\n",
      "Epoch [156/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007822951\n",
      "Epoch [156/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0007820609\n",
      "Epoch [156/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007818265\n",
      "Epoch [156/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0007815921\n",
      "Epoch [156/500], Batch [80/110], Train Loss: 0.0019, Val Loss: 0.0090, LR: 0.0007813576\n",
      "Epoch [156/500], Batch [90/110], Train Loss: 0.0069, Val Loss: 0.0086, LR: 0.0007811229\n",
      "Epoch [156/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007808882\n",
      "Epoch [156/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0007806534\n",
      "Epoch [157/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007804185\n",
      "Epoch [157/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007801836\n",
      "Epoch [157/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0007799485\n",
      "Epoch [157/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007797133\n",
      "Epoch [157/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007794781\n",
      "Epoch [157/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0007792427\n",
      "Epoch [157/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007790073\n",
      "Epoch [157/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0136, LR: 0.0007787718\n",
      "Epoch [157/500], Batch [90/110], Train Loss: 0.0209, Val Loss: 0.0127, LR: 0.0007785362\n",
      "Epoch [157/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007783005\n",
      "Epoch [157/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0084, LR: 0.0007780647\n",
      "Epoch [158/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007778288\n",
      "Epoch [158/500], Batch [20/110], Train Loss: 0.0166, Val Loss: 0.0091, LR: 0.0007775929\n",
      "Epoch [158/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0007773568\n",
      "Epoch [158/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0115, LR: 0.0007771207\n",
      "Epoch [158/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0087, LR: 0.0007768844\n",
      "Epoch [158/500], Batch [60/110], Train Loss: 0.0547, Val Loss: 0.0108, LR: 0.0007766481\n",
      "Epoch [158/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007764117\n",
      "Epoch [158/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007761752\n",
      "Epoch [158/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0109, LR: 0.0007759386\n",
      "Epoch [158/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0007757020\n",
      "Epoch [158/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0007754652\n",
      "Epoch [159/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0007752284\n",
      "Epoch [159/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0007749914\n",
      "Epoch [159/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0007747544\n",
      "Epoch [159/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0101, LR: 0.0007745173\n",
      "Epoch [159/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007742801\n",
      "Epoch [159/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0094, LR: 0.0007740428\n",
      "Epoch [159/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007738054\n",
      "Epoch [159/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0117, LR: 0.0007735679\n",
      "Epoch [159/500], Batch [90/110], Train Loss: 0.2546, Val Loss: 0.0158, LR: 0.0007733304\n",
      "Epoch [159/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007730927\n",
      "Epoch [159/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0007728550\n",
      "Epoch [160/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0007726172\n",
      "Epoch [160/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0007723793\n",
      "Epoch [160/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0007721413\n",
      "Epoch [160/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0007719032\n",
      "Epoch [160/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0007716651\n",
      "Epoch [160/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0007714268\n",
      "Epoch [160/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0122, LR: 0.0007711885\n",
      "Epoch [160/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0007709501\n",
      "Epoch [160/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007707115\n",
      "Epoch [160/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007704729\n",
      "Epoch [160/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007702343\n",
      "Epoch [161/500], Batch [10/110], Train Loss: 0.0026, Val Loss: 0.0086, LR: 0.0007699955\n",
      "Epoch [161/500], Batch [20/110], Train Loss: 0.0018, Val Loss: 0.0099, LR: 0.0007697566\n",
      "Epoch [161/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0102, LR: 0.0007695177\n",
      "Epoch [161/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0007692787\n",
      "Epoch [161/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0087, LR: 0.0007690395\n",
      "Epoch [161/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0007688003\n",
      "Epoch [161/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0083, LR: 0.0007685611\n",
      "Epoch [161/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0091, LR: 0.0007683217\n",
      "Epoch [161/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0007680822\n",
      "Epoch [161/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0083, LR: 0.0007678427\n",
      "Epoch [161/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007676030\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 161: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.036 sec , Memory Usage: 414.58 MB\n",
      "Epoch [162/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0007673633\n",
      "Epoch [162/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0007671235\n",
      "Epoch [162/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0007668836\n",
      "Epoch [162/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007666437\n",
      "Epoch [162/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0007664036\n",
      "Epoch [162/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0091, LR: 0.0007661634\n",
      "Epoch [162/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0080, LR: 0.0007659232\n",
      "Epoch [162/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007656829\n",
      "Epoch [162/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0007654425\n",
      "Epoch [162/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0089, LR: 0.0007652020\n",
      "Epoch [162/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007649614\n",
      "Epoch [163/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0089, LR: 0.0007647208\n",
      "Epoch [163/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0007644800\n",
      "Epoch [163/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007642392\n",
      "Epoch [163/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0202, LR: 0.0007639983\n",
      "Epoch [163/500], Batch [50/110], Train Loss: 0.0044, Val Loss: 0.0121, LR: 0.0007637573\n",
      "Epoch [163/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0007635162\n",
      "Epoch [163/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0108, LR: 0.0007632751\n",
      "Epoch [163/500], Batch [80/110], Train Loss: 0.0009, Val Loss: 0.0095, LR: 0.0007630338\n",
      "Epoch [163/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0088, LR: 0.0007627925\n",
      "Epoch [163/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0007625511\n",
      "Epoch [163/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0007623096\n",
      "Epoch [164/500], Batch [10/110], Train Loss: 0.0015, Val Loss: 0.0086, LR: 0.0007620680\n",
      "Epoch [164/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007618263\n",
      "Epoch [164/500], Batch [30/110], Train Loss: 0.0106, Val Loss: 0.0096, LR: 0.0007615846\n",
      "Epoch [164/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007613427\n",
      "Epoch [164/500], Batch [50/110], Train Loss: 0.0064, Val Loss: 0.0085, LR: 0.0007611008\n",
      "Epoch [164/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0007608588\n",
      "Epoch [164/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0108, LR: 0.0007606167\n",
      "Epoch [164/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0007603746\n",
      "Epoch [164/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007601323\n",
      "Epoch [164/500], Batch [100/110], Train Loss: 0.0016, Val Loss: 0.0099, LR: 0.0007598900\n",
      "Epoch [164/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0007596476\n",
      "Epoch [165/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007594051\n",
      "Epoch [165/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0007591625\n",
      "Epoch [165/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007589198\n",
      "Epoch [165/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0007586771\n",
      "Epoch [165/500], Batch [50/110], Train Loss: 0.0018, Val Loss: 0.0082, LR: 0.0007584342\n",
      "Epoch [165/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007581913\n",
      "Epoch [165/500], Batch [70/110], Train Loss: 0.0042, Val Loss: 0.0081, LR: 0.0007579483\n",
      "Epoch [165/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007577052\n",
      "Epoch [165/500], Batch [90/110], Train Loss: 0.1086, Val Loss: 0.0093, LR: 0.0007574621\n",
      "Epoch [165/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0007572188\n",
      "Epoch [165/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0085, LR: 0.0007569755\n",
      "Epoch [166/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0007567321\n",
      "Epoch [166/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0007564886\n",
      "Epoch [166/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007562450\n",
      "Epoch [166/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007560014\n",
      "Epoch [166/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0081, LR: 0.0007557576\n",
      "Epoch [166/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007555138\n",
      "Epoch [166/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007552699\n",
      "Epoch [166/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0007550259\n",
      "Epoch [166/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0007547819\n",
      "Epoch [166/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0007545377\n",
      "Epoch [166/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007542935\n",
      "Epoch [167/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0007540492\n",
      "Epoch [167/500], Batch [20/110], Train Loss: 0.0072, Val Loss: 0.0110, LR: 0.0007538048\n",
      "Epoch [167/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0007535603\n",
      "Epoch [167/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0167, LR: 0.0007533158\n",
      "Epoch [167/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007530711\n",
      "Epoch [167/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007528264\n",
      "Epoch [167/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0007525816\n",
      "Epoch [167/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007523367\n",
      "Epoch [167/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0007520918\n",
      "Epoch [167/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0007518467\n",
      "Epoch [167/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0007516016\n",
      "Epoch [168/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007513564\n",
      "Epoch [168/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0007511111\n",
      "Epoch [168/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0128, LR: 0.0007508658\n",
      "Epoch [168/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0007506204\n",
      "Epoch [168/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0007503748\n",
      "Epoch [168/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007501292\n",
      "Epoch [168/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0007498836\n",
      "Epoch [168/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0007496378\n",
      "Epoch [168/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0007493920\n",
      "Epoch [168/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0145, LR: 0.0007491460\n",
      "Epoch [168/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0007489000\n",
      "Epoch [169/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007486540\n",
      "Epoch [169/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0099, LR: 0.0007484078\n",
      "Epoch [169/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0007481616\n",
      "Epoch [169/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0007479152\n",
      "Epoch [169/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007476688\n",
      "Epoch [169/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0108, LR: 0.0007474224\n",
      "Epoch [169/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0122, LR: 0.0007471758\n",
      "Epoch [169/500], Batch [80/110], Train Loss: 0.0152, Val Loss: 0.0102, LR: 0.0007469292\n",
      "Epoch [169/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0118, LR: 0.0007466825\n",
      "Epoch [169/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007464357\n",
      "Epoch [169/500], Batch [110/110], Train Loss: 0.0009, Val Loss: 0.0090, LR: 0.0007461888\n",
      "Epoch [170/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0127, LR: 0.0007459419\n",
      "Epoch [170/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007456948\n",
      "Epoch [170/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0128, LR: 0.0007454477\n",
      "Epoch [170/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0007452005\n",
      "Epoch [170/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007449533\n",
      "Epoch [170/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0007447059\n",
      "Epoch [170/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0007444585\n",
      "Epoch [170/500], Batch [80/110], Train Loss: 0.0039, Val Loss: 0.0082, LR: 0.0007442110\n",
      "Epoch [170/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007439635\n",
      "Epoch [170/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0007437158\n",
      "Epoch [170/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007434681\n",
      "Epoch [171/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0120, LR: 0.0007432203\n",
      "Epoch [171/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0117, LR: 0.0007429724\n",
      "Epoch [171/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007427244\n",
      "Epoch [171/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007424764\n",
      "Epoch [171/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0094, LR: 0.0007422282\n",
      "Epoch [171/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0007419801\n",
      "Epoch [171/500], Batch [70/110], Train Loss: 0.0601, Val Loss: 0.0113, LR: 0.0007417318\n",
      "Epoch [171/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0007414834\n",
      "Epoch [171/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0007412350\n",
      "Epoch [171/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0091, LR: 0.0007409865\n",
      "Epoch [171/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0083, LR: 0.0007407379\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 171: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.036 sec , Memory Usage: 414.59 MB\n",
      "Epoch [172/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007404893\n",
      "Epoch [172/500], Batch [20/110], Train Loss: 0.0681, Val Loss: 0.0161, LR: 0.0007402405\n",
      "Epoch [172/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0007399917\n",
      "Epoch [172/500], Batch [40/110], Train Loss: 0.1481, Val Loss: 0.0080, LR: 0.0007397428\n",
      "Epoch [172/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0007394938\n",
      "Epoch [172/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0147, LR: 0.0007392448\n",
      "Epoch [172/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0007389957\n",
      "Epoch [172/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0085, LR: 0.0007387465\n",
      "Epoch [172/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0007384972\n",
      "Epoch [172/500], Batch [100/110], Train Loss: 0.0105, Val Loss: 0.0124, LR: 0.0007382479\n",
      "Epoch [172/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0007379984\n",
      "Epoch [173/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007377489\n",
      "Epoch [173/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007374994\n",
      "Epoch [173/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007372497\n",
      "Epoch [173/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007370000\n",
      "Epoch [173/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0082, LR: 0.0007367502\n",
      "Epoch [173/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0007365003\n",
      "Epoch [173/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0143, LR: 0.0007362504\n",
      "Epoch [173/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0081, LR: 0.0007360003\n",
      "Epoch [173/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0007357502\n",
      "Epoch [173/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0007355000\n",
      "Epoch [173/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0007352498\n",
      "Epoch [174/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0007349995\n",
      "Epoch [174/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0007347490\n",
      "Epoch [174/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0007344986\n",
      "Epoch [174/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0007342480\n",
      "Epoch [174/500], Batch [50/110], Train Loss: 0.0026, Val Loss: 0.0094, LR: 0.0007339974\n",
      "Epoch [174/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0007337467\n",
      "Epoch [174/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007334959\n",
      "Epoch [174/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0007332450\n",
      "Epoch [174/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0007329941\n",
      "Epoch [174/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007327431\n",
      "Epoch [174/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0080, LR: 0.0007324920\n",
      "Epoch [175/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0007322409\n",
      "Epoch [175/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0123, LR: 0.0007319897\n",
      "Epoch [175/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0007317384\n",
      "Epoch [175/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0085, LR: 0.0007314870\n",
      "Epoch [175/500], Batch [50/110], Train Loss: 0.0013, Val Loss: 0.0086, LR: 0.0007312355\n",
      "Epoch [175/500], Batch [60/110], Train Loss: 0.0093, Val Loss: 0.0106, LR: 0.0007309840\n",
      "Epoch [175/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007307324\n",
      "Epoch [175/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0007304807\n",
      "Epoch [175/500], Batch [90/110], Train Loss: 0.0022, Val Loss: 0.0078, LR: 0.0007302290\n",
      "Epoch [175/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007299772\n",
      "Epoch [175/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0007297253\n",
      "Epoch [176/500], Batch [10/110], Train Loss: 0.0120, Val Loss: 0.0095, LR: 0.0007294733\n",
      "Epoch [176/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007292213\n",
      "Epoch [176/500], Batch [30/110], Train Loss: 0.0018, Val Loss: 0.0087, LR: 0.0007289692\n",
      "Epoch [176/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0082, LR: 0.0007287170\n",
      "Epoch [176/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0007284648\n",
      "Epoch [176/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0007282124\n",
      "Epoch [176/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007279600\n",
      "Epoch [176/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0082, LR: 0.0007277075\n",
      "Epoch [176/500], Batch [90/110], Train Loss: 0.0255, Val Loss: 0.0081, LR: 0.0007274550\n",
      "Epoch [176/500], Batch [100/110], Train Loss: 0.0056, Val Loss: 0.0093, LR: 0.0007272024\n",
      "Epoch [176/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0105, LR: 0.0007269497\n",
      "Epoch [177/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0086, LR: 0.0007266969\n",
      "Epoch [177/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0007264441\n",
      "Epoch [177/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0106, LR: 0.0007261912\n",
      "Epoch [177/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0101, LR: 0.0007259382\n",
      "Epoch [177/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0096, LR: 0.0007256851\n",
      "Epoch [177/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0007254320\n",
      "Epoch [177/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0007251788\n",
      "Epoch [177/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0007249256\n",
      "Epoch [177/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0081, LR: 0.0007246722\n",
      "Epoch [177/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0098, LR: 0.0007244188\n",
      "Epoch [177/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0134, LR: 0.0007241653\n",
      "Epoch [178/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0007239118\n",
      "Epoch [178/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0007236581\n",
      "Epoch [178/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0125, LR: 0.0007234044\n",
      "Epoch [178/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0147, LR: 0.0007231507\n",
      "Epoch [178/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007228968\n",
      "Epoch [178/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0007226429\n",
      "Epoch [178/500], Batch [70/110], Train Loss: 0.0034, Val Loss: 0.0100, LR: 0.0007223889\n",
      "Epoch [178/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0007221349\n",
      "Epoch [178/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0007218808\n",
      "Epoch [178/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007216266\n",
      "Epoch [178/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007213723\n",
      "Epoch [179/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0007211180\n",
      "Epoch [179/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0007208636\n",
      "Epoch [179/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007206091\n",
      "Epoch [179/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007203545\n",
      "Epoch [179/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007200999\n",
      "Epoch [179/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007198452\n",
      "Epoch [179/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0119, LR: 0.0007195905\n",
      "Epoch [179/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0007193356\n",
      "Epoch [179/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0007190807\n",
      "Epoch [179/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007188258\n",
      "Epoch [179/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007185707\n",
      "Epoch [180/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0007183156\n",
      "Epoch [180/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007180605\n",
      "Epoch [180/500], Batch [30/110], Train Loss: 0.0061, Val Loss: 0.0130, LR: 0.0007178052\n",
      "Epoch [180/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007175499\n",
      "Epoch [180/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007172945\n",
      "Epoch [180/500], Batch [60/110], Train Loss: 0.0024, Val Loss: 0.0078, LR: 0.0007170391\n",
      "Epoch [180/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007167835\n",
      "Epoch [180/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0104, LR: 0.0007165279\n",
      "Epoch [180/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0007162723\n",
      "Epoch [180/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0082, LR: 0.0007160165\n",
      "Epoch [180/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0083, LR: 0.0007157607\n",
      "Epoch [181/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007155049\n",
      "Epoch [181/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007152489\n",
      "Epoch [181/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0007149929\n",
      "Epoch [181/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0007147369\n",
      "Epoch [181/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0113, LR: 0.0007144807\n",
      "Epoch [181/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0254, LR: 0.0007142245\n",
      "Epoch [181/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0007139682\n",
      "Epoch [181/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0007137119\n",
      "Epoch [181/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0095, LR: 0.0007134555\n",
      "Epoch [181/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0007131990\n",
      "Epoch [181/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0007129424\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 181: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 414.59 MB\n",
      "Epoch [182/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007126858\n",
      "Epoch [182/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0007124291\n",
      "Epoch [182/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0007121724\n",
      "Epoch [182/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0007119156\n",
      "Epoch [182/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0007116587\n",
      "Epoch [182/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0088, LR: 0.0007114017\n",
      "Epoch [182/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0137, LR: 0.0007111447\n",
      "Epoch [182/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0197, LR: 0.0007108876\n",
      "Epoch [182/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007106304\n",
      "Epoch [182/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007103732\n",
      "Epoch [182/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0087, LR: 0.0007101159\n",
      "Epoch [183/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0007098586\n",
      "Epoch [183/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0007096011\n",
      "Epoch [183/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007093436\n",
      "Epoch [183/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0007090861\n",
      "Epoch [183/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007088284\n",
      "Epoch [183/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0007085708\n",
      "Epoch [183/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0007083130\n",
      "Epoch [183/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007080552\n",
      "Epoch [183/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0007077973\n",
      "Epoch [183/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0088, LR: 0.0007075393\n",
      "Epoch [183/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0088, LR: 0.0007072813\n",
      "Epoch [184/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007070232\n",
      "Epoch [184/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0007067650\n",
      "Epoch [184/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0087, LR: 0.0007065068\n",
      "Epoch [184/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0007062485\n",
      "Epoch [184/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0007059902\n",
      "Epoch [184/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007057318\n",
      "Epoch [184/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0086, LR: 0.0007054733\n",
      "Epoch [184/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007052147\n",
      "Epoch [184/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007049561\n",
      "Epoch [184/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007046974\n",
      "Epoch [184/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0096, LR: 0.0007044387\n",
      "Epoch [185/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0143, LR: 0.0007041799\n",
      "Epoch [185/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007039210\n",
      "Epoch [185/500], Batch [30/110], Train Loss: 0.0475, Val Loss: 0.0127, LR: 0.0007036621\n",
      "Epoch [185/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007034031\n",
      "Epoch [185/500], Batch [50/110], Train Loss: 0.0010, Val Loss: 0.0087, LR: 0.0007031440\n",
      "Epoch [185/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0007028848\n",
      "Epoch [185/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0007026256\n",
      "Epoch [185/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0007023664\n",
      "Epoch [185/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007021071\n",
      "Epoch [185/500], Batch [100/110], Train Loss: 0.0013, Val Loss: 0.0084, LR: 0.0007018477\n",
      "Epoch [185/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0081, LR: 0.0007015882\n",
      "Epoch [186/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0088, LR: 0.0007013287\n",
      "Epoch [186/500], Batch [20/110], Train Loss: 0.0050, Val Loss: 0.0091, LR: 0.0007010691\n",
      "Epoch [186/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0087, LR: 0.0007008095\n",
      "Epoch [186/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0007005497\n",
      "Epoch [186/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0086, LR: 0.0007002900\n",
      "Epoch [186/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0090, LR: 0.0007000301\n",
      "Epoch [186/500], Batch [70/110], Train Loss: 0.0101, Val Loss: 0.0107, LR: 0.0006997702\n",
      "Epoch [186/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0006995102\n",
      "Epoch [186/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0082, LR: 0.0006992502\n",
      "Epoch [186/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0084, LR: 0.0006989901\n",
      "Epoch [186/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0083, LR: 0.0006987300\n",
      "Epoch [187/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006984697\n",
      "Epoch [187/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0006982095\n",
      "Epoch [187/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0006979491\n",
      "Epoch [187/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006976887\n",
      "Epoch [187/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0006974282\n",
      "Epoch [187/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0006971677\n",
      "Epoch [187/500], Batch [70/110], Train Loss: 0.0258, Val Loss: 0.0116, LR: 0.0006969071\n",
      "Epoch [187/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0095, LR: 0.0006966464\n",
      "Epoch [187/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0006963857\n",
      "Epoch [187/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006961249\n",
      "Epoch [187/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0006958641\n",
      "Epoch [188/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0006956032\n",
      "Epoch [188/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0006953422\n",
      "Epoch [188/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006950812\n",
      "Epoch [188/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0006948201\n",
      "Epoch [188/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006945589\n",
      "Epoch [188/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006942977\n",
      "Epoch [188/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0006940364\n",
      "Epoch [188/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0006937751\n",
      "Epoch [188/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006935136\n",
      "Epoch [188/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006932522\n",
      "Epoch [188/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006929907\n",
      "Epoch [189/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006927291\n",
      "Epoch [189/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0095, LR: 0.0006924674\n",
      "Epoch [189/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0006922057\n",
      "Epoch [189/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0088, LR: 0.0006919439\n",
      "Epoch [189/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006916821\n",
      "Epoch [189/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0006914202\n",
      "Epoch [189/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006911582\n",
      "Epoch [189/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0006908962\n",
      "Epoch [189/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0006906341\n",
      "Epoch [189/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0006903720\n",
      "Epoch [189/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0006901098\n",
      "Epoch [190/500], Batch [10/110], Train Loss: 0.0201, Val Loss: 0.0094, LR: 0.0006898475\n",
      "Epoch [190/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0095, LR: 0.0006895852\n",
      "Epoch [190/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0006893228\n",
      "Epoch [190/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0161, LR: 0.0006890604\n",
      "Epoch [190/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0006887979\n",
      "Epoch [190/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006885353\n",
      "Epoch [190/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0097, LR: 0.0006882727\n",
      "Epoch [190/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0006880100\n",
      "Epoch [190/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0006877473\n",
      "Epoch [190/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0006874845\n",
      "Epoch [190/500], Batch [110/110], Train Loss: 0.0378, Val Loss: 0.0091, LR: 0.0006872217\n",
      "Epoch [191/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0006869587\n",
      "Epoch [191/500], Batch [20/110], Train Loss: 0.0028, Val Loss: 0.0089, LR: 0.0006866958\n",
      "Epoch [191/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0080, LR: 0.0006864327\n",
      "Epoch [191/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0006861696\n",
      "Epoch [191/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006859065\n",
      "Epoch [191/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0006856433\n",
      "Epoch [191/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0006853800\n",
      "Epoch [191/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006851167\n",
      "Epoch [191/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0006848533\n",
      "Epoch [191/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0006845898\n",
      "Epoch [191/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0106, LR: 0.0006843263\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 191: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 414.59 MB\n",
      "Epoch [192/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006840627\n",
      "Epoch [192/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0006837991\n",
      "Epoch [192/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006835354\n",
      "Epoch [192/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0090, LR: 0.0006832717\n",
      "Epoch [192/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0006830079\n",
      "Epoch [192/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0089, LR: 0.0006827440\n",
      "Epoch [192/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006824801\n",
      "Epoch [192/500], Batch [80/110], Train Loss: 0.0009, Val Loss: 0.0094, LR: 0.0006822161\n",
      "Epoch [192/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0102, LR: 0.0006819521\n",
      "Epoch [192/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006816880\n",
      "Epoch [192/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006814239\n",
      "Epoch [193/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0006811597\n",
      "Epoch [193/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0006808954\n",
      "Epoch [193/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0006806311\n",
      "Epoch [193/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0006803667\n",
      "Epoch [193/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0157, LR: 0.0006801023\n",
      "Epoch [193/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0006798378\n",
      "Epoch [193/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006795732\n",
      "Epoch [193/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006793086\n",
      "Epoch [193/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0006790440\n",
      "Epoch [193/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006787793\n",
      "Epoch [193/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0006785145\n",
      "Epoch [194/500], Batch [10/110], Train Loss: 0.0009, Val Loss: 0.0085, LR: 0.0006782497\n",
      "Epoch [194/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0006779848\n",
      "Epoch [194/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0006777198\n",
      "Epoch [194/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0006774548\n",
      "Epoch [194/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0089, LR: 0.0006771898\n",
      "Epoch [194/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0091, LR: 0.0006769247\n",
      "Epoch [194/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0006766595\n",
      "Epoch [194/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006763943\n",
      "Epoch [194/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0093, LR: 0.0006761290\n",
      "Epoch [194/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0006758636\n",
      "Epoch [194/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0006755982\n",
      "Epoch [195/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0110, LR: 0.0006753328\n",
      "Epoch [195/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0006750673\n",
      "Epoch [195/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0132, LR: 0.0006748017\n",
      "Epoch [195/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0006745361\n",
      "Epoch [195/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0006742704\n",
      "Epoch [195/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0006740047\n",
      "Epoch [195/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0006737389\n",
      "Epoch [195/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0095, LR: 0.0006734731\n",
      "Epoch [195/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0006732072\n",
      "Epoch [195/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0006729413\n",
      "Epoch [195/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006726753\n",
      "Epoch [196/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006724092\n",
      "Epoch [196/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006721431\n",
      "Epoch [196/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0084, LR: 0.0006718769\n",
      "Epoch [196/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006716107\n",
      "Epoch [196/500], Batch [50/110], Train Loss: 0.0023, Val Loss: 0.0102, LR: 0.0006713444\n",
      "Epoch [196/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0123, LR: 0.0006710781\n",
      "Epoch [196/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0006708117\n",
      "Epoch [196/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0006705453\n",
      "Epoch [196/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0006702788\n",
      "Epoch [196/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0006700123\n",
      "Epoch [196/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006697457\n",
      "Epoch [197/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006694790\n",
      "Epoch [197/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006692123\n",
      "Epoch [197/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0084, LR: 0.0006689456\n",
      "Epoch [197/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0006686787\n",
      "Epoch [197/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0006684119\n",
      "Epoch [197/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0006681450\n",
      "Epoch [197/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0006678780\n",
      "Epoch [197/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0088, LR: 0.0006676110\n",
      "Epoch [197/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006673439\n",
      "Epoch [197/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006670768\n",
      "Epoch [197/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0006668096\n",
      "Epoch [198/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0173, LR: 0.0006665423\n",
      "Epoch [198/500], Batch [20/110], Train Loss: 0.0320, Val Loss: 0.0166, LR: 0.0006662750\n",
      "Epoch [198/500], Batch [30/110], Train Loss: 0.0109, Val Loss: 0.0108, LR: 0.0006660077\n",
      "Epoch [198/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006657403\n",
      "Epoch [198/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0006654729\n",
      "Epoch [198/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006652054\n",
      "Epoch [198/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0006649378\n",
      "Epoch [198/500], Batch [80/110], Train Loss: 0.0097, Val Loss: 0.0100, LR: 0.0006646702\n",
      "Epoch [198/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006644026\n",
      "Epoch [198/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0006641348\n",
      "Epoch [198/500], Batch [110/110], Train Loss: 0.0299, Val Loss: 0.0162, LR: 0.0006638671\n",
      "Epoch [199/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0090, LR: 0.0006635993\n",
      "Epoch [199/500], Batch [20/110], Train Loss: 0.0020, Val Loss: 0.0103, LR: 0.0006633314\n",
      "Epoch [199/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006630635\n",
      "Epoch [199/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006627955\n",
      "Epoch [199/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0128, LR: 0.0006625275\n",
      "Epoch [199/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0006622594\n",
      "Epoch [199/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006619913\n",
      "Epoch [199/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006617231\n",
      "Epoch [199/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006614549\n",
      "Epoch [199/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0006611867\n",
      "Epoch [199/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0006609183\n",
      "Epoch [200/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0006606500\n",
      "Epoch [200/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0006603815\n",
      "Epoch [200/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006601130\n",
      "Epoch [200/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006598445\n",
      "Epoch [200/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0086, LR: 0.0006595759\n",
      "Epoch [200/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0006593073\n",
      "Epoch [200/500], Batch [70/110], Train Loss: 0.0029, Val Loss: 0.0088, LR: 0.0006590386\n",
      "Epoch [200/500], Batch [80/110], Train Loss: 0.1357, Val Loss: 0.0087, LR: 0.0006587699\n",
      "Epoch [200/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0006585011\n",
      "Epoch [200/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0006582323\n",
      "Epoch [200/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0006579634\n",
      "Epoch [201/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0006576945\n",
      "Epoch [201/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0116, LR: 0.0006574255\n",
      "Epoch [201/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0006571565\n",
      "Epoch [201/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0006568874\n",
      "Epoch [201/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0006566183\n",
      "Epoch [201/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0102, LR: 0.0006563491\n",
      "Epoch [201/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0006560799\n",
      "Epoch [201/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006558106\n",
      "Epoch [201/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0006555413\n",
      "Epoch [201/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0006552719\n",
      "Epoch [201/500], Batch [110/110], Train Loss: 0.0021, Val Loss: 0.0086, LR: 0.0006550025\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 201: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 414.59 MB\n",
      "Epoch [202/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0006547330\n",
      "Epoch [202/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0154, LR: 0.0006544635\n",
      "Epoch [202/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006541939\n",
      "Epoch [202/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006539243\n",
      "Epoch [202/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0006536546\n",
      "Epoch [202/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0129, LR: 0.0006533849\n",
      "Epoch [202/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0084, LR: 0.0006531151\n",
      "Epoch [202/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006528453\n",
      "Epoch [202/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006525754\n",
      "Epoch [202/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0006523055\n",
      "Epoch [202/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0006520356\n",
      "Epoch [203/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0006517656\n",
      "Epoch [203/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0006514955\n",
      "Epoch [203/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006512254\n",
      "Epoch [203/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006509553\n",
      "Epoch [203/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0079, LR: 0.0006506851\n",
      "Epoch [203/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0006504148\n",
      "Epoch [203/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0006501445\n",
      "Epoch [203/500], Batch [80/110], Train Loss: 0.0021, Val Loss: 0.0082, LR: 0.0006498742\n",
      "Epoch [203/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0006496038\n",
      "Epoch [203/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006493334\n",
      "Epoch [203/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0091, LR: 0.0006490629\n",
      "Epoch [204/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0099, LR: 0.0006487924\n",
      "Epoch [204/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0006485218\n",
      "Epoch [204/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006482512\n",
      "Epoch [204/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0006479805\n",
      "Epoch [204/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0006477098\n",
      "Epoch [204/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006474390\n",
      "Epoch [204/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0006471682\n",
      "Epoch [204/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0094, LR: 0.0006468974\n",
      "Epoch [204/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006466265\n",
      "Epoch [204/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006463555\n",
      "Epoch [204/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0006460845\n",
      "Epoch [205/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0006458135\n",
      "Epoch [205/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006455424\n",
      "Epoch [205/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006452713\n",
      "Epoch [205/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006450001\n",
      "Epoch [205/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0006447289\n",
      "Epoch [205/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0006444576\n",
      "Epoch [205/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0175, LR: 0.0006441863\n",
      "Epoch [205/500], Batch [80/110], Train Loss: 0.0021, Val Loss: 0.0108, LR: 0.0006439149\n",
      "Epoch [205/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006436435\n",
      "Epoch [205/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006433721\n",
      "Epoch [205/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0006431006\n",
      "Epoch [206/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006428291\n",
      "Epoch [206/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006425575\n",
      "Epoch [206/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006422858\n",
      "Epoch [206/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0006420142\n",
      "Epoch [206/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0006417425\n",
      "Epoch [206/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006414707\n",
      "Epoch [206/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0006411989\n",
      "Epoch [206/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006409270\n",
      "Epoch [206/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0006406551\n",
      "Epoch [206/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0006403832\n",
      "Epoch [206/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0006401112\n",
      "Epoch [207/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0093, LR: 0.0006398392\n",
      "Epoch [207/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006395671\n",
      "Epoch [207/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0086, LR: 0.0006392950\n",
      "Epoch [207/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0006390228\n",
      "Epoch [207/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006387506\n",
      "Epoch [207/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006384784\n",
      "Epoch [207/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0006382061\n",
      "Epoch [207/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006379337\n",
      "Epoch [207/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0277, LR: 0.0006376614\n",
      "Epoch [207/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0006373889\n",
      "Epoch [207/500], Batch [110/110], Train Loss: 0.0534, Val Loss: 0.0091, LR: 0.0006371165\n",
      "Epoch [208/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0123, LR: 0.0006368440\n",
      "Epoch [208/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006365714\n",
      "Epoch [208/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006362988\n",
      "Epoch [208/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0112, LR: 0.0006360262\n",
      "Epoch [208/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0006357535\n",
      "Epoch [208/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006354808\n",
      "Epoch [208/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006352080\n",
      "Epoch [208/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0006349352\n",
      "Epoch [208/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006346624\n",
      "Epoch [208/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006343895\n",
      "Epoch [208/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0006341165\n",
      "Epoch [209/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0006338436\n",
      "Epoch [209/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006335706\n",
      "Epoch [209/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006332975\n",
      "Epoch [209/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0006330244\n",
      "Epoch [209/500], Batch [50/110], Train Loss: 0.0046, Val Loss: 0.0098, LR: 0.0006327512\n",
      "Epoch [209/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006324781\n",
      "Epoch [209/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0006322048\n",
      "Epoch [209/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006319316\n",
      "Epoch [209/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006316583\n",
      "Epoch [209/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0006313849\n",
      "Epoch [209/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006311115\n",
      "Epoch [210/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006308381\n",
      "Epoch [210/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0006305646\n",
      "Epoch [210/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0089, LR: 0.0006302911\n",
      "Epoch [210/500], Batch [40/110], Train Loss: 0.0040, Val Loss: 0.0103, LR: 0.0006300175\n",
      "Epoch [210/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0006297439\n",
      "Epoch [210/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0006294703\n",
      "Epoch [210/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006291966\n",
      "Epoch [210/500], Batch [80/110], Train Loss: 0.0014, Val Loss: 0.0085, LR: 0.0006289229\n",
      "Epoch [210/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0006286491\n",
      "Epoch [210/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0006283753\n",
      "Epoch [210/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006281015\n",
      "Epoch [211/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0006278276\n",
      "Epoch [211/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0006275537\n",
      "Epoch [211/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006272797\n",
      "Epoch [211/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006270057\n",
      "Epoch [211/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006267317\n",
      "Epoch [211/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006264576\n",
      "Epoch [211/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0006261835\n",
      "Epoch [211/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0100, LR: 0.0006259093\n",
      "Epoch [211/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0101, LR: 0.0006256351\n",
      "Epoch [211/500], Batch [100/110], Train Loss: 0.0075, Val Loss: 0.0093, LR: 0.0006253609\n",
      "Epoch [211/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0114, LR: 0.0006250866\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 211: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 414.59 MB\n",
      "Epoch [212/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0120, LR: 0.0006248123\n",
      "Epoch [212/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0006245379\n",
      "Epoch [212/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006242636\n",
      "Epoch [212/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0148, LR: 0.0006239891\n",
      "Epoch [212/500], Batch [50/110], Train Loss: 0.0013, Val Loss: 0.0099, LR: 0.0006237146\n",
      "Epoch [212/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006234401\n",
      "Epoch [212/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0006231656\n",
      "Epoch [212/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0006228910\n",
      "Epoch [212/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0082, LR: 0.0006226164\n",
      "Epoch [212/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006223417\n",
      "Epoch [212/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006220670\n",
      "Epoch [213/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0084, LR: 0.0006217923\n",
      "Epoch [213/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0006215175\n",
      "Epoch [213/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0084, LR: 0.0006212427\n",
      "Epoch [213/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006209678\n",
      "Epoch [213/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0006206929\n",
      "Epoch [213/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0184, LR: 0.0006204180\n",
      "Epoch [213/500], Batch [70/110], Train Loss: 0.0128, Val Loss: 0.0216, LR: 0.0006201430\n",
      "Epoch [213/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0006198680\n",
      "Epoch [213/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0006195930\n",
      "Epoch [213/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0006193179\n",
      "Epoch [213/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0006190428\n",
      "Epoch [214/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0082, LR: 0.0006187676\n",
      "Epoch [214/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0091, LR: 0.0006184924\n",
      "Epoch [214/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0006182172\n",
      "Epoch [214/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0084, LR: 0.0006179419\n",
      "Epoch [214/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0006176666\n",
      "Epoch [214/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0006173913\n",
      "Epoch [214/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0125, LR: 0.0006171159\n",
      "Epoch [214/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0006168405\n",
      "Epoch [214/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0109, LR: 0.0006165650\n",
      "Epoch [214/500], Batch [100/110], Train Loss: 0.0050, Val Loss: 0.0085, LR: 0.0006162895\n",
      "Epoch [214/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006160140\n",
      "Epoch [215/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0006157385\n",
      "Epoch [215/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0083, LR: 0.0006154629\n",
      "Epoch [215/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0006151872\n",
      "Epoch [215/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0006149116\n",
      "Epoch [215/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006146359\n",
      "Epoch [215/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006143601\n",
      "Epoch [215/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0115, LR: 0.0006140844\n",
      "Epoch [215/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0006138085\n",
      "Epoch [215/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006135327\n",
      "Epoch [215/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0006132568\n",
      "Epoch [215/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0006129809\n",
      "Epoch [216/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0092, LR: 0.0006127050\n",
      "Epoch [216/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0006124290\n",
      "Epoch [216/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0006121529\n",
      "Epoch [216/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0006118769\n",
      "Epoch [216/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0006116008\n",
      "Epoch [216/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0006113247\n",
      "Epoch [216/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0006110485\n",
      "Epoch [216/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0006107723\n",
      "Epoch [216/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0006104961\n",
      "Epoch [216/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006102198\n",
      "Epoch [216/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0094, LR: 0.0006099435\n",
      "Epoch [217/500], Batch [10/110], Train Loss: 0.0035, Val Loss: 0.0080, LR: 0.0006096672\n",
      "Epoch [217/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0006093908\n",
      "Epoch [217/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0006091144\n",
      "Epoch [217/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0006088380\n",
      "Epoch [217/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006085615\n",
      "Epoch [217/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006082850\n",
      "Epoch [217/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006080085\n",
      "Epoch [217/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006077319\n",
      "Epoch [217/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0006074553\n",
      "Epoch [217/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0006071787\n",
      "Epoch [217/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0080, LR: 0.0006069020\n",
      "Epoch [218/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0006066253\n",
      "Epoch [218/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006063486\n",
      "Epoch [218/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006060718\n",
      "Epoch [218/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0006057950\n",
      "Epoch [218/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0124, LR: 0.0006055181\n",
      "Epoch [218/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0093, LR: 0.0006052413\n",
      "Epoch [218/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0006049644\n",
      "Epoch [218/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0006046874\n",
      "Epoch [218/500], Batch [90/110], Train Loss: 0.0074, Val Loss: 0.0095, LR: 0.0006044105\n",
      "Epoch [218/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0006041335\n",
      "Epoch [218/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0006038564\n",
      "Epoch [219/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0006035794\n",
      "Epoch [219/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0006033023\n",
      "Epoch [219/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0006030252\n",
      "Epoch [219/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0006027480\n",
      "Epoch [219/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0086, LR: 0.0006024708\n",
      "Epoch [219/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0006021936\n",
      "Epoch [219/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0006019163\n",
      "Epoch [219/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0130, LR: 0.0006016390\n",
      "Epoch [219/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0006013617\n",
      "Epoch [219/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0006010844\n",
      "Epoch [219/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0006008070\n",
      "Epoch [220/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0006005296\n",
      "Epoch [220/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0089, LR: 0.0006002521\n",
      "Epoch [220/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0005999747\n",
      "Epoch [220/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0005996972\n",
      "Epoch [220/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0005994196\n",
      "Epoch [220/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0005991420\n",
      "Epoch [220/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0005988644\n",
      "Epoch [220/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005985868\n",
      "Epoch [220/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005983092\n",
      "Epoch [220/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0005980315\n",
      "Epoch [220/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0005977538\n",
      "Epoch [221/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0005974760\n",
      "Epoch [221/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0005971982\n",
      "Epoch [221/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0100, LR: 0.0005969204\n",
      "Epoch [221/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0107, LR: 0.0005966426\n",
      "Epoch [221/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0005963647\n",
      "Epoch [221/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0005960868\n",
      "Epoch [221/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0005958089\n",
      "Epoch [221/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0122, LR: 0.0005955309\n",
      "Epoch [221/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0005952529\n",
      "Epoch [221/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0005949749\n",
      "Epoch [221/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0005946969\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 221: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.036 sec , Memory Usage: 414.59 MB\n",
      "Epoch [222/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0005944188\n",
      "Epoch [222/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0005941407\n",
      "Epoch [222/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0108, LR: 0.0005938625\n",
      "Epoch [222/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0113, LR: 0.0005935844\n",
      "Epoch [222/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0005933062\n",
      "Epoch [222/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0005930280\n",
      "Epoch [222/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0005927497\n",
      "Epoch [222/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0005924714\n",
      "Epoch [222/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0005921931\n",
      "Epoch [222/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0005919148\n",
      "Epoch [222/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0005916364\n",
      "Epoch [223/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005913580\n",
      "Epoch [223/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005910796\n",
      "Epoch [223/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0005908012\n",
      "Epoch [223/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0005905227\n",
      "Epoch [223/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005902442\n",
      "Epoch [223/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0005899656\n",
      "Epoch [223/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0005896871\n",
      "Epoch [223/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0005894085\n",
      "Epoch [223/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0005891299\n",
      "Epoch [223/500], Batch [100/110], Train Loss: 0.0767, Val Loss: 0.0094, LR: 0.0005888512\n",
      "Epoch [223/500], Batch [110/110], Train Loss: 0.0024, Val Loss: 0.0096, LR: 0.0005885726\n",
      "Epoch [224/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0090, LR: 0.0005882939\n",
      "Epoch [224/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005880151\n",
      "Epoch [224/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0081, LR: 0.0005877364\n",
      "Epoch [224/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0005874576\n",
      "Epoch [224/500], Batch [50/110], Train Loss: 0.0019, Val Loss: 0.0091, LR: 0.0005871788\n",
      "Epoch [224/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0005869000\n",
      "Epoch [224/500], Batch [70/110], Train Loss: 0.0016, Val Loss: 0.0116, LR: 0.0005866211\n",
      "Epoch [224/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0005863422\n",
      "Epoch [224/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0005860633\n",
      "Epoch [224/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0005857844\n",
      "Epoch [224/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0005855054\n",
      "Epoch [225/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005852264\n",
      "Epoch [225/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005849474\n",
      "Epoch [225/500], Batch [30/110], Train Loss: 0.0056, Val Loss: 0.0095, LR: 0.0005846683\n",
      "Epoch [225/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0005843893\n",
      "Epoch [225/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0005841102\n",
      "Epoch [225/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0005838311\n",
      "Epoch [225/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0005835519\n",
      "Epoch [225/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005832727\n",
      "Epoch [225/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0005829935\n",
      "Epoch [225/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0005827143\n",
      "Epoch [225/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005824351\n",
      "Epoch [226/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0005821558\n",
      "Epoch [226/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0005818765\n",
      "Epoch [226/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0115, LR: 0.0005815972\n",
      "Epoch [226/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0005813178\n",
      "Epoch [226/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005810384\n",
      "Epoch [226/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0005807590\n",
      "Epoch [226/500], Batch [70/110], Train Loss: 0.0996, Val Loss: 0.0156, LR: 0.0005804796\n",
      "Epoch [226/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0005802002\n",
      "Epoch [226/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0005799207\n",
      "Epoch [226/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005796412\n",
      "Epoch [226/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0089, LR: 0.0005793617\n",
      "Epoch [227/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0098, LR: 0.0005790821\n",
      "Epoch [227/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0005788025\n",
      "Epoch [227/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0005785230\n",
      "Epoch [227/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005782433\n",
      "Epoch [227/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0005779637\n",
      "Epoch [227/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0131, LR: 0.0005776840\n",
      "Epoch [227/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0005774043\n",
      "Epoch [227/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005771246\n",
      "Epoch [227/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005768449\n",
      "Epoch [227/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005765651\n",
      "Epoch [227/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0005762853\n",
      "Epoch [228/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0005760055\n",
      "Epoch [228/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0005757257\n",
      "Epoch [228/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0005754458\n",
      "Epoch [228/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0005751660\n",
      "Epoch [228/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0005748861\n",
      "Epoch [228/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0005746061\n",
      "Epoch [228/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0112, LR: 0.0005743262\n",
      "Epoch [228/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0005740462\n",
      "Epoch [228/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0005737662\n",
      "Epoch [228/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0005734862\n",
      "Epoch [228/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0005732062\n",
      "Epoch [229/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0005729261\n",
      "Epoch [229/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0005726461\n",
      "Epoch [229/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0105, LR: 0.0005723660\n",
      "Epoch [229/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0005720858\n",
      "Epoch [229/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0005718057\n",
      "Epoch [229/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005715255\n",
      "Epoch [229/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0005712453\n",
      "Epoch [229/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0091, LR: 0.0005709651\n",
      "Epoch [229/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0005706849\n",
      "Epoch [229/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0005704046\n",
      "Epoch [229/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0122, LR: 0.0005701244\n",
      "Epoch [230/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0108, LR: 0.0005698441\n",
      "Epoch [230/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0005695637\n",
      "Epoch [230/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0005692834\n",
      "Epoch [230/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0005690030\n",
      "Epoch [230/500], Batch [50/110], Train Loss: 0.0058, Val Loss: 0.0092, LR: 0.0005687227\n",
      "Epoch [230/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0005684423\n",
      "Epoch [230/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0005681618\n",
      "Epoch [230/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0005678814\n",
      "Epoch [230/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0118, LR: 0.0005676009\n",
      "Epoch [230/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0005673205\n",
      "Epoch [230/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0005670400\n",
      "Epoch [231/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005667594\n",
      "Epoch [231/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0005664789\n",
      "Epoch [231/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005661983\n",
      "Epoch [231/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0005659177\n",
      "Epoch [231/500], Batch [50/110], Train Loss: 0.0119, Val Loss: 0.0184, LR: 0.0005656371\n",
      "Epoch [231/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0082, LR: 0.0005653565\n",
      "Epoch [231/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0005650759\n",
      "Epoch [231/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0005647952\n",
      "Epoch [231/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0125, LR: 0.0005645145\n",
      "Epoch [231/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0005642338\n",
      "Epoch [231/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005639531\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 231: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.041 sec , Memory Usage: 412.33 MB\n",
      "Epoch [232/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0005636724\n",
      "Epoch [232/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0005633916\n",
      "Epoch [232/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0005631108\n",
      "Epoch [232/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0005628300\n",
      "Epoch [232/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0005625492\n",
      "Epoch [232/500], Batch [60/110], Train Loss: 0.0016, Val Loss: 0.0081, LR: 0.0005622684\n",
      "Epoch [232/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005619875\n",
      "Epoch [232/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0005617066\n",
      "Epoch [232/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0005614257\n",
      "Epoch [232/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0005611448\n",
      "Epoch [232/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0005608639\n",
      "Epoch [233/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0005605830\n",
      "Epoch [233/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005603020\n",
      "Epoch [233/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0005600210\n",
      "Epoch [233/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005597400\n",
      "Epoch [233/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0082, LR: 0.0005594590\n",
      "Epoch [233/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0089, LR: 0.0005591780\n",
      "Epoch [233/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0005588969\n",
      "Epoch [233/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0005586158\n",
      "Epoch [233/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0005583348\n",
      "Epoch [233/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005580536\n",
      "Epoch [233/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0005577725\n",
      "Epoch [234/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0005574914\n",
      "Epoch [234/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0005572102\n",
      "Epoch [234/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0005569290\n",
      "Epoch [234/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0005566479\n",
      "Epoch [234/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005563666\n",
      "Epoch [234/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0005560854\n",
      "Epoch [234/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0005558042\n",
      "Epoch [234/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0111, LR: 0.0005555229\n",
      "Epoch [234/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0005552416\n",
      "Epoch [234/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0005549604\n",
      "Epoch [234/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0082, LR: 0.0005546790\n",
      "Epoch [235/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005543977\n",
      "Epoch [235/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0080, LR: 0.0005541164\n",
      "Epoch [235/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005538350\n",
      "Epoch [235/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0005535537\n",
      "Epoch [235/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0093, LR: 0.0005532723\n",
      "Epoch [235/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0005529909\n",
      "Epoch [235/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0005527094\n",
      "Epoch [235/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0005524280\n",
      "Epoch [235/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0005521466\n",
      "Epoch [235/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0005518651\n",
      "Epoch [235/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0095, LR: 0.0005515836\n",
      "Epoch [236/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0005513021\n",
      "Epoch [236/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0077, LR: 0.0005510206\n",
      "Epoch [236/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0005507391\n",
      "Epoch [236/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0005504575\n",
      "Epoch [236/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0125, LR: 0.0005501760\n",
      "Epoch [236/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0005498944\n",
      "Epoch [236/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0005496128\n",
      "Epoch [236/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005493312\n",
      "Epoch [236/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0005490496\n",
      "Epoch [236/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0005487680\n",
      "Epoch [236/500], Batch [110/110], Train Loss: 0.0017, Val Loss: 0.0110, LR: 0.0005484863\n",
      "Epoch [237/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0005482047\n",
      "Epoch [237/500], Batch [20/110], Train Loss: 0.0089, Val Loss: 0.0082, LR: 0.0005479230\n",
      "Epoch [237/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0005476413\n",
      "Epoch [237/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0005473596\n",
      "Epoch [237/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0124, LR: 0.0005470779\n",
      "Epoch [237/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0149, LR: 0.0005467962\n",
      "Epoch [237/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0005465144\n",
      "Epoch [237/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0005462327\n",
      "Epoch [237/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005459509\n",
      "Epoch [237/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0005456691\n",
      "Epoch [237/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0005453874\n",
      "Epoch [238/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0084, LR: 0.0005451055\n",
      "Epoch [238/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0005448237\n",
      "Epoch [238/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0005445419\n",
      "Epoch [238/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005442600\n",
      "Epoch [238/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0244, LR: 0.0005439782\n",
      "Epoch [238/500], Batch [60/110], Train Loss: 0.0011, Val Loss: 0.0080, LR: 0.0005436963\n",
      "Epoch [238/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0005434144\n",
      "Epoch [238/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0125, LR: 0.0005431325\n",
      "Epoch [238/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0141, LR: 0.0005428506\n",
      "Epoch [238/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0005425687\n",
      "Epoch [238/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0005422868\n",
      "Epoch [239/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005420048\n",
      "Epoch [239/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0078, LR: 0.0005417229\n",
      "Epoch [239/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0156, LR: 0.0005414409\n",
      "Epoch [239/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0005411589\n",
      "Epoch [239/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0005408769\n",
      "Epoch [239/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0005405949\n",
      "Epoch [239/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0005403129\n",
      "Epoch [239/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0005400309\n",
      "Epoch [239/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0005397488\n",
      "Epoch [239/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0005394668\n",
      "Epoch [239/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0005391847\n",
      "Epoch [240/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0005389026\n",
      "Epoch [240/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0005386206\n",
      "Epoch [240/500], Batch [30/110], Train Loss: 0.0498, Val Loss: 0.0157, LR: 0.0005383385\n",
      "Epoch [240/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005380564\n",
      "Epoch [240/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0005377742\n",
      "Epoch [240/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0081, LR: 0.0005374921\n",
      "Epoch [240/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0085, LR: 0.0005372100\n",
      "Epoch [240/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0005369278\n",
      "Epoch [240/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0005366457\n",
      "Epoch [240/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0005363635\n",
      "Epoch [240/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0005360813\n",
      "Epoch [241/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0005357991\n",
      "Epoch [241/500], Batch [20/110], Train Loss: 0.0018, Val Loss: 0.0081, LR: 0.0005355169\n",
      "Epoch [241/500], Batch [30/110], Train Loss: 0.0017, Val Loss: 0.0097, LR: 0.0005352347\n",
      "Epoch [241/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0005349525\n",
      "Epoch [241/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0005346703\n",
      "Epoch [241/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0005343880\n",
      "Epoch [241/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0005341058\n",
      "Epoch [241/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0118, LR: 0.0005338235\n",
      "Epoch [241/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0119, LR: 0.0005335412\n",
      "Epoch [241/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0005332590\n",
      "Epoch [241/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0005329767\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 241: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 412.33 MB\n",
      "Epoch [242/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0005326944\n",
      "Epoch [242/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0099, LR: 0.0005324121\n",
      "Epoch [242/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0005321298\n",
      "Epoch [242/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005318474\n",
      "Epoch [242/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0005315651\n",
      "Epoch [242/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0005312828\n",
      "Epoch [242/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0005310004\n",
      "Epoch [242/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0082, LR: 0.0005307181\n",
      "Epoch [242/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0005304357\n",
      "Epoch [242/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0005301533\n",
      "Epoch [242/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005298709\n",
      "Epoch [243/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0148, LR: 0.0005295885\n",
      "Epoch [243/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0005293061\n",
      "Epoch [243/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0005290237\n",
      "Epoch [243/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0005287413\n",
      "Epoch [243/500], Batch [50/110], Train Loss: 0.0021, Val Loss: 0.0098, LR: 0.0005284589\n",
      "Epoch [243/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0005281765\n",
      "Epoch [243/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0005278940\n",
      "Epoch [243/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0005276116\n",
      "Epoch [243/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0087, LR: 0.0005273291\n",
      "Epoch [243/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005270467\n",
      "Epoch [243/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005267642\n",
      "Epoch [244/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0005264817\n",
      "Epoch [244/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0005261993\n",
      "Epoch [244/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0005259168\n",
      "Epoch [244/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0005256343\n",
      "Epoch [244/500], Batch [50/110], Train Loss: 0.0015, Val Loss: 0.0086, LR: 0.0005253518\n",
      "Epoch [244/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0005250693\n",
      "Epoch [244/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0005247868\n",
      "Epoch [244/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0005245042\n",
      "Epoch [244/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0005242217\n",
      "Epoch [244/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0005239392\n",
      "Epoch [244/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005236566\n",
      "Epoch [245/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0005233741\n",
      "Epoch [245/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0117, LR: 0.0005230915\n",
      "Epoch [245/500], Batch [30/110], Train Loss: 0.0101, Val Loss: 0.0124, LR: 0.0005228090\n",
      "Epoch [245/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0094, LR: 0.0005225264\n",
      "Epoch [245/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0092, LR: 0.0005222439\n",
      "Epoch [245/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0005219613\n",
      "Epoch [245/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005216787\n",
      "Epoch [245/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0005213961\n",
      "Epoch [245/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0005211135\n",
      "Epoch [245/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0005208309\n",
      "Epoch [245/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0176, LR: 0.0005205483\n",
      "Epoch [246/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0141, LR: 0.0005202657\n",
      "Epoch [246/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005199831\n",
      "Epoch [246/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0005197005\n",
      "Epoch [246/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0005194179\n",
      "Epoch [246/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0005191352\n",
      "Epoch [246/500], Batch [60/110], Train Loss: 0.0256, Val Loss: 0.0089, LR: 0.0005188526\n",
      "Epoch [246/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005185700\n",
      "Epoch [246/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0005182873\n",
      "Epoch [246/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0005180047\n",
      "Epoch [246/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005177220\n",
      "Epoch [246/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0005174394\n",
      "Epoch [247/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0005171567\n",
      "Epoch [247/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0005168741\n",
      "Epoch [247/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0005165914\n",
      "Epoch [247/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0005163087\n",
      "Epoch [247/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0005160261\n",
      "Epoch [247/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0005157434\n",
      "Epoch [247/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0005154607\n",
      "Epoch [247/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0005151780\n",
      "Epoch [247/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0005148954\n",
      "Epoch [247/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0005146127\n",
      "Epoch [247/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0005143300\n",
      "Epoch [248/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0005140473\n",
      "Epoch [248/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0112, LR: 0.0005137646\n",
      "Epoch [248/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0005134819\n",
      "Epoch [248/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0005131992\n",
      "Epoch [248/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0005129165\n",
      "Epoch [248/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0005126338\n",
      "Epoch [248/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0005123511\n",
      "Epoch [248/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0005120683\n",
      "Epoch [248/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0005117856\n",
      "Epoch [248/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0088, LR: 0.0005115029\n",
      "Epoch [248/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0005112202\n",
      "Epoch [249/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0005109375\n",
      "Epoch [249/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0005106547\n",
      "Epoch [249/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0005103720\n",
      "Epoch [249/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005100893\n",
      "Epoch [249/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0005098066\n",
      "Epoch [249/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005095238\n",
      "Epoch [249/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005092411\n",
      "Epoch [249/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0104, LR: 0.0005089584\n",
      "Epoch [249/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0005086756\n",
      "Epoch [249/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0005083929\n",
      "Epoch [249/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005081102\n",
      "Epoch [250/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0005078274\n",
      "Epoch [250/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0005075447\n",
      "Epoch [250/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0005072619\n",
      "Epoch [250/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0005069792\n",
      "Epoch [250/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005066965\n",
      "Epoch [250/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005064137\n",
      "Epoch [250/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0005061310\n",
      "Epoch [250/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0005058482\n",
      "Epoch [250/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0005055655\n",
      "Epoch [250/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0005052827\n",
      "Epoch [250/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0005050000\n",
      "Epoch [251/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0123, LR: 0.0005047173\n",
      "Epoch [251/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0133, LR: 0.0005044345\n",
      "Epoch [251/500], Batch [30/110], Train Loss: 0.0040, Val Loss: 0.0132, LR: 0.0005041518\n",
      "Epoch [251/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0005038690\n",
      "Epoch [251/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0005035863\n",
      "Epoch [251/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0005033035\n",
      "Epoch [251/500], Batch [70/110], Train Loss: 0.0048, Val Loss: 0.0088, LR: 0.0005030208\n",
      "Epoch [251/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0005027381\n",
      "Epoch [251/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0091, LR: 0.0005024553\n",
      "Epoch [251/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0005021726\n",
      "Epoch [251/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0005018898\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 251: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 412.33 MB\n",
      "Epoch [252/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0005016071\n",
      "Epoch [252/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0005013244\n",
      "Epoch [252/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0088, LR: 0.0005010416\n",
      "Epoch [252/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0005007589\n",
      "Epoch [252/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0005004762\n",
      "Epoch [252/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0115, LR: 0.0005001934\n",
      "Epoch [252/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0164, LR: 0.0004999107\n",
      "Epoch [252/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0111, LR: 0.0004996280\n",
      "Epoch [252/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004993453\n",
      "Epoch [252/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0004990625\n",
      "Epoch [252/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0004987798\n",
      "Epoch [253/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0004984971\n",
      "Epoch [253/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0004982144\n",
      "Epoch [253/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0004979317\n",
      "Epoch [253/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0004976489\n",
      "Epoch [253/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0004973662\n",
      "Epoch [253/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0004970835\n",
      "Epoch [253/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0004968008\n",
      "Epoch [253/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0004965181\n",
      "Epoch [253/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0004962354\n",
      "Epoch [253/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004959527\n",
      "Epoch [253/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0004956700\n",
      "Epoch [254/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0004953873\n",
      "Epoch [254/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0004951046\n",
      "Epoch [254/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004948220\n",
      "Epoch [254/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0004945393\n",
      "Epoch [254/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0004942566\n",
      "Epoch [254/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004939739\n",
      "Epoch [254/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0004936913\n",
      "Epoch [254/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0004934086\n",
      "Epoch [254/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004931259\n",
      "Epoch [254/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004928433\n",
      "Epoch [254/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0004925606\n",
      "Epoch [255/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0100, LR: 0.0004922780\n",
      "Epoch [255/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0004919953\n",
      "Epoch [255/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0004917127\n",
      "Epoch [255/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004914300\n",
      "Epoch [255/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004911474\n",
      "Epoch [255/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0004908648\n",
      "Epoch [255/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0004905821\n",
      "Epoch [255/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0004902995\n",
      "Epoch [255/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004900169\n",
      "Epoch [255/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004897343\n",
      "Epoch [255/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0004894517\n",
      "Epoch [256/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0004891691\n",
      "Epoch [256/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0004888865\n",
      "Epoch [256/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0082, LR: 0.0004886039\n",
      "Epoch [256/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004883213\n",
      "Epoch [256/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004880387\n",
      "Epoch [256/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0004877561\n",
      "Epoch [256/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0004874736\n",
      "Epoch [256/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0004871910\n",
      "Epoch [256/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0079, LR: 0.0004869085\n",
      "Epoch [256/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004866259\n",
      "Epoch [256/500], Batch [110/110], Train Loss: 0.0098, Val Loss: 0.0134, LR: 0.0004863434\n",
      "Epoch [257/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0004860608\n",
      "Epoch [257/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0004857783\n",
      "Epoch [257/500], Batch [30/110], Train Loss: 0.0019, Val Loss: 0.0077, LR: 0.0004854958\n",
      "Epoch [257/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0004852132\n",
      "Epoch [257/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0004849307\n",
      "Epoch [257/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0004846482\n",
      "Epoch [257/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004843657\n",
      "Epoch [257/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004840832\n",
      "Epoch [257/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004838007\n",
      "Epoch [257/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0081, LR: 0.0004835183\n",
      "Epoch [257/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004832358\n",
      "Epoch [258/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0004829533\n",
      "Epoch [258/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004826709\n",
      "Epoch [258/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004823884\n",
      "Epoch [258/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0004821060\n",
      "Epoch [258/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0004818235\n",
      "Epoch [258/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004815411\n",
      "Epoch [258/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0082, LR: 0.0004812587\n",
      "Epoch [258/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0004809763\n",
      "Epoch [258/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0004806939\n",
      "Epoch [258/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004804115\n",
      "Epoch [258/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004801291\n",
      "Epoch [259/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004798467\n",
      "Epoch [259/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0004795643\n",
      "Epoch [259/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0004792819\n",
      "Epoch [259/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0004789996\n",
      "Epoch [259/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0004787172\n",
      "Epoch [259/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0004784349\n",
      "Epoch [259/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0004781526\n",
      "Epoch [259/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0004778702\n",
      "Epoch [259/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0004775879\n",
      "Epoch [259/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0004773056\n",
      "Epoch [259/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004770233\n",
      "Epoch [260/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004767410\n",
      "Epoch [260/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0004764588\n",
      "Epoch [260/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004761765\n",
      "Epoch [260/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0004758942\n",
      "Epoch [260/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0004756120\n",
      "Epoch [260/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0004753297\n",
      "Epoch [260/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004750475\n",
      "Epoch [260/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004747653\n",
      "Epoch [260/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0081, LR: 0.0004744831\n",
      "Epoch [260/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004742009\n",
      "Epoch [260/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0004739187\n",
      "Epoch [261/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0117, LR: 0.0004736365\n",
      "Epoch [261/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0136, LR: 0.0004733543\n",
      "Epoch [261/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0004730722\n",
      "Epoch [261/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0004727900\n",
      "Epoch [261/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0004725079\n",
      "Epoch [261/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0118, LR: 0.0004722258\n",
      "Epoch [261/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0080, LR: 0.0004719436\n",
      "Epoch [261/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0109, LR: 0.0004716615\n",
      "Epoch [261/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004713794\n",
      "Epoch [261/500], Batch [100/110], Train Loss: 0.0095, Val Loss: 0.0100, LR: 0.0004710974\n",
      "Epoch [261/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0004708153\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 261: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 412.33 MB\n",
      "Epoch [262/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004705332\n",
      "Epoch [262/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004702512\n",
      "Epoch [262/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004699691\n",
      "Epoch [262/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0004696871\n",
      "Epoch [262/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0004694051\n",
      "Epoch [262/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004691231\n",
      "Epoch [262/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004688411\n",
      "Epoch [262/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0004685591\n",
      "Epoch [262/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0004682771\n",
      "Epoch [262/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0110, LR: 0.0004679952\n",
      "Epoch [262/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0123, LR: 0.0004677132\n",
      "Epoch [263/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0004674313\n",
      "Epoch [263/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0004671494\n",
      "Epoch [263/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0004668675\n",
      "Epoch [263/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0004665856\n",
      "Epoch [263/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0004663037\n",
      "Epoch [263/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004660218\n",
      "Epoch [263/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0004657400\n",
      "Epoch [263/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004654581\n",
      "Epoch [263/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004651763\n",
      "Epoch [263/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0004648945\n",
      "Epoch [263/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0004646126\n",
      "Epoch [264/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0073, LR: 0.0004643309\n",
      "Epoch [264/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004640491\n",
      "Epoch [264/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004637673\n",
      "Epoch [264/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0075, LR: 0.0004634856\n",
      "Epoch [264/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0004632038\n",
      "Epoch [264/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004629221\n",
      "Epoch [264/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004626404\n",
      "Epoch [264/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0004623587\n",
      "Epoch [264/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0004620770\n",
      "Epoch [264/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0004617953\n",
      "Epoch [264/500], Batch [110/110], Train Loss: 0.0050, Val Loss: 0.0083, LR: 0.0004615137\n",
      "Epoch [265/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0004612320\n",
      "Epoch [265/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0004609504\n",
      "Epoch [265/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0126, LR: 0.0004606688\n",
      "Epoch [265/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0004603872\n",
      "Epoch [265/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004601056\n",
      "Epoch [265/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0004598240\n",
      "Epoch [265/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004595425\n",
      "Epoch [265/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004592609\n",
      "Epoch [265/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0004589794\n",
      "Epoch [265/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0004586979\n",
      "Epoch [265/500], Batch [110/110], Train Loss: 0.0678, Val Loss: 0.0083, LR: 0.0004584164\n",
      "Epoch [266/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0004581349\n",
      "Epoch [266/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0004578534\n",
      "Epoch [266/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004575720\n",
      "Epoch [266/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004572906\n",
      "Epoch [266/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0004570091\n",
      "Epoch [266/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0082, LR: 0.0004567277\n",
      "Epoch [266/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0004564463\n",
      "Epoch [266/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0004561650\n",
      "Epoch [266/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0004558836\n",
      "Epoch [266/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0082, LR: 0.0004556023\n",
      "Epoch [266/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004553210\n",
      "Epoch [267/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0004550396\n",
      "Epoch [267/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004547584\n",
      "Epoch [267/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0004544771\n",
      "Epoch [267/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0116, LR: 0.0004541958\n",
      "Epoch [267/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0004539146\n",
      "Epoch [267/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0004536334\n",
      "Epoch [267/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0004533521\n",
      "Epoch [267/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0004530710\n",
      "Epoch [267/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004527898\n",
      "Epoch [267/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0004525086\n",
      "Epoch [267/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0004522275\n",
      "Epoch [268/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0004519464\n",
      "Epoch [268/500], Batch [20/110], Train Loss: 0.0051, Val Loss: 0.0091, LR: 0.0004516652\n",
      "Epoch [268/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0004513842\n",
      "Epoch [268/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0004511031\n",
      "Epoch [268/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0004508220\n",
      "Epoch [268/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004505410\n",
      "Epoch [268/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0004502600\n",
      "Epoch [268/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0004499790\n",
      "Epoch [268/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0004496980\n",
      "Epoch [268/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0004494170\n",
      "Epoch [268/500], Batch [110/110], Train Loss: 0.0015, Val Loss: 0.0090, LR: 0.0004491361\n",
      "Epoch [269/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004488552\n",
      "Epoch [269/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004485743\n",
      "Epoch [269/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004482934\n",
      "Epoch [269/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004480125\n",
      "Epoch [269/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004477316\n",
      "Epoch [269/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004474508\n",
      "Epoch [269/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004471700\n",
      "Epoch [269/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0004468892\n",
      "Epoch [269/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0004466084\n",
      "Epoch [269/500], Batch [100/110], Train Loss: 0.0045, Val Loss: 0.0100, LR: 0.0004463276\n",
      "Epoch [269/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0004460469\n",
      "Epoch [270/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0004457662\n",
      "Epoch [270/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0004454855\n",
      "Epoch [270/500], Batch [30/110], Train Loss: 0.0026, Val Loss: 0.0087, LR: 0.0004452048\n",
      "Epoch [270/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0004449241\n",
      "Epoch [270/500], Batch [50/110], Train Loss: 0.0012, Val Loss: 0.0095, LR: 0.0004446435\n",
      "Epoch [270/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0004443629\n",
      "Epoch [270/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0004440823\n",
      "Epoch [270/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0004438017\n",
      "Epoch [270/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0004435211\n",
      "Epoch [270/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004432406\n",
      "Epoch [270/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0004429600\n",
      "Epoch [271/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0004426795\n",
      "Epoch [271/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004423991\n",
      "Epoch [271/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004421186\n",
      "Epoch [271/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0004418382\n",
      "Epoch [271/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0004415577\n",
      "Epoch [271/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0004412773\n",
      "Epoch [271/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0004409970\n",
      "Epoch [271/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004407166\n",
      "Epoch [271/500], Batch [90/110], Train Loss: 0.0276, Val Loss: 0.0087, LR: 0.0004404363\n",
      "Epoch [271/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0004401559\n",
      "Epoch [271/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004398756\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 271: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.036 sec , Memory Usage: 412.33 MB\n",
      "Epoch [272/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0004395954\n",
      "Epoch [272/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0117, LR: 0.0004393151\n",
      "Epoch [272/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0004390349\n",
      "Epoch [272/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0004387547\n",
      "Epoch [272/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0004384745\n",
      "Epoch [272/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0086, LR: 0.0004381943\n",
      "Epoch [272/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0004379142\n",
      "Epoch [272/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0091, LR: 0.0004376340\n",
      "Epoch [272/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0004373539\n",
      "Epoch [272/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0004370739\n",
      "Epoch [272/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0134, LR: 0.0004367938\n",
      "Epoch [273/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0004365138\n",
      "Epoch [273/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0082, LR: 0.0004362338\n",
      "Epoch [273/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004359538\n",
      "Epoch [273/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004356738\n",
      "Epoch [273/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0084, LR: 0.0004353939\n",
      "Epoch [273/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0004351139\n",
      "Epoch [273/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0004348340\n",
      "Epoch [273/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0004345542\n",
      "Epoch [273/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0004342743\n",
      "Epoch [273/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0004339945\n",
      "Epoch [273/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0120, LR: 0.0004337147\n",
      "Epoch [274/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0004334349\n",
      "Epoch [274/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0004331551\n",
      "Epoch [274/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0004328754\n",
      "Epoch [274/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0004325957\n",
      "Epoch [274/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0004323160\n",
      "Epoch [274/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0081, LR: 0.0004320363\n",
      "Epoch [274/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004317567\n",
      "Epoch [274/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0004314770\n",
      "Epoch [274/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0004311975\n",
      "Epoch [274/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0004309179\n",
      "Epoch [274/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0004306383\n",
      "Epoch [275/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0108, LR: 0.0004303588\n",
      "Epoch [275/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0004300793\n",
      "Epoch [275/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0101, LR: 0.0004297998\n",
      "Epoch [275/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0004295204\n",
      "Epoch [275/500], Batch [50/110], Train Loss: 0.0011, Val Loss: 0.0094, LR: 0.0004292410\n",
      "Epoch [275/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0004289616\n",
      "Epoch [275/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0004286822\n",
      "Epoch [275/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004284028\n",
      "Epoch [275/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0004281235\n",
      "Epoch [275/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0115, LR: 0.0004278442\n",
      "Epoch [275/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0116, LR: 0.0004275649\n",
      "Epoch [276/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0112, LR: 0.0004272857\n",
      "Epoch [276/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0004270065\n",
      "Epoch [276/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004267273\n",
      "Epoch [276/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0004264481\n",
      "Epoch [276/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0004261689\n",
      "Epoch [276/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0004258898\n",
      "Epoch [276/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0093, LR: 0.0004256107\n",
      "Epoch [276/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0004253317\n",
      "Epoch [276/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0004250526\n",
      "Epoch [276/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0004247736\n",
      "Epoch [276/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004244946\n",
      "Epoch [277/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0083, LR: 0.0004242156\n",
      "Epoch [277/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004239367\n",
      "Epoch [277/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0004236578\n",
      "Epoch [277/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0004233789\n",
      "Epoch [277/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0004231000\n",
      "Epoch [277/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0004228212\n",
      "Epoch [277/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0004225424\n",
      "Epoch [277/500], Batch [80/110], Train Loss: 0.0082, Val Loss: 0.0126, LR: 0.0004222636\n",
      "Epoch [277/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0004219849\n",
      "Epoch [277/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004217061\n",
      "Epoch [277/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0004214274\n",
      "Epoch [278/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004211488\n",
      "Epoch [278/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0004208701\n",
      "Epoch [278/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0004205915\n",
      "Epoch [278/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004203129\n",
      "Epoch [278/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0004200344\n",
      "Epoch [278/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0004197558\n",
      "Epoch [278/500], Batch [70/110], Train Loss: 0.0046, Val Loss: 0.0122, LR: 0.0004194773\n",
      "Epoch [278/500], Batch [80/110], Train Loss: 0.0513, Val Loss: 0.0106, LR: 0.0004191988\n",
      "Epoch [278/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0092, LR: 0.0004189204\n",
      "Epoch [278/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0004186420\n",
      "Epoch [278/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004183636\n",
      "Epoch [279/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0004180852\n",
      "Epoch [279/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0004178069\n",
      "Epoch [279/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0004175286\n",
      "Epoch [279/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0004172503\n",
      "Epoch [279/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0004169720\n",
      "Epoch [279/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0004166938\n",
      "Epoch [279/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0116, LR: 0.0004164156\n",
      "Epoch [279/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0004161375\n",
      "Epoch [279/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0004158593\n",
      "Epoch [279/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0004155812\n",
      "Epoch [279/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0004153031\n",
      "Epoch [280/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004150251\n",
      "Epoch [280/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004147471\n",
      "Epoch [280/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004144691\n",
      "Epoch [280/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0004141911\n",
      "Epoch [280/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0004139132\n",
      "Epoch [280/500], Batch [60/110], Train Loss: 0.0107, Val Loss: 0.0102, LR: 0.0004136353\n",
      "Epoch [280/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0004133574\n",
      "Epoch [280/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0088, LR: 0.0004130796\n",
      "Epoch [280/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004128018\n",
      "Epoch [280/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0004125240\n",
      "Epoch [280/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0083, LR: 0.0004122462\n",
      "Epoch [281/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0004119685\n",
      "Epoch [281/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0004116908\n",
      "Epoch [281/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004114132\n",
      "Epoch [281/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004111356\n",
      "Epoch [281/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004108580\n",
      "Epoch [281/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0004105804\n",
      "Epoch [281/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004103028\n",
      "Epoch [281/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004100253\n",
      "Epoch [281/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004097479\n",
      "Epoch [281/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0004094704\n",
      "Epoch [281/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0119, LR: 0.0004091930\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 281: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 412.33 MB\n",
      "Epoch [282/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0004089156\n",
      "Epoch [282/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004086383\n",
      "Epoch [282/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004083610\n",
      "Epoch [282/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004080837\n",
      "Epoch [282/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0004078064\n",
      "Epoch [282/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0004075292\n",
      "Epoch [282/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0120, LR: 0.0004072520\n",
      "Epoch [282/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0004069748\n",
      "Epoch [282/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0004066977\n",
      "Epoch [282/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0004064206\n",
      "Epoch [282/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0004061436\n",
      "Epoch [283/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0004058665\n",
      "Epoch [283/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0004055895\n",
      "Epoch [283/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0166, LR: 0.0004053126\n",
      "Epoch [283/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0161, LR: 0.0004050356\n",
      "Epoch [283/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004047587\n",
      "Epoch [283/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004044819\n",
      "Epoch [283/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0004042050\n",
      "Epoch [283/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0004039282\n",
      "Epoch [283/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0004036514\n",
      "Epoch [283/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0004033747\n",
      "Epoch [283/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0089, LR: 0.0004030980\n",
      "Epoch [284/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0004028213\n",
      "Epoch [284/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0004025447\n",
      "Epoch [284/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0004022681\n",
      "Epoch [284/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004019915\n",
      "Epoch [284/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0093, LR: 0.0004017150\n",
      "Epoch [284/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0004014385\n",
      "Epoch [284/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0004011620\n",
      "Epoch [284/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0004008856\n",
      "Epoch [284/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0004006092\n",
      "Epoch [284/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0004003328\n",
      "Epoch [284/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0004000565\n",
      "Epoch [285/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0116, LR: 0.0003997802\n",
      "Epoch [285/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0003995039\n",
      "Epoch [285/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0003992277\n",
      "Epoch [285/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0003989515\n",
      "Epoch [285/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0003986753\n",
      "Epoch [285/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0081, LR: 0.0003983992\n",
      "Epoch [285/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0003981231\n",
      "Epoch [285/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0099, LR: 0.0003978471\n",
      "Epoch [285/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0003975710\n",
      "Epoch [285/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0003972950\n",
      "Epoch [285/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003970191\n",
      "Epoch [286/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0003967432\n",
      "Epoch [286/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0003964673\n",
      "Epoch [286/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0003961915\n",
      "Epoch [286/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0003959156\n",
      "Epoch [286/500], Batch [50/110], Train Loss: 0.0043, Val Loss: 0.0103, LR: 0.0003956399\n",
      "Epoch [286/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0003953641\n",
      "Epoch [286/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0003950884\n",
      "Epoch [286/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0003948128\n",
      "Epoch [286/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0003945371\n",
      "Epoch [286/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0003942615\n",
      "Epoch [286/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0003939860\n",
      "Epoch [287/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003937105\n",
      "Epoch [287/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0003934350\n",
      "Epoch [287/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0003931595\n",
      "Epoch [287/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0003928841\n",
      "Epoch [287/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0003926087\n",
      "Epoch [287/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0003923334\n",
      "Epoch [287/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0003920581\n",
      "Epoch [287/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0003917828\n",
      "Epoch [287/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0003915076\n",
      "Epoch [287/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0003912324\n",
      "Epoch [287/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0003909572\n",
      "Epoch [288/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0003906821\n",
      "Epoch [288/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003904070\n",
      "Epoch [288/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0003901320\n",
      "Epoch [288/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0003898570\n",
      "Epoch [288/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0003895820\n",
      "Epoch [288/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0003893071\n",
      "Epoch [288/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0079, LR: 0.0003890322\n",
      "Epoch [288/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0003887573\n",
      "Epoch [288/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003884825\n",
      "Epoch [288/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0003882077\n",
      "Epoch [288/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003879330\n",
      "Epoch [289/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0003876583\n",
      "Epoch [289/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0086, LR: 0.0003873836\n",
      "Epoch [289/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003871090\n",
      "Epoch [289/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003868344\n",
      "Epoch [289/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0003865599\n",
      "Epoch [289/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0003862854\n",
      "Epoch [289/500], Batch [70/110], Train Loss: 0.0052, Val Loss: 0.0087, LR: 0.0003860109\n",
      "Epoch [289/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003857364\n",
      "Epoch [289/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003854621\n",
      "Epoch [289/500], Batch [100/110], Train Loss: 0.0018, Val Loss: 0.0094, LR: 0.0003851877\n",
      "Epoch [289/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0112, LR: 0.0003849134\n",
      "Epoch [290/500], Batch [10/110], Train Loss: 0.0057, Val Loss: 0.0128, LR: 0.0003846391\n",
      "Epoch [290/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0003843649\n",
      "Epoch [290/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0003840907\n",
      "Epoch [290/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0003838165\n",
      "Epoch [290/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0003835424\n",
      "Epoch [290/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0003832683\n",
      "Epoch [290/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0003829943\n",
      "Epoch [290/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0109, LR: 0.0003827203\n",
      "Epoch [290/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0003824463\n",
      "Epoch [290/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0003821724\n",
      "Epoch [290/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0003818985\n",
      "Epoch [291/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003816247\n",
      "Epoch [291/500], Batch [20/110], Train Loss: 0.0026, Val Loss: 0.0084, LR: 0.0003813509\n",
      "Epoch [291/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0003810771\n",
      "Epoch [291/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003808034\n",
      "Epoch [291/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0003805297\n",
      "Epoch [291/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0003802561\n",
      "Epoch [291/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003799825\n",
      "Epoch [291/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003797089\n",
      "Epoch [291/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003794354\n",
      "Epoch [291/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0003791619\n",
      "Epoch [291/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0003788885\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 291: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 412.33 MB\n",
      "Epoch [292/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0003786151\n",
      "Epoch [292/500], Batch [20/110], Train Loss: 0.0014, Val Loss: 0.0095, LR: 0.0003783417\n",
      "Epoch [292/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0106, LR: 0.0003780684\n",
      "Epoch [292/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0003777952\n",
      "Epoch [292/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0106, LR: 0.0003775219\n",
      "Epoch [292/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0003772488\n",
      "Epoch [292/500], Batch [70/110], Train Loss: 0.0018, Val Loss: 0.0078, LR: 0.0003769756\n",
      "Epoch [292/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0003767025\n",
      "Epoch [292/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0003764294\n",
      "Epoch [292/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0003761564\n",
      "Epoch [292/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003758835\n",
      "Epoch [293/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0003756105\n",
      "Epoch [293/500], Batch [20/110], Train Loss: 0.0061, Val Loss: 0.0094, LR: 0.0003753376\n",
      "Epoch [293/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0003750648\n",
      "Epoch [293/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0003747920\n",
      "Epoch [293/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0003745192\n",
      "Epoch [293/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0125, LR: 0.0003742465\n",
      "Epoch [293/500], Batch [70/110], Train Loss: 0.0014, Val Loss: 0.0108, LR: 0.0003739738\n",
      "Epoch [293/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0090, LR: 0.0003737012\n",
      "Epoch [293/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0003734286\n",
      "Epoch [293/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003731560\n",
      "Epoch [293/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0003728835\n",
      "Epoch [294/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0097, LR: 0.0003726111\n",
      "Epoch [294/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0003723386\n",
      "Epoch [294/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0003720663\n",
      "Epoch [294/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003717939\n",
      "Epoch [294/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0003715216\n",
      "Epoch [294/500], Batch [60/110], Train Loss: 0.0036, Val Loss: 0.0089, LR: 0.0003712494\n",
      "Epoch [294/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0003709772\n",
      "Epoch [294/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0003707050\n",
      "Epoch [294/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0003704329\n",
      "Epoch [294/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0003701608\n",
      "Epoch [294/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0003698888\n",
      "Epoch [295/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0003696168\n",
      "Epoch [295/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003693449\n",
      "Epoch [295/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0003690730\n",
      "Epoch [295/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0003688011\n",
      "Epoch [295/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0003685293\n",
      "Epoch [295/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0087, LR: 0.0003682575\n",
      "Epoch [295/500], Batch [70/110], Train Loss: 0.0056, Val Loss: 0.0093, LR: 0.0003679858\n",
      "Epoch [295/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0003677142\n",
      "Epoch [295/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0003674425\n",
      "Epoch [295/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0003671709\n",
      "Epoch [295/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0003668994\n",
      "Epoch [296/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0003666279\n",
      "Epoch [296/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0003663565\n",
      "Epoch [296/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0003660851\n",
      "Epoch [296/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0003658137\n",
      "Epoch [296/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0003655424\n",
      "Epoch [296/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0003652711\n",
      "Epoch [296/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0003649999\n",
      "Epoch [296/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0003647287\n",
      "Epoch [296/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0003644576\n",
      "Epoch [296/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0003641865\n",
      "Epoch [296/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0003639155\n",
      "Epoch [297/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0003636445\n",
      "Epoch [297/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003633735\n",
      "Epoch [297/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0003631026\n",
      "Epoch [297/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0003628318\n",
      "Epoch [297/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0003625610\n",
      "Epoch [297/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0003622902\n",
      "Epoch [297/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0092, LR: 0.0003620195\n",
      "Epoch [297/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0136, LR: 0.0003617488\n",
      "Epoch [297/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0137, LR: 0.0003614782\n",
      "Epoch [297/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0084, LR: 0.0003612076\n",
      "Epoch [297/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0003609371\n",
      "Epoch [298/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0003606666\n",
      "Epoch [298/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003603962\n",
      "Epoch [298/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0003601258\n",
      "Epoch [298/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0003598555\n",
      "Epoch [298/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0003595852\n",
      "Epoch [298/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0003593149\n",
      "Epoch [298/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0003590447\n",
      "Epoch [298/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003587746\n",
      "Epoch [298/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003585045\n",
      "Epoch [298/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0003582344\n",
      "Epoch [298/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0003579644\n",
      "Epoch [299/500], Batch [10/110], Train Loss: 0.0025, Val Loss: 0.0102, LR: 0.0003576945\n",
      "Epoch [299/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0003574246\n",
      "Epoch [299/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0003571547\n",
      "Epoch [299/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0003568849\n",
      "Epoch [299/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0087, LR: 0.0003566151\n",
      "Epoch [299/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0003563454\n",
      "Epoch [299/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0003560757\n",
      "Epoch [299/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0103, LR: 0.0003558061\n",
      "Epoch [299/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0003555365\n",
      "Epoch [299/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003552670\n",
      "Epoch [299/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003549975\n",
      "Epoch [300/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0003547281\n",
      "Epoch [300/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0003544587\n",
      "Epoch [300/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0003541894\n",
      "Epoch [300/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0003539201\n",
      "Epoch [300/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0003536509\n",
      "Epoch [300/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0003533817\n",
      "Epoch [300/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0003531126\n",
      "Epoch [300/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0003528435\n",
      "Epoch [300/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0003525745\n",
      "Epoch [300/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0003523055\n",
      "Epoch [300/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0119, LR: 0.0003520366\n",
      "Epoch [301/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0171, LR: 0.0003517677\n",
      "Epoch [301/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0003514989\n",
      "Epoch [301/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0003512301\n",
      "Epoch [301/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0094, LR: 0.0003509614\n",
      "Epoch [301/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0003506927\n",
      "Epoch [301/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0003504241\n",
      "Epoch [301/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0003501555\n",
      "Epoch [301/500], Batch [80/110], Train Loss: 0.0101, Val Loss: 0.0090, LR: 0.0003498870\n",
      "Epoch [301/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003496185\n",
      "Epoch [301/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003493500\n",
      "Epoch [301/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0089, LR: 0.0003490817\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 301: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.036 sec , Memory Usage: 412.33 MB\n",
      "Epoch [302/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003488133\n",
      "Epoch [302/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003485451\n",
      "Epoch [302/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003482769\n",
      "Epoch [302/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003480087\n",
      "Epoch [302/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0003477406\n",
      "Epoch [302/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0003474725\n",
      "Epoch [302/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0003472045\n",
      "Epoch [302/500], Batch [80/110], Train Loss: 0.0078, Val Loss: 0.0104, LR: 0.0003469365\n",
      "Epoch [302/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0116, LR: 0.0003466686\n",
      "Epoch [302/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0003464007\n",
      "Epoch [302/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0003461329\n",
      "Epoch [303/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003458652\n",
      "Epoch [303/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0003455974\n",
      "Epoch [303/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0003453298\n",
      "Epoch [303/500], Batch [40/110], Train Loss: 0.0085, Val Loss: 0.0088, LR: 0.0003450622\n",
      "Epoch [303/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0003447946\n",
      "Epoch [303/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003445271\n",
      "Epoch [303/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003442597\n",
      "Epoch [303/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003439923\n",
      "Epoch [303/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0003437250\n",
      "Epoch [303/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0003434577\n",
      "Epoch [303/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0003431904\n",
      "Epoch [304/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0003429232\n",
      "Epoch [304/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0003426561\n",
      "Epoch [304/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0106, LR: 0.0003423890\n",
      "Epoch [304/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0003421220\n",
      "Epoch [304/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0098, LR: 0.0003418550\n",
      "Epoch [304/500], Batch [60/110], Train Loss: 0.0038, Val Loss: 0.0143, LR: 0.0003415881\n",
      "Epoch [304/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0003413213\n",
      "Epoch [304/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003410544\n",
      "Epoch [304/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0003407877\n",
      "Epoch [304/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0003405210\n",
      "Epoch [304/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0003402543\n",
      "Epoch [305/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0003399877\n",
      "Epoch [305/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003397212\n",
      "Epoch [305/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003394547\n",
      "Epoch [305/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0003391883\n",
      "Epoch [305/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0003389219\n",
      "Epoch [305/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0003386556\n",
      "Epoch [305/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0003383893\n",
      "Epoch [305/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0003381231\n",
      "Epoch [305/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0003378569\n",
      "Epoch [305/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0003375908\n",
      "Epoch [305/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0003373247\n",
      "Epoch [306/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003370587\n",
      "Epoch [306/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003367928\n",
      "Epoch [306/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0003365269\n",
      "Epoch [306/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0003362611\n",
      "Epoch [306/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0003359953\n",
      "Epoch [306/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003357296\n",
      "Epoch [306/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003354639\n",
      "Epoch [306/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003351983\n",
      "Epoch [306/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0120, LR: 0.0003349327\n",
      "Epoch [306/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0003346672\n",
      "Epoch [306/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0080, LR: 0.0003344018\n",
      "Epoch [307/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0003341364\n",
      "Epoch [307/500], Batch [20/110], Train Loss: 0.0022, Val Loss: 0.0116, LR: 0.0003338710\n",
      "Epoch [307/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0126, LR: 0.0003336057\n",
      "Epoch [307/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0003333405\n",
      "Epoch [307/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0003330753\n",
      "Epoch [307/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0003328102\n",
      "Epoch [307/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0003325452\n",
      "Epoch [307/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0003322802\n",
      "Epoch [307/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0003320152\n",
      "Epoch [307/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0003317503\n",
      "Epoch [307/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0003314855\n",
      "Epoch [308/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0003312207\n",
      "Epoch [308/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0003309560\n",
      "Epoch [308/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0003306914\n",
      "Epoch [308/500], Batch [40/110], Train Loss: 0.0998, Val Loss: 0.0118, LR: 0.0003304268\n",
      "Epoch [308/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0122, LR: 0.0003301622\n",
      "Epoch [308/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0003298977\n",
      "Epoch [308/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0003296333\n",
      "Epoch [308/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0003293689\n",
      "Epoch [308/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0003291046\n",
      "Epoch [308/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0003288403\n",
      "Epoch [308/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0126, LR: 0.0003285761\n",
      "Epoch [309/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0122, LR: 0.0003283120\n",
      "Epoch [309/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0003280479\n",
      "Epoch [309/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0003277839\n",
      "Epoch [309/500], Batch [40/110], Train Loss: 0.0015, Val Loss: 0.0102, LR: 0.0003275199\n",
      "Epoch [309/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0133, LR: 0.0003272560\n",
      "Epoch [309/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0003269921\n",
      "Epoch [309/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0003267283\n",
      "Epoch [309/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0003264646\n",
      "Epoch [309/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003262009\n",
      "Epoch [309/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003259373\n",
      "Epoch [309/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003256737\n",
      "Epoch [310/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003254102\n",
      "Epoch [310/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0003251467\n",
      "Epoch [310/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0003248833\n",
      "Epoch [310/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0003246200\n",
      "Epoch [310/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0003243567\n",
      "Epoch [310/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0003240935\n",
      "Epoch [310/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0003238304\n",
      "Epoch [310/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0003235673\n",
      "Epoch [310/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0003233042\n",
      "Epoch [310/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0097, LR: 0.0003230413\n",
      "Epoch [310/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0003227783\n",
      "Epoch [311/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0003225155\n",
      "Epoch [311/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003222527\n",
      "Epoch [311/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0003219900\n",
      "Epoch [311/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0003217273\n",
      "Epoch [311/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0003214647\n",
      "Epoch [311/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0003212021\n",
      "Epoch [311/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0083, LR: 0.0003209396\n",
      "Epoch [311/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0003206772\n",
      "Epoch [311/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0003204148\n",
      "Epoch [311/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0003201525\n",
      "Epoch [311/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0003198902\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 311: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 412.33 MB\n",
      "Epoch [312/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0003196280\n",
      "Epoch [312/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0003193659\n",
      "Epoch [312/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0003191038\n",
      "Epoch [312/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0003188418\n",
      "Epoch [312/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003185798\n",
      "Epoch [312/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003183179\n",
      "Epoch [312/500], Batch [70/110], Train Loss: 0.0012, Val Loss: 0.0096, LR: 0.0003180561\n",
      "Epoch [312/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0003177943\n",
      "Epoch [312/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0003175326\n",
      "Epoch [312/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0137, LR: 0.0003172709\n",
      "Epoch [312/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0003170093\n",
      "Epoch [313/500], Batch [10/110], Train Loss: 0.0028, Val Loss: 0.0085, LR: 0.0003167478\n",
      "Epoch [313/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0003164864\n",
      "Epoch [313/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0003162249\n",
      "Epoch [313/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003159636\n",
      "Epoch [313/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0003157023\n",
      "Epoch [313/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0003154411\n",
      "Epoch [313/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0080, LR: 0.0003151799\n",
      "Epoch [313/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0003149188\n",
      "Epoch [313/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0003146578\n",
      "Epoch [313/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003143968\n",
      "Epoch [313/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003141359\n",
      "Epoch [314/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003138751\n",
      "Epoch [314/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0003136143\n",
      "Epoch [314/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0003133536\n",
      "Epoch [314/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0003130929\n",
      "Epoch [314/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0003128323\n",
      "Epoch [314/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0003125718\n",
      "Epoch [314/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0003123113\n",
      "Epoch [314/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0003120509\n",
      "Epoch [314/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0003117905\n",
      "Epoch [314/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0003115303\n",
      "Epoch [314/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0003112700\n",
      "Epoch [315/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0003110099\n",
      "Epoch [315/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0003107498\n",
      "Epoch [315/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0003104898\n",
      "Epoch [315/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0003102298\n",
      "Epoch [315/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0003099699\n",
      "Epoch [315/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003097100\n",
      "Epoch [315/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0115, LR: 0.0003094503\n",
      "Epoch [315/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0003091905\n",
      "Epoch [315/500], Batch [90/110], Train Loss: 0.0571, Val Loss: 0.0089, LR: 0.0003089309\n",
      "Epoch [315/500], Batch [100/110], Train Loss: 0.0364, Val Loss: 0.0088, LR: 0.0003086713\n",
      "Epoch [315/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0003084118\n",
      "Epoch [316/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0003081523\n",
      "Epoch [316/500], Batch [20/110], Train Loss: 0.0028, Val Loss: 0.0104, LR: 0.0003078929\n",
      "Epoch [316/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0003076336\n",
      "Epoch [316/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0003073744\n",
      "Epoch [316/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0003071152\n",
      "Epoch [316/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0003068560\n",
      "Epoch [316/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0003065969\n",
      "Epoch [316/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0003063379\n",
      "Epoch [316/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0003060790\n",
      "Epoch [316/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0003058201\n",
      "Epoch [316/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0003055613\n",
      "Epoch [317/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0003053026\n",
      "Epoch [317/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0003050439\n",
      "Epoch [317/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0003047853\n",
      "Epoch [317/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003045267\n",
      "Epoch [317/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0003042682\n",
      "Epoch [317/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0003040098\n",
      "Epoch [317/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0003037515\n",
      "Epoch [317/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0003034932\n",
      "Epoch [317/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0003032350\n",
      "Epoch [317/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0003029768\n",
      "Epoch [317/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0003027187\n",
      "Epoch [318/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0003024607\n",
      "Epoch [318/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0117, LR: 0.0003022027\n",
      "Epoch [318/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0129, LR: 0.0003019448\n",
      "Epoch [318/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0133, LR: 0.0003016870\n",
      "Epoch [318/500], Batch [50/110], Train Loss: 0.0026, Val Loss: 0.0113, LR: 0.0003014292\n",
      "Epoch [318/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0003011716\n",
      "Epoch [318/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0003009139\n",
      "Epoch [318/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0003006564\n",
      "Epoch [318/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0003003989\n",
      "Epoch [318/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0095, LR: 0.0003001414\n",
      "Epoch [318/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0002998841\n",
      "Epoch [319/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002996268\n",
      "Epoch [319/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002993696\n",
      "Epoch [319/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0097, LR: 0.0002991124\n",
      "Epoch [319/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0002988553\n",
      "Epoch [319/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0002985983\n",
      "Epoch [319/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0002983413\n",
      "Epoch [319/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002980844\n",
      "Epoch [319/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002978276\n",
      "Epoch [319/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0002975709\n",
      "Epoch [319/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002973142\n",
      "Epoch [319/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0002970576\n",
      "Epoch [320/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002968010\n",
      "Epoch [320/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002965445\n",
      "Epoch [320/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002962881\n",
      "Epoch [320/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002960318\n",
      "Epoch [320/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002957755\n",
      "Epoch [320/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002955193\n",
      "Epoch [320/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0002952631\n",
      "Epoch [320/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0002950071\n",
      "Epoch [320/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0002947511\n",
      "Epoch [320/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0002944951\n",
      "Epoch [320/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002942393\n",
      "Epoch [321/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0002939835\n",
      "Epoch [321/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0002937277\n",
      "Epoch [321/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0084, LR: 0.0002934721\n",
      "Epoch [321/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002932165\n",
      "Epoch [321/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0088, LR: 0.0002929609\n",
      "Epoch [321/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002927055\n",
      "Epoch [321/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0002924501\n",
      "Epoch [321/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002921948\n",
      "Epoch [321/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0083, LR: 0.0002919395\n",
      "Epoch [321/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0002916844\n",
      "Epoch [321/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002914293\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 321: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.036 sec , Memory Usage: 412.34 MB\n",
      "Epoch [322/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002911742\n",
      "Epoch [322/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002909193\n",
      "Epoch [322/500], Batch [30/110], Train Loss: 0.0090, Val Loss: 0.0085, LR: 0.0002906644\n",
      "Epoch [322/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0002904095\n",
      "Epoch [322/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002901548\n",
      "Epoch [322/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0091, LR: 0.0002899001\n",
      "Epoch [322/500], Batch [70/110], Train Loss: 0.0085, Val Loss: 0.0129, LR: 0.0002896455\n",
      "Epoch [322/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0112, LR: 0.0002893909\n",
      "Epoch [322/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0002891364\n",
      "Epoch [322/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002888820\n",
      "Epoch [322/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002886277\n",
      "Epoch [323/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0087, LR: 0.0002883734\n",
      "Epoch [323/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002881192\n",
      "Epoch [323/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002878651\n",
      "Epoch [323/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002876111\n",
      "Epoch [323/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002873571\n",
      "Epoch [323/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0002871032\n",
      "Epoch [323/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0002868493\n",
      "Epoch [323/500], Batch [80/110], Train Loss: 0.0465, Val Loss: 0.0127, LR: 0.0002865956\n",
      "Epoch [323/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0086, LR: 0.0002863419\n",
      "Epoch [323/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0002860882\n",
      "Epoch [323/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002858347\n",
      "Epoch [324/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002855812\n",
      "Epoch [324/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002853278\n",
      "Epoch [324/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002850744\n",
      "Epoch [324/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002848212\n",
      "Epoch [324/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002845680\n",
      "Epoch [324/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002843149\n",
      "Epoch [324/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0002840618\n",
      "Epoch [324/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0002838088\n",
      "Epoch [324/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0002835559\n",
      "Epoch [324/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0002833031\n",
      "Epoch [324/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002830503\n",
      "Epoch [325/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002827976\n",
      "Epoch [325/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0002825450\n",
      "Epoch [325/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0002822925\n",
      "Epoch [325/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002820400\n",
      "Epoch [325/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002817876\n",
      "Epoch [325/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002815352\n",
      "Epoch [325/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002812830\n",
      "Epoch [325/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0095, LR: 0.0002810308\n",
      "Epoch [325/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0002807787\n",
      "Epoch [325/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0002805267\n",
      "Epoch [325/500], Batch [110/110], Train Loss: 0.0022, Val Loss: 0.0101, LR: 0.0002802747\n",
      "Epoch [326/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0002800228\n",
      "Epoch [326/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0002797710\n",
      "Epoch [326/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0002795193\n",
      "Epoch [326/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0002792676\n",
      "Epoch [326/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0002790160\n",
      "Epoch [326/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0002787645\n",
      "Epoch [326/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0002785130\n",
      "Epoch [326/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0002782616\n",
      "Epoch [326/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0115, LR: 0.0002780103\n",
      "Epoch [326/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0002777591\n",
      "Epoch [326/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002775080\n",
      "Epoch [327/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0002772569\n",
      "Epoch [327/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002770059\n",
      "Epoch [327/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002767550\n",
      "Epoch [327/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002765041\n",
      "Epoch [327/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0002762533\n",
      "Epoch [327/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002760026\n",
      "Epoch [327/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002757520\n",
      "Epoch [327/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002755014\n",
      "Epoch [327/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0002752510\n",
      "Epoch [327/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0002750005\n",
      "Epoch [327/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0002747502\n",
      "Epoch [328/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0002745000\n",
      "Epoch [328/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0002742498\n",
      "Epoch [328/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002739997\n",
      "Epoch [328/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002737496\n",
      "Epoch [328/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002734997\n",
      "Epoch [328/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002732498\n",
      "Epoch [328/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0098, LR: 0.0002730000\n",
      "Epoch [328/500], Batch [80/110], Train Loss: 0.0078, Val Loss: 0.0118, LR: 0.0002727503\n",
      "Epoch [328/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0002725006\n",
      "Epoch [328/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0002722511\n",
      "Epoch [328/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002720016\n",
      "Epoch [329/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002717521\n",
      "Epoch [329/500], Batch [20/110], Train Loss: 0.0159, Val Loss: 0.0089, LR: 0.0002715028\n",
      "Epoch [329/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0002712535\n",
      "Epoch [329/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0002710043\n",
      "Epoch [329/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002707552\n",
      "Epoch [329/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0002705062\n",
      "Epoch [329/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0002702572\n",
      "Epoch [329/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002700083\n",
      "Epoch [329/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0002697595\n",
      "Epoch [329/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0002695107\n",
      "Epoch [329/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0002692621\n",
      "Epoch [330/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0002690135\n",
      "Epoch [330/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0002687650\n",
      "Epoch [330/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002685166\n",
      "Epoch [330/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002682682\n",
      "Epoch [330/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0002680199\n",
      "Epoch [330/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0002677718\n",
      "Epoch [330/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0002675236\n",
      "Epoch [330/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002672756\n",
      "Epoch [330/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0002670276\n",
      "Epoch [330/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0002667797\n",
      "Epoch [330/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002665319\n",
      "Epoch [331/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0002662842\n",
      "Epoch [331/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0002660365\n",
      "Epoch [331/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0002657890\n",
      "Epoch [331/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0002655415\n",
      "Epoch [331/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0002652941\n",
      "Epoch [331/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0002650467\n",
      "Epoch [331/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0002647995\n",
      "Epoch [331/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002645523\n",
      "Epoch [331/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002643052\n",
      "Epoch [331/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0094, LR: 0.0002640581\n",
      "Epoch [331/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0002638112\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 331: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.041 sec , Memory Usage: 412.34 MB\n",
      "Epoch [332/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002635643\n",
      "Epoch [332/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0002633175\n",
      "Epoch [332/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0002630708\n",
      "Epoch [332/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0002628242\n",
      "Epoch [332/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002625776\n",
      "Epoch [332/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0002623312\n",
      "Epoch [332/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0002620848\n",
      "Epoch [332/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0002618384\n",
      "Epoch [332/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002615922\n",
      "Epoch [332/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002613460\n",
      "Epoch [332/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0002611000\n",
      "Epoch [333/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0002608540\n",
      "Epoch [333/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002606080\n",
      "Epoch [333/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0002603622\n",
      "Epoch [333/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0002601164\n",
      "Epoch [333/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0107, LR: 0.0002598708\n",
      "Epoch [333/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002596252\n",
      "Epoch [333/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0002593796\n",
      "Epoch [333/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0002591342\n",
      "Epoch [333/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0002588889\n",
      "Epoch [333/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0088, LR: 0.0002586436\n",
      "Epoch [333/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002583984\n",
      "Epoch [334/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0002581533\n",
      "Epoch [334/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0106, LR: 0.0002579082\n",
      "Epoch [334/500], Batch [30/110], Train Loss: 0.0028, Val Loss: 0.0102, LR: 0.0002576633\n",
      "Epoch [334/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0002574184\n",
      "Epoch [334/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0002571736\n",
      "Epoch [334/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0002569289\n",
      "Epoch [334/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002566842\n",
      "Epoch [334/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002564397\n",
      "Epoch [334/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002561952\n",
      "Epoch [334/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002559508\n",
      "Epoch [334/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002557065\n",
      "Epoch [335/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002554623\n",
      "Epoch [335/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002552181\n",
      "Epoch [335/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002549741\n",
      "Epoch [335/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0002547301\n",
      "Epoch [335/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002544862\n",
      "Epoch [335/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002542424\n",
      "Epoch [335/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002539986\n",
      "Epoch [335/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002537550\n",
      "Epoch [335/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0002535114\n",
      "Epoch [335/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0002532679\n",
      "Epoch [335/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0092, LR: 0.0002530245\n",
      "Epoch [336/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002527812\n",
      "Epoch [336/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0002525379\n",
      "Epoch [336/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0002522948\n",
      "Epoch [336/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0002520517\n",
      "Epoch [336/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0097, LR: 0.0002518087\n",
      "Epoch [336/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002515658\n",
      "Epoch [336/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002513229\n",
      "Epoch [336/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002510802\n",
      "Epoch [336/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0002508375\n",
      "Epoch [336/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0002505949\n",
      "Epoch [336/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0002503524\n",
      "Epoch [337/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0002501100\n",
      "Epoch [337/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0002498677\n",
      "Epoch [337/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002496254\n",
      "Epoch [337/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0002493833\n",
      "Epoch [337/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0002491412\n",
      "Epoch [337/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002488992\n",
      "Epoch [337/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0085, LR: 0.0002486573\n",
      "Epoch [337/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002484154\n",
      "Epoch [337/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002481737\n",
      "Epoch [337/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002479320\n",
      "Epoch [337/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002476904\n",
      "Epoch [338/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0089, LR: 0.0002474489\n",
      "Epoch [338/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0002472075\n",
      "Epoch [338/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0111, LR: 0.0002469662\n",
      "Epoch [338/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0002467249\n",
      "Epoch [338/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0002464838\n",
      "Epoch [338/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002462427\n",
      "Epoch [338/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002460017\n",
      "Epoch [338/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0002457608\n",
      "Epoch [338/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0002455200\n",
      "Epoch [338/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002452792\n",
      "Epoch [338/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002450386\n",
      "Epoch [339/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002447980\n",
      "Epoch [339/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002445575\n",
      "Epoch [339/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0095, LR: 0.0002443171\n",
      "Epoch [339/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002440768\n",
      "Epoch [339/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002438366\n",
      "Epoch [339/500], Batch [60/110], Train Loss: 0.0054, Val Loss: 0.0088, LR: 0.0002435964\n",
      "Epoch [339/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002433563\n",
      "Epoch [339/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0002431164\n",
      "Epoch [339/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002428765\n",
      "Epoch [339/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0002426367\n",
      "Epoch [339/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0002423970\n",
      "Epoch [340/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0002421573\n",
      "Epoch [340/500], Batch [20/110], Train Loss: 0.0011, Val Loss: 0.0096, LR: 0.0002419178\n",
      "Epoch [340/500], Batch [30/110], Train Loss: 0.0016, Val Loss: 0.0152, LR: 0.0002416783\n",
      "Epoch [340/500], Batch [40/110], Train Loss: 0.0612, Val Loss: 0.0129, LR: 0.0002414389\n",
      "Epoch [340/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002411997\n",
      "Epoch [340/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0002409605\n",
      "Epoch [340/500], Batch [70/110], Train Loss: 0.0049, Val Loss: 0.0081, LR: 0.0002407213\n",
      "Epoch [340/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0002404823\n",
      "Epoch [340/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0002402434\n",
      "Epoch [340/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0002400045\n",
      "Epoch [340/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0002397657\n",
      "Epoch [341/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0002395271\n",
      "Epoch [341/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0002392885\n",
      "Epoch [341/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0002390499\n",
      "Epoch [341/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0002388115\n",
      "Epoch [341/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002385732\n",
      "Epoch [341/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0091, LR: 0.0002383349\n",
      "Epoch [341/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0002380968\n",
      "Epoch [341/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0096, LR: 0.0002378587\n",
      "Epoch [341/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0002376207\n",
      "Epoch [341/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0093, LR: 0.0002373828\n",
      "Epoch [341/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002371450\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 341: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.037 sec , Memory Usage: 393.13 MB\n",
      "Epoch [342/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002369073\n",
      "Epoch [342/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002366696\n",
      "Epoch [342/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002364321\n",
      "Epoch [342/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0002361946\n",
      "Epoch [342/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002359572\n",
      "Epoch [342/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002357199\n",
      "Epoch [342/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002354827\n",
      "Epoch [342/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002352456\n",
      "Epoch [342/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002350086\n",
      "Epoch [342/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002347716\n",
      "Epoch [342/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0002345348\n",
      "Epoch [343/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0002342980\n",
      "Epoch [343/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0101, LR: 0.0002340614\n",
      "Epoch [343/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0002338248\n",
      "Epoch [343/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002335883\n",
      "Epoch [343/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002333519\n",
      "Epoch [343/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002331156\n",
      "Epoch [343/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002328793\n",
      "Epoch [343/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002326432\n",
      "Epoch [343/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0089, LR: 0.0002324071\n",
      "Epoch [343/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0002321712\n",
      "Epoch [343/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0002319353\n",
      "Epoch [344/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002316995\n",
      "Epoch [344/500], Batch [20/110], Train Loss: 0.0408, Val Loss: 0.0081, LR: 0.0002314638\n",
      "Epoch [344/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002312282\n",
      "Epoch [344/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0002309927\n",
      "Epoch [344/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0002307573\n",
      "Epoch [344/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0002305219\n",
      "Epoch [344/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0002302867\n",
      "Epoch [344/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002300515\n",
      "Epoch [344/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002298164\n",
      "Epoch [344/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0002295815\n",
      "Epoch [344/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0002293466\n",
      "Epoch [345/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0002291118\n",
      "Epoch [345/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0100, LR: 0.0002288771\n",
      "Epoch [345/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0002286424\n",
      "Epoch [345/500], Batch [40/110], Train Loss: 0.0037, Val Loss: 0.0104, LR: 0.0002284079\n",
      "Epoch [345/500], Batch [50/110], Train Loss: 0.0063, Val Loss: 0.0098, LR: 0.0002281735\n",
      "Epoch [345/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002279391\n",
      "Epoch [345/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002277049\n",
      "Epoch [345/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002274707\n",
      "Epoch [345/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002272366\n",
      "Epoch [345/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002270026\n",
      "Epoch [345/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002267687\n",
      "Epoch [346/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002265349\n",
      "Epoch [346/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0093, LR: 0.0002263012\n",
      "Epoch [346/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0002260676\n",
      "Epoch [346/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0002258340\n",
      "Epoch [346/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0002256006\n",
      "Epoch [346/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0002253673\n",
      "Epoch [346/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0002251340\n",
      "Epoch [346/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0080, LR: 0.0002249008\n",
      "Epoch [346/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0002246678\n",
      "Epoch [346/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0002244348\n",
      "Epoch [346/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002242019\n",
      "Epoch [347/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0002239691\n",
      "Epoch [347/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002237364\n",
      "Epoch [347/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002235037\n",
      "Epoch [347/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002232712\n",
      "Epoch [347/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002230388\n",
      "Epoch [347/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002228064\n",
      "Epoch [347/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0002225742\n",
      "Epoch [347/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0002223420\n",
      "Epoch [347/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0002221100\n",
      "Epoch [347/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0002218780\n",
      "Epoch [347/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002216461\n",
      "Epoch [348/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002214143\n",
      "Epoch [348/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0002211826\n",
      "Epoch [348/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0002209510\n",
      "Epoch [348/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002207195\n",
      "Epoch [348/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0002204881\n",
      "Epoch [348/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002202567\n",
      "Epoch [348/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002200255\n",
      "Epoch [348/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0002197944\n",
      "Epoch [348/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002195633\n",
      "Epoch [348/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0002193324\n",
      "Epoch [348/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0002191015\n",
      "Epoch [349/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0002188707\n",
      "Epoch [349/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002186401\n",
      "Epoch [349/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0002184095\n",
      "Epoch [349/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0002181790\n",
      "Epoch [349/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0094, LR: 0.0002179486\n",
      "Epoch [349/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0002177183\n",
      "Epoch [349/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0002174881\n",
      "Epoch [349/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0002172580\n",
      "Epoch [349/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002170280\n",
      "Epoch [349/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002167980\n",
      "Epoch [349/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002165682\n",
      "Epoch [350/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0002163385\n",
      "Epoch [350/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002161088\n",
      "Epoch [350/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002158793\n",
      "Epoch [350/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0127, LR: 0.0002156498\n",
      "Epoch [350/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0002154205\n",
      "Epoch [350/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0002151912\n",
      "Epoch [350/500], Batch [70/110], Train Loss: 0.0326, Val Loss: 0.0080, LR: 0.0002149620\n",
      "Epoch [350/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0002147330\n",
      "Epoch [350/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0002145040\n",
      "Epoch [350/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0002142751\n",
      "Epoch [350/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0002140463\n",
      "Epoch [351/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002138176\n",
      "Epoch [351/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002135890\n",
      "Epoch [351/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002133605\n",
      "Epoch [351/500], Batch [40/110], Train Loss: 0.0268, Val Loss: 0.0091, LR: 0.0002131321\n",
      "Epoch [351/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002129038\n",
      "Epoch [351/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002126755\n",
      "Epoch [351/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002124474\n",
      "Epoch [351/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002122194\n",
      "Epoch [351/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002119915\n",
      "Epoch [351/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0002117636\n",
      "Epoch [351/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0002115359\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 351: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.037 sec , Memory Usage: 393.13 MB\n",
      "Epoch [352/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0002113082\n",
      "Epoch [352/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002110807\n",
      "Epoch [352/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0002108532\n",
      "Epoch [352/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0002106259\n",
      "Epoch [352/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0002103986\n",
      "Epoch [352/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0002101714\n",
      "Epoch [352/500], Batch [70/110], Train Loss: 0.0070, Val Loss: 0.0091, LR: 0.0002099444\n",
      "Epoch [352/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0002097174\n",
      "Epoch [352/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0002094905\n",
      "Epoch [352/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002092637\n",
      "Epoch [352/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0002090370\n",
      "Epoch [353/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0002088104\n",
      "Epoch [353/500], Batch [20/110], Train Loss: 0.0871, Val Loss: 0.0077, LR: 0.0002085840\n",
      "Epoch [353/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0002083576\n",
      "Epoch [353/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0002081313\n",
      "Epoch [353/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0002079051\n",
      "Epoch [353/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0002076789\n",
      "Epoch [353/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0002074529\n",
      "Epoch [353/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0002072270\n",
      "Epoch [353/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0002070012\n",
      "Epoch [353/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0002067755\n",
      "Epoch [353/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002065499\n",
      "Epoch [354/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002063244\n",
      "Epoch [354/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0002060989\n",
      "Epoch [354/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0002058736\n",
      "Epoch [354/500], Batch [40/110], Train Loss: 0.0301, Val Loss: 0.0096, LR: 0.0002056484\n",
      "Epoch [354/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0002054232\n",
      "Epoch [354/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002051982\n",
      "Epoch [354/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0002049733\n",
      "Epoch [354/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0002047484\n",
      "Epoch [354/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0002045237\n",
      "Epoch [354/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0002042990\n",
      "Epoch [354/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0002040745\n",
      "Epoch [355/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0002038501\n",
      "Epoch [355/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0002036257\n",
      "Epoch [355/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0002034015\n",
      "Epoch [355/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0002031773\n",
      "Epoch [355/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0002029533\n",
      "Epoch [355/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002027293\n",
      "Epoch [355/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002025054\n",
      "Epoch [355/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0002022817\n",
      "Epoch [355/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0002020580\n",
      "Epoch [355/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0002018345\n",
      "Epoch [355/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0002016110\n",
      "Epoch [356/500], Batch [10/110], Train Loss: 0.0109, Val Loss: 0.0087, LR: 0.0002013876\n",
      "Epoch [356/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0002011644\n",
      "Epoch [356/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0087, LR: 0.0002009412\n",
      "Epoch [356/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0002007182\n",
      "Epoch [356/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0002004952\n",
      "Epoch [356/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0002002723\n",
      "Epoch [356/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0002000496\n",
      "Epoch [356/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001998269\n",
      "Epoch [356/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0112, LR: 0.0001996043\n",
      "Epoch [356/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0001993819\n",
      "Epoch [356/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001991595\n",
      "Epoch [357/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001989372\n",
      "Epoch [357/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001987151\n",
      "Epoch [357/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001984930\n",
      "Epoch [357/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001982710\n",
      "Epoch [357/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001980492\n",
      "Epoch [357/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001978274\n",
      "Epoch [357/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001976057\n",
      "Epoch [357/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0092, LR: 0.0001973841\n",
      "Epoch [357/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001971627\n",
      "Epoch [357/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001969413\n",
      "Epoch [357/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0093, LR: 0.0001967200\n",
      "Epoch [358/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001964989\n",
      "Epoch [358/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001962778\n",
      "Epoch [358/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001960569\n",
      "Epoch [358/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001958360\n",
      "Epoch [358/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001956152\n",
      "Epoch [358/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001953946\n",
      "Epoch [358/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001951740\n",
      "Epoch [358/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001949535\n",
      "Epoch [358/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0001947332\n",
      "Epoch [358/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0001945129\n",
      "Epoch [358/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0001942928\n",
      "Epoch [359/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0001940727\n",
      "Epoch [359/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001938528\n",
      "Epoch [359/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001936329\n",
      "Epoch [359/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001934132\n",
      "Epoch [359/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001931935\n",
      "Epoch [359/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001929740\n",
      "Epoch [359/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001927545\n",
      "Epoch [359/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001925352\n",
      "Epoch [359/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0095, LR: 0.0001923159\n",
      "Epoch [359/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001920968\n",
      "Epoch [359/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0001918778\n",
      "Epoch [360/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001916588\n",
      "Epoch [360/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0091, LR: 0.0001914400\n",
      "Epoch [360/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0126, LR: 0.0001912213\n",
      "Epoch [360/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0001910026\n",
      "Epoch [360/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0001907841\n",
      "Epoch [360/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001905657\n",
      "Epoch [360/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001903474\n",
      "Epoch [360/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001901292\n",
      "Epoch [360/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001899110\n",
      "Epoch [360/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001896930\n",
      "Epoch [360/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001894751\n",
      "Epoch [361/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001892573\n",
      "Epoch [361/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001890396\n",
      "Epoch [361/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001888220\n",
      "Epoch [361/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001886045\n",
      "Epoch [361/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001883871\n",
      "Epoch [361/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001881698\n",
      "Epoch [361/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001879526\n",
      "Epoch [361/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001877356\n",
      "Epoch [361/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001875186\n",
      "Epoch [361/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001873017\n",
      "Epoch [361/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001870849\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 361: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.035 sec , Memory Usage: 393.13 MB\n",
      "Epoch [362/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001868683\n",
      "Epoch [362/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0001866517\n",
      "Epoch [362/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0001864352\n",
      "Epoch [362/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001862189\n",
      "Epoch [362/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001860026\n",
      "Epoch [362/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001857865\n",
      "Epoch [362/500], Batch [70/110], Train Loss: 0.0028, Val Loss: 0.0100, LR: 0.0001855704\n",
      "Epoch [362/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0001853545\n",
      "Epoch [362/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0118, LR: 0.0001851387\n",
      "Epoch [362/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0118, LR: 0.0001849229\n",
      "Epoch [362/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0001847073\n",
      "Epoch [363/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0001844918\n",
      "Epoch [363/500], Batch [20/110], Train Loss: 0.0141, Val Loss: 0.0099, LR: 0.0001842764\n",
      "Epoch [363/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001840610\n",
      "Epoch [363/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001838458\n",
      "Epoch [363/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0102, LR: 0.0001836307\n",
      "Epoch [363/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001834157\n",
      "Epoch [363/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0001832008\n",
      "Epoch [363/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001829860\n",
      "Epoch [363/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001827714\n",
      "Epoch [363/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001825568\n",
      "Epoch [363/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001823423\n",
      "Epoch [364/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001821279\n",
      "Epoch [364/500], Batch [20/110], Train Loss: 0.0125, Val Loss: 0.0087, LR: 0.0001819137\n",
      "Epoch [364/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001816995\n",
      "Epoch [364/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001814855\n",
      "Epoch [364/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001812715\n",
      "Epoch [364/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001810577\n",
      "Epoch [364/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001808439\n",
      "Epoch [364/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001806303\n",
      "Epoch [364/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001804168\n",
      "Epoch [364/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0096, LR: 0.0001802034\n",
      "Epoch [364/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001799901\n",
      "Epoch [365/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001797768\n",
      "Epoch [365/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001795637\n",
      "Epoch [365/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001793507\n",
      "Epoch [365/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001791379\n",
      "Epoch [365/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001789251\n",
      "Epoch [365/500], Batch [60/110], Train Loss: 0.0014, Val Loss: 0.0102, LR: 0.0001787124\n",
      "Epoch [365/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0101, LR: 0.0001784998\n",
      "Epoch [365/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0101, LR: 0.0001782874\n",
      "Epoch [365/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001780750\n",
      "Epoch [365/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001778628\n",
      "Epoch [365/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001776506\n",
      "Epoch [366/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001774386\n",
      "Epoch [366/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001772267\n",
      "Epoch [366/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0001770148\n",
      "Epoch [366/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001768031\n",
      "Epoch [366/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001765915\n",
      "Epoch [366/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001763800\n",
      "Epoch [366/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0001761686\n",
      "Epoch [366/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0001759573\n",
      "Epoch [366/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0103, LR: 0.0001757462\n",
      "Epoch [366/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0001755351\n",
      "Epoch [366/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0001753241\n",
      "Epoch [367/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001751133\n",
      "Epoch [367/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001749025\n",
      "Epoch [367/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001746919\n",
      "Epoch [367/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0001744813\n",
      "Epoch [367/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001742709\n",
      "Epoch [367/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001740606\n",
      "Epoch [367/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0001738504\n",
      "Epoch [367/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001736403\n",
      "Epoch [367/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001734303\n",
      "Epoch [367/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001732204\n",
      "Epoch [367/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001730106\n",
      "Epoch [368/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001728010\n",
      "Epoch [368/500], Batch [20/110], Train Loss: 0.0042, Val Loss: 0.0093, LR: 0.0001725914\n",
      "Epoch [368/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001723820\n",
      "Epoch [368/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0095, LR: 0.0001721726\n",
      "Epoch [368/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001719634\n",
      "Epoch [368/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001717543\n",
      "Epoch [368/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001715452\n",
      "Epoch [368/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001713363\n",
      "Epoch [368/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0098, LR: 0.0001711275\n",
      "Epoch [368/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001709188\n",
      "Epoch [368/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001707103\n",
      "Epoch [369/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001705018\n",
      "Epoch [369/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001702934\n",
      "Epoch [369/500], Batch [30/110], Train Loss: 0.0017, Val Loss: 0.0089, LR: 0.0001700852\n",
      "Epoch [369/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001698770\n",
      "Epoch [369/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0001696690\n",
      "Epoch [369/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0001694611\n",
      "Epoch [369/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001692532\n",
      "Epoch [369/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001690455\n",
      "Epoch [369/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0001688379\n",
      "Epoch [369/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0001686305\n",
      "Epoch [369/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0001684231\n",
      "Epoch [370/500], Batch [10/110], Train Loss: 0.0017, Val Loss: 0.0103, LR: 0.0001682158\n",
      "Epoch [370/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001680087\n",
      "Epoch [370/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001678016\n",
      "Epoch [370/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0001675947\n",
      "Epoch [370/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001673878\n",
      "Epoch [370/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001671811\n",
      "Epoch [370/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0001669745\n",
      "Epoch [370/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0001667680\n",
      "Epoch [370/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001665616\n",
      "Epoch [370/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0110, LR: 0.0001663553\n",
      "Epoch [370/500], Batch [110/110], Train Loss: 0.0249, Val Loss: 0.0112, LR: 0.0001661492\n",
      "Epoch [371/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0001659431\n",
      "Epoch [371/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0001657372\n",
      "Epoch [371/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0001655313\n",
      "Epoch [371/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0001653256\n",
      "Epoch [371/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0001651200\n",
      "Epoch [371/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0101, LR: 0.0001649145\n",
      "Epoch [371/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001647091\n",
      "Epoch [371/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001645038\n",
      "Epoch [371/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001642987\n",
      "Epoch [371/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001640936\n",
      "Epoch [371/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001638887\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 371: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 393.13 MB\n",
      "Epoch [372/500], Batch [10/110], Train Loss: 0.0381, Val Loss: 0.0089, LR: 0.0001636838\n",
      "Epoch [372/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001634791\n",
      "Epoch [372/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001632745\n",
      "Epoch [372/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001630700\n",
      "Epoch [372/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0001628656\n",
      "Epoch [372/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0001626613\n",
      "Epoch [372/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0001624572\n",
      "Epoch [372/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0001622531\n",
      "Epoch [372/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0001620492\n",
      "Epoch [372/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0001618453\n",
      "Epoch [372/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001616416\n",
      "Epoch [373/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001614380\n",
      "Epoch [373/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001612345\n",
      "Epoch [373/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0001610311\n",
      "Epoch [373/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0001608279\n",
      "Epoch [373/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0001606247\n",
      "Epoch [373/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0089, LR: 0.0001604217\n",
      "Epoch [373/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001602187\n",
      "Epoch [373/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001600159\n",
      "Epoch [373/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0001598132\n",
      "Epoch [373/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0001596106\n",
      "Epoch [373/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0001594081\n",
      "Epoch [374/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0001592057\n",
      "Epoch [374/500], Batch [20/110], Train Loss: 0.0091, Val Loss: 0.0090, LR: 0.0001590035\n",
      "Epoch [374/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001588013\n",
      "Epoch [374/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0001585993\n",
      "Epoch [374/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0001583974\n",
      "Epoch [374/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0001581956\n",
      "Epoch [374/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0001579939\n",
      "Epoch [374/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0001577923\n",
      "Epoch [374/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001575909\n",
      "Epoch [374/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001573895\n",
      "Epoch [374/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001571883\n",
      "Epoch [375/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001569871\n",
      "Epoch [375/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001567861\n",
      "Epoch [375/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0095, LR: 0.0001565852\n",
      "Epoch [375/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001563844\n",
      "Epoch [375/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001561838\n",
      "Epoch [375/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001559832\n",
      "Epoch [375/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001557828\n",
      "Epoch [375/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0087, LR: 0.0001555824\n",
      "Epoch [375/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0001553822\n",
      "Epoch [375/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001551821\n",
      "Epoch [375/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0100, LR: 0.0001549821\n",
      "Epoch [376/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001547823\n",
      "Epoch [376/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001545825\n",
      "Epoch [376/500], Batch [30/110], Train Loss: 0.0102, Val Loss: 0.0093, LR: 0.0001543829\n",
      "Epoch [376/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001541833\n",
      "Epoch [376/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001539839\n",
      "Epoch [376/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0001537846\n",
      "Epoch [376/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0001535854\n",
      "Epoch [376/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0001533864\n",
      "Epoch [376/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001531874\n",
      "Epoch [376/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001529886\n",
      "Epoch [376/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0001527898\n",
      "Epoch [377/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0001525912\n",
      "Epoch [377/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0001523927\n",
      "Epoch [377/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0001521943\n",
      "Epoch [377/500], Batch [40/110], Train Loss: 0.0025, Val Loss: 0.0103, LR: 0.0001519961\n",
      "Epoch [377/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001517979\n",
      "Epoch [377/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001515999\n",
      "Epoch [377/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001514020\n",
      "Epoch [377/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0088, LR: 0.0001512042\n",
      "Epoch [377/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001510065\n",
      "Epoch [377/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001508089\n",
      "Epoch [377/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001506114\n",
      "Epoch [378/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001504141\n",
      "Epoch [378/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001502169\n",
      "Epoch [378/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001500198\n",
      "Epoch [378/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001498228\n",
      "Epoch [378/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001496259\n",
      "Epoch [378/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001494291\n",
      "Epoch [378/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001492325\n",
      "Epoch [378/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0001490359\n",
      "Epoch [378/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0001488395\n",
      "Epoch [378/500], Batch [100/110], Train Loss: 0.0103, Val Loss: 0.0081, LR: 0.0001486432\n",
      "Epoch [378/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0001484470\n",
      "Epoch [379/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0001482510\n",
      "Epoch [379/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0001480550\n",
      "Epoch [379/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0001478592\n",
      "Epoch [379/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0001476635\n",
      "Epoch [379/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0001474679\n",
      "Epoch [379/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0001472724\n",
      "Epoch [379/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0001470770\n",
      "Epoch [379/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0001468818\n",
      "Epoch [379/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0120, LR: 0.0001466866\n",
      "Epoch [379/500], Batch [100/110], Train Loss: 0.0178, Val Loss: 0.0143, LR: 0.0001464916\n",
      "Epoch [379/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0142, LR: 0.0001462967\n",
      "Epoch [380/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0001461019\n",
      "Epoch [380/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001459073\n",
      "Epoch [380/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001457127\n",
      "Epoch [380/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001455183\n",
      "Epoch [380/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001453240\n",
      "Epoch [380/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001451298\n",
      "Epoch [380/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001449357\n",
      "Epoch [380/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001447417\n",
      "Epoch [380/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0001445479\n",
      "Epoch [380/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0001443541\n",
      "Epoch [380/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0001441605\n",
      "Epoch [381/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0001439670\n",
      "Epoch [381/500], Batch [20/110], Train Loss: 0.0160, Val Loss: 0.0087, LR: 0.0001437737\n",
      "Epoch [381/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001435804\n",
      "Epoch [381/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0001433873\n",
      "Epoch [381/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001431942\n",
      "Epoch [381/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001430013\n",
      "Epoch [381/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0001428086\n",
      "Epoch [381/500], Batch [80/110], Train Loss: 0.0039, Val Loss: 0.0084, LR: 0.0001426159\n",
      "Epoch [381/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0001424233\n",
      "Epoch [381/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0001422309\n",
      "Epoch [381/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0001420386\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 381: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.036 sec , Memory Usage: 393.13 MB\n",
      "Epoch [382/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0001418464\n",
      "Epoch [382/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0001416543\n",
      "Epoch [382/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0001414624\n",
      "Epoch [382/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0001412705\n",
      "Epoch [382/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0001410788\n",
      "Epoch [382/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0001408872\n",
      "Epoch [382/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0001406957\n",
      "Epoch [382/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001405044\n",
      "Epoch [382/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001403131\n",
      "Epoch [382/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001401220\n",
      "Epoch [382/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001399310\n",
      "Epoch [383/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001397401\n",
      "Epoch [383/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001395494\n",
      "Epoch [383/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0088, LR: 0.0001393587\n",
      "Epoch [383/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001391682\n",
      "Epoch [383/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0001389778\n",
      "Epoch [383/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0001387875\n",
      "Epoch [383/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001385973\n",
      "Epoch [383/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0001384073\n",
      "Epoch [383/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0107, LR: 0.0001382173\n",
      "Epoch [383/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0001380275\n",
      "Epoch [383/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0001378378\n",
      "Epoch [384/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001376483\n",
      "Epoch [384/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0001374588\n",
      "Epoch [384/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001372695\n",
      "Epoch [384/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0001370803\n",
      "Epoch [384/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001368912\n",
      "Epoch [384/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001367022\n",
      "Epoch [384/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001365133\n",
      "Epoch [384/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0001363246\n",
      "Epoch [384/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001361360\n",
      "Epoch [384/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001359475\n",
      "Epoch [384/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0001357591\n",
      "Epoch [385/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0001355709\n",
      "Epoch [385/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0001353828\n",
      "Epoch [385/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0001351947\n",
      "Epoch [385/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001350069\n",
      "Epoch [385/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001348191\n",
      "Epoch [385/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001346314\n",
      "Epoch [385/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001344439\n",
      "Epoch [385/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001342565\n",
      "Epoch [385/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001340692\n",
      "Epoch [385/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001338821\n",
      "Epoch [385/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001336950\n",
      "Epoch [386/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001335081\n",
      "Epoch [386/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001333213\n",
      "Epoch [386/500], Batch [30/110], Train Loss: 0.0083, Val Loss: 0.0092, LR: 0.0001331346\n",
      "Epoch [386/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001329481\n",
      "Epoch [386/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001327616\n",
      "Epoch [386/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001325753\n",
      "Epoch [386/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001323891\n",
      "Epoch [386/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001322031\n",
      "Epoch [386/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001320171\n",
      "Epoch [386/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001318313\n",
      "Epoch [386/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001316456\n",
      "Epoch [387/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001314600\n",
      "Epoch [387/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001312745\n",
      "Epoch [387/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001310892\n",
      "Epoch [387/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001309040\n",
      "Epoch [387/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001307189\n",
      "Epoch [387/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001305339\n",
      "Epoch [387/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0001303490\n",
      "Epoch [387/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001301643\n",
      "Epoch [387/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001299797\n",
      "Epoch [387/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001297952\n",
      "Epoch [387/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0001296109\n",
      "Epoch [388/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001294266\n",
      "Epoch [388/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001292425\n",
      "Epoch [388/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001290585\n",
      "Epoch [388/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001288746\n",
      "Epoch [388/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001286909\n",
      "Epoch [388/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001285072\n",
      "Epoch [388/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001283237\n",
      "Epoch [388/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001281404\n",
      "Epoch [388/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001279571\n",
      "Epoch [388/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001277740\n",
      "Epoch [388/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001275910\n",
      "Epoch [389/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001274081\n",
      "Epoch [389/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001272253\n",
      "Epoch [389/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001270427\n",
      "Epoch [389/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001268601\n",
      "Epoch [389/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001266777\n",
      "Epoch [389/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001264955\n",
      "Epoch [389/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001263133\n",
      "Epoch [389/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0001261313\n",
      "Epoch [389/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0001259494\n",
      "Epoch [389/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0110, LR: 0.0001257676\n",
      "Epoch [389/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0001255860\n",
      "Epoch [390/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0001254044\n",
      "Epoch [390/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001252230\n",
      "Epoch [390/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001250417\n",
      "Epoch [390/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001248606\n",
      "Epoch [390/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001246795\n",
      "Epoch [390/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001244986\n",
      "Epoch [390/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001243178\n",
      "Epoch [390/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001241372\n",
      "Epoch [390/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001239566\n",
      "Epoch [390/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0001237762\n",
      "Epoch [390/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001235959\n",
      "Epoch [391/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001234158\n",
      "Epoch [391/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0001232357\n",
      "Epoch [391/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0001230558\n",
      "Epoch [391/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001228760\n",
      "Epoch [391/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0112, LR: 0.0001226964\n",
      "Epoch [391/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0119, LR: 0.0001225168\n",
      "Epoch [391/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0117, LR: 0.0001223374\n",
      "Epoch [391/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0001221581\n",
      "Epoch [391/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0001219789\n",
      "Epoch [391/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0001217999\n",
      "Epoch [391/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0001216210\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 391: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 393.13 MB\n",
      "Epoch [392/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0001214422\n",
      "Epoch [392/500], Batch [20/110], Train Loss: 0.0245, Val Loss: 0.0119, LR: 0.0001212635\n",
      "Epoch [392/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0117, LR: 0.0001210850\n",
      "Epoch [392/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0112, LR: 0.0001209066\n",
      "Epoch [392/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0001207283\n",
      "Epoch [392/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0001205501\n",
      "Epoch [392/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0001203721\n",
      "Epoch [392/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0001201942\n",
      "Epoch [392/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0001200164\n",
      "Epoch [392/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001198387\n",
      "Epoch [392/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001196612\n",
      "Epoch [393/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001194837\n",
      "Epoch [393/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001193065\n",
      "Epoch [393/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001191293\n",
      "Epoch [393/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001189523\n",
      "Epoch [393/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001187754\n",
      "Epoch [393/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001185986\n",
      "Epoch [393/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001184219\n",
      "Epoch [393/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001182454\n",
      "Epoch [393/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001180690\n",
      "Epoch [393/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001178927\n",
      "Epoch [393/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001177165\n",
      "Epoch [394/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001175405\n",
      "Epoch [394/500], Batch [20/110], Train Loss: 0.0031, Val Loss: 0.0092, LR: 0.0001173646\n",
      "Epoch [394/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0091, LR: 0.0001171889\n",
      "Epoch [394/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001170132\n",
      "Epoch [394/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001168377\n",
      "Epoch [394/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001166623\n",
      "Epoch [394/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0001164870\n",
      "Epoch [394/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001163119\n",
      "Epoch [394/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001161369\n",
      "Epoch [394/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001159620\n",
      "Epoch [394/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0001157872\n",
      "Epoch [395/500], Batch [10/110], Train Loss: 0.0018, Val Loss: 0.0109, LR: 0.0001156126\n",
      "Epoch [395/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0001154381\n",
      "Epoch [395/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0001152637\n",
      "Epoch [395/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001150895\n",
      "Epoch [395/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001149153\n",
      "Epoch [395/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0095, LR: 0.0001147413\n",
      "Epoch [395/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001145675\n",
      "Epoch [395/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0096, LR: 0.0001143937\n",
      "Epoch [395/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001142201\n",
      "Epoch [395/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001140466\n",
      "Epoch [395/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001138733\n",
      "Epoch [396/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0001137000\n",
      "Epoch [396/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0091, LR: 0.0001135269\n",
      "Epoch [396/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001133540\n",
      "Epoch [396/500], Batch [40/110], Train Loss: 0.0171, Val Loss: 0.0092, LR: 0.0001131811\n",
      "Epoch [396/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001130084\n",
      "Epoch [396/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001128358\n",
      "Epoch [396/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001126633\n",
      "Epoch [396/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001124910\n",
      "Epoch [396/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001123188\n",
      "Epoch [396/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001121467\n",
      "Epoch [396/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0095, LR: 0.0001119748\n",
      "Epoch [397/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001118029\n",
      "Epoch [397/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001116312\n",
      "Epoch [397/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0001114597\n",
      "Epoch [397/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0100, LR: 0.0001112882\n",
      "Epoch [397/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001111169\n",
      "Epoch [397/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001109457\n",
      "Epoch [397/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001107747\n",
      "Epoch [397/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001106038\n",
      "Epoch [397/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001104330\n",
      "Epoch [397/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001102623\n",
      "Epoch [397/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0092, LR: 0.0001100918\n",
      "Epoch [398/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001099213\n",
      "Epoch [398/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001097511\n",
      "Epoch [398/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001095809\n",
      "Epoch [398/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001094109\n",
      "Epoch [398/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001092410\n",
      "Epoch [398/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001090712\n",
      "Epoch [398/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001089016\n",
      "Epoch [398/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001087321\n",
      "Epoch [398/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001085627\n",
      "Epoch [398/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001083935\n",
      "Epoch [398/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0001082243\n",
      "Epoch [399/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001080554\n",
      "Epoch [399/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001078865\n",
      "Epoch [399/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001077178\n",
      "Epoch [399/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001075492\n",
      "Epoch [399/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0001073807\n",
      "Epoch [399/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001072124\n",
      "Epoch [399/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001070441\n",
      "Epoch [399/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0001068761\n",
      "Epoch [399/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001067081\n",
      "Epoch [399/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001065403\n",
      "Epoch [399/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001063726\n",
      "Epoch [400/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0098, LR: 0.0001062050\n",
      "Epoch [400/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001060376\n",
      "Epoch [400/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001058703\n",
      "Epoch [400/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001057031\n",
      "Epoch [400/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001055361\n",
      "Epoch [400/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001053692\n",
      "Epoch [400/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0094, LR: 0.0001052024\n",
      "Epoch [400/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001050358\n",
      "Epoch [400/500], Batch [90/110], Train Loss: 0.0101, Val Loss: 0.0098, LR: 0.0001048692\n",
      "Epoch [400/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0001047028\n",
      "Epoch [400/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0001045366\n",
      "Epoch [401/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0001043705\n",
      "Epoch [401/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0001042045\n",
      "Epoch [401/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001040386\n",
      "Epoch [401/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001038729\n",
      "Epoch [401/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001037073\n",
      "Epoch [401/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001035418\n",
      "Epoch [401/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0001033764\n",
      "Epoch [401/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001032112\n",
      "Epoch [401/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001030462\n",
      "Epoch [401/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0001028812\n",
      "Epoch [401/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0001027164\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 401: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.036 sec , Memory Usage: 393.13 MB\n",
      "Epoch [402/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001025517\n",
      "Epoch [402/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0001023871\n",
      "Epoch [402/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0090, LR: 0.0001022227\n",
      "Epoch [402/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0001020584\n",
      "Epoch [402/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001018943\n",
      "Epoch [402/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0001017302\n",
      "Epoch [402/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0001015663\n",
      "Epoch [402/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0001014026\n",
      "Epoch [402/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0001012389\n",
      "Epoch [402/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0001010754\n",
      "Epoch [402/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0001009121\n",
      "Epoch [403/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0001007488\n",
      "Epoch [403/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0001005857\n",
      "Epoch [403/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0001004227\n",
      "Epoch [403/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0001002599\n",
      "Epoch [403/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0001000972\n",
      "Epoch [403/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000999346\n",
      "Epoch [403/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000997722\n",
      "Epoch [403/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000996099\n",
      "Epoch [403/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000994477\n",
      "Epoch [403/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000992856\n",
      "Epoch [403/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0099, LR: 0.0000991237\n",
      "Epoch [404/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000989619\n",
      "Epoch [404/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000988003\n",
      "Epoch [404/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000986387\n",
      "Epoch [404/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000984774\n",
      "Epoch [404/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000983161\n",
      "Epoch [404/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0000981550\n",
      "Epoch [404/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000979940\n",
      "Epoch [404/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000978331\n",
      "Epoch [404/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000976724\n",
      "Epoch [404/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000975118\n",
      "Epoch [404/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000973514\n",
      "Epoch [405/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0115, LR: 0.0000971910\n",
      "Epoch [405/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0115, LR: 0.0000970308\n",
      "Epoch [405/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0108, LR: 0.0000968708\n",
      "Epoch [405/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000967109\n",
      "Epoch [405/500], Batch [50/110], Train Loss: 0.0273, Val Loss: 0.0096, LR: 0.0000965511\n",
      "Epoch [405/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000963914\n",
      "Epoch [405/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000962319\n",
      "Epoch [405/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000960725\n",
      "Epoch [405/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000959132\n",
      "Epoch [405/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000957541\n",
      "Epoch [405/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0098, LR: 0.0000955951\n",
      "Epoch [406/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0097, LR: 0.0000954363\n",
      "Epoch [406/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000952775\n",
      "Epoch [406/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000951189\n",
      "Epoch [406/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000949605\n",
      "Epoch [406/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000948022\n",
      "Epoch [406/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000946440\n",
      "Epoch [406/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000944859\n",
      "Epoch [406/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000943280\n",
      "Epoch [406/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000941702\n",
      "Epoch [406/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000940125\n",
      "Epoch [406/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000938550\n",
      "Epoch [407/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000936976\n",
      "Epoch [407/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000935404\n",
      "Epoch [407/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000933833\n",
      "Epoch [407/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000932263\n",
      "Epoch [407/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000930694\n",
      "Epoch [407/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000929127\n",
      "Epoch [407/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000927561\n",
      "Epoch [407/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000925997\n",
      "Epoch [407/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0097, LR: 0.0000924434\n",
      "Epoch [407/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000922872\n",
      "Epoch [407/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0000921312\n",
      "Epoch [408/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000919753\n",
      "Epoch [408/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000918195\n",
      "Epoch [408/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000916639\n",
      "Epoch [408/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000915084\n",
      "Epoch [408/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000913530\n",
      "Epoch [408/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000911978\n",
      "Epoch [408/500], Batch [70/110], Train Loss: 0.0028, Val Loss: 0.0098, LR: 0.0000910427\n",
      "Epoch [408/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000908877\n",
      "Epoch [408/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000907329\n",
      "Epoch [408/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000905782\n",
      "Epoch [408/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000904236\n",
      "Epoch [409/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000902692\n",
      "Epoch [409/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000901149\n",
      "Epoch [409/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000899608\n",
      "Epoch [409/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000898067\n",
      "Epoch [409/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0000896529\n",
      "Epoch [409/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000894991\n",
      "Epoch [409/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000893455\n",
      "Epoch [409/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000891920\n",
      "Epoch [409/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000890387\n",
      "Epoch [409/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000888855\n",
      "Epoch [409/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000887324\n",
      "Epoch [410/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000885795\n",
      "Epoch [410/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000884267\n",
      "Epoch [410/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000882740\n",
      "Epoch [410/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000881215\n",
      "Epoch [410/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000879691\n",
      "Epoch [410/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000878169\n",
      "Epoch [410/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000876648\n",
      "Epoch [410/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0105, LR: 0.0000875128\n",
      "Epoch [410/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000873610\n",
      "Epoch [410/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000872092\n",
      "Epoch [410/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000870577\n",
      "Epoch [411/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000869062\n",
      "Epoch [411/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0098, LR: 0.0000867549\n",
      "Epoch [411/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000866038\n",
      "Epoch [411/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000864528\n",
      "Epoch [411/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000863019\n",
      "Epoch [411/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000861511\n",
      "Epoch [411/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000860005\n",
      "Epoch [411/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000858500\n",
      "Epoch [411/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000856997\n",
      "Epoch [411/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000855495\n",
      "Epoch [411/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000853994\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 411: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.045 sec , Memory Usage: 393.13 MB\n",
      "Epoch [412/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0095, LR: 0.0000852495\n",
      "Epoch [412/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000850997\n",
      "Epoch [412/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000849500\n",
      "Epoch [412/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000848005\n",
      "Epoch [412/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000846511\n",
      "Epoch [412/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000845019\n",
      "Epoch [412/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000843528\n",
      "Epoch [412/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0099, LR: 0.0000842038\n",
      "Epoch [412/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0000840550\n",
      "Epoch [412/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000839063\n",
      "Epoch [412/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000837577\n",
      "Epoch [413/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0000836093\n",
      "Epoch [413/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0000834610\n",
      "Epoch [413/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000833129\n",
      "Epoch [413/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000831649\n",
      "Epoch [413/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000830170\n",
      "Epoch [413/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000828693\n",
      "Epoch [413/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0000827217\n",
      "Epoch [413/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0000825742\n",
      "Epoch [413/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0000824269\n",
      "Epoch [413/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0000822797\n",
      "Epoch [413/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0000821327\n",
      "Epoch [414/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000819858\n",
      "Epoch [414/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000818390\n",
      "Epoch [414/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0092, LR: 0.0000816924\n",
      "Epoch [414/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000815459\n",
      "Epoch [414/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000813995\n",
      "Epoch [414/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000812533\n",
      "Epoch [414/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000811072\n",
      "Epoch [414/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0099, LR: 0.0000809613\n",
      "Epoch [414/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000808155\n",
      "Epoch [414/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000806698\n",
      "Epoch [414/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000805243\n",
      "Epoch [415/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000803789\n",
      "Epoch [415/500], Batch [20/110], Train Loss: 0.0062, Val Loss: 0.0097, LR: 0.0000802337\n",
      "Epoch [415/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000800886\n",
      "Epoch [415/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000799436\n",
      "Epoch [415/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000797988\n",
      "Epoch [415/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000796541\n",
      "Epoch [415/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0094, LR: 0.0000795095\n",
      "Epoch [415/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000793651\n",
      "Epoch [415/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000792208\n",
      "Epoch [415/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000790767\n",
      "Epoch [415/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000789327\n",
      "Epoch [416/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000787888\n",
      "Epoch [416/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000786451\n",
      "Epoch [416/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000785015\n",
      "Epoch [416/500], Batch [40/110], Train Loss: 0.0026, Val Loss: 0.0108, LR: 0.0000783581\n",
      "Epoch [416/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0000782148\n",
      "Epoch [416/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0000780716\n",
      "Epoch [416/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0000779286\n",
      "Epoch [416/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000777857\n",
      "Epoch [416/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0000776430\n",
      "Epoch [416/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0000775004\n",
      "Epoch [416/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0000773579\n",
      "Epoch [417/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000772156\n",
      "Epoch [417/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000770734\n",
      "Epoch [417/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000769313\n",
      "Epoch [417/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0000767894\n",
      "Epoch [417/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0000766477\n",
      "Epoch [417/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000765060\n",
      "Epoch [417/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0102, LR: 0.0000763646\n",
      "Epoch [417/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000762232\n",
      "Epoch [417/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000760820\n",
      "Epoch [417/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000759409\n",
      "Epoch [417/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000758000\n",
      "Epoch [418/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000756592\n",
      "Epoch [418/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000755186\n",
      "Epoch [418/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000753781\n",
      "Epoch [418/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000752377\n",
      "Epoch [418/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000750975\n",
      "Epoch [418/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000749574\n",
      "Epoch [418/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000748174\n",
      "Epoch [418/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000746776\n",
      "Epoch [418/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000745380\n",
      "Epoch [418/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000743984\n",
      "Epoch [418/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000742590\n",
      "Epoch [419/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000741198\n",
      "Epoch [419/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0097, LR: 0.0000739807\n",
      "Epoch [419/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000738417\n",
      "Epoch [419/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000737029\n",
      "Epoch [419/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000735642\n",
      "Epoch [419/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000734257\n",
      "Epoch [419/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000732873\n",
      "Epoch [419/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000731490\n",
      "Epoch [419/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000730109\n",
      "Epoch [419/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000728729\n",
      "Epoch [419/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000727351\n",
      "Epoch [420/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0000725974\n",
      "Epoch [420/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000724598\n",
      "Epoch [420/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0000723224\n",
      "Epoch [420/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000721851\n",
      "Epoch [420/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000720480\n",
      "Epoch [420/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000719110\n",
      "Epoch [420/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000717742\n",
      "Epoch [420/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000716375\n",
      "Epoch [420/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0000715009\n",
      "Epoch [420/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0000713645\n",
      "Epoch [420/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0000712282\n",
      "Epoch [421/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0000710921\n",
      "Epoch [421/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0000709561\n",
      "Epoch [421/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0000708202\n",
      "Epoch [421/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000706845\n",
      "Epoch [421/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000705489\n",
      "Epoch [421/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000704135\n",
      "Epoch [421/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000702782\n",
      "Epoch [421/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000701430\n",
      "Epoch [421/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000700080\n",
      "Epoch [421/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000698732\n",
      "Epoch [421/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000697384\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 421: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.043 sec , Memory Usage: 393.13 MB\n",
      "Epoch [422/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000696038\n",
      "Epoch [422/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000694694\n",
      "Epoch [422/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000693351\n",
      "Epoch [422/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000692010\n",
      "Epoch [422/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000690669\n",
      "Epoch [422/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000689331\n",
      "Epoch [422/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000687993\n",
      "Epoch [422/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000686657\n",
      "Epoch [422/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0106, LR: 0.0000685323\n",
      "Epoch [422/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0000683990\n",
      "Epoch [422/500], Batch [110/110], Train Loss: 0.0036, Val Loss: 0.0107, LR: 0.0000682658\n",
      "Epoch [423/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0107, LR: 0.0000681328\n",
      "Epoch [423/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0000680000\n",
      "Epoch [423/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000678672\n",
      "Epoch [423/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000677346\n",
      "Epoch [423/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000676022\n",
      "Epoch [423/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000674699\n",
      "Epoch [423/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000673377\n",
      "Epoch [423/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000672057\n",
      "Epoch [423/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0000670738\n",
      "Epoch [423/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0000669421\n",
      "Epoch [423/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0000668105\n",
      "Epoch [424/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0000666791\n",
      "Epoch [424/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0000665478\n",
      "Epoch [424/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0104, LR: 0.0000664166\n",
      "Epoch [424/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000662856\n",
      "Epoch [424/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000661547\n",
      "Epoch [424/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000660240\n",
      "Epoch [424/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000658934\n",
      "Epoch [424/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000657629\n",
      "Epoch [424/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000656326\n",
      "Epoch [424/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0000655025\n",
      "Epoch [424/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000653725\n",
      "Epoch [425/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000652426\n",
      "Epoch [425/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0000651129\n",
      "Epoch [425/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000649833\n",
      "Epoch [425/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0094, LR: 0.0000648538\n",
      "Epoch [425/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000647245\n",
      "Epoch [425/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000645954\n",
      "Epoch [425/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0000644664\n",
      "Epoch [425/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000643375\n",
      "Epoch [425/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000642088\n",
      "Epoch [425/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000640802\n",
      "Epoch [425/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0000639518\n",
      "Epoch [426/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0000638235\n",
      "Epoch [426/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0000636953\n",
      "Epoch [426/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0000635673\n",
      "Epoch [426/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0000634395\n",
      "Epoch [426/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0000633118\n",
      "Epoch [426/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0091, LR: 0.0000631842\n",
      "Epoch [426/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000630568\n",
      "Epoch [426/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000629295\n",
      "Epoch [426/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000628023\n",
      "Epoch [426/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000626753\n",
      "Epoch [426/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000625485\n",
      "Epoch [427/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000624218\n",
      "Epoch [427/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000622952\n",
      "Epoch [427/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000621688\n",
      "Epoch [427/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000620425\n",
      "Epoch [427/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000619164\n",
      "Epoch [427/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000617904\n",
      "Epoch [427/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000616646\n",
      "Epoch [427/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0093, LR: 0.0000615389\n",
      "Epoch [427/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000614134\n",
      "Epoch [427/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000612879\n",
      "Epoch [427/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000611627\n",
      "Epoch [428/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000610376\n",
      "Epoch [428/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0094, LR: 0.0000609126\n",
      "Epoch [428/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000607878\n",
      "Epoch [428/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000606631\n",
      "Epoch [428/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000605386\n",
      "Epoch [428/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000604142\n",
      "Epoch [428/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000602899\n",
      "Epoch [428/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000601658\n",
      "Epoch [428/500], Batch [90/110], Train Loss: 0.0032, Val Loss: 0.0093, LR: 0.0000600419\n",
      "Epoch [428/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000599181\n",
      "Epoch [428/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000597944\n",
      "Epoch [429/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000596709\n",
      "Epoch [429/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000595475\n",
      "Epoch [429/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000594243\n",
      "Epoch [429/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000593012\n",
      "Epoch [429/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000591783\n",
      "Epoch [429/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0000590555\n",
      "Epoch [429/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0000589328\n",
      "Epoch [429/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0000588103\n",
      "Epoch [429/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0000586880\n",
      "Epoch [429/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0107, LR: 0.0000585658\n",
      "Epoch [429/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0104, LR: 0.0000584437\n",
      "Epoch [430/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000583218\n",
      "Epoch [430/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000582000\n",
      "Epoch [430/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000580784\n",
      "Epoch [430/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000579569\n",
      "Epoch [430/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000578356\n",
      "Epoch [430/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000577144\n",
      "Epoch [430/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000575933\n",
      "Epoch [430/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000574724\n",
      "Epoch [430/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000573517\n",
      "Epoch [430/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000572311\n",
      "Epoch [430/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0000571106\n",
      "Epoch [431/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000569903\n",
      "Epoch [431/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000568701\n",
      "Epoch [431/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000567501\n",
      "Epoch [431/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000566302\n",
      "Epoch [431/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000565105\n",
      "Epoch [431/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000563909\n",
      "Epoch [431/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000562715\n",
      "Epoch [431/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0000561522\n",
      "Epoch [431/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000560331\n",
      "Epoch [431/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000559141\n",
      "Epoch [431/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000557952\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 431: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.036 sec , Memory Usage: 393.13 MB\n",
      "Epoch [432/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000556765\n",
      "Epoch [432/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000555580\n",
      "Epoch [432/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0093, LR: 0.0000554395\n",
      "Epoch [432/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0000553213\n",
      "Epoch [432/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000552032\n",
      "Epoch [432/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000550852\n",
      "Epoch [432/500], Batch [70/110], Train Loss: 0.0053, Val Loss: 0.0094, LR: 0.0000549674\n",
      "Epoch [432/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000548497\n",
      "Epoch [432/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000547322\n",
      "Epoch [432/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000546148\n",
      "Epoch [432/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000544975\n",
      "Epoch [433/500], Batch [10/110], Train Loss: 0.0032, Val Loss: 0.0098, LR: 0.0000543805\n",
      "Epoch [433/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000542635\n",
      "Epoch [433/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000541467\n",
      "Epoch [433/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0097, LR: 0.0000540301\n",
      "Epoch [433/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000539136\n",
      "Epoch [433/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0000537972\n",
      "Epoch [433/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0000536810\n",
      "Epoch [433/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0000535650\n",
      "Epoch [433/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0000534490\n",
      "Epoch [433/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0000533333\n",
      "Epoch [433/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0000532177\n",
      "Epoch [434/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0108, LR: 0.0000531022\n",
      "Epoch [434/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0000529869\n",
      "Epoch [434/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0000528717\n",
      "Epoch [434/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0000527567\n",
      "Epoch [434/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0000526418\n",
      "Epoch [434/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0104, LR: 0.0000525271\n",
      "Epoch [434/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000524125\n",
      "Epoch [434/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000522980\n",
      "Epoch [434/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000521838\n",
      "Epoch [434/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0000520696\n",
      "Epoch [434/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000519556\n",
      "Epoch [435/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000518418\n",
      "Epoch [435/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000517281\n",
      "Epoch [435/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000516145\n",
      "Epoch [435/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000515011\n",
      "Epoch [435/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000513879\n",
      "Epoch [435/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000512748\n",
      "Epoch [435/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000511618\n",
      "Epoch [435/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000510490\n",
      "Epoch [435/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000509363\n",
      "Epoch [435/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000508238\n",
      "Epoch [435/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000507115\n",
      "Epoch [436/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000505992\n",
      "Epoch [436/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000504872\n",
      "Epoch [436/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000503753\n",
      "Epoch [436/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000502635\n",
      "Epoch [436/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000501519\n",
      "Epoch [436/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0097, LR: 0.0000500404\n",
      "Epoch [436/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000499291\n",
      "Epoch [436/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000498179\n",
      "Epoch [436/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000497068\n",
      "Epoch [436/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000495960\n",
      "Epoch [436/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000494852\n",
      "Epoch [437/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000493747\n",
      "Epoch [437/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000492642\n",
      "Epoch [437/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000491539\n",
      "Epoch [437/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000490438\n",
      "Epoch [437/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000489338\n",
      "Epoch [437/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000488240\n",
      "Epoch [437/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000487143\n",
      "Epoch [437/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000486047\n",
      "Epoch [437/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000484953\n",
      "Epoch [437/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000483861\n",
      "Epoch [437/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000482770\n",
      "Epoch [438/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000481680\n",
      "Epoch [438/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000480592\n",
      "Epoch [438/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000479506\n",
      "Epoch [438/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000478421\n",
      "Epoch [438/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0000477338\n",
      "Epoch [438/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000476255\n",
      "Epoch [438/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000475175\n",
      "Epoch [438/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000474096\n",
      "Epoch [438/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000473018\n",
      "Epoch [438/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000471942\n",
      "Epoch [438/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000470868\n",
      "Epoch [439/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000469795\n",
      "Epoch [439/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000468723\n",
      "Epoch [439/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000467653\n",
      "Epoch [439/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000466585\n",
      "Epoch [439/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000465518\n",
      "Epoch [439/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000464452\n",
      "Epoch [439/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000463388\n",
      "Epoch [439/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000462325\n",
      "Epoch [439/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000461264\n",
      "Epoch [439/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000460205\n",
      "Epoch [439/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000459146\n",
      "Epoch [440/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000458090\n",
      "Epoch [440/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000457035\n",
      "Epoch [440/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000455981\n",
      "Epoch [440/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000454929\n",
      "Epoch [440/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000453878\n",
      "Epoch [440/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000452829\n",
      "Epoch [440/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000451782\n",
      "Epoch [440/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000450736\n",
      "Epoch [440/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0102, LR: 0.0000449691\n",
      "Epoch [440/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000448648\n",
      "Epoch [440/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000447606\n",
      "Epoch [441/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000446566\n",
      "Epoch [441/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000445528\n",
      "Epoch [441/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000444491\n",
      "Epoch [441/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0100, LR: 0.0000443455\n",
      "Epoch [441/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000442421\n",
      "Epoch [441/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000441388\n",
      "Epoch [441/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000440357\n",
      "Epoch [441/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000439328\n",
      "Epoch [441/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000438300\n",
      "Epoch [441/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000437273\n",
      "Epoch [441/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000436248\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 441: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.035 sec , Memory Usage: 393.13 MB\n",
      "Epoch [442/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000435224\n",
      "Epoch [442/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000434202\n",
      "Epoch [442/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000433182\n",
      "Epoch [442/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000432163\n",
      "Epoch [442/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000431145\n",
      "Epoch [442/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000430129\n",
      "Epoch [442/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000429115\n",
      "Epoch [442/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000428102\n",
      "Epoch [442/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000427090\n",
      "Epoch [442/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000426080\n",
      "Epoch [442/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000425072\n",
      "Epoch [443/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000424065\n",
      "Epoch [443/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0098, LR: 0.0000423059\n",
      "Epoch [443/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0098, LR: 0.0000422055\n",
      "Epoch [443/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000421053\n",
      "Epoch [443/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000420052\n",
      "Epoch [443/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000419053\n",
      "Epoch [443/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000418055\n",
      "Epoch [443/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000417058\n",
      "Epoch [443/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000416063\n",
      "Epoch [443/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000415070\n",
      "Epoch [443/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000414078\n",
      "Epoch [444/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000413088\n",
      "Epoch [444/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000412099\n",
      "Epoch [444/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000411112\n",
      "Epoch [444/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000410126\n",
      "Epoch [444/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000409141\n",
      "Epoch [444/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000408159\n",
      "Epoch [444/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000407177\n",
      "Epoch [444/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0000406198\n",
      "Epoch [444/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000405219\n",
      "Epoch [444/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000404243\n",
      "Epoch [444/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000403267\n",
      "Epoch [445/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000402294\n",
      "Epoch [445/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000401322\n",
      "Epoch [445/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000400351\n",
      "Epoch [445/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000399382\n",
      "Epoch [445/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000398414\n",
      "Epoch [445/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000397448\n",
      "Epoch [445/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000396483\n",
      "Epoch [445/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000395520\n",
      "Epoch [445/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000394559\n",
      "Epoch [445/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000393599\n",
      "Epoch [445/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000392640\n",
      "Epoch [446/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000391683\n",
      "Epoch [446/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000390728\n",
      "Epoch [446/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000389774\n",
      "Epoch [446/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000388821\n",
      "Epoch [446/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000387870\n",
      "Epoch [446/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000386921\n",
      "Epoch [446/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000385973\n",
      "Epoch [446/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000385027\n",
      "Epoch [446/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000384082\n",
      "Epoch [446/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000383139\n",
      "Epoch [446/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000382197\n",
      "Epoch [447/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000381257\n",
      "Epoch [447/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000380318\n",
      "Epoch [447/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000379381\n",
      "Epoch [447/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000378445\n",
      "Epoch [447/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0000377511\n",
      "Epoch [447/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000376578\n",
      "Epoch [447/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0097, LR: 0.0000375647\n",
      "Epoch [447/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000374717\n",
      "Epoch [447/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000373789\n",
      "Epoch [447/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000372863\n",
      "Epoch [447/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000371938\n",
      "Epoch [448/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000371014\n",
      "Epoch [448/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000370092\n",
      "Epoch [448/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000369172\n",
      "Epoch [448/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0000368253\n",
      "Epoch [448/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000367336\n",
      "Epoch [448/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000366420\n",
      "Epoch [448/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000365505\n",
      "Epoch [448/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000364593\n",
      "Epoch [448/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000363681\n",
      "Epoch [448/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000362772\n",
      "Epoch [448/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000361863\n",
      "Epoch [449/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000360957\n",
      "Epoch [449/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000360052\n",
      "Epoch [449/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000359148\n",
      "Epoch [449/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000358246\n",
      "Epoch [449/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000357345\n",
      "Epoch [449/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000356446\n",
      "Epoch [449/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000355549\n",
      "Epoch [449/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000354653\n",
      "Epoch [449/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000353758\n",
      "Epoch [449/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000352865\n",
      "Epoch [449/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000351974\n",
      "Epoch [450/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000351084\n",
      "Epoch [450/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000350196\n",
      "Epoch [450/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000349309\n",
      "Epoch [450/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000348424\n",
      "Epoch [450/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000347540\n",
      "Epoch [450/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000346658\n",
      "Epoch [450/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000345777\n",
      "Epoch [450/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000344898\n",
      "Epoch [450/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000344021\n",
      "Epoch [450/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000343145\n",
      "Epoch [450/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000342270\n",
      "Epoch [451/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000341397\n",
      "Epoch [451/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000340526\n",
      "Epoch [451/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000339656\n",
      "Epoch [451/500], Batch [40/110], Train Loss: 0.0041, Val Loss: 0.0097, LR: 0.0000338788\n",
      "Epoch [451/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000337921\n",
      "Epoch [451/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000337056\n",
      "Epoch [451/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000336192\n",
      "Epoch [451/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000335330\n",
      "Epoch [451/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0000334469\n",
      "Epoch [451/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000333610\n",
      "Epoch [451/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000332752\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 451: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.042 sec , Memory Usage: 393.13 MB\n",
      "Epoch [452/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000331896\n",
      "Epoch [452/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000331042\n",
      "Epoch [452/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0000330189\n",
      "Epoch [452/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000329337\n",
      "Epoch [452/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000328487\n",
      "Epoch [452/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000327639\n",
      "Epoch [452/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000326792\n",
      "Epoch [452/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000325947\n",
      "Epoch [452/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000325103\n",
      "Epoch [452/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000324261\n",
      "Epoch [452/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000323421\n",
      "Epoch [453/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000322581\n",
      "Epoch [453/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000321744\n",
      "Epoch [453/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0000320908\n",
      "Epoch [453/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000320073\n",
      "Epoch [453/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000319240\n",
      "Epoch [453/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000318409\n",
      "Epoch [453/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000317579\n",
      "Epoch [453/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000316751\n",
      "Epoch [453/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000315924\n",
      "Epoch [453/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000315099\n",
      "Epoch [453/500], Batch [110/110], Train Loss: 0.0014, Val Loss: 0.0098, LR: 0.0000314275\n",
      "Epoch [454/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000313453\n",
      "Epoch [454/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000312633\n",
      "Epoch [454/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000311814\n",
      "Epoch [454/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000310996\n",
      "Epoch [454/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0097, LR: 0.0000310180\n",
      "Epoch [454/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000309366\n",
      "Epoch [454/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000308553\n",
      "Epoch [454/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000307742\n",
      "Epoch [454/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000306932\n",
      "Epoch [454/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000306124\n",
      "Epoch [454/500], Batch [110/110], Train Loss: 0.0021, Val Loss: 0.0097, LR: 0.0000305317\n",
      "Epoch [455/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000304512\n",
      "Epoch [455/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000303708\n",
      "Epoch [455/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000302906\n",
      "Epoch [455/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000302106\n",
      "Epoch [455/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000301307\n",
      "Epoch [455/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000300510\n",
      "Epoch [455/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000299714\n",
      "Epoch [455/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000298920\n",
      "Epoch [455/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000298127\n",
      "Epoch [455/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0000297336\n",
      "Epoch [455/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000296546\n",
      "Epoch [456/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000295758\n",
      "Epoch [456/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000294972\n",
      "Epoch [456/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000294187\n",
      "Epoch [456/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0101, LR: 0.0000293403\n",
      "Epoch [456/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000292622\n",
      "Epoch [456/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000291841\n",
      "Epoch [456/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000291062\n",
      "Epoch [456/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000290285\n",
      "Epoch [456/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000289510\n",
      "Epoch [456/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000288736\n",
      "Epoch [456/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000287963\n",
      "Epoch [457/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000287192\n",
      "Epoch [457/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0000286423\n",
      "Epoch [457/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0000285655\n",
      "Epoch [457/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0000284888\n",
      "Epoch [457/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000284124\n",
      "Epoch [457/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000283360\n",
      "Epoch [457/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000282599\n",
      "Epoch [457/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000281839\n",
      "Epoch [457/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000281080\n",
      "Epoch [457/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000280323\n",
      "Epoch [457/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0103, LR: 0.0000279568\n",
      "Epoch [458/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000278814\n",
      "Epoch [458/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000278062\n",
      "Epoch [458/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0102, LR: 0.0000277311\n",
      "Epoch [458/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000276562\n",
      "Epoch [458/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000275814\n",
      "Epoch [458/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000275068\n",
      "Epoch [458/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000274323\n",
      "Epoch [458/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000273580\n",
      "Epoch [458/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000272839\n",
      "Epoch [458/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000272099\n",
      "Epoch [458/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000271361\n",
      "Epoch [459/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000270624\n",
      "Epoch [459/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000269889\n",
      "Epoch [459/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0098, LR: 0.0000269155\n",
      "Epoch [459/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000268423\n",
      "Epoch [459/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000267693\n",
      "Epoch [459/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000266964\n",
      "Epoch [459/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000266237\n",
      "Epoch [459/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000265511\n",
      "Epoch [459/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000264786\n",
      "Epoch [459/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000264064\n",
      "Epoch [459/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000263343\n",
      "Epoch [460/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000262623\n",
      "Epoch [460/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000261905\n",
      "Epoch [460/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000261189\n",
      "Epoch [460/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000260474\n",
      "Epoch [460/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000259760\n",
      "Epoch [460/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000259049\n",
      "Epoch [460/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000258338\n",
      "Epoch [460/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000257630\n",
      "Epoch [460/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000256923\n",
      "Epoch [460/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000256217\n",
      "Epoch [460/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0000255513\n",
      "Epoch [461/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000254811\n",
      "Epoch [461/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000254110\n",
      "Epoch [461/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000253411\n",
      "Epoch [461/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000252713\n",
      "Epoch [461/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000252017\n",
      "Epoch [461/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000251323\n",
      "Epoch [461/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0101, LR: 0.0000250630\n",
      "Epoch [461/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000249938\n",
      "Epoch [461/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000249248\n",
      "Epoch [461/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000248560\n",
      "Epoch [461/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000247873\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 461: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.040 sec , Memory Usage: 359.45 MB\n",
      "Epoch [462/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000247188\n",
      "Epoch [462/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000246505\n",
      "Epoch [462/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000245823\n",
      "Epoch [462/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000245142\n",
      "Epoch [462/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000244463\n",
      "Epoch [462/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0000243786\n",
      "Epoch [462/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000243110\n",
      "Epoch [462/500], Batch [80/110], Train Loss: 0.0020, Val Loss: 0.0099, LR: 0.0000242436\n",
      "Epoch [462/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000241763\n",
      "Epoch [462/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000241092\n",
      "Epoch [462/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000240423\n",
      "Epoch [463/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000239755\n",
      "Epoch [463/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000239089\n",
      "Epoch [463/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000238424\n",
      "Epoch [463/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000237761\n",
      "Epoch [463/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000237099\n",
      "Epoch [463/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000236439\n",
      "Epoch [463/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000235781\n",
      "Epoch [463/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000235124\n",
      "Epoch [463/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0098, LR: 0.0000234468\n",
      "Epoch [463/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0098, LR: 0.0000233815\n",
      "Epoch [463/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000233162\n",
      "Epoch [464/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000232512\n",
      "Epoch [464/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000231863\n",
      "Epoch [464/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000231215\n",
      "Epoch [464/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000230569\n",
      "Epoch [464/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000229925\n",
      "Epoch [464/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000229282\n",
      "Epoch [464/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000228641\n",
      "Epoch [464/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000228001\n",
      "Epoch [464/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000227363\n",
      "Epoch [464/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000226727\n",
      "Epoch [464/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0000226092\n",
      "Epoch [465/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000225459\n",
      "Epoch [465/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000224827\n",
      "Epoch [465/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000224197\n",
      "Epoch [465/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000223568\n",
      "Epoch [465/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000222941\n",
      "Epoch [465/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000222316\n",
      "Epoch [465/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000221692\n",
      "Epoch [465/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0000221069\n",
      "Epoch [465/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0000220449\n",
      "Epoch [465/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000219830\n",
      "Epoch [465/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000219212\n",
      "Epoch [466/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000218596\n",
      "Epoch [466/500], Batch [20/110], Train Loss: 0.0070, Val Loss: 0.0093, LR: 0.0000217982\n",
      "Epoch [466/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0000217369\n",
      "Epoch [466/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0000216757\n",
      "Epoch [466/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000216148\n",
      "Epoch [466/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000215540\n",
      "Epoch [466/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000214933\n",
      "Epoch [466/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000214328\n",
      "Epoch [466/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000213725\n",
      "Epoch [466/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000213123\n",
      "Epoch [466/500], Batch [110/110], Train Loss: 0.0083, Val Loss: 0.0098, LR: 0.0000212523\n",
      "Epoch [467/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000211924\n",
      "Epoch [467/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000211327\n",
      "Epoch [467/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000210732\n",
      "Epoch [467/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000210138\n",
      "Epoch [467/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0103, LR: 0.0000209545\n",
      "Epoch [467/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000208955\n",
      "Epoch [467/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0103, LR: 0.0000208365\n",
      "Epoch [467/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000207778\n",
      "Epoch [467/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000207192\n",
      "Epoch [467/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000206607\n",
      "Epoch [467/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000206025\n",
      "Epoch [468/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000205443\n",
      "Epoch [468/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000204864\n",
      "Epoch [468/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000204285\n",
      "Epoch [468/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000203709\n",
      "Epoch [468/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000203134\n",
      "Epoch [468/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000202561\n",
      "Epoch [468/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000201989\n",
      "Epoch [468/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000201419\n",
      "Epoch [468/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000200850\n",
      "Epoch [468/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000200283\n",
      "Epoch [468/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0000199717\n",
      "Epoch [469/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000199154\n",
      "Epoch [469/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000198591\n",
      "Epoch [469/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000198031\n",
      "Epoch [469/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000197471\n",
      "Epoch [469/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000196914\n",
      "Epoch [469/500], Batch [60/110], Train Loss: 0.0020, Val Loss: 0.0099, LR: 0.0000196358\n",
      "Epoch [469/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000195804\n",
      "Epoch [469/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000195251\n",
      "Epoch [469/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000194700\n",
      "Epoch [469/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0099, LR: 0.0000194150\n",
      "Epoch [469/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000193602\n",
      "Epoch [470/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000193055\n",
      "Epoch [470/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0098, LR: 0.0000192511\n",
      "Epoch [470/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0098, LR: 0.0000191967\n",
      "Epoch [470/500], Batch [40/110], Train Loss: 0.0227, Val Loss: 0.0097, LR: 0.0000191426\n",
      "Epoch [470/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000190886\n",
      "Epoch [470/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000190347\n",
      "Epoch [470/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000189810\n",
      "Epoch [470/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000189275\n",
      "Epoch [470/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000188741\n",
      "Epoch [470/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0000188209\n",
      "Epoch [470/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0000187678\n",
      "Epoch [471/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000187149\n",
      "Epoch [471/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0098, LR: 0.0000186622\n",
      "Epoch [471/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000186096\n",
      "Epoch [471/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0000185572\n",
      "Epoch [471/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0000185049\n",
      "Epoch [471/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000184528\n",
      "Epoch [471/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000184008\n",
      "Epoch [471/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000183490\n",
      "Epoch [471/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0000182974\n",
      "Epoch [471/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000182459\n",
      "Epoch [471/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000181946\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 471: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.035 sec , Memory Usage: 359.45 MB\n",
      "Epoch [472/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000181435\n",
      "Epoch [472/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000180925\n",
      "Epoch [472/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000180416\n",
      "Epoch [472/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000179910\n",
      "Epoch [472/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000179404\n",
      "Epoch [472/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000178901\n",
      "Epoch [472/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000178399\n",
      "Epoch [472/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000177898\n",
      "Epoch [472/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000177399\n",
      "Epoch [472/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000176902\n",
      "Epoch [472/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000176407\n",
      "Epoch [473/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000175912\n",
      "Epoch [473/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000175420\n",
      "Epoch [473/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000174929\n",
      "Epoch [473/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000174440\n",
      "Epoch [473/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0000173952\n",
      "Epoch [473/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0000173466\n",
      "Epoch [473/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0000172981\n",
      "Epoch [473/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0000172499\n",
      "Epoch [473/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0000172017\n",
      "Epoch [473/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0000171537\n",
      "Epoch [473/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0000171059\n",
      "Epoch [474/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0106, LR: 0.0000170583\n",
      "Epoch [474/500], Batch [20/110], Train Loss: 0.0226, Val Loss: 0.0106, LR: 0.0000170108\n",
      "Epoch [474/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0000169634\n",
      "Epoch [474/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0000169163\n",
      "Epoch [474/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0000168692\n",
      "Epoch [474/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000168224\n",
      "Epoch [474/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000167757\n",
      "Epoch [474/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0000167291\n",
      "Epoch [474/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0000166827\n",
      "Epoch [474/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0000166365\n",
      "Epoch [474/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0000165905\n",
      "Epoch [475/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0000165446\n",
      "Epoch [475/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0000164988\n",
      "Epoch [475/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0000164532\n",
      "Epoch [475/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000164078\n",
      "Epoch [475/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000163625\n",
      "Epoch [475/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0105, LR: 0.0000163174\n",
      "Epoch [475/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000162725\n",
      "Epoch [475/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0000162277\n",
      "Epoch [475/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000161831\n",
      "Epoch [475/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0105, LR: 0.0000161386\n",
      "Epoch [475/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000160943\n",
      "Epoch [476/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000160501\n",
      "Epoch [476/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000160061\n",
      "Epoch [476/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000159623\n",
      "Epoch [476/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0000159186\n",
      "Epoch [476/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0103, LR: 0.0000158751\n",
      "Epoch [476/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000158318\n",
      "Epoch [476/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000157886\n",
      "Epoch [476/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000157455\n",
      "Epoch [476/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000157027\n",
      "Epoch [476/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000156599\n",
      "Epoch [476/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000156174\n",
      "Epoch [477/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000155750\n",
      "Epoch [477/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000155328\n",
      "Epoch [477/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0000154907\n",
      "Epoch [477/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000154488\n",
      "Epoch [477/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000154070\n",
      "Epoch [477/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000153654\n",
      "Epoch [477/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000153240\n",
      "Epoch [477/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000152827\n",
      "Epoch [477/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000152416\n",
      "Epoch [477/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000152006\n",
      "Epoch [477/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000151598\n",
      "Epoch [478/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000151192\n",
      "Epoch [478/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0000150787\n",
      "Epoch [478/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000150384\n",
      "Epoch [478/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000149982\n",
      "Epoch [478/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000149582\n",
      "Epoch [478/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000149184\n",
      "Epoch [478/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000148787\n",
      "Epoch [478/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000148392\n",
      "Epoch [478/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000147998\n",
      "Epoch [478/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000147606\n",
      "Epoch [478/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000147216\n",
      "Epoch [479/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000146827\n",
      "Epoch [479/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000146440\n",
      "Epoch [479/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0102, LR: 0.0000146054\n",
      "Epoch [479/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000145670\n",
      "Epoch [479/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000145288\n",
      "Epoch [479/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000144907\n",
      "Epoch [479/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000144528\n",
      "Epoch [479/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000144150\n",
      "Epoch [479/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000143774\n",
      "Epoch [479/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000143400\n",
      "Epoch [479/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000143027\n",
      "Epoch [480/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000142656\n",
      "Epoch [480/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000142286\n",
      "Epoch [480/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000141918\n",
      "Epoch [480/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000141552\n",
      "Epoch [480/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000141187\n",
      "Epoch [480/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000140824\n",
      "Epoch [480/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000140463\n",
      "Epoch [480/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000140103\n",
      "Epoch [480/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000139744\n",
      "Epoch [480/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000139387\n",
      "Epoch [480/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0103, LR: 0.0000139032\n",
      "Epoch [481/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000138679\n",
      "Epoch [481/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000138327\n",
      "Epoch [481/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000137976\n",
      "Epoch [481/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000137628\n",
      "Epoch [481/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000137280\n",
      "Epoch [481/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000136935\n",
      "Epoch [481/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000136591\n",
      "Epoch [481/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000136249\n",
      "Epoch [481/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000135908\n",
      "Epoch [481/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000135569\n",
      "Epoch [481/500], Batch [110/110], Train Loss: 0.0076, Val Loss: 0.0102, LR: 0.0000135231\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 481: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 359.45 MB\n",
      "Epoch [482/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000134895\n",
      "Epoch [482/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000134561\n",
      "Epoch [482/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0102, LR: 0.0000134228\n",
      "Epoch [482/500], Batch [40/110], Train Loss: 0.0022, Val Loss: 0.0102, LR: 0.0000133897\n",
      "Epoch [482/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000133567\n",
      "Epoch [482/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000133240\n",
      "Epoch [482/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000132913\n",
      "Epoch [482/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000132589\n",
      "Epoch [482/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000132265\n",
      "Epoch [482/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000131944\n",
      "Epoch [482/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0102, LR: 0.0000131624\n",
      "Epoch [483/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0101, LR: 0.0000131306\n",
      "Epoch [483/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000130989\n",
      "Epoch [483/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000130674\n",
      "Epoch [483/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000130360\n",
      "Epoch [483/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000130049\n",
      "Epoch [483/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0102, LR: 0.0000129738\n",
      "Epoch [483/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000129430\n",
      "Epoch [483/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000129123\n",
      "Epoch [483/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0102, LR: 0.0000128817\n",
      "Epoch [483/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000128513\n",
      "Epoch [483/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000128211\n",
      "Epoch [484/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000127910\n",
      "Epoch [484/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000127611\n",
      "Epoch [484/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000127314\n",
      "Epoch [484/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000127018\n",
      "Epoch [484/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000126724\n",
      "Epoch [484/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000126431\n",
      "Epoch [484/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000126140\n",
      "Epoch [484/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000125851\n",
      "Epoch [484/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000125563\n",
      "Epoch [484/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000125277\n",
      "Epoch [484/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000124992\n",
      "Epoch [485/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000124710\n",
      "Epoch [485/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0000124428\n",
      "Epoch [485/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000124148\n",
      "Epoch [485/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000123870\n",
      "Epoch [485/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000123594\n",
      "Epoch [485/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000123319\n",
      "Epoch [485/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000123045\n",
      "Epoch [485/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000122774\n",
      "Epoch [485/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000122504\n",
      "Epoch [485/500], Batch [100/110], Train Loss: 0.0022, Val Loss: 0.0101, LR: 0.0000122235\n",
      "Epoch [485/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000121968\n",
      "Epoch [486/500], Batch [10/110], Train Loss: 0.0197, Val Loss: 0.0100, LR: 0.0000121703\n",
      "Epoch [486/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000121439\n",
      "Epoch [486/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000121177\n",
      "Epoch [486/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000120917\n",
      "Epoch [486/500], Batch [50/110], Train Loss: 0.0010, Val Loss: 0.0100, LR: 0.0000120658\n",
      "Epoch [486/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000120401\n",
      "Epoch [486/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000120145\n",
      "Epoch [486/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000119891\n",
      "Epoch [486/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000119639\n",
      "Epoch [486/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000119388\n",
      "Epoch [486/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0100, LR: 0.0000119139\n",
      "Epoch [487/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000118891\n",
      "Epoch [487/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000118645\n",
      "Epoch [487/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000118401\n",
      "Epoch [487/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000118158\n",
      "Epoch [487/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000117917\n",
      "Epoch [487/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000117677\n",
      "Epoch [487/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000117439\n",
      "Epoch [487/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000117203\n",
      "Epoch [487/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000116968\n",
      "Epoch [487/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0000116735\n",
      "Epoch [487/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000116504\n",
      "Epoch [488/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000116274\n",
      "Epoch [488/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000116045\n",
      "Epoch [488/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000115819\n",
      "Epoch [488/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000115594\n",
      "Epoch [488/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000115370\n",
      "Epoch [488/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000115148\n",
      "Epoch [488/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000114928\n",
      "Epoch [488/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000114710\n",
      "Epoch [488/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000114493\n",
      "Epoch [488/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000114277\n",
      "Epoch [488/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000114063\n",
      "Epoch [489/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000113851\n",
      "Epoch [489/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000113641\n",
      "Epoch [489/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000113432\n",
      "Epoch [489/500], Batch [40/110], Train Loss: 0.0015, Val Loss: 0.0099, LR: 0.0000113224\n",
      "Epoch [489/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000113019\n",
      "Epoch [489/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000112815\n",
      "Epoch [489/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000112612\n",
      "Epoch [489/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000112411\n",
      "Epoch [489/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000112212\n",
      "Epoch [489/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000112014\n",
      "Epoch [489/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000111818\n",
      "Epoch [490/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000111624\n",
      "Epoch [490/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0000111431\n",
      "Epoch [490/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0000111240\n",
      "Epoch [490/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000111050\n",
      "Epoch [490/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000110862\n",
      "Epoch [490/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0000110676\n",
      "Epoch [490/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0098, LR: 0.0000110491\n",
      "Epoch [490/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000110308\n",
      "Epoch [490/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000110126\n",
      "Epoch [490/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000109946\n",
      "Epoch [490/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000109768\n",
      "Epoch [491/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000109591\n",
      "Epoch [491/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0000109416\n",
      "Epoch [491/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000109242\n",
      "Epoch [491/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000109070\n",
      "Epoch [491/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000108900\n",
      "Epoch [491/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000108731\n",
      "Epoch [491/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000108564\n",
      "Epoch [491/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000108399\n",
      "Epoch [491/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0000108235\n",
      "Epoch [491/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0000108073\n",
      "Epoch [491/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0000107912\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 491: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.039 sec , Memory Usage: 359.45 MB\n",
      "Epoch [492/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000107753\n",
      "Epoch [492/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0102, LR: 0.0000107596\n",
      "Epoch [492/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000107440\n",
      "Epoch [492/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000107286\n",
      "Epoch [492/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000107133\n",
      "Epoch [492/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000106983\n",
      "Epoch [492/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000106833\n",
      "Epoch [492/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000106686\n",
      "Epoch [492/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000106539\n",
      "Epoch [492/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0102, LR: 0.0000106395\n",
      "Epoch [492/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000106252\n",
      "Epoch [493/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000106111\n",
      "Epoch [493/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000105971\n",
      "Epoch [493/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0105, LR: 0.0000105833\n",
      "Epoch [493/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000105697\n",
      "Epoch [493/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000105562\n",
      "Epoch [493/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000105429\n",
      "Epoch [493/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0000105297\n",
      "Epoch [493/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000105167\n",
      "Epoch [493/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000105039\n",
      "Epoch [493/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000104912\n",
      "Epoch [493/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000104787\n",
      "Epoch [494/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000104663\n",
      "Epoch [494/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0000104542\n",
      "Epoch [494/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000104421\n",
      "Epoch [494/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000104303\n",
      "Epoch [494/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0103, LR: 0.0000104186\n",
      "Epoch [494/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0103, LR: 0.0000104070\n",
      "Epoch [494/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000103956\n",
      "Epoch [494/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000103844\n",
      "Epoch [494/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000103733\n",
      "Epoch [494/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000103624\n",
      "Epoch [494/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000103517\n",
      "Epoch [495/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000103411\n",
      "Epoch [495/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000103307\n",
      "Epoch [495/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000103205\n",
      "Epoch [495/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000103104\n",
      "Epoch [495/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000103004\n",
      "Epoch [495/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000102907\n",
      "Epoch [495/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000102811\n",
      "Epoch [495/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000102716\n",
      "Epoch [495/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000102623\n",
      "Epoch [495/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000102532\n",
      "Epoch [495/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000102443\n",
      "Epoch [496/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000102355\n",
      "Epoch [496/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0103, LR: 0.0000102268\n",
      "Epoch [496/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000102183\n",
      "Epoch [496/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000102100\n",
      "Epoch [496/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000102019\n",
      "Epoch [496/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000101939\n",
      "Epoch [496/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000101860\n",
      "Epoch [496/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000101784\n",
      "Epoch [496/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000101709\n",
      "Epoch [496/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000101635\n",
      "Epoch [496/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000101563\n",
      "Epoch [497/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000101493\n",
      "Epoch [497/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000101424\n",
      "Epoch [497/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000101357\n",
      "Epoch [497/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000101292\n",
      "Epoch [497/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000101228\n",
      "Epoch [497/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0000101166\n",
      "Epoch [497/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000101105\n",
      "Epoch [497/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000101047\n",
      "Epoch [497/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100989\n",
      "Epoch [497/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100933\n",
      "Epoch [497/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100879\n",
      "Epoch [498/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100827\n",
      "Epoch [498/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0102, LR: 0.0000100776\n",
      "Epoch [498/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100727\n",
      "Epoch [498/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100679\n",
      "Epoch [498/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000100633\n",
      "Epoch [498/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000100589\n",
      "Epoch [498/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000100546\n",
      "Epoch [498/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000100505\n",
      "Epoch [498/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000100465\n",
      "Epoch [498/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0000100427\n",
      "Epoch [498/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000100391\n",
      "Epoch [499/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000100356\n",
      "Epoch [499/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000100323\n",
      "Epoch [499/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000100292\n",
      "Epoch [499/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0104, LR: 0.0000100262\n",
      "Epoch [499/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0000100233\n",
      "Epoch [499/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000100207\n",
      "Epoch [499/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000100182\n",
      "Epoch [499/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000100158\n",
      "Epoch [499/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000100136\n",
      "Epoch [499/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100116\n",
      "Epoch [499/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100098\n",
      "Epoch [500/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100081\n",
      "Epoch [500/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100065\n",
      "Epoch [500/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100052\n",
      "Epoch [500/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0102, LR: 0.0000100040\n",
      "Epoch [500/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0000100029\n",
      "Epoch [500/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100020\n",
      "Epoch [500/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100013\n",
      "Epoch [500/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100007\n",
      "Epoch [500/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100003\n",
      "Epoch [500/500], Batch [100/110], Train Loss: 0.0026, Val Loss: 0.0102, LR: 0.0000100001\n",
      "Epoch [500/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0000100000\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 361.75 MB\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "epochs = 500\n",
    "SL = len(train_loader) * epochs\n",
    "model = MODEL().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=SL, eta_min=1e-5)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"=========== TP: {total_params:,} ===========\")\n",
    "\n",
    "train(model, criterion, optimizer, scheduler, epochs, train_loader, val_loader, test_loader)\n",
    "RES = test(model, test_loader)\n",
    "print(RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "456f26ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== SEED: 28 , FOLD: 1/4, D: cpu ===========\n",
      " Label\n",
      "1    5675\n",
      "0    4325\n",
      "Name: count, dtype: int64\n",
      "=========== TP: 2,879 ===========\n",
      "Epoch [1/500], Batch [10/110], Train Loss: 0.7080, Val Loss: 0.6806, LR: 0.0009999999\n",
      "Epoch [1/500], Batch [20/110], Train Loss: 0.6097, Val Loss: 0.6747, LR: 0.0009999997\n",
      "Epoch [1/500], Batch [30/110], Train Loss: 0.6234, Val Loss: 0.6648, LR: 0.0009999993\n",
      "Epoch [1/500], Batch [40/110], Train Loss: 0.6848, Val Loss: 0.6483, LR: 0.0009999987\n",
      "Epoch [1/500], Batch [50/110], Train Loss: 0.6563, Val Loss: 0.6167, LR: 0.0009999980\n",
      "Epoch [1/500], Batch [60/110], Train Loss: 0.5491, Val Loss: 0.5572, LR: 0.0009999971\n",
      "Epoch [1/500], Batch [70/110], Train Loss: 0.4208, Val Loss: 0.4740, LR: 0.0009999960\n",
      "Epoch [1/500], Batch [80/110], Train Loss: 0.4427, Val Loss: 0.3834, LR: 0.0009999948\n",
      "Epoch [1/500], Batch [90/110], Train Loss: 0.3122, Val Loss: 0.2946, LR: 0.0009999935\n",
      "Epoch [1/500], Batch [100/110], Train Loss: 0.1351, Val Loss: 0.2197, LR: 0.0009999919\n",
      "Epoch [1/500], Batch [110/110], Train Loss: 0.1719, Val Loss: 0.1641, LR: 0.0009999902\n",
      "Confusion Matrix:\n",
      "[[577  60]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99827   0.90581   0.94979       637\n",
      "           1    0.93492   0.99884   0.96583       863\n",
      "\n",
      "    accuracy                        0.95933      1500\n",
      "   macro avg    0.96660   0.95232   0.95781      1500\n",
      "weighted avg    0.96182   0.95933   0.95902      1500\n",
      "\n",
      "Total Errors: 61\n",
      "Index: 28, Predicted: 1, Actual: 0\n",
      "Index: 34, Predicted: 1, Actual: 0\n",
      "Index: 41, Predicted: 1, Actual: 0\n",
      "Index: 44, Predicted: 1, Actual: 0\n",
      "Index: 63, Predicted: 1, Actual: 0\n",
      "Epoch 1: OK- Accuracy: 0.95933, Precision: 0.93492, Recall: 0.99884, F1: 0.96583, ROC AUC: 0.95232, AUPR (PR-AUC): 0.93451, Sensitivity: 0.99884, Specificity: 0.90581, Far: 0.09419152276295134, False Positive Rate (FPR): 0.09419, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 198.66 MB\n",
      "Epoch [2/500], Batch [10/110], Train Loss: 0.1104, Val Loss: 0.1349, LR: 0.0009999884\n",
      "Epoch [2/500], Batch [20/110], Train Loss: 0.0927, Val Loss: 0.1226, LR: 0.0009999864\n",
      "Epoch [2/500], Batch [30/110], Train Loss: 0.1623, Val Loss: 0.1200, LR: 0.0009999842\n",
      "Epoch [2/500], Batch [40/110], Train Loss: 0.1807, Val Loss: 0.0945, LR: 0.0009999818\n",
      "Epoch [2/500], Batch [50/110], Train Loss: 0.0295, Val Loss: 0.0884, LR: 0.0009999793\n",
      "Epoch [2/500], Batch [60/110], Train Loss: 0.1959, Val Loss: 0.0877, LR: 0.0009999767\n",
      "Epoch [2/500], Batch [70/110], Train Loss: 0.0259, Val Loss: 0.0900, LR: 0.0009999738\n",
      "Epoch [2/500], Batch [80/110], Train Loss: 0.0122, Val Loss: 0.0894, LR: 0.0009999708\n",
      "Epoch [2/500], Batch [90/110], Train Loss: 0.0153, Val Loss: 0.0824, LR: 0.0009999677\n",
      "Epoch [2/500], Batch [100/110], Train Loss: 0.0187, Val Loss: 0.0734, LR: 0.0009999644\n",
      "Epoch [2/500], Batch [110/110], Train Loss: 0.1467, Val Loss: 0.0721, LR: 0.0009999609\n",
      "Epoch [3/500], Batch [10/110], Train Loss: 0.1287, Val Loss: 0.1030, LR: 0.0009999573\n",
      "Epoch [3/500], Batch [20/110], Train Loss: 0.0086, Val Loss: 0.0693, LR: 0.0009999535\n",
      "Epoch [3/500], Batch [30/110], Train Loss: 0.1870, Val Loss: 0.0725, LR: 0.0009999495\n",
      "Epoch [3/500], Batch [40/110], Train Loss: 0.1030, Val Loss: 0.0661, LR: 0.0009999454\n",
      "Epoch [3/500], Batch [50/110], Train Loss: 0.0117, Val Loss: 0.0720, LR: 0.0009999411\n",
      "Epoch [3/500], Batch [60/110], Train Loss: 0.0091, Val Loss: 0.0651, LR: 0.0009999367\n",
      "Epoch [3/500], Batch [70/110], Train Loss: 0.1169, Val Loss: 0.0705, LR: 0.0009999321\n",
      "Epoch [3/500], Batch [80/110], Train Loss: 0.0140, Val Loss: 0.0587, LR: 0.0009999273\n",
      "Epoch [3/500], Batch [90/110], Train Loss: 0.0091, Val Loss: 0.0656, LR: 0.0009999224\n",
      "Epoch [3/500], Batch [100/110], Train Loss: 0.2009, Val Loss: 0.0603, LR: 0.0009999173\n",
      "Epoch [3/500], Batch [110/110], Train Loss: 0.0210, Val Loss: 0.0595, LR: 0.0009999121\n",
      "Epoch [4/500], Batch [10/110], Train Loss: 0.0084, Val Loss: 0.0714, LR: 0.0009999067\n",
      "Epoch [4/500], Batch [20/110], Train Loss: 0.0105, Val Loss: 0.0670, LR: 0.0009999011\n",
      "Epoch [4/500], Batch [30/110], Train Loss: 0.0111, Val Loss: 0.0577, LR: 0.0009998953\n",
      "Epoch [4/500], Batch [40/110], Train Loss: 0.0024, Val Loss: 0.0542, LR: 0.0009998895\n",
      "Epoch [4/500], Batch [50/110], Train Loss: 0.1681, Val Loss: 0.0645, LR: 0.0009998834\n",
      "Epoch [4/500], Batch [60/110], Train Loss: 0.0109, Val Loss: 0.0580, LR: 0.0009998772\n",
      "Epoch [4/500], Batch [70/110], Train Loss: 0.0666, Val Loss: 0.0487, LR: 0.0009998708\n",
      "Epoch [4/500], Batch [80/110], Train Loss: 0.0339, Val Loss: 0.0478, LR: 0.0009998643\n",
      "Epoch [4/500], Batch [90/110], Train Loss: 0.0076, Val Loss: 0.0620, LR: 0.0009998576\n",
      "Epoch [4/500], Batch [100/110], Train Loss: 0.0566, Val Loss: 0.0513, LR: 0.0009998507\n",
      "Epoch [4/500], Batch [110/110], Train Loss: 0.1062, Val Loss: 0.0590, LR: 0.0009998437\n",
      "Epoch [5/500], Batch [10/110], Train Loss: 0.0496, Val Loss: 0.0468, LR: 0.0009998365\n",
      "Epoch [5/500], Batch [20/110], Train Loss: 0.1402, Val Loss: 0.0629, LR: 0.0009998291\n",
      "Epoch [5/500], Batch [30/110], Train Loss: 0.0253, Val Loss: 0.0453, LR: 0.0009998216\n",
      "Epoch [5/500], Batch [40/110], Train Loss: 0.0221, Val Loss: 0.0621, LR: 0.0009998140\n",
      "Epoch [5/500], Batch [50/110], Train Loss: 0.0118, Val Loss: 0.0450, LR: 0.0009998061\n",
      "Epoch [5/500], Batch [60/110], Train Loss: 0.0252, Val Loss: 0.0528, LR: 0.0009997981\n",
      "Epoch [5/500], Batch [70/110], Train Loss: 0.0023, Val Loss: 0.0485, LR: 0.0009997900\n",
      "Epoch [5/500], Batch [80/110], Train Loss: 0.1405, Val Loss: 0.0531, LR: 0.0009997817\n",
      "Epoch [5/500], Batch [90/110], Train Loss: 0.0253, Val Loss: 0.0466, LR: 0.0009997732\n",
      "Epoch [5/500], Batch [100/110], Train Loss: 0.0609, Val Loss: 0.0434, LR: 0.0009997645\n",
      "Epoch [5/500], Batch [110/110], Train Loss: 0.0132, Val Loss: 0.0408, LR: 0.0009997557\n",
      "Epoch [6/500], Batch [10/110], Train Loss: 0.0144, Val Loss: 0.0487, LR: 0.0009997468\n",
      "Epoch [6/500], Batch [20/110], Train Loss: 0.0051, Val Loss: 0.0446, LR: 0.0009997377\n",
      "Epoch [6/500], Batch [30/110], Train Loss: 0.1176, Val Loss: 0.0513, LR: 0.0009997284\n",
      "Epoch [6/500], Batch [40/110], Train Loss: 0.2374, Val Loss: 0.0463, LR: 0.0009997189\n",
      "Epoch [6/500], Batch [50/110], Train Loss: 0.1249, Val Loss: 0.0390, LR: 0.0009997093\n",
      "Epoch [6/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0430, LR: 0.0009996996\n",
      "Epoch [6/500], Batch [70/110], Train Loss: 0.1438, Val Loss: 0.0394, LR: 0.0009996896\n",
      "Epoch [6/500], Batch [80/110], Train Loss: 0.0065, Val Loss: 0.0379, LR: 0.0009996795\n",
      "Epoch [6/500], Batch [90/110], Train Loss: 0.1073, Val Loss: 0.0513, LR: 0.0009996693\n",
      "Epoch [6/500], Batch [100/110], Train Loss: 0.0136, Val Loss: 0.0398, LR: 0.0009996589\n",
      "Epoch [6/500], Batch [110/110], Train Loss: 0.1289, Val Loss: 0.0562, LR: 0.0009996483\n",
      "Epoch [7/500], Batch [10/110], Train Loss: 0.0702, Val Loss: 0.0384, LR: 0.0009996376\n",
      "Epoch [7/500], Batch [20/110], Train Loss: 0.1098, Val Loss: 0.0534, LR: 0.0009996267\n",
      "Epoch [7/500], Batch [30/110], Train Loss: 0.0068, Val Loss: 0.0351, LR: 0.0009996156\n",
      "Epoch [7/500], Batch [40/110], Train Loss: 0.0769, Val Loss: 0.0360, LR: 0.0009996044\n",
      "Epoch [7/500], Batch [50/110], Train Loss: 0.0020, Val Loss: 0.0470, LR: 0.0009995930\n",
      "Epoch [7/500], Batch [60/110], Train Loss: 0.0340, Val Loss: 0.0349, LR: 0.0009995814\n",
      "Epoch [7/500], Batch [70/110], Train Loss: 0.0034, Val Loss: 0.0447, LR: 0.0009995697\n",
      "Epoch [7/500], Batch [80/110], Train Loss: 0.0026, Val Loss: 0.0536, LR: 0.0009995579\n",
      "Epoch [7/500], Batch [90/110], Train Loss: 0.0424, Val Loss: 0.0340, LR: 0.0009995458\n",
      "Epoch [7/500], Batch [100/110], Train Loss: 0.0055, Val Loss: 0.0416, LR: 0.0009995337\n",
      "Epoch [7/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0409, LR: 0.0009995213\n",
      "Epoch [8/500], Batch [10/110], Train Loss: 0.0107, Val Loss: 0.0408, LR: 0.0009995088\n",
      "Epoch [8/500], Batch [20/110], Train Loss: 0.0079, Val Loss: 0.0423, LR: 0.0009994961\n",
      "Epoch [8/500], Batch [30/110], Train Loss: 0.0027, Val Loss: 0.0363, LR: 0.0009994833\n",
      "Epoch [8/500], Batch [40/110], Train Loss: 0.0087, Val Loss: 0.0389, LR: 0.0009994703\n",
      "Epoch [8/500], Batch [50/110], Train Loss: 0.0063, Val Loss: 0.0334, LR: 0.0009994571\n",
      "Epoch [8/500], Batch [60/110], Train Loss: 0.0857, Val Loss: 0.0404, LR: 0.0009994438\n",
      "Epoch [8/500], Batch [70/110], Train Loss: 0.0734, Val Loss: 0.0387, LR: 0.0009994303\n",
      "Epoch [8/500], Batch [80/110], Train Loss: 0.0121, Val Loss: 0.0345, LR: 0.0009994167\n",
      "Epoch [8/500], Batch [90/110], Train Loss: 0.0015, Val Loss: 0.0392, LR: 0.0009994029\n",
      "Epoch [8/500], Batch [100/110], Train Loss: 0.0093, Val Loss: 0.0338, LR: 0.0009993889\n",
      "Epoch [8/500], Batch [110/110], Train Loss: 0.1262, Val Loss: 0.0565, LR: 0.0009993748\n",
      "Epoch [9/500], Batch [10/110], Train Loss: 0.0042, Val Loss: 0.0477, LR: 0.0009993605\n",
      "Epoch [9/500], Batch [20/110], Train Loss: 0.0712, Val Loss: 0.0324, LR: 0.0009993461\n",
      "Epoch [9/500], Batch [30/110], Train Loss: 0.1013, Val Loss: 0.0481, LR: 0.0009993314\n",
      "Epoch [9/500], Batch [40/110], Train Loss: 0.1171, Val Loss: 0.0315, LR: 0.0009993167\n",
      "Epoch [9/500], Batch [50/110], Train Loss: 0.0024, Val Loss: 0.0506, LR: 0.0009993017\n",
      "Epoch [9/500], Batch [60/110], Train Loss: 0.0057, Val Loss: 0.0396, LR: 0.0009992867\n",
      "Epoch [9/500], Batch [70/110], Train Loss: 0.0168, Val Loss: 0.0333, LR: 0.0009992714\n",
      "Epoch [9/500], Batch [80/110], Train Loss: 0.0236, Val Loss: 0.0329, LR: 0.0009992560\n",
      "Epoch [9/500], Batch [90/110], Train Loss: 0.0049, Val Loss: 0.0319, LR: 0.0009992404\n",
      "Epoch [9/500], Batch [100/110], Train Loss: 0.0164, Val Loss: 0.0321, LR: 0.0009992247\n",
      "Epoch [9/500], Batch [110/110], Train Loss: 0.0173, Val Loss: 0.0367, LR: 0.0009992088\n",
      "Epoch [10/500], Batch [10/110], Train Loss: 0.0431, Val Loss: 0.0298, LR: 0.0009991927\n",
      "Epoch [10/500], Batch [20/110], Train Loss: 0.0026, Val Loss: 0.0307, LR: 0.0009991765\n",
      "Epoch [10/500], Batch [30/110], Train Loss: 0.0030, Val Loss: 0.0425, LR: 0.0009991601\n",
      "Epoch [10/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0300, LR: 0.0009991436\n",
      "Epoch [10/500], Batch [50/110], Train Loss: 0.0193, Val Loss: 0.0297, LR: 0.0009991269\n",
      "Epoch [10/500], Batch [60/110], Train Loss: 0.0020, Val Loss: 0.0366, LR: 0.0009991100\n",
      "Epoch [10/500], Batch [70/110], Train Loss: 0.0026, Val Loss: 0.0365, LR: 0.0009990930\n",
      "Epoch [10/500], Batch [80/110], Train Loss: 0.0169, Val Loss: 0.0318, LR: 0.0009990758\n",
      "Epoch [10/500], Batch [90/110], Train Loss: 0.0870, Val Loss: 0.0387, LR: 0.0009990584\n",
      "Epoch [10/500], Batch [100/110], Train Loss: 0.0022, Val Loss: 0.0284, LR: 0.0009990409\n",
      "Epoch [10/500], Batch [110/110], Train Loss: 0.2832, Val Loss: 0.0394, LR: 0.0009990232\n",
      "Epoch [11/500], Batch [10/110], Train Loss: 0.1356, Val Loss: 0.0305, LR: 0.0009990054\n",
      "Epoch [11/500], Batch [20/110], Train Loss: 0.0180, Val Loss: 0.0367, LR: 0.0009989874\n",
      "Epoch [11/500], Batch [30/110], Train Loss: 0.0146, Val Loss: 0.0287, LR: 0.0009989692\n",
      "Epoch [11/500], Batch [40/110], Train Loss: 0.0095, Val Loss: 0.0302, LR: 0.0009989509\n",
      "Epoch [11/500], Batch [50/110], Train Loss: 0.0593, Val Loss: 0.0309, LR: 0.0009989324\n",
      "Epoch [11/500], Batch [60/110], Train Loss: 0.0025, Val Loss: 0.0354, LR: 0.0009989138\n",
      "Epoch [11/500], Batch [70/110], Train Loss: 0.0131, Val Loss: 0.0280, LR: 0.0009988950\n",
      "Epoch [11/500], Batch [80/110], Train Loss: 0.0201, Val Loss: 0.0319, LR: 0.0009988760\n",
      "Epoch [11/500], Batch [90/110], Train Loss: 0.0180, Val Loss: 0.0394, LR: 0.0009988569\n",
      "Epoch [11/500], Batch [100/110], Train Loss: 0.0040, Val Loss: 0.0274, LR: 0.0009988376\n",
      "Epoch [11/500], Batch [110/110], Train Loss: 0.0022, Val Loss: 0.0274, LR: 0.0009988182\n",
      "Confusion Matrix:\n",
      "[[617  20]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99838   0.96860   0.98327       637\n",
      "           1    0.97732   0.99884   0.98797       863\n",
      "\n",
      "    accuracy                        0.98600      1500\n",
      "   macro avg    0.98785   0.98372   0.98562      1500\n",
      "weighted avg    0.98627   0.98600   0.98597      1500\n",
      "\n",
      "Total Errors: 21\n",
      "Index: 41, Predicted: 1, Actual: 0\n",
      "Index: 313, Predicted: 1, Actual: 0\n",
      "Index: 460, Predicted: 1, Actual: 0\n",
      "Index: 575, Predicted: 1, Actual: 0\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 11: OK- Accuracy: 0.98600, Precision: 0.97732, Recall: 0.99884, F1: 0.98797, ROC AUC: 0.98372, AUPR (PR-AUC): 0.97686, Sensitivity: 0.99884, Specificity: 0.96860, Far: 0.03139717425431711, False Positive Rate (FPR): 0.03140, False Negative Rate (FNR): 0.00116, Runtime: 0.038 sec , Memory Usage: 198.63 MB\n",
      "Epoch [12/500], Batch [10/110], Train Loss: 0.0106, Val Loss: 0.0273, LR: 0.0009987986\n",
      "Epoch [12/500], Batch [20/110], Train Loss: 0.0061, Val Loss: 0.0273, LR: 0.0009987788\n",
      "Epoch [12/500], Batch [30/110], Train Loss: 0.1067, Val Loss: 0.0271, LR: 0.0009987589\n",
      "Epoch [12/500], Batch [40/110], Train Loss: 0.1097, Val Loss: 0.0289, LR: 0.0009987388\n",
      "Epoch [12/500], Batch [50/110], Train Loss: 0.0224, Val Loss: 0.0287, LR: 0.0009987185\n",
      "Epoch [12/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0340, LR: 0.0009986981\n",
      "Epoch [12/500], Batch [70/110], Train Loss: 0.0463, Val Loss: 0.0261, LR: 0.0009986776\n",
      "Epoch [12/500], Batch [80/110], Train Loss: 0.0186, Val Loss: 0.0355, LR: 0.0009986568\n",
      "Epoch [12/500], Batch [90/110], Train Loss: 0.0859, Val Loss: 0.0385, LR: 0.0009986359\n",
      "Epoch [12/500], Batch [100/110], Train Loss: 0.0148, Val Loss: 0.0280, LR: 0.0009986149\n",
      "Epoch [12/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0392, LR: 0.0009985937\n",
      "Epoch [13/500], Batch [10/110], Train Loss: 0.0036, Val Loss: 0.0267, LR: 0.0009985723\n",
      "Epoch [13/500], Batch [20/110], Train Loss: 0.0919, Val Loss: 0.0379, LR: 0.0009985507\n",
      "Epoch [13/500], Batch [30/110], Train Loss: 0.0142, Val Loss: 0.0290, LR: 0.0009985290\n",
      "Epoch [13/500], Batch [40/110], Train Loss: 0.0472, Val Loss: 0.0258, LR: 0.0009985072\n",
      "Epoch [13/500], Batch [50/110], Train Loss: 0.0554, Val Loss: 0.0259, LR: 0.0009984852\n",
      "Epoch [13/500], Batch [60/110], Train Loss: 0.0016, Val Loss: 0.0263, LR: 0.0009984630\n",
      "Epoch [13/500], Batch [70/110], Train Loss: 0.0021, Val Loss: 0.0257, LR: 0.0009984406\n",
      "Epoch [13/500], Batch [80/110], Train Loss: 0.0070, Val Loss: 0.0277, LR: 0.0009984181\n",
      "Epoch [13/500], Batch [90/110], Train Loss: 0.0021, Val Loss: 0.0393, LR: 0.0009983955\n",
      "Epoch [13/500], Batch [100/110], Train Loss: 0.0046, Val Loss: 0.0257, LR: 0.0009983726\n",
      "Epoch [13/500], Batch [110/110], Train Loss: 0.1013, Val Loss: 0.0392, LR: 0.0009983496\n",
      "Epoch [14/500], Batch [10/110], Train Loss: 0.0083, Val Loss: 0.0253, LR: 0.0009983265\n",
      "Epoch [14/500], Batch [20/110], Train Loss: 0.0091, Val Loss: 0.0251, LR: 0.0009983032\n",
      "Epoch [14/500], Batch [30/110], Train Loss: 0.0060, Val Loss: 0.0281, LR: 0.0009982797\n",
      "Epoch [14/500], Batch [40/110], Train Loss: 0.0264, Val Loss: 0.0258, LR: 0.0009982561\n",
      "Epoch [14/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0248, LR: 0.0009982323\n",
      "Epoch [14/500], Batch [60/110], Train Loss: 0.0023, Val Loss: 0.0245, LR: 0.0009982083\n",
      "Epoch [14/500], Batch [70/110], Train Loss: 0.0957, Val Loss: 0.0415, LR: 0.0009981842\n",
      "Epoch [14/500], Batch [80/110], Train Loss: 0.0118, Val Loss: 0.0246, LR: 0.0009981599\n",
      "Epoch [14/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0295, LR: 0.0009981355\n",
      "Epoch [14/500], Batch [100/110], Train Loss: 0.0046, Val Loss: 0.0274, LR: 0.0009981109\n",
      "Epoch [14/500], Batch [110/110], Train Loss: 0.0272, Val Loss: 0.0242, LR: 0.0009980861\n",
      "Epoch [15/500], Batch [10/110], Train Loss: 0.0621, Val Loss: 0.0329, LR: 0.0009980612\n",
      "Epoch [15/500], Batch [20/110], Train Loss: 0.0016, Val Loss: 0.0299, LR: 0.0009980361\n",
      "Epoch [15/500], Batch [30/110], Train Loss: 0.0314, Val Loss: 0.0237, LR: 0.0009980109\n",
      "Epoch [15/500], Batch [40/110], Train Loss: 0.0550, Val Loss: 0.0241, LR: 0.0009979855\n",
      "Epoch [15/500], Batch [50/110], Train Loss: 0.0010, Val Loss: 0.0265, LR: 0.0009979599\n",
      "Epoch [15/500], Batch [60/110], Train Loss: 0.0085, Val Loss: 0.0242, LR: 0.0009979342\n",
      "Epoch [15/500], Batch [70/110], Train Loss: 0.0292, Val Loss: 0.0280, LR: 0.0009979083\n",
      "Epoch [15/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0245, LR: 0.0009978823\n",
      "Epoch [15/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0286, LR: 0.0009978561\n",
      "Epoch [15/500], Batch [100/110], Train Loss: 0.0023, Val Loss: 0.0309, LR: 0.0009978297\n",
      "Epoch [15/500], Batch [110/110], Train Loss: 0.0207, Val Loss: 0.0310, LR: 0.0009978032\n",
      "Epoch [16/500], Batch [10/110], Train Loss: 0.0160, Val Loss: 0.0254, LR: 0.0009977765\n",
      "Epoch [16/500], Batch [20/110], Train Loss: 0.0663, Val Loss: 0.0244, LR: 0.0009977496\n",
      "Epoch [16/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0301, LR: 0.0009977226\n",
      "Epoch [16/500], Batch [40/110], Train Loss: 0.0010, Val Loss: 0.0230, LR: 0.0009976955\n",
      "Epoch [16/500], Batch [50/110], Train Loss: 0.0544, Val Loss: 0.0260, LR: 0.0009976681\n",
      "Epoch [16/500], Batch [60/110], Train Loss: 0.0026, Val Loss: 0.0256, LR: 0.0009976406\n",
      "Epoch [16/500], Batch [70/110], Train Loss: 0.0186, Val Loss: 0.0234, LR: 0.0009976130\n",
      "Epoch [16/500], Batch [80/110], Train Loss: 0.0033, Val Loss: 0.0245, LR: 0.0009975852\n",
      "Epoch [16/500], Batch [90/110], Train Loss: 0.0830, Val Loss: 0.0330, LR: 0.0009975572\n",
      "Epoch [16/500], Batch [100/110], Train Loss: 0.0453, Val Loss: 0.0226, LR: 0.0009975290\n",
      "Epoch [16/500], Batch [110/110], Train Loss: 0.0048, Val Loss: 0.0223, LR: 0.0009975008\n",
      "Epoch [17/500], Batch [10/110], Train Loss: 0.0073, Val Loss: 0.0265, LR: 0.0009974723\n",
      "Epoch [17/500], Batch [20/110], Train Loss: 0.0427, Val Loss: 0.0235, LR: 0.0009974437\n",
      "Epoch [17/500], Batch [30/110], Train Loss: 0.0717, Val Loss: 0.0287, LR: 0.0009974149\n",
      "Epoch [17/500], Batch [40/110], Train Loss: 0.0608, Val Loss: 0.0228, LR: 0.0009973860\n",
      "Epoch [17/500], Batch [50/110], Train Loss: 0.0708, Val Loss: 0.0232, LR: 0.0009973569\n",
      "Epoch [17/500], Batch [60/110], Train Loss: 0.0035, Val Loss: 0.0227, LR: 0.0009973276\n",
      "Epoch [17/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0234, LR: 0.0009972982\n",
      "Epoch [17/500], Batch [80/110], Train Loss: 0.0301, Val Loss: 0.0231, LR: 0.0009972686\n",
      "Epoch [17/500], Batch [90/110], Train Loss: 0.1095, Val Loss: 0.0341, LR: 0.0009972389\n",
      "Epoch [17/500], Batch [100/110], Train Loss: 0.0051, Val Loss: 0.0336, LR: 0.0009972090\n",
      "Epoch [17/500], Batch [110/110], Train Loss: 0.0073, Val Loss: 0.0223, LR: 0.0009971789\n",
      "Epoch [18/500], Batch [10/110], Train Loss: 0.0050, Val Loss: 0.0252, LR: 0.0009971487\n",
      "Epoch [18/500], Batch [20/110], Train Loss: 0.0394, Val Loss: 0.0237, LR: 0.0009971183\n",
      "Epoch [18/500], Batch [30/110], Train Loss: 0.0062, Val Loss: 0.0224, LR: 0.0009970877\n",
      "Epoch [18/500], Batch [40/110], Train Loss: 0.0162, Val Loss: 0.0273, LR: 0.0009970570\n",
      "Epoch [18/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0305, LR: 0.0009970262\n",
      "Epoch [18/500], Batch [60/110], Train Loss: 0.2597, Val Loss: 0.0225, LR: 0.0009969951\n",
      "Epoch [18/500], Batch [70/110], Train Loss: 0.0020, Val Loss: 0.0240, LR: 0.0009969640\n",
      "Epoch [18/500], Batch [80/110], Train Loss: 0.0229, Val Loss: 0.0224, LR: 0.0009969326\n",
      "Epoch [18/500], Batch [90/110], Train Loss: 0.0840, Val Loss: 0.0329, LR: 0.0009969011\n",
      "Epoch [18/500], Batch [100/110], Train Loss: 0.0014, Val Loss: 0.0259, LR: 0.0009968694\n",
      "Epoch [18/500], Batch [110/110], Train Loss: 0.0028, Val Loss: 0.0213, LR: 0.0009968376\n",
      "Epoch [19/500], Batch [10/110], Train Loss: 0.0174, Val Loss: 0.0212, LR: 0.0009968056\n",
      "Epoch [19/500], Batch [20/110], Train Loss: 0.0617, Val Loss: 0.0239, LR: 0.0009967735\n",
      "Epoch [19/500], Batch [30/110], Train Loss: 0.0297, Val Loss: 0.0212, LR: 0.0009967411\n",
      "Epoch [19/500], Batch [40/110], Train Loss: 0.0015, Val Loss: 0.0216, LR: 0.0009967087\n",
      "Epoch [19/500], Batch [50/110], Train Loss: 0.0284, Val Loss: 0.0211, LR: 0.0009966760\n",
      "Epoch [19/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0335, LR: 0.0009966433\n",
      "Epoch [19/500], Batch [70/110], Train Loss: 0.0042, Val Loss: 0.0292, LR: 0.0009966103\n",
      "Epoch [19/500], Batch [80/110], Train Loss: 0.0314, Val Loss: 0.0212, LR: 0.0009965772\n",
      "Epoch [19/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0326, LR: 0.0009965439\n",
      "Epoch [19/500], Batch [100/110], Train Loss: 0.0267, Val Loss: 0.0213, LR: 0.0009965105\n",
      "Epoch [19/500], Batch [110/110], Train Loss: 0.0059, Val Loss: 0.0215, LR: 0.0009964769\n",
      "Epoch [20/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0224, LR: 0.0009964431\n",
      "Epoch [20/500], Batch [20/110], Train Loss: 0.0507, Val Loss: 0.0240, LR: 0.0009964092\n",
      "Epoch [20/500], Batch [30/110], Train Loss: 0.0021, Val Loss: 0.0248, LR: 0.0009963751\n",
      "Epoch [20/500], Batch [40/110], Train Loss: 0.0038, Val Loss: 0.0243, LR: 0.0009963409\n",
      "Epoch [20/500], Batch [50/110], Train Loss: 0.0053, Val Loss: 0.0315, LR: 0.0009963065\n",
      "Epoch [20/500], Batch [60/110], Train Loss: 0.0236, Val Loss: 0.0217, LR: 0.0009962720\n",
      "Epoch [20/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0210, LR: 0.0009962372\n",
      "Epoch [20/500], Batch [80/110], Train Loss: 0.0308, Val Loss: 0.0213, LR: 0.0009962024\n",
      "Epoch [20/500], Batch [90/110], Train Loss: 0.0028, Val Loss: 0.0215, LR: 0.0009961673\n",
      "Epoch [20/500], Batch [100/110], Train Loss: 0.0102, Val Loss: 0.0214, LR: 0.0009961321\n",
      "Epoch [20/500], Batch [110/110], Train Loss: 0.0092, Val Loss: 0.0218, LR: 0.0009960968\n",
      "Epoch [21/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0214, LR: 0.0009960613\n",
      "Epoch [21/500], Batch [20/110], Train Loss: 0.0029, Val Loss: 0.0209, LR: 0.0009960256\n",
      "Epoch [21/500], Batch [30/110], Train Loss: 0.0032, Val Loss: 0.0208, LR: 0.0009959897\n",
      "Epoch [21/500], Batch [40/110], Train Loss: 0.0361, Val Loss: 0.0219, LR: 0.0009959537\n",
      "Epoch [21/500], Batch [50/110], Train Loss: 0.0028, Val Loss: 0.0213, LR: 0.0009959176\n",
      "Epoch [21/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0201, LR: 0.0009958813\n",
      "Epoch [21/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0203, LR: 0.0009958448\n",
      "Epoch [21/500], Batch [80/110], Train Loss: 0.0062, Val Loss: 0.0214, LR: 0.0009958082\n",
      "Epoch [21/500], Batch [90/110], Train Loss: 0.0044, Val Loss: 0.0280, LR: 0.0009957714\n",
      "Epoch [21/500], Batch [100/110], Train Loss: 0.0036, Val Loss: 0.0220, LR: 0.0009957344\n",
      "Epoch [21/500], Batch [110/110], Train Loss: 0.0292, Val Loss: 0.0210, LR: 0.0009956973\n",
      "Confusion Matrix:\n",
      "[[632   5]\n",
      " [  9 854]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98596   0.99215   0.98905       637\n",
      "           1    0.99418   0.98957   0.99187       863\n",
      "\n",
      "    accuracy                        0.99067      1500\n",
      "   macro avg    0.99007   0.99086   0.99046      1500\n",
      "weighted avg    0.99069   0.99067   0.99067      1500\n",
      "\n",
      "Total Errors: 14\n",
      "Index: 407, Predicted: 0, Actual: 1\n",
      "Index: 554, Predicted: 0, Actual: 1\n",
      "Index: 569, Predicted: 0, Actual: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Index: 719, Predicted: 1, Actual: 0\n",
      "Epoch 21: OK- Accuracy: 0.99067, Precision: 0.99418, Recall: 0.98957, F1: 0.99187, ROC AUC: 0.99086, AUPR (PR-AUC): 0.98981, Sensitivity: 0.98957, Specificity: 0.99215, Far: 0.007849293563579277, False Positive Rate (FPR): 0.00785, False Negative Rate (FNR): 0.01043, Runtime: 0.039 sec , Memory Usage: 198.65 MB\n",
      "Epoch [22/500], Batch [10/110], Train Loss: 0.0068, Val Loss: 0.0218, LR: 0.0009956600\n",
      "Epoch [22/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0232, LR: 0.0009956226\n",
      "Epoch [22/500], Batch [30/110], Train Loss: 0.0079, Val Loss: 0.0247, LR: 0.0009955850\n",
      "Epoch [22/500], Batch [40/110], Train Loss: 0.0133, Val Loss: 0.0204, LR: 0.0009955472\n",
      "Epoch [22/500], Batch [50/110], Train Loss: 0.0016, Val Loss: 0.0235, LR: 0.0009955093\n",
      "Epoch [22/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0205, LR: 0.0009954712\n",
      "Epoch [22/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0210, LR: 0.0009954330\n",
      "Epoch [22/500], Batch [80/110], Train Loss: 0.1107, Val Loss: 0.0229, LR: 0.0009953946\n",
      "Epoch [22/500], Batch [90/110], Train Loss: 0.0450, Val Loss: 0.0202, LR: 0.0009953560\n",
      "Epoch [22/500], Batch [100/110], Train Loss: 0.0493, Val Loss: 0.0207, LR: 0.0009953173\n",
      "Epoch [22/500], Batch [110/110], Train Loss: 0.0373, Val Loss: 0.0213, LR: 0.0009952784\n",
      "Epoch [23/500], Batch [10/110], Train Loss: 0.0244, Val Loss: 0.0198, LR: 0.0009952394\n",
      "Epoch [23/500], Batch [20/110], Train Loss: 0.0242, Val Loss: 0.0204, LR: 0.0009952002\n",
      "Epoch [23/500], Batch [30/110], Train Loss: 0.0679, Val Loss: 0.0201, LR: 0.0009951608\n",
      "Epoch [23/500], Batch [40/110], Train Loss: 0.0701, Val Loss: 0.0243, LR: 0.0009951213\n",
      "Epoch [23/500], Batch [50/110], Train Loss: 0.0330, Val Loss: 0.0198, LR: 0.0009950816\n",
      "Epoch [23/500], Batch [60/110], Train Loss: 0.0024, Val Loss: 0.0207, LR: 0.0009950418\n",
      "Epoch [23/500], Batch [70/110], Train Loss: 0.1450, Val Loss: 0.0228, LR: 0.0009950018\n",
      "Epoch [23/500], Batch [80/110], Train Loss: 0.0434, Val Loss: 0.0199, LR: 0.0009949616\n",
      "Epoch [23/500], Batch [90/110], Train Loss: 0.0066, Val Loss: 0.0205, LR: 0.0009949213\n",
      "Epoch [23/500], Batch [100/110], Train Loss: 0.0021, Val Loss: 0.0200, LR: 0.0009948808\n",
      "Epoch [23/500], Batch [110/110], Train Loss: 0.0366, Val Loss: 0.0195, LR: 0.0009948402\n",
      "Epoch [24/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0216, LR: 0.0009947994\n",
      "Epoch [24/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0203, LR: 0.0009947584\n",
      "Epoch [24/500], Batch [30/110], Train Loss: 0.0129, Val Loss: 0.0248, LR: 0.0009947173\n",
      "Epoch [24/500], Batch [40/110], Train Loss: 0.0756, Val Loss: 0.0261, LR: 0.0009946760\n",
      "Epoch [24/500], Batch [50/110], Train Loss: 0.0649, Val Loss: 0.0232, LR: 0.0009946346\n",
      "Epoch [24/500], Batch [60/110], Train Loss: 0.0446, Val Loss: 0.0192, LR: 0.0009945930\n",
      "Epoch [24/500], Batch [70/110], Train Loss: 0.0479, Val Loss: 0.0198, LR: 0.0009945512\n",
      "Epoch [24/500], Batch [80/110], Train Loss: 0.0811, Val Loss: 0.0331, LR: 0.0009945093\n",
      "Epoch [24/500], Batch [90/110], Train Loss: 0.0121, Val Loss: 0.0211, LR: 0.0009944672\n",
      "Epoch [24/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0208, LR: 0.0009944250\n",
      "Epoch [24/500], Batch [110/110], Train Loss: 0.0091, Val Loss: 0.0227, LR: 0.0009943826\n",
      "Epoch [25/500], Batch [10/110], Train Loss: 0.0685, Val Loss: 0.0205, LR: 0.0009943401\n",
      "Epoch [25/500], Batch [20/110], Train Loss: 0.0180, Val Loss: 0.0198, LR: 0.0009942973\n",
      "Epoch [25/500], Batch [30/110], Train Loss: 0.0148, Val Loss: 0.0190, LR: 0.0009942545\n",
      "Epoch [25/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0193, LR: 0.0009942114\n",
      "Epoch [25/500], Batch [50/110], Train Loss: 0.0638, Val Loss: 0.0188, LR: 0.0009941682\n",
      "Epoch [25/500], Batch [60/110], Train Loss: 0.0639, Val Loss: 0.0188, LR: 0.0009941249\n",
      "Epoch [25/500], Batch [70/110], Train Loss: 0.0017, Val Loss: 0.0225, LR: 0.0009940814\n",
      "Epoch [25/500], Batch [80/110], Train Loss: 0.0027, Val Loss: 0.0186, LR: 0.0009940377\n",
      "Epoch [25/500], Batch [90/110], Train Loss: 0.0490, Val Loss: 0.0186, LR: 0.0009939939\n",
      "Epoch [25/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0207, LR: 0.0009939499\n",
      "Epoch [25/500], Batch [110/110], Train Loss: 0.0026, Val Loss: 0.0212, LR: 0.0009939057\n",
      "Epoch [26/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0214, LR: 0.0009938614\n",
      "Epoch [26/500], Batch [20/110], Train Loss: 0.0321, Val Loss: 0.0204, LR: 0.0009938169\n",
      "Epoch [26/500], Batch [30/110], Train Loss: 0.0014, Val Loss: 0.0193, LR: 0.0009937723\n",
      "Epoch [26/500], Batch [40/110], Train Loss: 0.0023, Val Loss: 0.0220, LR: 0.0009937275\n",
      "Epoch [26/500], Batch [50/110], Train Loss: 0.0034, Val Loss: 0.0200, LR: 0.0009936826\n",
      "Epoch [26/500], Batch [60/110], Train Loss: 0.0208, Val Loss: 0.0184, LR: 0.0009936375\n",
      "Epoch [26/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0190, LR: 0.0009935922\n",
      "Epoch [26/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0190, LR: 0.0009935468\n",
      "Epoch [26/500], Batch [90/110], Train Loss: 0.0053, Val Loss: 0.0194, LR: 0.0009935012\n",
      "Epoch [26/500], Batch [100/110], Train Loss: 0.0042, Val Loss: 0.0210, LR: 0.0009934554\n",
      "Epoch [26/500], Batch [110/110], Train Loss: 0.0055, Val Loss: 0.0336, LR: 0.0009934095\n",
      "Epoch [27/500], Batch [10/110], Train Loss: 0.0027, Val Loss: 0.0204, LR: 0.0009933635\n",
      "Epoch [27/500], Batch [20/110], Train Loss: 0.0331, Val Loss: 0.0190, LR: 0.0009933173\n",
      "Epoch [27/500], Batch [30/110], Train Loss: 0.0371, Val Loss: 0.0191, LR: 0.0009932709\n",
      "Epoch [27/500], Batch [40/110], Train Loss: 0.0666, Val Loss: 0.0190, LR: 0.0009932243\n",
      "Epoch [27/500], Batch [50/110], Train Loss: 0.0019, Val Loss: 0.0247, LR: 0.0009931776\n",
      "Epoch [27/500], Batch [60/110], Train Loss: 0.0018, Val Loss: 0.0185, LR: 0.0009931308\n",
      "Epoch [27/500], Batch [70/110], Train Loss: 0.0052, Val Loss: 0.0185, LR: 0.0009930837\n",
      "Epoch [27/500], Batch [80/110], Train Loss: 0.0546, Val Loss: 0.0247, LR: 0.0009930366\n",
      "Epoch [27/500], Batch [90/110], Train Loss: 0.0227, Val Loss: 0.0192, LR: 0.0009929892\n",
      "Epoch [27/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0257, LR: 0.0009929417\n",
      "Epoch [27/500], Batch [110/110], Train Loss: 0.0373, Val Loss: 0.0181, LR: 0.0009928941\n",
      "Epoch [28/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0178, LR: 0.0009928463\n",
      "Epoch [28/500], Batch [20/110], Train Loss: 0.0190, Val Loss: 0.0181, LR: 0.0009927983\n",
      "Epoch [28/500], Batch [30/110], Train Loss: 0.0245, Val Loss: 0.0194, LR: 0.0009927501\n",
      "Epoch [28/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0271, LR: 0.0009927019\n",
      "Epoch [28/500], Batch [50/110], Train Loss: 0.0309, Val Loss: 0.0182, LR: 0.0009926534\n",
      "Epoch [28/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0194, LR: 0.0009926048\n",
      "Epoch [28/500], Batch [70/110], Train Loss: 0.0025, Val Loss: 0.0180, LR: 0.0009925560\n",
      "Epoch [28/500], Batch [80/110], Train Loss: 0.0161, Val Loss: 0.0186, LR: 0.0009925071\n",
      "Epoch [28/500], Batch [90/110], Train Loss: 0.0020, Val Loss: 0.0241, LR: 0.0009924580\n",
      "Epoch [28/500], Batch [100/110], Train Loss: 0.0383, Val Loss: 0.0190, LR: 0.0009924088\n",
      "Epoch [28/500], Batch [110/110], Train Loss: 0.0099, Val Loss: 0.0213, LR: 0.0009923593\n",
      "Epoch [29/500], Batch [10/110], Train Loss: 0.0521, Val Loss: 0.0180, LR: 0.0009923098\n",
      "Epoch [29/500], Batch [20/110], Train Loss: 0.0436, Val Loss: 0.0189, LR: 0.0009922601\n",
      "Epoch [29/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0209, LR: 0.0009922102\n",
      "Epoch [29/500], Batch [40/110], Train Loss: 0.0534, Val Loss: 0.0187, LR: 0.0009921601\n",
      "Epoch [29/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0191, LR: 0.0009921099\n",
      "Epoch [29/500], Batch [60/110], Train Loss: 0.0118, Val Loss: 0.0232, LR: 0.0009920596\n",
      "Epoch [29/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0218, LR: 0.0009920090\n",
      "Epoch [29/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0203, LR: 0.0009919584\n",
      "Epoch [29/500], Batch [90/110], Train Loss: 0.0170, Val Loss: 0.0202, LR: 0.0009919075\n",
      "Epoch [29/500], Batch [100/110], Train Loss: 0.0012, Val Loss: 0.0183, LR: 0.0009918565\n",
      "Epoch [29/500], Batch [110/110], Train Loss: 0.0327, Val Loss: 0.0183, LR: 0.0009918054\n",
      "Epoch [30/500], Batch [10/110], Train Loss: 0.0027, Val Loss: 0.0195, LR: 0.0009917541\n",
      "Epoch [30/500], Batch [20/110], Train Loss: 0.0103, Val Loss: 0.0187, LR: 0.0009917026\n",
      "Epoch [30/500], Batch [30/110], Train Loss: 0.0279, Val Loss: 0.0177, LR: 0.0009916510\n",
      "Epoch [30/500], Batch [40/110], Train Loss: 0.0128, Val Loss: 0.0262, LR: 0.0009915992\n",
      "Epoch [30/500], Batch [50/110], Train Loss: 0.0041, Val Loss: 0.0176, LR: 0.0009915472\n",
      "Epoch [30/500], Batch [60/110], Train Loss: 0.0108, Val Loss: 0.0196, LR: 0.0009914951\n",
      "Epoch [30/500], Batch [70/110], Train Loss: 0.0710, Val Loss: 0.0231, LR: 0.0009914428\n",
      "Epoch [30/500], Batch [80/110], Train Loss: 0.0155, Val Loss: 0.0191, LR: 0.0009913904\n",
      "Epoch [30/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0243, LR: 0.0009913378\n",
      "Epoch [30/500], Batch [100/110], Train Loss: 0.0184, Val Loss: 0.0187, LR: 0.0009912851\n",
      "Epoch [30/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0366, LR: 0.0009912322\n",
      "Epoch [31/500], Batch [10/110], Train Loss: 0.0013, Val Loss: 0.0189, LR: 0.0009911791\n",
      "Epoch [31/500], Batch [20/110], Train Loss: 0.0065, Val Loss: 0.0239, LR: 0.0009911259\n",
      "Epoch [31/500], Batch [30/110], Train Loss: 0.0017, Val Loss: 0.0176, LR: 0.0009910725\n",
      "Epoch [31/500], Batch [40/110], Train Loss: 0.0496, Val Loss: 0.0188, LR: 0.0009910190\n",
      "Epoch [31/500], Batch [50/110], Train Loss: 0.0738, Val Loss: 0.0235, LR: 0.0009909653\n",
      "Epoch [31/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0204, LR: 0.0009909114\n",
      "Epoch [31/500], Batch [70/110], Train Loss: 0.0264, Val Loss: 0.0177, LR: 0.0009908574\n",
      "Epoch [31/500], Batch [80/110], Train Loss: 0.0700, Val Loss: 0.0176, LR: 0.0009908033\n",
      "Epoch [31/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0181, LR: 0.0009907489\n",
      "Epoch [31/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0223, LR: 0.0009906945\n",
      "Epoch [31/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0324, LR: 0.0009906398\n",
      "Confusion Matrix:\n",
      "[[618  19]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99838   0.97017   0.98408       637\n",
      "           1    0.97843   0.99884   0.98853       863\n",
      "\n",
      "    accuracy                        0.98667      1500\n",
      "   macro avg    0.98841   0.98451   0.98630      1500\n",
      "weighted avg    0.98691   0.98667   0.98664      1500\n",
      "\n",
      "Total Errors: 20\n",
      "Index: 41, Predicted: 1, Actual: 0\n",
      "Index: 313, Predicted: 1, Actual: 0\n",
      "Index: 460, Predicted: 1, Actual: 0\n",
      "Index: 575, Predicted: 1, Actual: 0\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 31: OK- Accuracy: 0.98667, Precision: 0.97843, Recall: 0.99884, F1: 0.98853, ROC AUC: 0.98451, AUPR (PR-AUC): 0.97797, Sensitivity: 0.99884, Specificity: 0.97017, Far: 0.029827315541601257, False Positive Rate (FPR): 0.02983, False Negative Rate (FNR): 0.00116, Runtime: 0.033 sec , Memory Usage: 198.67 MB\n",
      "Epoch [32/500], Batch [10/110], Train Loss: 0.0020, Val Loss: 0.0172, LR: 0.0009905850\n",
      "Epoch [32/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0175, LR: 0.0009905300\n",
      "Epoch [32/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0194, LR: 0.0009904749\n",
      "Epoch [32/500], Batch [40/110], Train Loss: 0.0219, Val Loss: 0.0175, LR: 0.0009904196\n",
      "Epoch [32/500], Batch [50/110], Train Loss: 0.0022, Val Loss: 0.0173, LR: 0.0009903642\n",
      "Epoch [32/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0206, LR: 0.0009903086\n",
      "Epoch [32/500], Batch [70/110], Train Loss: 0.0026, Val Loss: 0.0253, LR: 0.0009902529\n",
      "Epoch [32/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0193, LR: 0.0009901969\n",
      "Epoch [32/500], Batch [90/110], Train Loss: 0.0031, Val Loss: 0.0207, LR: 0.0009901409\n",
      "Epoch [32/500], Batch [100/110], Train Loss: 0.0431, Val Loss: 0.0175, LR: 0.0009900846\n",
      "Epoch [32/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0171, LR: 0.0009900283\n",
      "Epoch [33/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0169, LR: 0.0009899717\n",
      "Epoch [33/500], Batch [20/110], Train Loss: 0.0440, Val Loss: 0.0173, LR: 0.0009899150\n",
      "Epoch [33/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0170, LR: 0.0009898581\n",
      "Epoch [33/500], Batch [40/110], Train Loss: 0.0107, Val Loss: 0.0243, LR: 0.0009898011\n",
      "Epoch [33/500], Batch [50/110], Train Loss: 0.0552, Val Loss: 0.0238, LR: 0.0009897439\n",
      "Epoch [33/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0166, LR: 0.0009896866\n",
      "Epoch [33/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0165, LR: 0.0009896291\n",
      "Epoch [33/500], Batch [80/110], Train Loss: 0.0495, Val Loss: 0.0187, LR: 0.0009895715\n",
      "Epoch [33/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0180, LR: 0.0009895136\n",
      "Epoch [33/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0172, LR: 0.0009894557\n",
      "Epoch [33/500], Batch [110/110], Train Loss: 0.0587, Val Loss: 0.0180, LR: 0.0009893975\n",
      "Epoch [34/500], Batch [10/110], Train Loss: 0.0162, Val Loss: 0.0203, LR: 0.0009893393\n",
      "Epoch [34/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0177, LR: 0.0009892808\n",
      "Epoch [34/500], Batch [30/110], Train Loss: 0.0160, Val Loss: 0.0166, LR: 0.0009892222\n",
      "Epoch [34/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0185, LR: 0.0009891635\n",
      "Epoch [34/500], Batch [50/110], Train Loss: 0.2410, Val Loss: 0.0163, LR: 0.0009891045\n",
      "Epoch [34/500], Batch [60/110], Train Loss: 0.0176, Val Loss: 0.0165, LR: 0.0009890455\n",
      "Epoch [34/500], Batch [70/110], Train Loss: 0.0199, Val Loss: 0.0169, LR: 0.0009889862\n",
      "Epoch [34/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0186, LR: 0.0009889268\n",
      "Epoch [34/500], Batch [90/110], Train Loss: 0.0487, Val Loss: 0.0197, LR: 0.0009888673\n",
      "Epoch [34/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0162, LR: 0.0009888076\n",
      "Epoch [34/500], Batch [110/110], Train Loss: 0.0293, Val Loss: 0.0165, LR: 0.0009887477\n",
      "Epoch [35/500], Batch [10/110], Train Loss: 0.1564, Val Loss: 0.0167, LR: 0.0009886877\n",
      "Epoch [35/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0210, LR: 0.0009886275\n",
      "Epoch [35/500], Batch [30/110], Train Loss: 0.0350, Val Loss: 0.0194, LR: 0.0009885672\n",
      "Epoch [35/500], Batch [40/110], Train Loss: 0.0013, Val Loss: 0.0161, LR: 0.0009885067\n",
      "Epoch [35/500], Batch [50/110], Train Loss: 0.0117, Val Loss: 0.0167, LR: 0.0009884460\n",
      "Epoch [35/500], Batch [60/110], Train Loss: 0.0794, Val Loss: 0.0204, LR: 0.0009883852\n",
      "Epoch [35/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0197, LR: 0.0009883243\n",
      "Epoch [35/500], Batch [80/110], Train Loss: 0.0008, Val Loss: 0.0224, LR: 0.0009882631\n",
      "Epoch [35/500], Batch [90/110], Train Loss: 0.0043, Val Loss: 0.0162, LR: 0.0009882018\n",
      "Epoch [35/500], Batch [100/110], Train Loss: 0.0016, Val Loss: 0.0168, LR: 0.0009881404\n",
      "Epoch [35/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0161, LR: 0.0009880788\n",
      "Epoch [36/500], Batch [10/110], Train Loss: 0.0417, Val Loss: 0.0159, LR: 0.0009880170\n",
      "Epoch [36/500], Batch [20/110], Train Loss: 0.0031, Val Loss: 0.0159, LR: 0.0009879551\n",
      "Epoch [36/500], Batch [30/110], Train Loss: 0.0317, Val Loss: 0.0167, LR: 0.0009878931\n",
      "Epoch [36/500], Batch [40/110], Train Loss: 0.0093, Val Loss: 0.0284, LR: 0.0009878308\n",
      "Epoch [36/500], Batch [50/110], Train Loss: 0.0941, Val Loss: 0.0177, LR: 0.0009877684\n",
      "Epoch [36/500], Batch [60/110], Train Loss: 0.0038, Val Loss: 0.0158, LR: 0.0009877059\n",
      "Epoch [36/500], Batch [70/110], Train Loss: 0.0022, Val Loss: 0.0157, LR: 0.0009876432\n",
      "Epoch [36/500], Batch [80/110], Train Loss: 0.0025, Val Loss: 0.0169, LR: 0.0009875803\n",
      "Epoch [36/500], Batch [90/110], Train Loss: 0.0069, Val Loss: 0.0204, LR: 0.0009875173\n",
      "Epoch [36/500], Batch [100/110], Train Loss: 0.0027, Val Loss: 0.0157, LR: 0.0009874541\n",
      "Epoch [36/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0194, LR: 0.0009873908\n",
      "Epoch [37/500], Batch [10/110], Train Loss: 0.0289, Val Loss: 0.0156, LR: 0.0009873273\n",
      "Epoch [37/500], Batch [20/110], Train Loss: 0.0072, Val Loss: 0.0178, LR: 0.0009872637\n",
      "Epoch [37/500], Batch [30/110], Train Loss: 0.0326, Val Loss: 0.0164, LR: 0.0009871999\n",
      "Epoch [37/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0167, LR: 0.0009871359\n",
      "Epoch [37/500], Batch [50/110], Train Loss: 0.0062, Val Loss: 0.0213, LR: 0.0009870718\n",
      "Epoch [37/500], Batch [60/110], Train Loss: 0.0338, Val Loss: 0.0161, LR: 0.0009870075\n",
      "Epoch [37/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0195, LR: 0.0009869431\n",
      "Epoch [37/500], Batch [80/110], Train Loss: 0.0012, Val Loss: 0.0179, LR: 0.0009868785\n",
      "Epoch [37/500], Batch [90/110], Train Loss: 0.0514, Val Loss: 0.0162, LR: 0.0009868137\n",
      "Epoch [37/500], Batch [100/110], Train Loss: 0.0021, Val Loss: 0.0157, LR: 0.0009867488\n",
      "Epoch [37/500], Batch [110/110], Train Loss: 0.1754, Val Loss: 0.0177, LR: 0.0009866838\n",
      "Epoch [38/500], Batch [10/110], Train Loss: 0.0199, Val Loss: 0.0202, LR: 0.0009866185\n",
      "Epoch [38/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0164, LR: 0.0009865532\n",
      "Epoch [38/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0229, LR: 0.0009864876\n",
      "Epoch [38/500], Batch [40/110], Train Loss: 0.0537, Val Loss: 0.0169, LR: 0.0009864219\n",
      "Epoch [38/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0180, LR: 0.0009863561\n",
      "Epoch [38/500], Batch [60/110], Train Loss: 0.2065, Val Loss: 0.0231, LR: 0.0009862901\n",
      "Epoch [38/500], Batch [70/110], Train Loss: 0.0015, Val Loss: 0.0264, LR: 0.0009862239\n",
      "Epoch [38/500], Batch [80/110], Train Loss: 0.0126, Val Loss: 0.0157, LR: 0.0009861576\n",
      "Epoch [38/500], Batch [90/110], Train Loss: 0.0074, Val Loss: 0.0155, LR: 0.0009860911\n",
      "Epoch [38/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0158, LR: 0.0009860245\n",
      "Epoch [38/500], Batch [110/110], Train Loss: 0.0283, Val Loss: 0.0155, LR: 0.0009859577\n",
      "Epoch [39/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0179, LR: 0.0009858908\n",
      "Epoch [39/500], Batch [20/110], Train Loss: 0.0431, Val Loss: 0.0163, LR: 0.0009858237\n",
      "Epoch [39/500], Batch [30/110], Train Loss: 0.0207, Val Loss: 0.0158, LR: 0.0009857564\n",
      "Epoch [39/500], Batch [40/110], Train Loss: 0.2092, Val Loss: 0.0178, LR: 0.0009856890\n",
      "Epoch [39/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0172, LR: 0.0009856214\n",
      "Epoch [39/500], Batch [60/110], Train Loss: 0.0199, Val Loss: 0.0179, LR: 0.0009855537\n",
      "Epoch [39/500], Batch [70/110], Train Loss: 0.0540, Val Loss: 0.0158, LR: 0.0009854858\n",
      "Epoch [39/500], Batch [80/110], Train Loss: 0.0250, Val Loss: 0.0160, LR: 0.0009854177\n",
      "Epoch [39/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0201, LR: 0.0009853495\n",
      "Epoch [39/500], Batch [100/110], Train Loss: 0.0326, Val Loss: 0.0158, LR: 0.0009852812\n",
      "Epoch [39/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0157, LR: 0.0009852127\n",
      "Epoch [40/500], Batch [10/110], Train Loss: 0.0365, Val Loss: 0.0160, LR: 0.0009851440\n",
      "Epoch [40/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0168, LR: 0.0009850752\n",
      "Epoch [40/500], Batch [30/110], Train Loss: 0.0793, Val Loss: 0.0153, LR: 0.0009850062\n",
      "Epoch [40/500], Batch [40/110], Train Loss: 0.0522, Val Loss: 0.0154, LR: 0.0009849370\n",
      "Epoch [40/500], Batch [50/110], Train Loss: 0.0194, Val Loss: 0.0185, LR: 0.0009848677\n",
      "Epoch [40/500], Batch [60/110], Train Loss: 0.0722, Val Loss: 0.0184, LR: 0.0009847983\n",
      "Epoch [40/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0154, LR: 0.0009847287\n",
      "Epoch [40/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0153, LR: 0.0009846589\n",
      "Epoch [40/500], Batch [90/110], Train Loss: 0.0478, Val Loss: 0.0157, LR: 0.0009845890\n",
      "Epoch [40/500], Batch [100/110], Train Loss: 0.0322, Val Loss: 0.0168, LR: 0.0009845189\n",
      "Epoch [40/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0205, LR: 0.0009844487\n",
      "Epoch [41/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0155, LR: 0.0009843783\n",
      "Epoch [41/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0249, LR: 0.0009843077\n",
      "Epoch [41/500], Batch [30/110], Train Loss: 0.0239, Val Loss: 0.0152, LR: 0.0009842370\n",
      "Epoch [41/500], Batch [40/110], Train Loss: 0.0725, Val Loss: 0.0167, LR: 0.0009841662\n",
      "Epoch [41/500], Batch [50/110], Train Loss: 0.0195, Val Loss: 0.0151, LR: 0.0009840951\n",
      "Epoch [41/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0165, LR: 0.0009840240\n",
      "Epoch [41/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0152, LR: 0.0009839526\n",
      "Epoch [41/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0152, LR: 0.0009838811\n",
      "Epoch [41/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0153, LR: 0.0009838095\n",
      "Epoch [41/500], Batch [100/110], Train Loss: 0.0463, Val Loss: 0.0163, LR: 0.0009837377\n",
      "Epoch [41/500], Batch [110/110], Train Loss: 0.0884, Val Loss: 0.0171, LR: 0.0009836657\n",
      "Confusion Matrix:\n",
      "[[621  16]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99839   0.97488   0.98650       637\n",
      "           1    0.98178   0.99884   0.99024       863\n",
      "\n",
      "    accuracy                        0.98867      1500\n",
      "   macro avg    0.99008   0.98686   0.98837      1500\n",
      "weighted avg    0.98883   0.98867   0.98865      1500\n",
      "\n",
      "Total Errors: 17\n",
      "Index: 41, Predicted: 1, Actual: 0\n",
      "Index: 313, Predicted: 1, Actual: 0\n",
      "Index: 575, Predicted: 1, Actual: 0\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Index: 715, Predicted: 1, Actual: 0\n",
      "Epoch 41: OK- Accuracy: 0.98867, Precision: 0.98178, Recall: 0.99884, F1: 0.99024, ROC AUC: 0.98686, AUPR (PR-AUC): 0.98131, Sensitivity: 0.99884, Specificity: 0.97488, Far: 0.02511773940345369, False Positive Rate (FPR): 0.02512, False Negative Rate (FNR): 0.00116, Runtime: 0.037 sec , Memory Usage: 198.68 MB\n",
      "Epoch [42/500], Batch [10/110], Train Loss: 0.0180, Val Loss: 0.0170, LR: 0.0009835936\n",
      "Epoch [42/500], Batch [20/110], Train Loss: 0.0041, Val Loss: 0.0151, LR: 0.0009835214\n",
      "Epoch [42/500], Batch [30/110], Train Loss: 0.0045, Val Loss: 0.0167, LR: 0.0009834489\n",
      "Epoch [42/500], Batch [40/110], Train Loss: 0.0293, Val Loss: 0.0161, LR: 0.0009833763\n",
      "Epoch [42/500], Batch [50/110], Train Loss: 0.0145, Val Loss: 0.0173, LR: 0.0009833036\n",
      "Epoch [42/500], Batch [60/110], Train Loss: 0.0364, Val Loss: 0.0150, LR: 0.0009832307\n",
      "Epoch [42/500], Batch [70/110], Train Loss: 0.0167, Val Loss: 0.0176, LR: 0.0009831577\n",
      "Epoch [42/500], Batch [80/110], Train Loss: 0.0547, Val Loss: 0.0161, LR: 0.0009830845\n",
      "Epoch [42/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0160, LR: 0.0009830111\n",
      "Epoch [42/500], Batch [100/110], Train Loss: 0.0189, Val Loss: 0.0151, LR: 0.0009829376\n",
      "Epoch [42/500], Batch [110/110], Train Loss: 0.0103, Val Loss: 0.0163, LR: 0.0009828639\n",
      "Epoch [43/500], Batch [10/110], Train Loss: 0.0237, Val Loss: 0.0152, LR: 0.0009827901\n",
      "Epoch [43/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0201, LR: 0.0009827161\n",
      "Epoch [43/500], Batch [30/110], Train Loss: 0.0126, Val Loss: 0.0295, LR: 0.0009826420\n",
      "Epoch [43/500], Batch [40/110], Train Loss: 0.0655, Val Loss: 0.0191, LR: 0.0009825677\n",
      "Epoch [43/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0172, LR: 0.0009824932\n",
      "Epoch [43/500], Batch [60/110], Train Loss: 0.0014, Val Loss: 0.0207, LR: 0.0009824186\n",
      "Epoch [43/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0170, LR: 0.0009823438\n",
      "Epoch [43/500], Batch [80/110], Train Loss: 0.0014, Val Loss: 0.0151, LR: 0.0009822689\n",
      "Epoch [43/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0245, LR: 0.0009821938\n",
      "Epoch [43/500], Batch [100/110], Train Loss: 0.0596, Val Loss: 0.0150, LR: 0.0009821186\n",
      "Epoch [43/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0164, LR: 0.0009820432\n",
      "Epoch [44/500], Batch [10/110], Train Loss: 0.0034, Val Loss: 0.0204, LR: 0.0009819677\n",
      "Epoch [44/500], Batch [20/110], Train Loss: 0.0173, Val Loss: 0.0185, LR: 0.0009818920\n",
      "Epoch [44/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0154, LR: 0.0009818161\n",
      "Epoch [44/500], Batch [40/110], Train Loss: 0.0932, Val Loss: 0.0165, LR: 0.0009817401\n",
      "Epoch [44/500], Batch [50/110], Train Loss: 0.0317, Val Loss: 0.0152, LR: 0.0009816640\n",
      "Epoch [44/500], Batch [60/110], Train Loss: 0.1102, Val Loss: 0.0171, LR: 0.0009815876\n",
      "Epoch [44/500], Batch [70/110], Train Loss: 0.0029, Val Loss: 0.0159, LR: 0.0009815112\n",
      "Epoch [44/500], Batch [80/110], Train Loss: 0.0783, Val Loss: 0.0150, LR: 0.0009814345\n",
      "Epoch [44/500], Batch [90/110], Train Loss: 0.0422, Val Loss: 0.0149, LR: 0.0009813577\n",
      "Epoch [44/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0149, LR: 0.0009812808\n",
      "Epoch [44/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0149, LR: 0.0009812037\n",
      "Epoch [45/500], Batch [10/110], Train Loss: 0.0034, Val Loss: 0.0173, LR: 0.0009811264\n",
      "Epoch [45/500], Batch [20/110], Train Loss: 0.0014, Val Loss: 0.0163, LR: 0.0009810490\n",
      "Epoch [45/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0198, LR: 0.0009809715\n",
      "Epoch [45/500], Batch [40/110], Train Loss: 0.0204, Val Loss: 0.0160, LR: 0.0009808938\n",
      "Epoch [45/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0166, LR: 0.0009808159\n",
      "Epoch [45/500], Batch [60/110], Train Loss: 0.0139, Val Loss: 0.0151, LR: 0.0009807378\n",
      "Epoch [45/500], Batch [70/110], Train Loss: 0.0704, Val Loss: 0.0150, LR: 0.0009806597\n",
      "Epoch [45/500], Batch [80/110], Train Loss: 0.0341, Val Loss: 0.0156, LR: 0.0009805813\n",
      "Epoch [45/500], Batch [90/110], Train Loss: 0.0284, Val Loss: 0.0148, LR: 0.0009805028\n",
      "Epoch [45/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0145, LR: 0.0009804242\n",
      "Epoch [45/500], Batch [110/110], Train Loss: 0.0291, Val Loss: 0.0180, LR: 0.0009803454\n",
      "Epoch [46/500], Batch [10/110], Train Loss: 0.0020, Val Loss: 0.0150, LR: 0.0009802664\n",
      "Epoch [46/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0154, LR: 0.0009801873\n",
      "Epoch [46/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0145, LR: 0.0009801080\n",
      "Epoch [46/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0146, LR: 0.0009800286\n",
      "Epoch [46/500], Batch [50/110], Train Loss: 0.0087, Val Loss: 0.0164, LR: 0.0009799490\n",
      "Epoch [46/500], Batch [60/110], Train Loss: 0.0187, Val Loss: 0.0146, LR: 0.0009798693\n",
      "Epoch [46/500], Batch [70/110], Train Loss: 0.0023, Val Loss: 0.0149, LR: 0.0009797894\n",
      "Epoch [46/500], Batch [80/110], Train Loss: 0.0012, Val Loss: 0.0164, LR: 0.0009797094\n",
      "Epoch [46/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0179, LR: 0.0009796292\n",
      "Epoch [46/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0159, LR: 0.0009795488\n",
      "Epoch [46/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0165, LR: 0.0009794683\n",
      "Epoch [47/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0192, LR: 0.0009793876\n",
      "Epoch [47/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0157, LR: 0.0009793068\n",
      "Epoch [47/500], Batch [30/110], Train Loss: 0.0072, Val Loss: 0.0201, LR: 0.0009792258\n",
      "Epoch [47/500], Batch [40/110], Train Loss: 0.0249, Val Loss: 0.0146, LR: 0.0009791447\n",
      "Epoch [47/500], Batch [50/110], Train Loss: 0.0010, Val Loss: 0.0166, LR: 0.0009790634\n",
      "Epoch [47/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0147, LR: 0.0009789820\n",
      "Epoch [47/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0150, LR: 0.0009789004\n",
      "Epoch [47/500], Batch [80/110], Train Loss: 0.0013, Val Loss: 0.0147, LR: 0.0009788186\n",
      "Epoch [47/500], Batch [90/110], Train Loss: 0.0145, Val Loss: 0.0145, LR: 0.0009787367\n",
      "Epoch [47/500], Batch [100/110], Train Loss: 0.0523, Val Loss: 0.0145, LR: 0.0009786547\n",
      "Epoch [47/500], Batch [110/110], Train Loss: 0.0109, Val Loss: 0.0167, LR: 0.0009785725\n",
      "Epoch [48/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0148, LR: 0.0009784901\n",
      "Epoch [48/500], Batch [20/110], Train Loss: 0.0099, Val Loss: 0.0141, LR: 0.0009784076\n",
      "Epoch [48/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0144, LR: 0.0009783249\n",
      "Epoch [48/500], Batch [40/110], Train Loss: 0.0655, Val Loss: 0.0153, LR: 0.0009782421\n",
      "Epoch [48/500], Batch [50/110], Train Loss: 0.0134, Val Loss: 0.0141, LR: 0.0009781591\n",
      "Epoch [48/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0149, LR: 0.0009780760\n",
      "Epoch [48/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0150, LR: 0.0009779927\n",
      "Epoch [48/500], Batch [80/110], Train Loss: 0.1209, Val Loss: 0.0153, LR: 0.0009779092\n",
      "Epoch [48/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0158, LR: 0.0009778256\n",
      "Epoch [48/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0153, LR: 0.0009777419\n",
      "Epoch [48/500], Batch [110/110], Train Loss: 0.0171, Val Loss: 0.0155, LR: 0.0009776579\n",
      "Epoch [49/500], Batch [10/110], Train Loss: 0.0021, Val Loss: 0.0142, LR: 0.0009775739\n",
      "Epoch [49/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0171, LR: 0.0009774897\n",
      "Epoch [49/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0149, LR: 0.0009774053\n",
      "Epoch [49/500], Batch [40/110], Train Loss: 0.0030, Val Loss: 0.0167, LR: 0.0009773208\n",
      "Epoch [49/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0145, LR: 0.0009772361\n",
      "Epoch [49/500], Batch [60/110], Train Loss: 0.0017, Val Loss: 0.0161, LR: 0.0009771513\n",
      "Epoch [49/500], Batch [70/110], Train Loss: 0.0493, Val Loss: 0.0153, LR: 0.0009770663\n",
      "Epoch [49/500], Batch [80/110], Train Loss: 0.0964, Val Loss: 0.0169, LR: 0.0009769811\n",
      "Epoch [49/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0180, LR: 0.0009768958\n",
      "Epoch [49/500], Batch [100/110], Train Loss: 0.0519, Val Loss: 0.0153, LR: 0.0009768104\n",
      "Epoch [49/500], Batch [110/110], Train Loss: 0.0123, Val Loss: 0.0144, LR: 0.0009767248\n",
      "Epoch [50/500], Batch [10/110], Train Loss: 0.0055, Val Loss: 0.0142, LR: 0.0009766390\n",
      "Epoch [50/500], Batch [20/110], Train Loss: 0.0095, Val Loss: 0.0148, LR: 0.0009765531\n",
      "Epoch [50/500], Batch [30/110], Train Loss: 0.0586, Val Loss: 0.0161, LR: 0.0009764670\n",
      "Epoch [50/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0140, LR: 0.0009763808\n",
      "Epoch [50/500], Batch [50/110], Train Loss: 0.0254, Val Loss: 0.0140, LR: 0.0009762944\n",
      "Epoch [50/500], Batch [60/110], Train Loss: 0.0040, Val Loss: 0.0163, LR: 0.0009762079\n",
      "Epoch [50/500], Batch [70/110], Train Loss: 0.0220, Val Loss: 0.0144, LR: 0.0009761212\n",
      "Epoch [50/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0170, LR: 0.0009760344\n",
      "Epoch [50/500], Batch [90/110], Train Loss: 0.0071, Val Loss: 0.0140, LR: 0.0009759474\n",
      "Epoch [50/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0140, LR: 0.0009758603\n",
      "Epoch [50/500], Batch [110/110], Train Loss: 0.0133, Val Loss: 0.0191, LR: 0.0009757730\n",
      "Epoch [51/500], Batch [10/110], Train Loss: 0.1755, Val Loss: 0.0147, LR: 0.0009756855\n",
      "Epoch [51/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0139, LR: 0.0009755979\n",
      "Epoch [51/500], Batch [30/110], Train Loss: 0.0072, Val Loss: 0.0166, LR: 0.0009755102\n",
      "Epoch [51/500], Batch [40/110], Train Loss: 0.0010, Val Loss: 0.0141, LR: 0.0009754223\n",
      "Epoch [51/500], Batch [50/110], Train Loss: 0.0092, Val Loss: 0.0138, LR: 0.0009753342\n",
      "Epoch [51/500], Batch [60/110], Train Loss: 0.0308, Val Loss: 0.0159, LR: 0.0009752460\n",
      "Epoch [51/500], Batch [70/110], Train Loss: 0.0030, Val Loss: 0.0144, LR: 0.0009751576\n",
      "Epoch [51/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0143, LR: 0.0009750691\n",
      "Epoch [51/500], Batch [90/110], Train Loss: 0.0530, Val Loss: 0.0158, LR: 0.0009749804\n",
      "Epoch [51/500], Batch [100/110], Train Loss: 0.0046, Val Loss: 0.0142, LR: 0.0009748916\n",
      "Epoch [51/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0147, LR: 0.0009748026\n",
      "Confusion Matrix:\n",
      "[[633   4]\n",
      " [  9 854]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98598   0.99372   0.98984       637\n",
      "           1    0.99534   0.98957   0.99245       863\n",
      "\n",
      "    accuracy                        0.99133      1500\n",
      "   macro avg    0.99066   0.99165   0.99114      1500\n",
      "weighted avg    0.99136   0.99133   0.99134      1500\n",
      "\n",
      "Total Errors: 13\n",
      "Index: 407, Predicted: 0, Actual: 1\n",
      "Index: 554, Predicted: 0, Actual: 1\n",
      "Index: 569, Predicted: 0, Actual: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Index: 719, Predicted: 1, Actual: 0\n",
      "Epoch 51: OK- Accuracy: 0.99133, Precision: 0.99534, Recall: 0.98957, F1: 0.99245, ROC AUC: 0.99165, AUPR (PR-AUC): 0.99096, Sensitivity: 0.98957, Specificity: 0.99372, Far: 0.006279434850863423, False Positive Rate (FPR): 0.00628, False Negative Rate (FNR): 0.01043, Runtime: 0.041 sec , Memory Usage: 198.69 MB\n",
      "Epoch [52/500], Batch [10/110], Train Loss: 0.0027, Val Loss: 0.0142, LR: 0.0009747135\n",
      "Epoch [52/500], Batch [20/110], Train Loss: 0.0350, Val Loss: 0.0153, LR: 0.0009746242\n",
      "Epoch [52/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0139, LR: 0.0009745347\n",
      "Epoch [52/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0161, LR: 0.0009744451\n",
      "Epoch [52/500], Batch [50/110], Train Loss: 0.0030, Val Loss: 0.0145, LR: 0.0009743554\n",
      "Epoch [52/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0153, LR: 0.0009742655\n",
      "Epoch [52/500], Batch [70/110], Train Loss: 0.0163, Val Loss: 0.0138, LR: 0.0009741754\n",
      "Epoch [52/500], Batch [80/110], Train Loss: 0.0227, Val Loss: 0.0151, LR: 0.0009740852\n",
      "Epoch [52/500], Batch [90/110], Train Loss: 0.0063, Val Loss: 0.0141, LR: 0.0009739948\n",
      "Epoch [52/500], Batch [100/110], Train Loss: 0.0866, Val Loss: 0.0141, LR: 0.0009739043\n",
      "Epoch [52/500], Batch [110/110], Train Loss: 0.0326, Val Loss: 0.0139, LR: 0.0009738137\n",
      "Epoch [53/500], Batch [10/110], Train Loss: 0.0456, Val Loss: 0.0144, LR: 0.0009737228\n",
      "Epoch [53/500], Batch [20/110], Train Loss: 0.0106, Val Loss: 0.0169, LR: 0.0009736319\n",
      "Epoch [53/500], Batch [30/110], Train Loss: 0.0517, Val Loss: 0.0142, LR: 0.0009735407\n",
      "Epoch [53/500], Batch [40/110], Train Loss: 0.0221, Val Loss: 0.0144, LR: 0.0009734495\n",
      "Epoch [53/500], Batch [50/110], Train Loss: 0.0191, Val Loss: 0.0143, LR: 0.0009733580\n",
      "Epoch [53/500], Batch [60/110], Train Loss: 0.0123, Val Loss: 0.0143, LR: 0.0009732664\n",
      "Epoch [53/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0139, LR: 0.0009731747\n",
      "Epoch [53/500], Batch [80/110], Train Loss: 0.0048, Val Loss: 0.0144, LR: 0.0009730828\n",
      "Epoch [53/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0139, LR: 0.0009729908\n",
      "Epoch [53/500], Batch [100/110], Train Loss: 0.0284, Val Loss: 0.0149, LR: 0.0009728986\n",
      "Epoch [53/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0138, LR: 0.0009728062\n",
      "Epoch [54/500], Batch [10/110], Train Loss: 0.0991, Val Loss: 0.0137, LR: 0.0009727137\n",
      "Epoch [54/500], Batch [20/110], Train Loss: 0.0033, Val Loss: 0.0138, LR: 0.0009726211\n",
      "Epoch [54/500], Batch [30/110], Train Loss: 0.1067, Val Loss: 0.0140, LR: 0.0009725283\n",
      "Epoch [54/500], Batch [40/110], Train Loss: 0.0033, Val Loss: 0.0144, LR: 0.0009724353\n",
      "Epoch [54/500], Batch [50/110], Train Loss: 0.0849, Val Loss: 0.0152, LR: 0.0009723422\n",
      "Epoch [54/500], Batch [60/110], Train Loss: 0.0281, Val Loss: 0.0143, LR: 0.0009722489\n",
      "Epoch [54/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0145, LR: 0.0009721555\n",
      "Epoch [54/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0142, LR: 0.0009720619\n",
      "Epoch [54/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0150, LR: 0.0009719682\n",
      "Epoch [54/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0147, LR: 0.0009718743\n",
      "Epoch [54/500], Batch [110/110], Train Loss: 0.0071, Val Loss: 0.0140, LR: 0.0009717803\n",
      "Epoch [55/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0148, LR: 0.0009716861\n",
      "Epoch [55/500], Batch [20/110], Train Loss: 0.0414, Val Loss: 0.0140, LR: 0.0009715918\n",
      "Epoch [55/500], Batch [30/110], Train Loss: 0.2126, Val Loss: 0.0147, LR: 0.0009714973\n",
      "Epoch [55/500], Batch [40/110], Train Loss: 0.0013, Val Loss: 0.0140, LR: 0.0009714027\n",
      "Epoch [55/500], Batch [50/110], Train Loss: 0.0447, Val Loss: 0.0135, LR: 0.0009713079\n",
      "Epoch [55/500], Batch [60/110], Train Loss: 0.0386, Val Loss: 0.0136, LR: 0.0009712130\n",
      "Epoch [55/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0163, LR: 0.0009711179\n",
      "Epoch [55/500], Batch [80/110], Train Loss: 0.0077, Val Loss: 0.0173, LR: 0.0009710226\n",
      "Epoch [55/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0140, LR: 0.0009709272\n",
      "Epoch [55/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0133, LR: 0.0009708317\n",
      "Epoch [55/500], Batch [110/110], Train Loss: 0.0021, Val Loss: 0.0137, LR: 0.0009707360\n",
      "Epoch [56/500], Batch [10/110], Train Loss: 0.1748, Val Loss: 0.0196, LR: 0.0009706401\n",
      "Epoch [56/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0142, LR: 0.0009705441\n",
      "Epoch [56/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0182, LR: 0.0009704480\n",
      "Epoch [56/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0132, LR: 0.0009703517\n",
      "Epoch [56/500], Batch [50/110], Train Loss: 0.0101, Val Loss: 0.0165, LR: 0.0009702552\n",
      "Epoch [56/500], Batch [60/110], Train Loss: 0.0181, Val Loss: 0.0140, LR: 0.0009701586\n",
      "Epoch [56/500], Batch [70/110], Train Loss: 0.0231, Val Loss: 0.0232, LR: 0.0009700618\n",
      "Epoch [56/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0274, LR: 0.0009699649\n",
      "Epoch [56/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0135, LR: 0.0009698678\n",
      "Epoch [56/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0131, LR: 0.0009697706\n",
      "Epoch [56/500], Batch [110/110], Train Loss: 0.0244, Val Loss: 0.0136, LR: 0.0009696733\n",
      "Epoch [57/500], Batch [10/110], Train Loss: 0.0049, Val Loss: 0.0164, LR: 0.0009695757\n",
      "Epoch [57/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0142, LR: 0.0009694781\n",
      "Epoch [57/500], Batch [30/110], Train Loss: 0.0021, Val Loss: 0.0142, LR: 0.0009693802\n",
      "Epoch [57/500], Batch [40/110], Train Loss: 0.0177, Val Loss: 0.0135, LR: 0.0009692823\n",
      "Epoch [57/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0132, LR: 0.0009691841\n",
      "Epoch [57/500], Batch [60/110], Train Loss: 0.0408, Val Loss: 0.0131, LR: 0.0009690859\n",
      "Epoch [57/500], Batch [70/110], Train Loss: 0.0294, Val Loss: 0.0130, LR: 0.0009689874\n",
      "Epoch [57/500], Batch [80/110], Train Loss: 0.0444, Val Loss: 0.0130, LR: 0.0009688888\n",
      "Epoch [57/500], Batch [90/110], Train Loss: 0.0260, Val Loss: 0.0131, LR: 0.0009687901\n",
      "Epoch [57/500], Batch [100/110], Train Loss: 0.0472, Val Loss: 0.0130, LR: 0.0009686912\n",
      "Epoch [57/500], Batch [110/110], Train Loss: 0.0094, Val Loss: 0.0127, LR: 0.0009685922\n",
      "Epoch [58/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0128, LR: 0.0009684930\n",
      "Epoch [58/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0151, LR: 0.0009683937\n",
      "Epoch [58/500], Batch [30/110], Train Loss: 0.0037, Val Loss: 0.0145, LR: 0.0009682942\n",
      "Epoch [58/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0130, LR: 0.0009681945\n",
      "Epoch [58/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0132, LR: 0.0009680947\n",
      "Epoch [58/500], Batch [60/110], Train Loss: 0.0052, Val Loss: 0.0132, LR: 0.0009679948\n",
      "Epoch [58/500], Batch [70/110], Train Loss: 0.0355, Val Loss: 0.0139, LR: 0.0009678947\n",
      "Epoch [58/500], Batch [80/110], Train Loss: 0.0104, Val Loss: 0.0133, LR: 0.0009677945\n",
      "Epoch [58/500], Batch [90/110], Train Loss: 0.0065, Val Loss: 0.0136, LR: 0.0009676941\n",
      "Epoch [58/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0166, LR: 0.0009675935\n",
      "Epoch [58/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0135, LR: 0.0009674928\n",
      "Epoch [59/500], Batch [10/110], Train Loss: 0.0469, Val Loss: 0.0168, LR: 0.0009673920\n",
      "Epoch [59/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0136, LR: 0.0009672910\n",
      "Epoch [59/500], Batch [30/110], Train Loss: 0.0029, Val Loss: 0.0130, LR: 0.0009671898\n",
      "Epoch [59/500], Batch [40/110], Train Loss: 0.0364, Val Loss: 0.0127, LR: 0.0009670885\n",
      "Epoch [59/500], Batch [50/110], Train Loss: 0.0029, Val Loss: 0.0131, LR: 0.0009669871\n",
      "Epoch [59/500], Batch [60/110], Train Loss: 0.0418, Val Loss: 0.0136, LR: 0.0009668855\n",
      "Epoch [59/500], Batch [70/110], Train Loss: 0.0201, Val Loss: 0.0132, LR: 0.0009667837\n",
      "Epoch [59/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0171, LR: 0.0009666818\n",
      "Epoch [59/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0144, LR: 0.0009665798\n",
      "Epoch [59/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0132, LR: 0.0009664776\n",
      "Epoch [59/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0138, LR: 0.0009663752\n",
      "Epoch [60/500], Batch [10/110], Train Loss: 0.0009, Val Loss: 0.0137, LR: 0.0009662727\n",
      "Epoch [60/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0129, LR: 0.0009661700\n",
      "Epoch [60/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0140, LR: 0.0009660672\n",
      "Epoch [60/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0134, LR: 0.0009659643\n",
      "Epoch [60/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0136, LR: 0.0009658612\n",
      "Epoch [60/500], Batch [60/110], Train Loss: 0.0600, Val Loss: 0.0134, LR: 0.0009657579\n",
      "Epoch [60/500], Batch [70/110], Train Loss: 0.0042, Val Loss: 0.0130, LR: 0.0009656545\n",
      "Epoch [60/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0130, LR: 0.0009655509\n",
      "Epoch [60/500], Batch [90/110], Train Loss: 0.0015, Val Loss: 0.0131, LR: 0.0009654472\n",
      "Epoch [60/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0133, LR: 0.0009653434\n",
      "Epoch [60/500], Batch [110/110], Train Loss: 0.1331, Val Loss: 0.0130, LR: 0.0009652394\n",
      "Epoch [61/500], Batch [10/110], Train Loss: 0.1129, Val Loss: 0.0152, LR: 0.0009651352\n",
      "Epoch [61/500], Batch [20/110], Train Loss: 0.0010, Val Loss: 0.0135, LR: 0.0009650309\n",
      "Epoch [61/500], Batch [30/110], Train Loss: 0.0067, Val Loss: 0.0165, LR: 0.0009649264\n",
      "Epoch [61/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0144, LR: 0.0009648218\n",
      "Epoch [61/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0142, LR: 0.0009647171\n",
      "Epoch [61/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0138, LR: 0.0009646122\n",
      "Epoch [61/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0133, LR: 0.0009645071\n",
      "Epoch [61/500], Batch [80/110], Train Loss: 0.0333, Val Loss: 0.0137, LR: 0.0009644019\n",
      "Epoch [61/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0134, LR: 0.0009642965\n",
      "Epoch [61/500], Batch [100/110], Train Loss: 0.0017, Val Loss: 0.0155, LR: 0.0009641910\n",
      "Epoch [61/500], Batch [110/110], Train Loss: 0.0078, Val Loss: 0.0137, LR: 0.0009640854\n",
      "Confusion Matrix:\n",
      "[[629   8]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99841   0.98744   0.99290       637\n",
      "           1    0.99080   0.99884   0.99481       863\n",
      "\n",
      "    accuracy                        0.99400      1500\n",
      "   macro avg    0.99461   0.99314   0.99385      1500\n",
      "weighted avg    0.99404   0.99400   0.99400      1500\n",
      "\n",
      "Total Errors: 9\n",
      "Index: 41, Predicted: 1, Actual: 0\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Index: 719, Predicted: 1, Actual: 0\n",
      "Index: 721, Predicted: 1, Actual: 0\n",
      "Index: 750, Predicted: 1, Actual: 0\n",
      "Epoch 61: OK- Accuracy: 0.99400, Precision: 0.99080, Recall: 0.99884, F1: 0.99481, ROC AUC: 0.99314, AUPR (PR-AUC): 0.99032, Sensitivity: 0.99884, Specificity: 0.98744, Far: 0.012558869701726845, False Positive Rate (FPR): 0.01256, False Negative Rate (FNR): 0.00116, Runtime: 0.032 sec , Memory Usage: 198.69 MB\n",
      "Epoch [62/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0177, LR: 0.0009639795\n",
      "Epoch [62/500], Batch [20/110], Train Loss: 0.0052, Val Loss: 0.0132, LR: 0.0009638736\n",
      "Epoch [62/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0125, LR: 0.0009637675\n",
      "Epoch [62/500], Batch [40/110], Train Loss: 0.0167, Val Loss: 0.0135, LR: 0.0009636612\n",
      "Epoch [62/500], Batch [50/110], Train Loss: 0.0013, Val Loss: 0.0131, LR: 0.0009635548\n",
      "Epoch [62/500], Batch [60/110], Train Loss: 0.0013, Val Loss: 0.0128, LR: 0.0009634482\n",
      "Epoch [62/500], Batch [70/110], Train Loss: 0.0679, Val Loss: 0.0125, LR: 0.0009633415\n",
      "Epoch [62/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0147, LR: 0.0009632347\n",
      "Epoch [62/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0127, LR: 0.0009631277\n",
      "Epoch [62/500], Batch [100/110], Train Loss: 0.0019, Val Loss: 0.0141, LR: 0.0009630205\n",
      "Epoch [62/500], Batch [110/110], Train Loss: 0.0098, Val Loss: 0.0142, LR: 0.0009629132\n",
      "Epoch [63/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0171, LR: 0.0009628058\n",
      "Epoch [63/500], Batch [20/110], Train Loss: 0.0112, Val Loss: 0.0237, LR: 0.0009626982\n",
      "Epoch [63/500], Batch [30/110], Train Loss: 0.0018, Val Loss: 0.0139, LR: 0.0009625904\n",
      "Epoch [63/500], Batch [40/110], Train Loss: 0.0032, Val Loss: 0.0181, LR: 0.0009624825\n",
      "Epoch [63/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0126, LR: 0.0009623745\n",
      "Epoch [63/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0137, LR: 0.0009622662\n",
      "Epoch [63/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0141, LR: 0.0009621579\n",
      "Epoch [63/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0138, LR: 0.0009620494\n",
      "Epoch [63/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0124, LR: 0.0009619408\n",
      "Epoch [63/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0125, LR: 0.0009618320\n",
      "Epoch [63/500], Batch [110/110], Train Loss: 0.0022, Val Loss: 0.0130, LR: 0.0009617230\n",
      "Epoch [64/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0142, LR: 0.0009616139\n",
      "Epoch [64/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0138, LR: 0.0009615047\n",
      "Epoch [64/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0126, LR: 0.0009613953\n",
      "Epoch [64/500], Batch [40/110], Train Loss: 0.3316, Val Loss: 0.0129, LR: 0.0009612857\n",
      "Epoch [64/500], Batch [50/110], Train Loss: 0.2216, Val Loss: 0.0123, LR: 0.0009611760\n",
      "Epoch [64/500], Batch [60/110], Train Loss: 0.0017, Val Loss: 0.0132, LR: 0.0009610662\n",
      "Epoch [64/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0126, LR: 0.0009609562\n",
      "Epoch [64/500], Batch [80/110], Train Loss: 0.0375, Val Loss: 0.0130, LR: 0.0009608461\n",
      "Epoch [64/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0139, LR: 0.0009607358\n",
      "Epoch [64/500], Batch [100/110], Train Loss: 0.0486, Val Loss: 0.0134, LR: 0.0009606253\n",
      "Epoch [64/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0139, LR: 0.0009605148\n",
      "Epoch [65/500], Batch [10/110], Train Loss: 0.0121, Val Loss: 0.0126, LR: 0.0009604040\n",
      "Epoch [65/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0131, LR: 0.0009602932\n",
      "Epoch [65/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0127, LR: 0.0009601821\n",
      "Epoch [65/500], Batch [40/110], Train Loss: 0.0095, Val Loss: 0.0154, LR: 0.0009600709\n",
      "Epoch [65/500], Batch [50/110], Train Loss: 0.0382, Val Loss: 0.0127, LR: 0.0009599596\n",
      "Epoch [65/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0127, LR: 0.0009598481\n",
      "Epoch [65/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0164, LR: 0.0009597365\n",
      "Epoch [65/500], Batch [80/110], Train Loss: 0.0148, Val Loss: 0.0123, LR: 0.0009596247\n",
      "Epoch [65/500], Batch [90/110], Train Loss: 0.0743, Val Loss: 0.0184, LR: 0.0009595128\n",
      "Epoch [65/500], Batch [100/110], Train Loss: 0.0931, Val Loss: 0.0125, LR: 0.0009594008\n",
      "Epoch [65/500], Batch [110/110], Train Loss: 0.0194, Val Loss: 0.0132, LR: 0.0009592885\n",
      "Epoch [66/500], Batch [10/110], Train Loss: 0.1918, Val Loss: 0.0140, LR: 0.0009591762\n",
      "Epoch [66/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0128, LR: 0.0009590637\n",
      "Epoch [66/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0120, LR: 0.0009589510\n",
      "Epoch [66/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0127, LR: 0.0009588382\n",
      "Epoch [66/500], Batch [50/110], Train Loss: 0.0453, Val Loss: 0.0122, LR: 0.0009587252\n",
      "Epoch [66/500], Batch [60/110], Train Loss: 0.1306, Val Loss: 0.0145, LR: 0.0009586121\n",
      "Epoch [66/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0121, LR: 0.0009584989\n",
      "Epoch [66/500], Batch [80/110], Train Loss: 0.0024, Val Loss: 0.0143, LR: 0.0009583855\n",
      "Epoch [66/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0124, LR: 0.0009582719\n",
      "Epoch [66/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0120, LR: 0.0009581582\n",
      "Epoch [66/500], Batch [110/110], Train Loss: 0.0378, Val Loss: 0.0133, LR: 0.0009580444\n",
      "Epoch [67/500], Batch [10/110], Train Loss: 0.0144, Val Loss: 0.0117, LR: 0.0009579304\n",
      "Epoch [67/500], Batch [20/110], Train Loss: 0.0227, Val Loss: 0.0120, LR: 0.0009578162\n",
      "Epoch [67/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0123, LR: 0.0009577020\n",
      "Epoch [67/500], Batch [40/110], Train Loss: 0.0802, Val Loss: 0.0140, LR: 0.0009575875\n",
      "Epoch [67/500], Batch [50/110], Train Loss: 0.0196, Val Loss: 0.0121, LR: 0.0009574729\n",
      "Epoch [67/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0128, LR: 0.0009573582\n",
      "Epoch [67/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0150, LR: 0.0009572433\n",
      "Epoch [67/500], Batch [80/110], Train Loss: 0.0261, Val Loss: 0.0120, LR: 0.0009571283\n",
      "Epoch [67/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0149, LR: 0.0009570131\n",
      "Epoch [67/500], Batch [100/110], Train Loss: 0.0098, Val Loss: 0.0121, LR: 0.0009568978\n",
      "Epoch [67/500], Batch [110/110], Train Loss: 0.0009, Val Loss: 0.0120, LR: 0.0009567823\n",
      "Epoch [68/500], Batch [10/110], Train Loss: 0.0422, Val Loss: 0.0116, LR: 0.0009566667\n",
      "Epoch [68/500], Batch [20/110], Train Loss: 0.0229, Val Loss: 0.0117, LR: 0.0009565510\n",
      "Epoch [68/500], Batch [30/110], Train Loss: 0.0083, Val Loss: 0.0123, LR: 0.0009564350\n",
      "Epoch [68/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0155, LR: 0.0009563190\n",
      "Epoch [68/500], Batch [50/110], Train Loss: 0.0120, Val Loss: 0.0119, LR: 0.0009562028\n",
      "Epoch [68/500], Batch [60/110], Train Loss: 0.0040, Val Loss: 0.0230, LR: 0.0009560864\n",
      "Epoch [68/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0132, LR: 0.0009559699\n",
      "Epoch [68/500], Batch [80/110], Train Loss: 0.0015, Val Loss: 0.0114, LR: 0.0009558533\n",
      "Epoch [68/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0115, LR: 0.0009557365\n",
      "Epoch [68/500], Batch [100/110], Train Loss: 0.0030, Val Loss: 0.0141, LR: 0.0009556195\n",
      "Epoch [68/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0120, LR: 0.0009555025\n",
      "Epoch [69/500], Batch [10/110], Train Loss: 0.0044, Val Loss: 0.0150, LR: 0.0009553852\n",
      "Epoch [69/500], Batch [20/110], Train Loss: 0.0175, Val Loss: 0.0124, LR: 0.0009552678\n",
      "Epoch [69/500], Batch [30/110], Train Loss: 0.0043, Val Loss: 0.0144, LR: 0.0009551503\n",
      "Epoch [69/500], Batch [40/110], Train Loss: 0.0485, Val Loss: 0.0117, LR: 0.0009550326\n",
      "Epoch [69/500], Batch [50/110], Train Loss: 0.0058, Val Loss: 0.0118, LR: 0.0009549148\n",
      "Epoch [69/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0129, LR: 0.0009547968\n",
      "Epoch [69/500], Batch [70/110], Train Loss: 0.0140, Val Loss: 0.0116, LR: 0.0009546787\n",
      "Epoch [69/500], Batch [80/110], Train Loss: 0.0105, Val Loss: 0.0125, LR: 0.0009545605\n",
      "Epoch [69/500], Batch [90/110], Train Loss: 0.0189, Val Loss: 0.0121, LR: 0.0009544420\n",
      "Epoch [69/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0121, LR: 0.0009543235\n",
      "Epoch [69/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0118, LR: 0.0009542048\n",
      "Epoch [70/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0173, LR: 0.0009540859\n",
      "Epoch [70/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0132, LR: 0.0009539669\n",
      "Epoch [70/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0158, LR: 0.0009538478\n",
      "Epoch [70/500], Batch [40/110], Train Loss: 0.0041, Val Loss: 0.0126, LR: 0.0009537285\n",
      "Epoch [70/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0121, LR: 0.0009536091\n",
      "Epoch [70/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0118, LR: 0.0009534895\n",
      "Epoch [70/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0121, LR: 0.0009533698\n",
      "Epoch [70/500], Batch [80/110], Train Loss: 0.0097, Val Loss: 0.0121, LR: 0.0009532499\n",
      "Epoch [70/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0128, LR: 0.0009531299\n",
      "Epoch [70/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0119, LR: 0.0009530097\n",
      "Epoch [70/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0120, LR: 0.0009528894\n",
      "Epoch [71/500], Batch [10/110], Train Loss: 0.0022, Val Loss: 0.0118, LR: 0.0009527689\n",
      "Epoch [71/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0117, LR: 0.0009526483\n",
      "Epoch [71/500], Batch [30/110], Train Loss: 0.0017, Val Loss: 0.0140, LR: 0.0009525276\n",
      "Epoch [71/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0114, LR: 0.0009524067\n",
      "Epoch [71/500], Batch [50/110], Train Loss: 0.0149, Val Loss: 0.0117, LR: 0.0009522856\n",
      "Epoch [71/500], Batch [60/110], Train Loss: 0.0110, Val Loss: 0.0145, LR: 0.0009521644\n",
      "Epoch [71/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0131, LR: 0.0009520431\n",
      "Epoch [71/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0125, LR: 0.0009519216\n",
      "Epoch [71/500], Batch [90/110], Train Loss: 0.0030, Val Loss: 0.0140, LR: 0.0009518000\n",
      "Epoch [71/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0126, LR: 0.0009516782\n",
      "Epoch [71/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0135, LR: 0.0009515563\n",
      "Confusion Matrix:\n",
      "[[634   3]\n",
      " [  9 854]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98600   0.99529   0.99062       637\n",
      "           1    0.99650   0.98957   0.99302       863\n",
      "\n",
      "    accuracy                        0.99200      1500\n",
      "   macro avg    0.99125   0.99243   0.99182      1500\n",
      "weighted avg    0.99204   0.99200   0.99200      1500\n",
      "\n",
      "Total Errors: 12\n",
      "Index: 407, Predicted: 0, Actual: 1\n",
      "Index: 554, Predicted: 0, Actual: 1\n",
      "Index: 569, Predicted: 0, Actual: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Index: 721, Predicted: 1, Actual: 0\n",
      "Epoch 71: OK- Accuracy: 0.99200, Precision: 0.99650, Recall: 0.98957, F1: 0.99302, ROC AUC: 0.99243, AUPR (PR-AUC): 0.99211, Sensitivity: 0.98957, Specificity: 0.99529, Far: 0.004709576138147566, False Positive Rate (FPR): 0.00471, False Negative Rate (FNR): 0.01043, Runtime: 0.038 sec , Memory Usage: 198.71 MB\n",
      "Epoch [72/500], Batch [10/110], Train Loss: 0.0316, Val Loss: 0.0118, LR: 0.0009514342\n",
      "Epoch [72/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0120, LR: 0.0009513120\n",
      "Epoch [72/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0117, LR: 0.0009511897\n",
      "Epoch [72/500], Batch [40/110], Train Loss: 0.0058, Val Loss: 0.0147, LR: 0.0009510672\n",
      "Epoch [72/500], Batch [50/110], Train Loss: 0.0197, Val Loss: 0.0129, LR: 0.0009509445\n",
      "Epoch [72/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0124, LR: 0.0009508217\n",
      "Epoch [72/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0120, LR: 0.0009506988\n",
      "Epoch [72/500], Batch [80/110], Train Loss: 0.0020, Val Loss: 0.0120, LR: 0.0009505757\n",
      "Epoch [72/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0116, LR: 0.0009504525\n",
      "Epoch [72/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0134, LR: 0.0009503291\n",
      "Epoch [72/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0118, LR: 0.0009502056\n",
      "Epoch [73/500], Batch [10/110], Train Loss: 0.0222, Val Loss: 0.0124, LR: 0.0009500819\n",
      "Epoch [73/500], Batch [20/110], Train Loss: 0.0117, Val Loss: 0.0136, LR: 0.0009499581\n",
      "Epoch [73/500], Batch [30/110], Train Loss: 0.0120, Val Loss: 0.0124, LR: 0.0009498342\n",
      "Epoch [73/500], Batch [40/110], Train Loss: 0.0046, Val Loss: 0.0135, LR: 0.0009497101\n",
      "Epoch [73/500], Batch [50/110], Train Loss: 0.0025, Val Loss: 0.0186, LR: 0.0009495858\n",
      "Epoch [73/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0129, LR: 0.0009494614\n",
      "Epoch [73/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0116, LR: 0.0009493369\n",
      "Epoch [73/500], Batch [80/110], Train Loss: 0.0547, Val Loss: 0.0108, LR: 0.0009492122\n",
      "Epoch [73/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0110, LR: 0.0009490874\n",
      "Epoch [73/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0112, LR: 0.0009489624\n",
      "Epoch [73/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0144, LR: 0.0009488373\n",
      "Epoch [74/500], Batch [10/110], Train Loss: 0.0610, Val Loss: 0.0113, LR: 0.0009487121\n",
      "Epoch [74/500], Batch [20/110], Train Loss: 0.0029, Val Loss: 0.0115, LR: 0.0009485866\n",
      "Epoch [74/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0131, LR: 0.0009484611\n",
      "Epoch [74/500], Batch [40/110], Train Loss: 0.0181, Val Loss: 0.0119, LR: 0.0009483354\n",
      "Epoch [74/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0148, LR: 0.0009482096\n",
      "Epoch [74/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0139, LR: 0.0009480836\n",
      "Epoch [74/500], Batch [70/110], Train Loss: 0.3496, Val Loss: 0.0118, LR: 0.0009479575\n",
      "Epoch [74/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0118, LR: 0.0009478312\n",
      "Epoch [74/500], Batch [90/110], Train Loss: 0.0239, Val Loss: 0.0114, LR: 0.0009477048\n",
      "Epoch [74/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0120, LR: 0.0009475782\n",
      "Epoch [74/500], Batch [110/110], Train Loss: 0.0232, Val Loss: 0.0115, LR: 0.0009474515\n",
      "Epoch [75/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0116, LR: 0.0009473247\n",
      "Epoch [75/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0119, LR: 0.0009471977\n",
      "Epoch [75/500], Batch [30/110], Train Loss: 0.0275, Val Loss: 0.0142, LR: 0.0009470705\n",
      "Epoch [75/500], Batch [40/110], Train Loss: 0.0024, Val Loss: 0.0119, LR: 0.0009469432\n",
      "Epoch [75/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0120, LR: 0.0009468158\n",
      "Epoch [75/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0122, LR: 0.0009466882\n",
      "Epoch [75/500], Batch [70/110], Train Loss: 0.0025, Val Loss: 0.0151, LR: 0.0009465605\n",
      "Epoch [75/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0147, LR: 0.0009464327\n",
      "Epoch [75/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0116, LR: 0.0009463047\n",
      "Epoch [75/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0159, LR: 0.0009461765\n",
      "Epoch [75/500], Batch [110/110], Train Loss: 0.0401, Val Loss: 0.0119, LR: 0.0009460482\n",
      "Epoch [76/500], Batch [10/110], Train Loss: 0.0018, Val Loss: 0.0124, LR: 0.0009459198\n",
      "Epoch [76/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0115, LR: 0.0009457912\n",
      "Epoch [76/500], Batch [30/110], Train Loss: 0.2353, Val Loss: 0.0114, LR: 0.0009456625\n",
      "Epoch [76/500], Batch [40/110], Train Loss: 0.0055, Val Loss: 0.0116, LR: 0.0009455336\n",
      "Epoch [76/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0009454046\n",
      "Epoch [76/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0119, LR: 0.0009452755\n",
      "Epoch [76/500], Batch [70/110], Train Loss: 0.0179, Val Loss: 0.0116, LR: 0.0009451462\n",
      "Epoch [76/500], Batch [80/110], Train Loss: 0.0217, Val Loss: 0.0120, LR: 0.0009450167\n",
      "Epoch [76/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0126, LR: 0.0009448871\n",
      "Epoch [76/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0120, LR: 0.0009447574\n",
      "Epoch [76/500], Batch [110/110], Train Loss: 0.1474, Val Loss: 0.0116, LR: 0.0009446275\n",
      "Epoch [77/500], Batch [10/110], Train Loss: 0.0052, Val Loss: 0.0140, LR: 0.0009444975\n",
      "Epoch [77/500], Batch [20/110], Train Loss: 0.0217, Val Loss: 0.0122, LR: 0.0009443674\n",
      "Epoch [77/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0128, LR: 0.0009442371\n",
      "Epoch [77/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0119, LR: 0.0009441066\n",
      "Epoch [77/500], Batch [50/110], Train Loss: 0.0043, Val Loss: 0.0117, LR: 0.0009439760\n",
      "Epoch [77/500], Batch [60/110], Train Loss: 0.0095, Val Loss: 0.0118, LR: 0.0009438453\n",
      "Epoch [77/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0117, LR: 0.0009437144\n",
      "Epoch [77/500], Batch [80/110], Train Loss: 0.0036, Val Loss: 0.0118, LR: 0.0009435834\n",
      "Epoch [77/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0113, LR: 0.0009434522\n",
      "Epoch [77/500], Batch [100/110], Train Loss: 0.0149, Val Loss: 0.0108, LR: 0.0009433209\n",
      "Epoch [77/500], Batch [110/110], Train Loss: 0.0418, Val Loss: 0.0106, LR: 0.0009431895\n",
      "Epoch [78/500], Batch [10/110], Train Loss: 0.0034, Val Loss: 0.0105, LR: 0.0009430579\n",
      "Epoch [78/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0009429262\n",
      "Epoch [78/500], Batch [30/110], Train Loss: 0.0491, Val Loss: 0.0108, LR: 0.0009427943\n",
      "Epoch [78/500], Batch [40/110], Train Loss: 0.0104, Val Loss: 0.0105, LR: 0.0009426623\n",
      "Epoch [78/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0105, LR: 0.0009425301\n",
      "Epoch [78/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0107, LR: 0.0009423978\n",
      "Epoch [78/500], Batch [70/110], Train Loss: 0.0204, Val Loss: 0.0111, LR: 0.0009422654\n",
      "Epoch [78/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0120, LR: 0.0009421328\n",
      "Epoch [78/500], Batch [90/110], Train Loss: 0.0073, Val Loss: 0.0133, LR: 0.0009420000\n",
      "Epoch [78/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0155, LR: 0.0009418672\n",
      "Epoch [78/500], Batch [110/110], Train Loss: 0.0116, Val Loss: 0.0115, LR: 0.0009417342\n",
      "Epoch [79/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0138, LR: 0.0009416010\n",
      "Epoch [79/500], Batch [20/110], Train Loss: 0.0072, Val Loss: 0.0124, LR: 0.0009414677\n",
      "Epoch [79/500], Batch [30/110], Train Loss: 0.0069, Val Loss: 0.0119, LR: 0.0009413343\n",
      "Epoch [79/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0134, LR: 0.0009412007\n",
      "Epoch [79/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0110, LR: 0.0009410669\n",
      "Epoch [79/500], Batch [60/110], Train Loss: 0.0020, Val Loss: 0.0143, LR: 0.0009409331\n",
      "Epoch [79/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0147, LR: 0.0009407990\n",
      "Epoch [79/500], Batch [80/110], Train Loss: 0.0015, Val Loss: 0.0111, LR: 0.0009406649\n",
      "Epoch [79/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0114, LR: 0.0009405306\n",
      "Epoch [79/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0107, LR: 0.0009403962\n",
      "Epoch [79/500], Batch [110/110], Train Loss: 0.0160, Val Loss: 0.0110, LR: 0.0009402616\n",
      "Epoch [80/500], Batch [10/110], Train Loss: 0.1308, Val Loss: 0.0121, LR: 0.0009401268\n",
      "Epoch [80/500], Batch [20/110], Train Loss: 0.0199, Val Loss: 0.0108, LR: 0.0009399920\n",
      "Epoch [80/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0107, LR: 0.0009398570\n",
      "Epoch [80/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0111, LR: 0.0009397218\n",
      "Epoch [80/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0109, LR: 0.0009395865\n",
      "Epoch [80/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0118, LR: 0.0009394511\n",
      "Epoch [80/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0116, LR: 0.0009393155\n",
      "Epoch [80/500], Batch [80/110], Train Loss: 0.2152, Val Loss: 0.0108, LR: 0.0009391798\n",
      "Epoch [80/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0137, LR: 0.0009390439\n",
      "Epoch [80/500], Batch [100/110], Train Loss: 0.0370, Val Loss: 0.0117, LR: 0.0009389079\n",
      "Epoch [80/500], Batch [110/110], Train Loss: 0.0135, Val Loss: 0.0108, LR: 0.0009387718\n",
      "Epoch [81/500], Batch [10/110], Train Loss: 0.0117, Val Loss: 0.0108, LR: 0.0009386355\n",
      "Epoch [81/500], Batch [20/110], Train Loss: 0.0014, Val Loss: 0.0109, LR: 0.0009384991\n",
      "Epoch [81/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0113, LR: 0.0009383625\n",
      "Epoch [81/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0157, LR: 0.0009382258\n",
      "Epoch [81/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0116, LR: 0.0009380890\n",
      "Epoch [81/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0113, LR: 0.0009379520\n",
      "Epoch [81/500], Batch [70/110], Train Loss: 0.0707, Val Loss: 0.0114, LR: 0.0009378149\n",
      "Epoch [81/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0113, LR: 0.0009376776\n",
      "Epoch [81/500], Batch [90/110], Train Loss: 0.0554, Val Loss: 0.0133, LR: 0.0009375402\n",
      "Epoch [81/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0113, LR: 0.0009374026\n",
      "Epoch [81/500], Batch [110/110], Train Loss: 0.0220, Val Loss: 0.0115, LR: 0.0009372649\n",
      "Confusion Matrix:\n",
      "[[634   3]\n",
      " [  2 861]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99686   0.99529   0.99607       637\n",
      "           1    0.99653   0.99768   0.99710       863\n",
      "\n",
      "    accuracy                        0.99667      1500\n",
      "   macro avg    0.99669   0.99649   0.99659      1500\n",
      "weighted avg    0.99667   0.99667   0.99667      1500\n",
      "\n",
      "Total Errors: 5\n",
      "Index: 407, Predicted: 0, Actual: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Index: 721, Predicted: 1, Actual: 0\n",
      "Index: 750, Predicted: 1, Actual: 0\n",
      "Index: 1197, Predicted: 1, Actual: 0\n",
      "Epoch 81: OK- Accuracy: 0.99667, Precision: 0.99653, Recall: 0.99768, F1: 0.99710, ROC AUC: 0.99649, AUPR (PR-AUC): 0.99555, Sensitivity: 0.99768, Specificity: 0.99529, Far: 0.004709576138147566, False Positive Rate (FPR): 0.00471, False Negative Rate (FNR): 0.00232, Runtime: 0.033 sec , Memory Usage: 198.71 MB\n",
      "Epoch [82/500], Batch [10/110], Train Loss: 0.0204, Val Loss: 0.0137, LR: 0.0009371271\n",
      "Epoch [82/500], Batch [20/110], Train Loss: 0.2365, Val Loss: 0.0112, LR: 0.0009369891\n",
      "Epoch [82/500], Batch [30/110], Train Loss: 0.0118, Val Loss: 0.0118, LR: 0.0009368510\n",
      "Epoch [82/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0114, LR: 0.0009367127\n",
      "Epoch [82/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0108, LR: 0.0009365743\n",
      "Epoch [82/500], Batch [60/110], Train Loss: 0.0077, Val Loss: 0.0102, LR: 0.0009364358\n",
      "Epoch [82/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0168, LR: 0.0009362971\n",
      "Epoch [82/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0009361583\n",
      "Epoch [82/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0162, LR: 0.0009360193\n",
      "Epoch [82/500], Batch [100/110], Train Loss: 0.0180, Val Loss: 0.0110, LR: 0.0009358802\n",
      "Epoch [82/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0119, LR: 0.0009357410\n",
      "Epoch [83/500], Batch [10/110], Train Loss: 0.0488, Val Loss: 0.0110, LR: 0.0009356016\n",
      "Epoch [83/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0009354620\n",
      "Epoch [83/500], Batch [30/110], Train Loss: 0.0195, Val Loss: 0.0108, LR: 0.0009353224\n",
      "Epoch [83/500], Batch [40/110], Train Loss: 0.0405, Val Loss: 0.0111, LR: 0.0009351826\n",
      "Epoch [83/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0120, LR: 0.0009350426\n",
      "Epoch [83/500], Batch [60/110], Train Loss: 0.2076, Val Loss: 0.0112, LR: 0.0009349025\n",
      "Epoch [83/500], Batch [70/110], Train Loss: 0.0081, Val Loss: 0.0124, LR: 0.0009347623\n",
      "Epoch [83/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0130, LR: 0.0009346219\n",
      "Epoch [83/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0114, LR: 0.0009344814\n",
      "Epoch [83/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0113, LR: 0.0009343408\n",
      "Epoch [83/500], Batch [110/110], Train Loss: 0.0258, Val Loss: 0.0111, LR: 0.0009342000\n",
      "Epoch [84/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0112, LR: 0.0009340591\n",
      "Epoch [84/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0110, LR: 0.0009339180\n",
      "Epoch [84/500], Batch [30/110], Train Loss: 0.0378, Val Loss: 0.0110, LR: 0.0009337768\n",
      "Epoch [84/500], Batch [40/110], Train Loss: 0.0142, Val Loss: 0.0113, LR: 0.0009336354\n",
      "Epoch [84/500], Batch [50/110], Train Loss: 0.0266, Val Loss: 0.0119, LR: 0.0009334940\n",
      "Epoch [84/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0118, LR: 0.0009333523\n",
      "Epoch [84/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0116, LR: 0.0009332106\n",
      "Epoch [84/500], Batch [80/110], Train Loss: 0.0026, Val Loss: 0.0112, LR: 0.0009330687\n",
      "Epoch [84/500], Batch [90/110], Train Loss: 0.0085, Val Loss: 0.0110, LR: 0.0009329266\n",
      "Epoch [84/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0111, LR: 0.0009327844\n",
      "Epoch [84/500], Batch [110/110], Train Loss: 0.0061, Val Loss: 0.0136, LR: 0.0009326421\n",
      "Epoch [85/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0121, LR: 0.0009324996\n",
      "Epoch [85/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0113, LR: 0.0009323570\n",
      "Epoch [85/500], Batch [30/110], Train Loss: 0.0113, Val Loss: 0.0109, LR: 0.0009322143\n",
      "Epoch [85/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0112, LR: 0.0009320714\n",
      "Epoch [85/500], Batch [50/110], Train Loss: 0.0280, Val Loss: 0.0107, LR: 0.0009319284\n",
      "Epoch [85/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0009317852\n",
      "Epoch [85/500], Batch [70/110], Train Loss: 0.0175, Val Loss: 0.0107, LR: 0.0009316419\n",
      "Epoch [85/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0109, LR: 0.0009314985\n",
      "Epoch [85/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0110, LR: 0.0009313549\n",
      "Epoch [85/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0116, LR: 0.0009312112\n",
      "Epoch [85/500], Batch [110/110], Train Loss: 0.0050, Val Loss: 0.0116, LR: 0.0009310673\n",
      "Epoch [86/500], Batch [10/110], Train Loss: 0.0021, Val Loss: 0.0110, LR: 0.0009309233\n",
      "Epoch [86/500], Batch [20/110], Train Loss: 0.1099, Val Loss: 0.0134, LR: 0.0009307792\n",
      "Epoch [86/500], Batch [30/110], Train Loss: 0.0019, Val Loss: 0.0109, LR: 0.0009306349\n",
      "Epoch [86/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0009304905\n",
      "Epoch [86/500], Batch [50/110], Train Loss: 0.0014, Val Loss: 0.0117, LR: 0.0009303459\n",
      "Epoch [86/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0120, LR: 0.0009302012\n",
      "Epoch [86/500], Batch [70/110], Train Loss: 0.0219, Val Loss: 0.0111, LR: 0.0009300564\n",
      "Epoch [86/500], Batch [80/110], Train Loss: 0.0269, Val Loss: 0.0114, LR: 0.0009299114\n",
      "Epoch [86/500], Batch [90/110], Train Loss: 0.0230, Val Loss: 0.0106, LR: 0.0009297663\n",
      "Epoch [86/500], Batch [100/110], Train Loss: 0.0268, Val Loss: 0.0105, LR: 0.0009296211\n",
      "Epoch [86/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0107, LR: 0.0009294757\n",
      "Epoch [87/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0110, LR: 0.0009293302\n",
      "Epoch [87/500], Batch [20/110], Train Loss: 0.0335, Val Loss: 0.0109, LR: 0.0009291845\n",
      "Epoch [87/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0009290387\n",
      "Epoch [87/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0109, LR: 0.0009288928\n",
      "Epoch [87/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0133, LR: 0.0009287467\n",
      "Epoch [87/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0009286005\n",
      "Epoch [87/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0145, LR: 0.0009284541\n",
      "Epoch [87/500], Batch [80/110], Train Loss: 0.0330, Val Loss: 0.0112, LR: 0.0009283076\n",
      "Epoch [87/500], Batch [90/110], Train Loss: 0.0021, Val Loss: 0.0110, LR: 0.0009281610\n",
      "Epoch [87/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0108, LR: 0.0009280142\n",
      "Epoch [87/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0108, LR: 0.0009278673\n",
      "Epoch [88/500], Batch [10/110], Train Loss: 0.0092, Val Loss: 0.0106, LR: 0.0009277203\n",
      "Epoch [88/500], Batch [20/110], Train Loss: 0.0091, Val Loss: 0.0110, LR: 0.0009275731\n",
      "Epoch [88/500], Batch [30/110], Train Loss: 0.0101, Val Loss: 0.0109, LR: 0.0009274258\n",
      "Epoch [88/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0151, LR: 0.0009272783\n",
      "Epoch [88/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0117, LR: 0.0009271307\n",
      "Epoch [88/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0110, LR: 0.0009269830\n",
      "Epoch [88/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0009268351\n",
      "Epoch [88/500], Batch [80/110], Train Loss: 0.0140, Val Loss: 0.0106, LR: 0.0009266871\n",
      "Epoch [88/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0009265390\n",
      "Epoch [88/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0009263907\n",
      "Epoch [88/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0103, LR: 0.0009262423\n",
      "Epoch [89/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0009260937\n",
      "Epoch [89/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0112, LR: 0.0009259450\n",
      "Epoch [89/500], Batch [30/110], Train Loss: 0.0040, Val Loss: 0.0124, LR: 0.0009257962\n",
      "Epoch [89/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0009256472\n",
      "Epoch [89/500], Batch [50/110], Train Loss: 0.0033, Val Loss: 0.0116, LR: 0.0009254981\n",
      "Epoch [89/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0112, LR: 0.0009253489\n",
      "Epoch [89/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0120, LR: 0.0009251995\n",
      "Epoch [89/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0153, LR: 0.0009250500\n",
      "Epoch [89/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0115, LR: 0.0009249003\n",
      "Epoch [89/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0112, LR: 0.0009247505\n",
      "Epoch [89/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0119, LR: 0.0009246006\n",
      "Epoch [90/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0139, LR: 0.0009244505\n",
      "Epoch [90/500], Batch [20/110], Train Loss: 0.0556, Val Loss: 0.0139, LR: 0.0009243003\n",
      "Epoch [90/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0133, LR: 0.0009241500\n",
      "Epoch [90/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0109, LR: 0.0009239995\n",
      "Epoch [90/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0150, LR: 0.0009238489\n",
      "Epoch [90/500], Batch [60/110], Train Loss: 0.0648, Val Loss: 0.0107, LR: 0.0009236981\n",
      "Epoch [90/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0009235472\n",
      "Epoch [90/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0009233962\n",
      "Epoch [90/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0009232451\n",
      "Epoch [90/500], Batch [100/110], Train Loss: 0.0268, Val Loss: 0.0108, LR: 0.0009230938\n",
      "Epoch [90/500], Batch [110/110], Train Loss: 0.0162, Val Loss: 0.0110, LR: 0.0009229423\n",
      "Epoch [91/500], Batch [10/110], Train Loss: 0.0209, Val Loss: 0.0110, LR: 0.0009227908\n",
      "Epoch [91/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0111, LR: 0.0009226390\n",
      "Epoch [91/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0108, LR: 0.0009224872\n",
      "Epoch [91/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0009223352\n",
      "Epoch [91/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0107, LR: 0.0009221831\n",
      "Epoch [91/500], Batch [60/110], Train Loss: 0.0138, Val Loss: 0.0109, LR: 0.0009220309\n",
      "Epoch [91/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0117, LR: 0.0009218785\n",
      "Epoch [91/500], Batch [80/110], Train Loss: 0.0154, Val Loss: 0.0114, LR: 0.0009217260\n",
      "Epoch [91/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0142, LR: 0.0009215733\n",
      "Epoch [91/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0115, LR: 0.0009214205\n",
      "Epoch [91/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0009212676\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 91: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 198.71 MB\n",
      "Epoch [92/500], Batch [10/110], Train Loss: 0.0096, Val Loss: 0.0114, LR: 0.0009211145\n",
      "Epoch [92/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0009209613\n",
      "Epoch [92/500], Batch [30/110], Train Loss: 0.0065, Val Loss: 0.0108, LR: 0.0009208080\n",
      "Epoch [92/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0109, LR: 0.0009206545\n",
      "Epoch [92/500], Batch [50/110], Train Loss: 0.0157, Val Loss: 0.0107, LR: 0.0009205009\n",
      "Epoch [92/500], Batch [60/110], Train Loss: 0.0068, Val Loss: 0.0111, LR: 0.0009203471\n",
      "Epoch [92/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0128, LR: 0.0009201933\n",
      "Epoch [92/500], Batch [80/110], Train Loss: 0.0355, Val Loss: 0.0108, LR: 0.0009200392\n",
      "Epoch [92/500], Batch [90/110], Train Loss: 0.0042, Val Loss: 0.0107, LR: 0.0009198851\n",
      "Epoch [92/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0105, LR: 0.0009197308\n",
      "Epoch [92/500], Batch [110/110], Train Loss: 0.0225, Val Loss: 0.0106, LR: 0.0009195764\n",
      "Epoch [93/500], Batch [10/110], Train Loss: 0.0015, Val Loss: 0.0106, LR: 0.0009194218\n",
      "Epoch [93/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0009192671\n",
      "Epoch [93/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0108, LR: 0.0009191123\n",
      "Epoch [93/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0120, LR: 0.0009189573\n",
      "Epoch [93/500], Batch [50/110], Train Loss: 0.0608, Val Loss: 0.0163, LR: 0.0009188022\n",
      "Epoch [93/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0130, LR: 0.0009186470\n",
      "Epoch [93/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0139, LR: 0.0009184916\n",
      "Epoch [93/500], Batch [80/110], Train Loss: 0.0038, Val Loss: 0.0126, LR: 0.0009183361\n",
      "Epoch [93/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0127, LR: 0.0009181805\n",
      "Epoch [93/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0116, LR: 0.0009180247\n",
      "Epoch [93/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0009178688\n",
      "Epoch [94/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0009177128\n",
      "Epoch [94/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0114, LR: 0.0009175566\n",
      "Epoch [94/500], Batch [30/110], Train Loss: 0.0030, Val Loss: 0.0116, LR: 0.0009174003\n",
      "Epoch [94/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0110, LR: 0.0009172439\n",
      "Epoch [94/500], Batch [50/110], Train Loss: 0.0335, Val Loss: 0.0120, LR: 0.0009170873\n",
      "Epoch [94/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0110, LR: 0.0009169306\n",
      "Epoch [94/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0132, LR: 0.0009167737\n",
      "Epoch [94/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0113, LR: 0.0009166167\n",
      "Epoch [94/500], Batch [90/110], Train Loss: 0.0170, Val Loss: 0.0127, LR: 0.0009164596\n",
      "Epoch [94/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0112, LR: 0.0009163024\n",
      "Epoch [94/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0105, LR: 0.0009161450\n",
      "Epoch [95/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0009159875\n",
      "Epoch [95/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0105, LR: 0.0009158298\n",
      "Epoch [95/500], Batch [30/110], Train Loss: 0.0205, Val Loss: 0.0099, LR: 0.0009156720\n",
      "Epoch [95/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0009155141\n",
      "Epoch [95/500], Batch [50/110], Train Loss: 0.0012, Val Loss: 0.0103, LR: 0.0009153560\n",
      "Epoch [95/500], Batch [60/110], Train Loss: 0.0212, Val Loss: 0.0103, LR: 0.0009151978\n",
      "Epoch [95/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0106, LR: 0.0009150395\n",
      "Epoch [95/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0144, LR: 0.0009148811\n",
      "Epoch [95/500], Batch [90/110], Train Loss: 0.0189, Val Loss: 0.0110, LR: 0.0009147225\n",
      "Epoch [95/500], Batch [100/110], Train Loss: 0.0164, Val Loss: 0.0137, LR: 0.0009145637\n",
      "Epoch [95/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0009144049\n",
      "Epoch [96/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0126, LR: 0.0009142459\n",
      "Epoch [96/500], Batch [20/110], Train Loss: 0.0046, Val Loss: 0.0102, LR: 0.0009140868\n",
      "Epoch [96/500], Batch [30/110], Train Loss: 0.0148, Val Loss: 0.0102, LR: 0.0009139275\n",
      "Epoch [96/500], Batch [40/110], Train Loss: 0.0030, Val Loss: 0.0117, LR: 0.0009137681\n",
      "Epoch [96/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0009136086\n",
      "Epoch [96/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0009134489\n",
      "Epoch [96/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0009132891\n",
      "Epoch [96/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0009131292\n",
      "Epoch [96/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0009129692\n",
      "Epoch [96/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0100, LR: 0.0009128090\n",
      "Epoch [96/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0009126486\n",
      "Epoch [97/500], Batch [10/110], Train Loss: 0.0220, Val Loss: 0.0102, LR: 0.0009124882\n",
      "Epoch [97/500], Batch [20/110], Train Loss: 0.0045, Val Loss: 0.0130, LR: 0.0009123276\n",
      "Epoch [97/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0141, LR: 0.0009121669\n",
      "Epoch [97/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0009120060\n",
      "Epoch [97/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0105, LR: 0.0009118450\n",
      "Epoch [97/500], Batch [60/110], Train Loss: 0.0054, Val Loss: 0.0103, LR: 0.0009116839\n",
      "Epoch [97/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0009115226\n",
      "Epoch [97/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0127, LR: 0.0009113613\n",
      "Epoch [97/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0009111997\n",
      "Epoch [97/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0106, LR: 0.0009110381\n",
      "Epoch [97/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0009108763\n",
      "Epoch [98/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0009107144\n",
      "Epoch [98/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0009105523\n",
      "Epoch [98/500], Batch [30/110], Train Loss: 0.0032, Val Loss: 0.0107, LR: 0.0009103901\n",
      "Epoch [98/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0009102278\n",
      "Epoch [98/500], Batch [50/110], Train Loss: 0.0217, Val Loss: 0.0115, LR: 0.0009100654\n",
      "Epoch [98/500], Batch [60/110], Train Loss: 0.0149, Val Loss: 0.0096, LR: 0.0009099028\n",
      "Epoch [98/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0096, LR: 0.0009097401\n",
      "Epoch [98/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0116, LR: 0.0009095773\n",
      "Epoch [98/500], Batch [90/110], Train Loss: 0.0327, Val Loss: 0.0117, LR: 0.0009094143\n",
      "Epoch [98/500], Batch [100/110], Train Loss: 0.0024, Val Loss: 0.0103, LR: 0.0009092512\n",
      "Epoch [98/500], Batch [110/110], Train Loss: 0.0100, Val Loss: 0.0120, LR: 0.0009090879\n",
      "Epoch [99/500], Batch [10/110], Train Loss: 0.0103, Val Loss: 0.0110, LR: 0.0009089246\n",
      "Epoch [99/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0009087611\n",
      "Epoch [99/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0106, LR: 0.0009085974\n",
      "Epoch [99/500], Batch [40/110], Train Loss: 0.0074, Val Loss: 0.0118, LR: 0.0009084337\n",
      "Epoch [99/500], Batch [50/110], Train Loss: 0.0142, Val Loss: 0.0108, LR: 0.0009082698\n",
      "Epoch [99/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0107, LR: 0.0009081057\n",
      "Epoch [99/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0107, LR: 0.0009079416\n",
      "Epoch [99/500], Batch [80/110], Train Loss: 0.0144, Val Loss: 0.0113, LR: 0.0009077773\n",
      "Epoch [99/500], Batch [90/110], Train Loss: 0.0077, Val Loss: 0.0110, LR: 0.0009076129\n",
      "Epoch [99/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0009074483\n",
      "Epoch [99/500], Batch [110/110], Train Loss: 0.0059, Val Loss: 0.0113, LR: 0.0009072836\n",
      "Epoch [100/500], Batch [10/110], Train Loss: 0.0022, Val Loss: 0.0104, LR: 0.0009071188\n",
      "Epoch [100/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0122, LR: 0.0009069538\n",
      "Epoch [100/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0009067888\n",
      "Epoch [100/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0130, LR: 0.0009066236\n",
      "Epoch [100/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0099, LR: 0.0009064582\n",
      "Epoch [100/500], Batch [60/110], Train Loss: 0.0092, Val Loss: 0.0097, LR: 0.0009062927\n",
      "Epoch [100/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0009061271\n",
      "Epoch [100/500], Batch [80/110], Train Loss: 0.0059, Val Loss: 0.0109, LR: 0.0009059614\n",
      "Epoch [100/500], Batch [90/110], Train Loss: 0.0326, Val Loss: 0.0102, LR: 0.0009057955\n",
      "Epoch [100/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0009056295\n",
      "Epoch [100/500], Batch [110/110], Train Loss: 0.0014, Val Loss: 0.0097, LR: 0.0009054634\n",
      "Epoch [101/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0103, LR: 0.0009052972\n",
      "Epoch [101/500], Batch [20/110], Train Loss: 0.0564, Val Loss: 0.0107, LR: 0.0009051308\n",
      "Epoch [101/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0009049642\n",
      "Epoch [101/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0009047976\n",
      "Epoch [101/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0112, LR: 0.0009046308\n",
      "Epoch [101/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0111, LR: 0.0009044639\n",
      "Epoch [101/500], Batch [70/110], Train Loss: 0.0012, Val Loss: 0.0108, LR: 0.0009042969\n",
      "Epoch [101/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0009041297\n",
      "Epoch [101/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0009039624\n",
      "Epoch [101/500], Batch [100/110], Train Loss: 0.0036, Val Loss: 0.0101, LR: 0.0009037950\n",
      "Epoch [101/500], Batch [110/110], Train Loss: 0.0020, Val Loss: 0.0099, LR: 0.0009036274\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 101: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 198.71 MB\n",
      "Epoch [102/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0009034597\n",
      "Epoch [102/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0009032919\n",
      "Epoch [102/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0105, LR: 0.0009031239\n",
      "Epoch [102/500], Batch [40/110], Train Loss: 0.0223, Val Loss: 0.0105, LR: 0.0009029559\n",
      "Epoch [102/500], Batch [50/110], Train Loss: 0.0052, Val Loss: 0.0123, LR: 0.0009027876\n",
      "Epoch [102/500], Batch [60/110], Train Loss: 0.0089, Val Loss: 0.0120, LR: 0.0009026193\n",
      "Epoch [102/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0009024508\n",
      "Epoch [102/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0117, LR: 0.0009022822\n",
      "Epoch [102/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0009021135\n",
      "Epoch [102/500], Batch [100/110], Train Loss: 0.0046, Val Loss: 0.0103, LR: 0.0009019446\n",
      "Epoch [102/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0098, LR: 0.0009017757\n",
      "Epoch [103/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0101, LR: 0.0009016065\n",
      "Epoch [103/500], Batch [20/110], Train Loss: 0.0030, Val Loss: 0.0096, LR: 0.0009014373\n",
      "Epoch [103/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0009012679\n",
      "Epoch [103/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0096, LR: 0.0009010984\n",
      "Epoch [103/500], Batch [50/110], Train Loss: 0.0029, Val Loss: 0.0096, LR: 0.0009009288\n",
      "Epoch [103/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0009007590\n",
      "Epoch [103/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0100, LR: 0.0009005891\n",
      "Epoch [103/500], Batch [80/110], Train Loss: 0.0033, Val Loss: 0.0108, LR: 0.0009004191\n",
      "Epoch [103/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0009002489\n",
      "Epoch [103/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0009000787\n",
      "Epoch [103/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0110, LR: 0.0008999082\n",
      "Epoch [104/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0102, LR: 0.0008997377\n",
      "Epoch [104/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0008995670\n",
      "Epoch [104/500], Batch [30/110], Train Loss: 0.0050, Val Loss: 0.0108, LR: 0.0008993962\n",
      "Epoch [104/500], Batch [40/110], Train Loss: 0.0045, Val Loss: 0.0097, LR: 0.0008992253\n",
      "Epoch [104/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0100, LR: 0.0008990543\n",
      "Epoch [104/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0008988831\n",
      "Epoch [104/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0118, LR: 0.0008987118\n",
      "Epoch [104/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0008985403\n",
      "Epoch [104/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0099, LR: 0.0008983688\n",
      "Epoch [104/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0008981971\n",
      "Epoch [104/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0134, LR: 0.0008980252\n",
      "Epoch [105/500], Batch [10/110], Train Loss: 0.0689, Val Loss: 0.0110, LR: 0.0008978533\n",
      "Epoch [105/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0008976812\n",
      "Epoch [105/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0008975090\n",
      "Epoch [105/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0105, LR: 0.0008973367\n",
      "Epoch [105/500], Batch [50/110], Train Loss: 0.0042, Val Loss: 0.0100, LR: 0.0008971642\n",
      "Epoch [105/500], Batch [60/110], Train Loss: 0.0048, Val Loss: 0.0109, LR: 0.0008969916\n",
      "Epoch [105/500], Batch [70/110], Train Loss: 0.0085, Val Loss: 0.0100, LR: 0.0008968189\n",
      "Epoch [105/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0008966460\n",
      "Epoch [105/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0008964731\n",
      "Epoch [105/500], Batch [100/110], Train Loss: 0.0062, Val Loss: 0.0096, LR: 0.0008963000\n",
      "Epoch [105/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0008961267\n",
      "Epoch [106/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0008959534\n",
      "Epoch [106/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0008957799\n",
      "Epoch [106/500], Batch [30/110], Train Loss: 0.0057, Val Loss: 0.0100, LR: 0.0008956063\n",
      "Epoch [106/500], Batch [40/110], Train Loss: 0.0095, Val Loss: 0.0098, LR: 0.0008954325\n",
      "Epoch [106/500], Batch [50/110], Train Loss: 0.0036, Val Loss: 0.0116, LR: 0.0008952587\n",
      "Epoch [106/500], Batch [60/110], Train Loss: 0.1512, Val Loss: 0.0150, LR: 0.0008950847\n",
      "Epoch [106/500], Batch [70/110], Train Loss: 0.0982, Val Loss: 0.0109, LR: 0.0008949105\n",
      "Epoch [106/500], Batch [80/110], Train Loss: 0.0035, Val Loss: 0.0115, LR: 0.0008947363\n",
      "Epoch [106/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0008945619\n",
      "Epoch [106/500], Batch [100/110], Train Loss: 0.0044, Val Loss: 0.0112, LR: 0.0008943874\n",
      "Epoch [106/500], Batch [110/110], Train Loss: 0.0035, Val Loss: 0.0112, LR: 0.0008942128\n",
      "Epoch [107/500], Batch [10/110], Train Loss: 0.0039, Val Loss: 0.0098, LR: 0.0008940380\n",
      "Epoch [107/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0115, LR: 0.0008938631\n",
      "Epoch [107/500], Batch [30/110], Train Loss: 0.0032, Val Loss: 0.0096, LR: 0.0008936881\n",
      "Epoch [107/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0008935130\n",
      "Epoch [107/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0008933377\n",
      "Epoch [107/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0110, LR: 0.0008931623\n",
      "Epoch [107/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0008929868\n",
      "Epoch [107/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0008928111\n",
      "Epoch [107/500], Batch [90/110], Train Loss: 0.0152, Val Loss: 0.0106, LR: 0.0008926354\n",
      "Epoch [107/500], Batch [100/110], Train Loss: 0.0165, Val Loss: 0.0101, LR: 0.0008924595\n",
      "Epoch [107/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0008922835\n",
      "Epoch [108/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0008921073\n",
      "Epoch [108/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0008919310\n",
      "Epoch [108/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0102, LR: 0.0008917546\n",
      "Epoch [108/500], Batch [40/110], Train Loss: 0.0032, Val Loss: 0.0103, LR: 0.0008915781\n",
      "Epoch [108/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0008914014\n",
      "Epoch [108/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0124, LR: 0.0008912246\n",
      "Epoch [108/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0090, LR: 0.0008910477\n",
      "Epoch [108/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0126, LR: 0.0008908707\n",
      "Epoch [108/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0008906935\n",
      "Epoch [108/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0008905163\n",
      "Epoch [108/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0008903388\n",
      "Epoch [109/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0008901613\n",
      "Epoch [109/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0008899836\n",
      "Epoch [109/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0095, LR: 0.0008898058\n",
      "Epoch [109/500], Batch [40/110], Train Loss: 0.0138, Val Loss: 0.0113, LR: 0.0008896279\n",
      "Epoch [109/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0008894499\n",
      "Epoch [109/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0008892717\n",
      "Epoch [109/500], Batch [70/110], Train Loss: 0.0178, Val Loss: 0.0107, LR: 0.0008890934\n",
      "Epoch [109/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0116, LR: 0.0008889150\n",
      "Epoch [109/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0008887365\n",
      "Epoch [109/500], Batch [100/110], Train Loss: 0.0034, Val Loss: 0.0100, LR: 0.0008885578\n",
      "Epoch [109/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0097, LR: 0.0008883790\n",
      "Epoch [110/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0008882001\n",
      "Epoch [110/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0008880211\n",
      "Epoch [110/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008878419\n",
      "Epoch [110/500], Batch [40/110], Train Loss: 0.0017, Val Loss: 0.0096, LR: 0.0008876626\n",
      "Epoch [110/500], Batch [50/110], Train Loss: 0.0094, Val Loss: 0.0089, LR: 0.0008874832\n",
      "Epoch [110/500], Batch [60/110], Train Loss: 0.0054, Val Loss: 0.0095, LR: 0.0008873036\n",
      "Epoch [110/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0008871240\n",
      "Epoch [110/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0008869442\n",
      "Epoch [110/500], Batch [90/110], Train Loss: 0.0051, Val Loss: 0.0097, LR: 0.0008867643\n",
      "Epoch [110/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0102, LR: 0.0008865842\n",
      "Epoch [110/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0122, LR: 0.0008864041\n",
      "Epoch [111/500], Batch [10/110], Train Loss: 0.0357, Val Loss: 0.0115, LR: 0.0008862238\n",
      "Epoch [111/500], Batch [20/110], Train Loss: 0.0080, Val Loss: 0.0098, LR: 0.0008860434\n",
      "Epoch [111/500], Batch [30/110], Train Loss: 0.0070, Val Loss: 0.0094, LR: 0.0008858628\n",
      "Epoch [111/500], Batch [40/110], Train Loss: 0.0025, Val Loss: 0.0093, LR: 0.0008856822\n",
      "Epoch [111/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0008855014\n",
      "Epoch [111/500], Batch [60/110], Train Loss: 0.0206, Val Loss: 0.0090, LR: 0.0008853205\n",
      "Epoch [111/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0008851394\n",
      "Epoch [111/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008849583\n",
      "Epoch [111/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0090, LR: 0.0008847770\n",
      "Epoch [111/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008845956\n",
      "Epoch [111/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0090, LR: 0.0008844140\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 111: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 198.72 MB\n",
      "Epoch [112/500], Batch [10/110], Train Loss: 0.0109, Val Loss: 0.0094, LR: 0.0008842324\n",
      "Epoch [112/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0008840506\n",
      "Epoch [112/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0110, LR: 0.0008838687\n",
      "Epoch [112/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0008836867\n",
      "Epoch [112/500], Batch [50/110], Train Loss: 0.0032, Val Loss: 0.0096, LR: 0.0008835045\n",
      "Epoch [112/500], Batch [60/110], Train Loss: 0.0151, Val Loss: 0.0099, LR: 0.0008833223\n",
      "Epoch [112/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0094, LR: 0.0008831399\n",
      "Epoch [112/500], Batch [80/110], Train Loss: 0.0081, Val Loss: 0.0099, LR: 0.0008829573\n",
      "Epoch [112/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008827747\n",
      "Epoch [112/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0090, LR: 0.0008825919\n",
      "Epoch [112/500], Batch [110/110], Train Loss: 0.0112, Val Loss: 0.0090, LR: 0.0008824090\n",
      "Epoch [113/500], Batch [10/110], Train Loss: 0.0064, Val Loss: 0.0095, LR: 0.0008822260\n",
      "Epoch [113/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0008820429\n",
      "Epoch [113/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0099, LR: 0.0008818596\n",
      "Epoch [113/500], Batch [40/110], Train Loss: 0.0020, Val Loss: 0.0091, LR: 0.0008816763\n",
      "Epoch [113/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008814928\n",
      "Epoch [113/500], Batch [60/110], Train Loss: 0.0130, Val Loss: 0.0089, LR: 0.0008813091\n",
      "Epoch [113/500], Batch [70/110], Train Loss: 0.0012, Val Loss: 0.0095, LR: 0.0008811254\n",
      "Epoch [113/500], Batch [80/110], Train Loss: 0.0045, Val Loss: 0.0091, LR: 0.0008809415\n",
      "Epoch [113/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0119, LR: 0.0008807575\n",
      "Epoch [113/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008805734\n",
      "Epoch [113/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0087, LR: 0.0008803891\n",
      "Epoch [114/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008802048\n",
      "Epoch [114/500], Batch [20/110], Train Loss: 0.0020, Val Loss: 0.0087, LR: 0.0008800203\n",
      "Epoch [114/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008798357\n",
      "Epoch [114/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0008796510\n",
      "Epoch [114/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0008794661\n",
      "Epoch [114/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0008792811\n",
      "Epoch [114/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008790960\n",
      "Epoch [114/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0008789108\n",
      "Epoch [114/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0092, LR: 0.0008787255\n",
      "Epoch [114/500], Batch [100/110], Train Loss: 0.0036, Val Loss: 0.0096, LR: 0.0008785400\n",
      "Epoch [114/500], Batch [110/110], Train Loss: 0.0029, Val Loss: 0.0091, LR: 0.0008783544\n",
      "Epoch [115/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0008781687\n",
      "Epoch [115/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0096, LR: 0.0008779829\n",
      "Epoch [115/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0008777969\n",
      "Epoch [115/500], Batch [40/110], Train Loss: 0.0322, Val Loss: 0.0100, LR: 0.0008776109\n",
      "Epoch [115/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0008774247\n",
      "Epoch [115/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0008772384\n",
      "Epoch [115/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0104, LR: 0.0008770519\n",
      "Epoch [115/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0008768654\n",
      "Epoch [115/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0104, LR: 0.0008766787\n",
      "Epoch [115/500], Batch [100/110], Train Loss: 0.0021, Val Loss: 0.0097, LR: 0.0008764919\n",
      "Epoch [115/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0091, LR: 0.0008763050\n",
      "Epoch [116/500], Batch [10/110], Train Loss: 0.0013, Val Loss: 0.0091, LR: 0.0008761179\n",
      "Epoch [116/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0107, LR: 0.0008759308\n",
      "Epoch [116/500], Batch [30/110], Train Loss: 0.0023, Val Loss: 0.0087, LR: 0.0008757435\n",
      "Epoch [116/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008755561\n",
      "Epoch [116/500], Batch [50/110], Train Loss: 0.0119, Val Loss: 0.0087, LR: 0.0008753686\n",
      "Epoch [116/500], Batch [60/110], Train Loss: 0.0075, Val Loss: 0.0096, LR: 0.0008751809\n",
      "Epoch [116/500], Batch [70/110], Train Loss: 0.0091, Val Loss: 0.0103, LR: 0.0008749931\n",
      "Epoch [116/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008748053\n",
      "Epoch [116/500], Batch [90/110], Train Loss: 0.0134, Val Loss: 0.0086, LR: 0.0008746172\n",
      "Epoch [116/500], Batch [100/110], Train Loss: 0.0087, Val Loss: 0.0093, LR: 0.0008744291\n",
      "Epoch [116/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008742409\n",
      "Epoch [117/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0087, LR: 0.0008740525\n",
      "Epoch [117/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0008738640\n",
      "Epoch [117/500], Batch [30/110], Train Loss: 0.0065, Val Loss: 0.0092, LR: 0.0008736754\n",
      "Epoch [117/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0008734867\n",
      "Epoch [117/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0008732978\n",
      "Epoch [117/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0008731088\n",
      "Epoch [117/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0084, LR: 0.0008729197\n",
      "Epoch [117/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0008727305\n",
      "Epoch [117/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0008725412\n",
      "Epoch [117/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0080, LR: 0.0008723517\n",
      "Epoch [117/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0008721622\n",
      "Epoch [118/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0088, LR: 0.0008719725\n",
      "Epoch [118/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0097, LR: 0.0008717827\n",
      "Epoch [118/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0008715927\n",
      "Epoch [118/500], Batch [40/110], Train Loss: 0.0060, Val Loss: 0.0090, LR: 0.0008714027\n",
      "Epoch [118/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0008712125\n",
      "Epoch [118/500], Batch [60/110], Train Loss: 0.0123, Val Loss: 0.0092, LR: 0.0008710222\n",
      "Epoch [118/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0105, LR: 0.0008708318\n",
      "Epoch [118/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0008706413\n",
      "Epoch [118/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0008704506\n",
      "Epoch [118/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0008702599\n",
      "Epoch [118/500], Batch [110/110], Train Loss: 0.0087, Val Loss: 0.0107, LR: 0.0008700690\n",
      "Epoch [119/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008698780\n",
      "Epoch [119/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0096, LR: 0.0008696869\n",
      "Epoch [119/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0008694956\n",
      "Epoch [119/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0008693043\n",
      "Epoch [119/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0095, LR: 0.0008691128\n",
      "Epoch [119/500], Batch [60/110], Train Loss: 0.0229, Val Loss: 0.0100, LR: 0.0008689212\n",
      "Epoch [119/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0088, LR: 0.0008687295\n",
      "Epoch [119/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0008685376\n",
      "Epoch [119/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008683457\n",
      "Epoch [119/500], Batch [100/110], Train Loss: 0.0107, Val Loss: 0.0088, LR: 0.0008681536\n",
      "Epoch [119/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0008679614\n",
      "Epoch [120/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008677691\n",
      "Epoch [120/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0103, LR: 0.0008675767\n",
      "Epoch [120/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0008673841\n",
      "Epoch [120/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0108, LR: 0.0008671914\n",
      "Epoch [120/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0008669987\n",
      "Epoch [120/500], Batch [60/110], Train Loss: 0.0067, Val Loss: 0.0102, LR: 0.0008668058\n",
      "Epoch [120/500], Batch [70/110], Train Loss: 0.0048, Val Loss: 0.0093, LR: 0.0008666127\n",
      "Epoch [120/500], Batch [80/110], Train Loss: 0.0023, Val Loss: 0.0092, LR: 0.0008664196\n",
      "Epoch [120/500], Batch [90/110], Train Loss: 0.0097, Val Loss: 0.0093, LR: 0.0008662263\n",
      "Epoch [120/500], Batch [100/110], Train Loss: 0.0974, Val Loss: 0.0087, LR: 0.0008660330\n",
      "Epoch [120/500], Batch [110/110], Train Loss: 0.0023, Val Loss: 0.0093, LR: 0.0008658395\n",
      "Epoch [121/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008656459\n",
      "Epoch [121/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0008654521\n",
      "Epoch [121/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0008652583\n",
      "Epoch [121/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0008650643\n",
      "Epoch [121/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008648702\n",
      "Epoch [121/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0008646760\n",
      "Epoch [121/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0008644817\n",
      "Epoch [121/500], Batch [80/110], Train Loss: 0.0084, Val Loss: 0.0089, LR: 0.0008642873\n",
      "Epoch [121/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008640927\n",
      "Epoch [121/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0008638981\n",
      "Epoch [121/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0008637033\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  1 862]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99922       637\n",
      "           1    1.00000   0.99884   0.99942       863\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99922   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 578, Predicted: 0, Actual: 1\n",
      "Epoch 121: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 198.73 MB\n",
      "Epoch [122/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008635084\n",
      "Epoch [122/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0116, LR: 0.0008633134\n",
      "Epoch [122/500], Batch [30/110], Train Loss: 0.0013, Val Loss: 0.0092, LR: 0.0008631182\n",
      "Epoch [122/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0008629230\n",
      "Epoch [122/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0008627276\n",
      "Epoch [122/500], Batch [60/110], Train Loss: 0.0028, Val Loss: 0.0102, LR: 0.0008625321\n",
      "Epoch [122/500], Batch [70/110], Train Loss: 0.0050, Val Loss: 0.0086, LR: 0.0008623365\n",
      "Epoch [122/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0090, LR: 0.0008621408\n",
      "Epoch [122/500], Batch [90/110], Train Loss: 0.0029, Val Loss: 0.0088, LR: 0.0008619450\n",
      "Epoch [122/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0008617490\n",
      "Epoch [122/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0082, LR: 0.0008615530\n",
      "Epoch [123/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0008613568\n",
      "Epoch [123/500], Batch [20/110], Train Loss: 0.0009, Val Loss: 0.0087, LR: 0.0008611605\n",
      "Epoch [123/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0008609641\n",
      "Epoch [123/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0008607675\n",
      "Epoch [123/500], Batch [50/110], Train Loss: 0.0023, Val Loss: 0.0081, LR: 0.0008605709\n",
      "Epoch [123/500], Batch [60/110], Train Loss: 0.2065, Val Loss: 0.0086, LR: 0.0008603741\n",
      "Epoch [123/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008601772\n",
      "Epoch [123/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0084, LR: 0.0008599802\n",
      "Epoch [123/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008597831\n",
      "Epoch [123/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008595859\n",
      "Epoch [123/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008593886\n",
      "Epoch [124/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0008591911\n",
      "Epoch [124/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0087, LR: 0.0008589935\n",
      "Epoch [124/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008587958\n",
      "Epoch [124/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0084, LR: 0.0008585980\n",
      "Epoch [124/500], Batch [50/110], Train Loss: 0.0066, Val Loss: 0.0083, LR: 0.0008584001\n",
      "Epoch [124/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008582021\n",
      "Epoch [124/500], Batch [70/110], Train Loss: 0.0016, Val Loss: 0.0081, LR: 0.0008580039\n",
      "Epoch [124/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0008578057\n",
      "Epoch [124/500], Batch [90/110], Train Loss: 0.0038, Val Loss: 0.0098, LR: 0.0008576073\n",
      "Epoch [124/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0008574088\n",
      "Epoch [124/500], Batch [110/110], Train Loss: 0.0029, Val Loss: 0.0124, LR: 0.0008572102\n",
      "Epoch [125/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008570114\n",
      "Epoch [125/500], Batch [20/110], Train Loss: 0.0078, Val Loss: 0.0115, LR: 0.0008568126\n",
      "Epoch [125/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0091, LR: 0.0008566136\n",
      "Epoch [125/500], Batch [40/110], Train Loss: 0.0044, Val Loss: 0.0096, LR: 0.0008564146\n",
      "Epoch [125/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008562154\n",
      "Epoch [125/500], Batch [60/110], Train Loss: 0.0162, Val Loss: 0.0088, LR: 0.0008560161\n",
      "Epoch [125/500], Batch [70/110], Train Loss: 0.0036, Val Loss: 0.0087, LR: 0.0008558167\n",
      "Epoch [125/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008556171\n",
      "Epoch [125/500], Batch [90/110], Train Loss: 0.2679, Val Loss: 0.0097, LR: 0.0008554175\n",
      "Epoch [125/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008552177\n",
      "Epoch [125/500], Batch [110/110], Train Loss: 0.0041, Val Loss: 0.0088, LR: 0.0008550179\n",
      "Epoch [126/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0008548179\n",
      "Epoch [126/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0008546178\n",
      "Epoch [126/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0091, LR: 0.0008544176\n",
      "Epoch [126/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008542172\n",
      "Epoch [126/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0090, LR: 0.0008540168\n",
      "Epoch [126/500], Batch [60/110], Train Loss: 0.0115, Val Loss: 0.0085, LR: 0.0008538162\n",
      "Epoch [126/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008536156\n",
      "Epoch [126/500], Batch [80/110], Train Loss: 0.0026, Val Loss: 0.0085, LR: 0.0008534148\n",
      "Epoch [126/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008532139\n",
      "Epoch [126/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0089, LR: 0.0008530129\n",
      "Epoch [126/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0090, LR: 0.0008528117\n",
      "Epoch [127/500], Batch [10/110], Train Loss: 0.0009, Val Loss: 0.0089, LR: 0.0008526105\n",
      "Epoch [127/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0105, LR: 0.0008524091\n",
      "Epoch [127/500], Batch [30/110], Train Loss: 0.0936, Val Loss: 0.0113, LR: 0.0008522077\n",
      "Epoch [127/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0008520061\n",
      "Epoch [127/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0008518044\n",
      "Epoch [127/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0008516026\n",
      "Epoch [127/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0008514007\n",
      "Epoch [127/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008511987\n",
      "Epoch [127/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008509965\n",
      "Epoch [127/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0008507943\n",
      "Epoch [127/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0008505919\n",
      "Epoch [128/500], Batch [10/110], Train Loss: 0.0036, Val Loss: 0.0086, LR: 0.0008503894\n",
      "Epoch [128/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0008501868\n",
      "Epoch [128/500], Batch [30/110], Train Loss: 0.0029, Val Loss: 0.0088, LR: 0.0008499841\n",
      "Epoch [128/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0097, LR: 0.0008497813\n",
      "Epoch [128/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0008495783\n",
      "Epoch [128/500], Batch [60/110], Train Loss: 0.0331, Val Loss: 0.0093, LR: 0.0008493753\n",
      "Epoch [128/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008491721\n",
      "Epoch [128/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008489689\n",
      "Epoch [128/500], Batch [90/110], Train Loss: 0.0028, Val Loss: 0.0086, LR: 0.0008487655\n",
      "Epoch [128/500], Batch [100/110], Train Loss: 0.0048, Val Loss: 0.0081, LR: 0.0008485620\n",
      "Epoch [128/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008483584\n",
      "Epoch [129/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0008481547\n",
      "Epoch [129/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0008479508\n",
      "Epoch [129/500], Batch [30/110], Train Loss: 0.0048, Val Loss: 0.0090, LR: 0.0008477469\n",
      "Epoch [129/500], Batch [40/110], Train Loss: 0.0018, Val Loss: 0.0086, LR: 0.0008475428\n",
      "Epoch [129/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0008473387\n",
      "Epoch [129/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0096, LR: 0.0008471344\n",
      "Epoch [129/500], Batch [70/110], Train Loss: 0.0017, Val Loss: 0.0096, LR: 0.0008469300\n",
      "Epoch [129/500], Batch [80/110], Train Loss: 0.0009, Val Loss: 0.0084, LR: 0.0008467255\n",
      "Epoch [129/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0008465209\n",
      "Epoch [129/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0008463162\n",
      "Epoch [129/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0008461113\n",
      "Epoch [130/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0008459064\n",
      "Epoch [130/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0008457013\n",
      "Epoch [130/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0008454962\n",
      "Epoch [130/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0008452909\n",
      "Epoch [130/500], Batch [50/110], Train Loss: 0.0014, Val Loss: 0.0089, LR: 0.0008450855\n",
      "Epoch [130/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0008448800\n",
      "Epoch [130/500], Batch [70/110], Train Loss: 0.0053, Val Loss: 0.0087, LR: 0.0008446744\n",
      "Epoch [130/500], Batch [80/110], Train Loss: 0.0074, Val Loss: 0.0087, LR: 0.0008444687\n",
      "Epoch [130/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008442628\n",
      "Epoch [130/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0089, LR: 0.0008440569\n",
      "Epoch [130/500], Batch [110/110], Train Loss: 0.0040, Val Loss: 0.0088, LR: 0.0008438508\n",
      "Epoch [131/500], Batch [10/110], Train Loss: 0.0014, Val Loss: 0.0085, LR: 0.0008436447\n",
      "Epoch [131/500], Batch [20/110], Train Loss: 0.0042, Val Loss: 0.0084, LR: 0.0008434384\n",
      "Epoch [131/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0084, LR: 0.0008432320\n",
      "Epoch [131/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008430255\n",
      "Epoch [131/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008428189\n",
      "Epoch [131/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008426122\n",
      "Epoch [131/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008424053\n",
      "Epoch [131/500], Batch [80/110], Train Loss: 0.0050, Val Loss: 0.0085, LR: 0.0008421984\n",
      "Epoch [131/500], Batch [90/110], Train Loss: 0.0030, Val Loss: 0.0084, LR: 0.0008419913\n",
      "Epoch [131/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008417842\n",
      "Epoch [131/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008415769\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 131: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 198.73 MB\n",
      "Epoch [132/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0008413695\n",
      "Epoch [132/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008411621\n",
      "Epoch [132/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008409545\n",
      "Epoch [132/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0085, LR: 0.0008407468\n",
      "Epoch [132/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008405389\n",
      "Epoch [132/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008403310\n",
      "Epoch [132/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008401230\n",
      "Epoch [132/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008399148\n",
      "Epoch [132/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0086, LR: 0.0008397066\n",
      "Epoch [132/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0088, LR: 0.0008394982\n",
      "Epoch [132/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0008392897\n",
      "Epoch [133/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008390812\n",
      "Epoch [133/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0008388725\n",
      "Epoch [133/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0008386637\n",
      "Epoch [133/500], Batch [40/110], Train Loss: 0.0073, Val Loss: 0.0091, LR: 0.0008384548\n",
      "Epoch [133/500], Batch [50/110], Train Loss: 0.0175, Val Loss: 0.0118, LR: 0.0008382457\n",
      "Epoch [133/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0008380366\n",
      "Epoch [133/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0091, LR: 0.0008378274\n",
      "Epoch [133/500], Batch [80/110], Train Loss: 0.0010, Val Loss: 0.0089, LR: 0.0008376180\n",
      "Epoch [133/500], Batch [90/110], Train Loss: 0.0024, Val Loss: 0.0089, LR: 0.0008374086\n",
      "Epoch [133/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0008371990\n",
      "Epoch [133/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0008369894\n",
      "Epoch [134/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008367796\n",
      "Epoch [134/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008365697\n",
      "Epoch [134/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008363597\n",
      "Epoch [134/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0088, LR: 0.0008361496\n",
      "Epoch [134/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0008359394\n",
      "Epoch [134/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0091, LR: 0.0008357291\n",
      "Epoch [134/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0008355187\n",
      "Epoch [134/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0008353081\n",
      "Epoch [134/500], Batch [90/110], Train Loss: 0.0029, Val Loss: 0.0088, LR: 0.0008350975\n",
      "Epoch [134/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0085, LR: 0.0008348867\n",
      "Epoch [134/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008346759\n",
      "Epoch [135/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0008344649\n",
      "Epoch [135/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0008342538\n",
      "Epoch [135/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0008340427\n",
      "Epoch [135/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008338314\n",
      "Epoch [135/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0093, LR: 0.0008336200\n",
      "Epoch [135/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0113, LR: 0.0008334085\n",
      "Epoch [135/500], Batch [70/110], Train Loss: 0.0021, Val Loss: 0.0089, LR: 0.0008331969\n",
      "Epoch [135/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0088, LR: 0.0008329852\n",
      "Epoch [135/500], Batch [90/110], Train Loss: 0.0013, Val Loss: 0.0089, LR: 0.0008327733\n",
      "Epoch [135/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008325614\n",
      "Epoch [135/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0084, LR: 0.0008323494\n",
      "Epoch [136/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0084, LR: 0.0008321372\n",
      "Epoch [136/500], Batch [20/110], Train Loss: 0.1014, Val Loss: 0.0081, LR: 0.0008319250\n",
      "Epoch [136/500], Batch [30/110], Train Loss: 0.0021, Val Loss: 0.0081, LR: 0.0008317126\n",
      "Epoch [136/500], Batch [40/110], Train Loss: 0.0037, Val Loss: 0.0082, LR: 0.0008315002\n",
      "Epoch [136/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0083, LR: 0.0008312876\n",
      "Epoch [136/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0084, LR: 0.0008310749\n",
      "Epoch [136/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008308621\n",
      "Epoch [136/500], Batch [80/110], Train Loss: 0.0026, Val Loss: 0.0082, LR: 0.0008306493\n",
      "Epoch [136/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008304363\n",
      "Epoch [136/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0086, LR: 0.0008302232\n",
      "Epoch [136/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0008300099\n",
      "Epoch [137/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0095, LR: 0.0008297966\n",
      "Epoch [137/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008295832\n",
      "Epoch [137/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008293697\n",
      "Epoch [137/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0097, LR: 0.0008291561\n",
      "Epoch [137/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008289423\n",
      "Epoch [137/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0096, LR: 0.0008287285\n",
      "Epoch [137/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008285145\n",
      "Epoch [137/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0157, LR: 0.0008283005\n",
      "Epoch [137/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0008280863\n",
      "Epoch [137/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0089, LR: 0.0008278721\n",
      "Epoch [137/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008276577\n",
      "Epoch [138/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008274432\n",
      "Epoch [138/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008272286\n",
      "Epoch [138/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008270140\n",
      "Epoch [138/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008267992\n",
      "Epoch [138/500], Batch [50/110], Train Loss: 0.0053, Val Loss: 0.0082, LR: 0.0008265843\n",
      "Epoch [138/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0086, LR: 0.0008263693\n",
      "Epoch [138/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0008261542\n",
      "Epoch [138/500], Batch [80/110], Train Loss: 0.0413, Val Loss: 0.0083, LR: 0.0008259390\n",
      "Epoch [138/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008257236\n",
      "Epoch [138/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0008255082\n",
      "Epoch [138/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008252927\n",
      "Epoch [139/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0082, LR: 0.0008250771\n",
      "Epoch [139/500], Batch [20/110], Train Loss: 0.0018, Val Loss: 0.0081, LR: 0.0008248613\n",
      "Epoch [139/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0008246455\n",
      "Epoch [139/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008244296\n",
      "Epoch [139/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0008242135\n",
      "Epoch [139/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0087, LR: 0.0008239974\n",
      "Epoch [139/500], Batch [70/110], Train Loss: 0.0128, Val Loss: 0.0099, LR: 0.0008237811\n",
      "Epoch [139/500], Batch [80/110], Train Loss: 0.0051, Val Loss: 0.0084, LR: 0.0008235648\n",
      "Epoch [139/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0114, LR: 0.0008233483\n",
      "Epoch [139/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008231317\n",
      "Epoch [139/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008229151\n",
      "Epoch [140/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0008226983\n",
      "Epoch [140/500], Batch [20/110], Train Loss: 0.0045, Val Loss: 0.0092, LR: 0.0008224814\n",
      "Epoch [140/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0008222644\n",
      "Epoch [140/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0008220474\n",
      "Epoch [140/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0008218302\n",
      "Epoch [140/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0008216129\n",
      "Epoch [140/500], Batch [70/110], Train Loss: 0.0107, Val Loss: 0.0101, LR: 0.0008213955\n",
      "Epoch [140/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0094, LR: 0.0008211780\n",
      "Epoch [140/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008209604\n",
      "Epoch [140/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008207427\n",
      "Epoch [140/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0088, LR: 0.0008205249\n",
      "Epoch [141/500], Batch [10/110], Train Loss: 0.0035, Val Loss: 0.0091, LR: 0.0008203070\n",
      "Epoch [141/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0098, LR: 0.0008200890\n",
      "Epoch [141/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0089, LR: 0.0008198708\n",
      "Epoch [141/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008196526\n",
      "Epoch [141/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008194343\n",
      "Epoch [141/500], Batch [60/110], Train Loss: 0.0015, Val Loss: 0.0087, LR: 0.0008192159\n",
      "Epoch [141/500], Batch [70/110], Train Loss: 0.0018, Val Loss: 0.0088, LR: 0.0008189974\n",
      "Epoch [141/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0008187787\n",
      "Epoch [141/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008185600\n",
      "Epoch [141/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0008183412\n",
      "Epoch [141/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0082, LR: 0.0008181222\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 141: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.042 sec , Memory Usage: 198.73 MB\n",
      "Epoch [142/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0086, LR: 0.0008179032\n",
      "Epoch [142/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0008176841\n",
      "Epoch [142/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0120, LR: 0.0008174648\n",
      "Epoch [142/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0008172455\n",
      "Epoch [142/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008170260\n",
      "Epoch [142/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0008168065\n",
      "Epoch [142/500], Batch [70/110], Train Loss: 0.0110, Val Loss: 0.0096, LR: 0.0008165868\n",
      "Epoch [142/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0008163671\n",
      "Epoch [142/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0091, LR: 0.0008161472\n",
      "Epoch [142/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0095, LR: 0.0008159273\n",
      "Epoch [142/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0088, LR: 0.0008157072\n",
      "Epoch [143/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0087, LR: 0.0008154871\n",
      "Epoch [143/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0087, LR: 0.0008152668\n",
      "Epoch [143/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0084, LR: 0.0008150465\n",
      "Epoch [143/500], Batch [40/110], Train Loss: 0.0014, Val Loss: 0.0086, LR: 0.0008148260\n",
      "Epoch [143/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008146054\n",
      "Epoch [143/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0008143848\n",
      "Epoch [143/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008141640\n",
      "Epoch [143/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0008139431\n",
      "Epoch [143/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0008137222\n",
      "Epoch [143/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008135011\n",
      "Epoch [143/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008132800\n",
      "Epoch [144/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0092, LR: 0.0008130587\n",
      "Epoch [144/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008128373\n",
      "Epoch [144/500], Batch [30/110], Train Loss: 0.0026, Val Loss: 0.0084, LR: 0.0008126159\n",
      "Epoch [144/500], Batch [40/110], Train Loss: 0.0027, Val Loss: 0.0082, LR: 0.0008123943\n",
      "Epoch [144/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008121726\n",
      "Epoch [144/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008119508\n",
      "Epoch [144/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008117290\n",
      "Epoch [144/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008115070\n",
      "Epoch [144/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0087, LR: 0.0008112849\n",
      "Epoch [144/500], Batch [100/110], Train Loss: 0.0045, Val Loss: 0.0091, LR: 0.0008110628\n",
      "Epoch [144/500], Batch [110/110], Train Loss: 0.0067, Val Loss: 0.0099, LR: 0.0008108405\n",
      "Epoch [145/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0008106181\n",
      "Epoch [145/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0091, LR: 0.0008103957\n",
      "Epoch [145/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0008101731\n",
      "Epoch [145/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0008099504\n",
      "Epoch [145/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008097277\n",
      "Epoch [145/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0008095048\n",
      "Epoch [145/500], Batch [70/110], Train Loss: 0.0019, Val Loss: 0.0083, LR: 0.0008092818\n",
      "Epoch [145/500], Batch [80/110], Train Loss: 0.0011, Val Loss: 0.0085, LR: 0.0008090588\n",
      "Epoch [145/500], Batch [90/110], Train Loss: 0.0049, Val Loss: 0.0089, LR: 0.0008088356\n",
      "Epoch [145/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0008086124\n",
      "Epoch [145/500], Batch [110/110], Train Loss: 0.0018, Val Loss: 0.0085, LR: 0.0008083890\n",
      "Epoch [146/500], Batch [10/110], Train Loss: 0.0034, Val Loss: 0.0086, LR: 0.0008081655\n",
      "Epoch [146/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0008079420\n",
      "Epoch [146/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0096, LR: 0.0008077183\n",
      "Epoch [146/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0008074946\n",
      "Epoch [146/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008072707\n",
      "Epoch [146/500], Batch [60/110], Train Loss: 0.0019, Val Loss: 0.0088, LR: 0.0008070467\n",
      "Epoch [146/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0008068227\n",
      "Epoch [146/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008065985\n",
      "Epoch [146/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008063743\n",
      "Epoch [146/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008061499\n",
      "Epoch [146/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0008059255\n",
      "Epoch [147/500], Batch [10/110], Train Loss: 0.0052, Val Loss: 0.0091, LR: 0.0008057010\n",
      "Epoch [147/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008054763\n",
      "Epoch [147/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008052516\n",
      "Epoch [147/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0085, LR: 0.0008050267\n",
      "Epoch [147/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008048018\n",
      "Epoch [147/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0082, LR: 0.0008045768\n",
      "Epoch [147/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0008043516\n",
      "Epoch [147/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008041264\n",
      "Epoch [147/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0086, LR: 0.0008039011\n",
      "Epoch [147/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0092, LR: 0.0008036756\n",
      "Epoch [147/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008034501\n",
      "Epoch [148/500], Batch [10/110], Train Loss: 0.1465, Val Loss: 0.0091, LR: 0.0008032245\n",
      "Epoch [148/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008029988\n",
      "Epoch [148/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008027730\n",
      "Epoch [148/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0095, LR: 0.0008025471\n",
      "Epoch [148/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0008023211\n",
      "Epoch [148/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0077, LR: 0.0008020949\n",
      "Epoch [148/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0008018687\n",
      "Epoch [148/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0008016424\n",
      "Epoch [148/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008014160\n",
      "Epoch [148/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008011896\n",
      "Epoch [148/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0008009630\n",
      "Epoch [149/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0008007363\n",
      "Epoch [149/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0122, LR: 0.0008005095\n",
      "Epoch [149/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0008002826\n",
      "Epoch [149/500], Batch [40/110], Train Loss: 0.0024, Val Loss: 0.0075, LR: 0.0008000556\n",
      "Epoch [149/500], Batch [50/110], Train Loss: 0.0043, Val Loss: 0.0079, LR: 0.0007998286\n",
      "Epoch [149/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007996014\n",
      "Epoch [149/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0077, LR: 0.0007993741\n",
      "Epoch [149/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0007991468\n",
      "Epoch [149/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007989193\n",
      "Epoch [149/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0007986918\n",
      "Epoch [149/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007984641\n",
      "Epoch [150/500], Batch [10/110], Train Loss: 0.0093, Val Loss: 0.0098, LR: 0.0007982364\n",
      "Epoch [150/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0007980085\n",
      "Epoch [150/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007977806\n",
      "Epoch [150/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007975526\n",
      "Epoch [150/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007973245\n",
      "Epoch [150/500], Batch [60/110], Train Loss: 0.2289, Val Loss: 0.0085, LR: 0.0007970962\n",
      "Epoch [150/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007968679\n",
      "Epoch [150/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0007966395\n",
      "Epoch [150/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007964110\n",
      "Epoch [150/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007961824\n",
      "Epoch [150/500], Batch [110/110], Train Loss: 0.0014, Val Loss: 0.0081, LR: 0.0007959537\n",
      "Epoch [151/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007957249\n",
      "Epoch [151/500], Batch [20/110], Train Loss: 0.0076, Val Loss: 0.0083, LR: 0.0007954960\n",
      "Epoch [151/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007952670\n",
      "Epoch [151/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007950380\n",
      "Epoch [151/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007948088\n",
      "Epoch [151/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0086, LR: 0.0007945795\n",
      "Epoch [151/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007943502\n",
      "Epoch [151/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0007941207\n",
      "Epoch [151/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007938912\n",
      "Epoch [151/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0083, LR: 0.0007936615\n",
      "Epoch [151/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007934318\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 151: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 198.73 MB\n",
      "Epoch [152/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007932020\n",
      "Epoch [152/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007929720\n",
      "Epoch [152/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0086, LR: 0.0007927420\n",
      "Epoch [152/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007925119\n",
      "Epoch [152/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007922817\n",
      "Epoch [152/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0007920514\n",
      "Epoch [152/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007918210\n",
      "Epoch [152/500], Batch [80/110], Train Loss: 0.0073, Val Loss: 0.0088, LR: 0.0007915905\n",
      "Epoch [152/500], Batch [90/110], Train Loss: 0.1694, Val Loss: 0.0088, LR: 0.0007913599\n",
      "Epoch [152/500], Batch [100/110], Train Loss: 0.0089, Val Loss: 0.0100, LR: 0.0007911293\n",
      "Epoch [152/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0085, LR: 0.0007908985\n",
      "Epoch [153/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0087, LR: 0.0007906676\n",
      "Epoch [153/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007904367\n",
      "Epoch [153/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007902056\n",
      "Epoch [153/500], Batch [40/110], Train Loss: 0.0084, Val Loss: 0.0085, LR: 0.0007899745\n",
      "Epoch [153/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0007897433\n",
      "Epoch [153/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0007895119\n",
      "Epoch [153/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0007892805\n",
      "Epoch [153/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0007890490\n",
      "Epoch [153/500], Batch [90/110], Train Loss: 0.0024, Val Loss: 0.0083, LR: 0.0007888174\n",
      "Epoch [153/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0085, LR: 0.0007885857\n",
      "Epoch [153/500], Batch [110/110], Train Loss: 0.0028, Val Loss: 0.0081, LR: 0.0007883539\n",
      "Epoch [154/500], Batch [10/110], Train Loss: 0.0020, Val Loss: 0.0080, LR: 0.0007881220\n",
      "Epoch [154/500], Batch [20/110], Train Loss: 0.0021, Val Loss: 0.0086, LR: 0.0007878900\n",
      "Epoch [154/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007876580\n",
      "Epoch [154/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007874258\n",
      "Epoch [154/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007871936\n",
      "Epoch [154/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0007869612\n",
      "Epoch [154/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0083, LR: 0.0007867288\n",
      "Epoch [154/500], Batch [80/110], Train Loss: 0.0026, Val Loss: 0.0085, LR: 0.0007864963\n",
      "Epoch [154/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0007862636\n",
      "Epoch [154/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007860309\n",
      "Epoch [154/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0007857981\n",
      "Epoch [155/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007855652\n",
      "Epoch [155/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007853322\n",
      "Epoch [155/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007850992\n",
      "Epoch [155/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0076, LR: 0.0007848660\n",
      "Epoch [155/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0007846327\n",
      "Epoch [155/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0007843994\n",
      "Epoch [155/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0007841660\n",
      "Epoch [155/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0007839324\n",
      "Epoch [155/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007836988\n",
      "Epoch [155/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007834651\n",
      "Epoch [155/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0007832313\n",
      "Epoch [156/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007829974\n",
      "Epoch [156/500], Batch [20/110], Train Loss: 0.0026, Val Loss: 0.0081, LR: 0.0007827634\n",
      "Epoch [156/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007825293\n",
      "Epoch [156/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007822951\n",
      "Epoch [156/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007820609\n",
      "Epoch [156/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007818265\n",
      "Epoch [156/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007815921\n",
      "Epoch [156/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0084, LR: 0.0007813576\n",
      "Epoch [156/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007811229\n",
      "Epoch [156/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007808882\n",
      "Epoch [156/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0007806534\n",
      "Epoch [157/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0084, LR: 0.0007804185\n",
      "Epoch [157/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0007801836\n",
      "Epoch [157/500], Batch [30/110], Train Loss: 0.0013, Val Loss: 0.0086, LR: 0.0007799485\n",
      "Epoch [157/500], Batch [40/110], Train Loss: 0.0051, Val Loss: 0.0089, LR: 0.0007797133\n",
      "Epoch [157/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0089, LR: 0.0007794781\n",
      "Epoch [157/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007792427\n",
      "Epoch [157/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007790073\n",
      "Epoch [157/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007787718\n",
      "Epoch [157/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007785362\n",
      "Epoch [157/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007783005\n",
      "Epoch [157/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007780647\n",
      "Epoch [158/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007778288\n",
      "Epoch [158/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0007775929\n",
      "Epoch [158/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0076, LR: 0.0007773568\n",
      "Epoch [158/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007771207\n",
      "Epoch [158/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0007768844\n",
      "Epoch [158/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0007766481\n",
      "Epoch [158/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0007764117\n",
      "Epoch [158/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0078, LR: 0.0007761752\n",
      "Epoch [158/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0007759386\n",
      "Epoch [158/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0079, LR: 0.0007757020\n",
      "Epoch [158/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007754652\n",
      "Epoch [159/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0007752284\n",
      "Epoch [159/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0007749914\n",
      "Epoch [159/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007747544\n",
      "Epoch [159/500], Batch [40/110], Train Loss: 0.0069, Val Loss: 0.0087, LR: 0.0007745173\n",
      "Epoch [159/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007742801\n",
      "Epoch [159/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007740428\n",
      "Epoch [159/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007738054\n",
      "Epoch [159/500], Batch [80/110], Train Loss: 0.0021, Val Loss: 0.0076, LR: 0.0007735679\n",
      "Epoch [159/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007733304\n",
      "Epoch [159/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007730927\n",
      "Epoch [159/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007728550\n",
      "Epoch [160/500], Batch [10/110], Train Loss: 0.0090, Val Loss: 0.0080, LR: 0.0007726172\n",
      "Epoch [160/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0007723793\n",
      "Epoch [160/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007721413\n",
      "Epoch [160/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007719032\n",
      "Epoch [160/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0082, LR: 0.0007716651\n",
      "Epoch [160/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007714268\n",
      "Epoch [160/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007711885\n",
      "Epoch [160/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007709501\n",
      "Epoch [160/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0080, LR: 0.0007707115\n",
      "Epoch [160/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0007704729\n",
      "Epoch [160/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007702343\n",
      "Epoch [161/500], Batch [10/110], Train Loss: 0.0036, Val Loss: 0.0077, LR: 0.0007699955\n",
      "Epoch [161/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0007697566\n",
      "Epoch [161/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007695177\n",
      "Epoch [161/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007692787\n",
      "Epoch [161/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007690395\n",
      "Epoch [161/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0070, LR: 0.0007688003\n",
      "Epoch [161/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007685611\n",
      "Epoch [161/500], Batch [80/110], Train Loss: 0.0008, Val Loss: 0.0076, LR: 0.0007683217\n",
      "Epoch [161/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0007680822\n",
      "Epoch [161/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0007678427\n",
      "Epoch [161/500], Batch [110/110], Train Loss: 0.0019, Val Loss: 0.0076, LR: 0.0007676030\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 161: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 198.73 MB\n",
      "Epoch [162/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0007673633\n",
      "Epoch [162/500], Batch [20/110], Train Loss: 0.0082, Val Loss: 0.0077, LR: 0.0007671235\n",
      "Epoch [162/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0007668836\n",
      "Epoch [162/500], Batch [40/110], Train Loss: 0.0015, Val Loss: 0.0074, LR: 0.0007666437\n",
      "Epoch [162/500], Batch [50/110], Train Loss: 0.0011, Val Loss: 0.0074, LR: 0.0007664036\n",
      "Epoch [162/500], Batch [60/110], Train Loss: 0.0026, Val Loss: 0.0074, LR: 0.0007661634\n",
      "Epoch [162/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0007659232\n",
      "Epoch [162/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0007656829\n",
      "Epoch [162/500], Batch [90/110], Train Loss: 0.0013, Val Loss: 0.0074, LR: 0.0007654425\n",
      "Epoch [162/500], Batch [100/110], Train Loss: 0.0967, Val Loss: 0.0075, LR: 0.0007652020\n",
      "Epoch [162/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0076, LR: 0.0007649614\n",
      "Epoch [163/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0093, LR: 0.0007647208\n",
      "Epoch [163/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0007644800\n",
      "Epoch [163/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0127, LR: 0.0007642392\n",
      "Epoch [163/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0125, LR: 0.0007639983\n",
      "Epoch [163/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0095, LR: 0.0007637573\n",
      "Epoch [163/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0007635162\n",
      "Epoch [163/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0086, LR: 0.0007632751\n",
      "Epoch [163/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0007630338\n",
      "Epoch [163/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007627925\n",
      "Epoch [163/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0007625511\n",
      "Epoch [163/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0077, LR: 0.0007623096\n",
      "Epoch [164/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007620680\n",
      "Epoch [164/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0007618263\n",
      "Epoch [164/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0082, LR: 0.0007615846\n",
      "Epoch [164/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0106, LR: 0.0007613427\n",
      "Epoch [164/500], Batch [50/110], Train Loss: 0.1504, Val Loss: 0.0084, LR: 0.0007611008\n",
      "Epoch [164/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007608588\n",
      "Epoch [164/500], Batch [70/110], Train Loss: 0.0047, Val Loss: 0.0082, LR: 0.0007606167\n",
      "Epoch [164/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007603746\n",
      "Epoch [164/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007601323\n",
      "Epoch [164/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0007598900\n",
      "Epoch [164/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0007596476\n",
      "Epoch [165/500], Batch [10/110], Train Loss: 0.0009, Val Loss: 0.0074, LR: 0.0007594051\n",
      "Epoch [165/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0076, LR: 0.0007591625\n",
      "Epoch [165/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0077, LR: 0.0007589198\n",
      "Epoch [165/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007586771\n",
      "Epoch [165/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0007584342\n",
      "Epoch [165/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007581913\n",
      "Epoch [165/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0070, LR: 0.0007579483\n",
      "Epoch [165/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007577052\n",
      "Epoch [165/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0007574621\n",
      "Epoch [165/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0007572188\n",
      "Epoch [165/500], Batch [110/110], Train Loss: 0.0021, Val Loss: 0.0072, LR: 0.0007569755\n",
      "Epoch [166/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007567321\n",
      "Epoch [166/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0007564886\n",
      "Epoch [166/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0075, LR: 0.0007562450\n",
      "Epoch [166/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0007560014\n",
      "Epoch [166/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0007557576\n",
      "Epoch [166/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0007555138\n",
      "Epoch [166/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0007552699\n",
      "Epoch [166/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0063, LR: 0.0007550259\n",
      "Epoch [166/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0060, LR: 0.0007547819\n",
      "Epoch [166/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0007545377\n",
      "Epoch [166/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0007542935\n",
      "Epoch [167/500], Batch [10/110], Train Loss: 0.0836, Val Loss: 0.0076, LR: 0.0007540492\n",
      "Epoch [167/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0007538048\n",
      "Epoch [167/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0007535603\n",
      "Epoch [167/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0079, LR: 0.0007533158\n",
      "Epoch [167/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0083, LR: 0.0007530711\n",
      "Epoch [167/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007528264\n",
      "Epoch [167/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0076, LR: 0.0007525816\n",
      "Epoch [167/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0077, LR: 0.0007523367\n",
      "Epoch [167/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007520918\n",
      "Epoch [167/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007518467\n",
      "Epoch [167/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0007516016\n",
      "Epoch [168/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0007513564\n",
      "Epoch [168/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0007511111\n",
      "Epoch [168/500], Batch [30/110], Train Loss: 0.0045, Val Loss: 0.0071, LR: 0.0007508658\n",
      "Epoch [168/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0071, LR: 0.0007506204\n",
      "Epoch [168/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0007503748\n",
      "Epoch [168/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0007501292\n",
      "Epoch [168/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0007498836\n",
      "Epoch [168/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0007496378\n",
      "Epoch [168/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0007493920\n",
      "Epoch [168/500], Batch [100/110], Train Loss: 0.0029, Val Loss: 0.0083, LR: 0.0007491460\n",
      "Epoch [168/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007489000\n",
      "Epoch [169/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0007486540\n",
      "Epoch [169/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0073, LR: 0.0007484078\n",
      "Epoch [169/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007481616\n",
      "Epoch [169/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007479152\n",
      "Epoch [169/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0067, LR: 0.0007476688\n",
      "Epoch [169/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0007474224\n",
      "Epoch [169/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0007471758\n",
      "Epoch [169/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0007469292\n",
      "Epoch [169/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0007466825\n",
      "Epoch [169/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0007464357\n",
      "Epoch [169/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007461888\n",
      "Epoch [170/500], Batch [10/110], Train Loss: 0.0014, Val Loss: 0.0075, LR: 0.0007459419\n",
      "Epoch [170/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0007456948\n",
      "Epoch [170/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0007454477\n",
      "Epoch [170/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0007452005\n",
      "Epoch [170/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0007449533\n",
      "Epoch [170/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007447059\n",
      "Epoch [170/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0007444585\n",
      "Epoch [170/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0007442110\n",
      "Epoch [170/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0007439635\n",
      "Epoch [170/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007437158\n",
      "Epoch [170/500], Batch [110/110], Train Loss: 0.0021, Val Loss: 0.0069, LR: 0.0007434681\n",
      "Epoch [171/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0007432203\n",
      "Epoch [171/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0071, LR: 0.0007429724\n",
      "Epoch [171/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007427244\n",
      "Epoch [171/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007424764\n",
      "Epoch [171/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0007422282\n",
      "Epoch [171/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0007419801\n",
      "Epoch [171/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0007417318\n",
      "Epoch [171/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0068, LR: 0.0007414834\n",
      "Epoch [171/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007412350\n",
      "Epoch [171/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007409865\n",
      "Epoch [171/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007407379\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 171: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 198.75 MB\n",
      "Epoch [172/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007404893\n",
      "Epoch [172/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0007402405\n",
      "Epoch [172/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0071, LR: 0.0007399917\n",
      "Epoch [172/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0007397428\n",
      "Epoch [172/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007394938\n",
      "Epoch [172/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0073, LR: 0.0007392448\n",
      "Epoch [172/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0067, LR: 0.0007389957\n",
      "Epoch [172/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0007387465\n",
      "Epoch [172/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0007384972\n",
      "Epoch [172/500], Batch [100/110], Train Loss: 0.0537, Val Loss: 0.0062, LR: 0.0007382479\n",
      "Epoch [172/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007379984\n",
      "Epoch [173/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0007377489\n",
      "Epoch [173/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0007374994\n",
      "Epoch [173/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0070, LR: 0.0007372497\n",
      "Epoch [173/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007370000\n",
      "Epoch [173/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007367502\n",
      "Epoch [173/500], Batch [60/110], Train Loss: 0.0016, Val Loss: 0.0070, LR: 0.0007365003\n",
      "Epoch [173/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0066, LR: 0.0007362504\n",
      "Epoch [173/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0007360003\n",
      "Epoch [173/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0007357502\n",
      "Epoch [173/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0062, LR: 0.0007355000\n",
      "Epoch [173/500], Batch [110/110], Train Loss: 0.0022, Val Loss: 0.0057, LR: 0.0007352498\n",
      "Epoch [174/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0056, LR: 0.0007349995\n",
      "Epoch [174/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0007347490\n",
      "Epoch [174/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0007344986\n",
      "Epoch [174/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0007342480\n",
      "Epoch [174/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0068, LR: 0.0007339974\n",
      "Epoch [174/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007337467\n",
      "Epoch [174/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0007334959\n",
      "Epoch [174/500], Batch [80/110], Train Loss: 0.0030, Val Loss: 0.0070, LR: 0.0007332450\n",
      "Epoch [174/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0007329941\n",
      "Epoch [174/500], Batch [100/110], Train Loss: 0.0028, Val Loss: 0.0079, LR: 0.0007327431\n",
      "Epoch [174/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007324920\n",
      "Epoch [175/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0007322409\n",
      "Epoch [175/500], Batch [20/110], Train Loss: 0.0009, Val Loss: 0.0069, LR: 0.0007319897\n",
      "Epoch [175/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007317384\n",
      "Epoch [175/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007314870\n",
      "Epoch [175/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0007312355\n",
      "Epoch [175/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0007309840\n",
      "Epoch [175/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0007307324\n",
      "Epoch [175/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0007304807\n",
      "Epoch [175/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0007302290\n",
      "Epoch [175/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0007299772\n",
      "Epoch [175/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0007297253\n",
      "Epoch [176/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0007294733\n",
      "Epoch [176/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0070, LR: 0.0007292213\n",
      "Epoch [176/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007289692\n",
      "Epoch [176/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0007287170\n",
      "Epoch [176/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0080, LR: 0.0007284648\n",
      "Epoch [176/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0007282124\n",
      "Epoch [176/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0007279600\n",
      "Epoch [176/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007277075\n",
      "Epoch [176/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007274550\n",
      "Epoch [176/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007272024\n",
      "Epoch [176/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007269497\n",
      "Epoch [177/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007266969\n",
      "Epoch [177/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0007264441\n",
      "Epoch [177/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007261912\n",
      "Epoch [177/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0071, LR: 0.0007259382\n",
      "Epoch [177/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007256851\n",
      "Epoch [177/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0069, LR: 0.0007254320\n",
      "Epoch [177/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0069, LR: 0.0007251788\n",
      "Epoch [177/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0069, LR: 0.0007249256\n",
      "Epoch [177/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0007246722\n",
      "Epoch [177/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0007244188\n",
      "Epoch [177/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0007241653\n",
      "Epoch [178/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007239118\n",
      "Epoch [178/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0070, LR: 0.0007236581\n",
      "Epoch [178/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0073, LR: 0.0007234044\n",
      "Epoch [178/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0074, LR: 0.0007231507\n",
      "Epoch [178/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0007228968\n",
      "Epoch [178/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0077, LR: 0.0007226429\n",
      "Epoch [178/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0078, LR: 0.0007223889\n",
      "Epoch [178/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0007221349\n",
      "Epoch [178/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007218808\n",
      "Epoch [178/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0007216266\n",
      "Epoch [178/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0007213723\n",
      "Epoch [179/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0007211180\n",
      "Epoch [179/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0007208636\n",
      "Epoch [179/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0071, LR: 0.0007206091\n",
      "Epoch [179/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0077, LR: 0.0007203545\n",
      "Epoch [179/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007200999\n",
      "Epoch [179/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0007198452\n",
      "Epoch [179/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007195905\n",
      "Epoch [179/500], Batch [80/110], Train Loss: 0.0902, Val Loss: 0.0078, LR: 0.0007193356\n",
      "Epoch [179/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0007190807\n",
      "Epoch [179/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0007188258\n",
      "Epoch [179/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0073, LR: 0.0007185707\n",
      "Epoch [180/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0073, LR: 0.0007183156\n",
      "Epoch [180/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007180605\n",
      "Epoch [180/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0007178052\n",
      "Epoch [180/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0007175499\n",
      "Epoch [180/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007172945\n",
      "Epoch [180/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007170391\n",
      "Epoch [180/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0070, LR: 0.0007167835\n",
      "Epoch [180/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007165279\n",
      "Epoch [180/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007162723\n",
      "Epoch [180/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007160165\n",
      "Epoch [180/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0007157607\n",
      "Epoch [181/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0007155049\n",
      "Epoch [181/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0076, LR: 0.0007152489\n",
      "Epoch [181/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0007149929\n",
      "Epoch [181/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0007147369\n",
      "Epoch [181/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0007144807\n",
      "Epoch [181/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007142245\n",
      "Epoch [181/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0079, LR: 0.0007139682\n",
      "Epoch [181/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007137119\n",
      "Epoch [181/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0007134555\n",
      "Epoch [181/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0007131990\n",
      "Epoch [181/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0007129424\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 181: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 198.75 MB\n",
      "Epoch [182/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0007126858\n",
      "Epoch [182/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0007124291\n",
      "Epoch [182/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007121724\n",
      "Epoch [182/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0074, LR: 0.0007119156\n",
      "Epoch [182/500], Batch [50/110], Train Loss: 0.0011, Val Loss: 0.0074, LR: 0.0007116587\n",
      "Epoch [182/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0073, LR: 0.0007114017\n",
      "Epoch [182/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007111447\n",
      "Epoch [182/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0079, LR: 0.0007108876\n",
      "Epoch [182/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007106304\n",
      "Epoch [182/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0007103732\n",
      "Epoch [182/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0007101159\n",
      "Epoch [183/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0062, LR: 0.0007098586\n",
      "Epoch [183/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0060, LR: 0.0007096011\n",
      "Epoch [183/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0007093436\n",
      "Epoch [183/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0007090861\n",
      "Epoch [183/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0007088284\n",
      "Epoch [183/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0007085708\n",
      "Epoch [183/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0007083130\n",
      "Epoch [183/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0007080552\n",
      "Epoch [183/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0007077973\n",
      "Epoch [183/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0007075393\n",
      "Epoch [183/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0007072813\n",
      "Epoch [184/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007070232\n",
      "Epoch [184/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007067650\n",
      "Epoch [184/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0007065068\n",
      "Epoch [184/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0057, LR: 0.0007062485\n",
      "Epoch [184/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0007059902\n",
      "Epoch [184/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0064, LR: 0.0007057318\n",
      "Epoch [184/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0007054733\n",
      "Epoch [184/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0064, LR: 0.0007052147\n",
      "Epoch [184/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0007049561\n",
      "Epoch [184/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0007046974\n",
      "Epoch [184/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0068, LR: 0.0007044387\n",
      "Epoch [185/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0007041799\n",
      "Epoch [185/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0007039210\n",
      "Epoch [185/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0007036621\n",
      "Epoch [185/500], Batch [40/110], Train Loss: 0.0831, Val Loss: 0.0068, LR: 0.0007034031\n",
      "Epoch [185/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0007031440\n",
      "Epoch [185/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0007028848\n",
      "Epoch [185/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0007026256\n",
      "Epoch [185/500], Batch [80/110], Train Loss: 0.0008, Val Loss: 0.0072, LR: 0.0007023664\n",
      "Epoch [185/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0071, LR: 0.0007021071\n",
      "Epoch [185/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0007018477\n",
      "Epoch [185/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0007015882\n",
      "Epoch [186/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0069, LR: 0.0007013287\n",
      "Epoch [186/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0007010691\n",
      "Epoch [186/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0007008095\n",
      "Epoch [186/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0062, LR: 0.0007005497\n",
      "Epoch [186/500], Batch [50/110], Train Loss: 0.0010, Val Loss: 0.0066, LR: 0.0007002900\n",
      "Epoch [186/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0007000301\n",
      "Epoch [186/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006997702\n",
      "Epoch [186/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006995102\n",
      "Epoch [186/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0067, LR: 0.0006992502\n",
      "Epoch [186/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0067, LR: 0.0006989901\n",
      "Epoch [186/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006987300\n",
      "Epoch [187/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0066, LR: 0.0006984697\n",
      "Epoch [187/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0006982095\n",
      "Epoch [187/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0066, LR: 0.0006979491\n",
      "Epoch [187/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0068, LR: 0.0006976887\n",
      "Epoch [187/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0006974282\n",
      "Epoch [187/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0006971677\n",
      "Epoch [187/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006969071\n",
      "Epoch [187/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006966464\n",
      "Epoch [187/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0006963857\n",
      "Epoch [187/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0006961249\n",
      "Epoch [187/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0056, LR: 0.0006958641\n",
      "Epoch [188/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0057, LR: 0.0006956032\n",
      "Epoch [188/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006953422\n",
      "Epoch [188/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006950812\n",
      "Epoch [188/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0066, LR: 0.0006948201\n",
      "Epoch [188/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006945589\n",
      "Epoch [188/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006942977\n",
      "Epoch [188/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0068, LR: 0.0006940364\n",
      "Epoch [188/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006937751\n",
      "Epoch [188/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006935136\n",
      "Epoch [188/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0066, LR: 0.0006932522\n",
      "Epoch [188/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0006929907\n",
      "Epoch [189/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0006927291\n",
      "Epoch [189/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0006924674\n",
      "Epoch [189/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0006922057\n",
      "Epoch [189/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006919439\n",
      "Epoch [189/500], Batch [50/110], Train Loss: 0.0536, Val Loss: 0.0061, LR: 0.0006916821\n",
      "Epoch [189/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0006914202\n",
      "Epoch [189/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0006911582\n",
      "Epoch [189/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006908962\n",
      "Epoch [189/500], Batch [90/110], Train Loss: 0.0017, Val Loss: 0.0067, LR: 0.0006906341\n",
      "Epoch [189/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0006903720\n",
      "Epoch [189/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006901098\n",
      "Epoch [190/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006898475\n",
      "Epoch [190/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0006895852\n",
      "Epoch [190/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0064, LR: 0.0006893228\n",
      "Epoch [190/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006890604\n",
      "Epoch [190/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0006887979\n",
      "Epoch [190/500], Batch [60/110], Train Loss: 0.0019, Val Loss: 0.0068, LR: 0.0006885353\n",
      "Epoch [190/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0006882727\n",
      "Epoch [190/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0006880100\n",
      "Epoch [190/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0006877473\n",
      "Epoch [190/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0006874845\n",
      "Epoch [190/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0006872217\n",
      "Epoch [191/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0076, LR: 0.0006869587\n",
      "Epoch [191/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0006866958\n",
      "Epoch [191/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0006864327\n",
      "Epoch [191/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0006861696\n",
      "Epoch [191/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0006859065\n",
      "Epoch [191/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0006856433\n",
      "Epoch [191/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0006853800\n",
      "Epoch [191/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006851167\n",
      "Epoch [191/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0006848533\n",
      "Epoch [191/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006845898\n",
      "Epoch [191/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0006843263\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 191: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.042 sec , Memory Usage: 198.76 MB\n",
      "Epoch [192/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0062, LR: 0.0006840627\n",
      "Epoch [192/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0006837991\n",
      "Epoch [192/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0006835354\n",
      "Epoch [192/500], Batch [40/110], Train Loss: 0.0009, Val Loss: 0.0080, LR: 0.0006832717\n",
      "Epoch [192/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0080, LR: 0.0006830079\n",
      "Epoch [192/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0006827440\n",
      "Epoch [192/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0076, LR: 0.0006824801\n",
      "Epoch [192/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0076, LR: 0.0006822161\n",
      "Epoch [192/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0076, LR: 0.0006819521\n",
      "Epoch [192/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0006816880\n",
      "Epoch [192/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0078, LR: 0.0006814239\n",
      "Epoch [193/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0006811597\n",
      "Epoch [193/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0006808954\n",
      "Epoch [193/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0006806311\n",
      "Epoch [193/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0006803667\n",
      "Epoch [193/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0006801023\n",
      "Epoch [193/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0006798378\n",
      "Epoch [193/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006795732\n",
      "Epoch [193/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006793086\n",
      "Epoch [193/500], Batch [90/110], Train Loss: 0.0013, Val Loss: 0.0070, LR: 0.0006790440\n",
      "Epoch [193/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006787793\n",
      "Epoch [193/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006785145\n",
      "Epoch [194/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0006782497\n",
      "Epoch [194/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0006779848\n",
      "Epoch [194/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006777198\n",
      "Epoch [194/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0006774548\n",
      "Epoch [194/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006771898\n",
      "Epoch [194/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0066, LR: 0.0006769247\n",
      "Epoch [194/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006766595\n",
      "Epoch [194/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0083, LR: 0.0006763943\n",
      "Epoch [194/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0006761290\n",
      "Epoch [194/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0006758636\n",
      "Epoch [194/500], Batch [110/110], Train Loss: 0.0057, Val Loss: 0.0081, LR: 0.0006755982\n",
      "Epoch [195/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0006753328\n",
      "Epoch [195/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0075, LR: 0.0006750673\n",
      "Epoch [195/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0006748017\n",
      "Epoch [195/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0076, LR: 0.0006745361\n",
      "Epoch [195/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0006742704\n",
      "Epoch [195/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0006740047\n",
      "Epoch [195/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0006737389\n",
      "Epoch [195/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0006734731\n",
      "Epoch [195/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0006732072\n",
      "Epoch [195/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006729413\n",
      "Epoch [195/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0006726753\n",
      "Epoch [196/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006724092\n",
      "Epoch [196/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006721431\n",
      "Epoch [196/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0006718769\n",
      "Epoch [196/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0006716107\n",
      "Epoch [196/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0059, LR: 0.0006713444\n",
      "Epoch [196/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0006710781\n",
      "Epoch [196/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0006708117\n",
      "Epoch [196/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0006705453\n",
      "Epoch [196/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0006702788\n",
      "Epoch [196/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0006700123\n",
      "Epoch [196/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0006697457\n",
      "Epoch [197/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0006694790\n",
      "Epoch [197/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0006692123\n",
      "Epoch [197/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0006689456\n",
      "Epoch [197/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0076, LR: 0.0006686787\n",
      "Epoch [197/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0006684119\n",
      "Epoch [197/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0006681450\n",
      "Epoch [197/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0068, LR: 0.0006678780\n",
      "Epoch [197/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0006676110\n",
      "Epoch [197/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006673439\n",
      "Epoch [197/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0006670768\n",
      "Epoch [197/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0006668096\n",
      "Epoch [198/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006665423\n",
      "Epoch [198/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006662750\n",
      "Epoch [198/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0006660077\n",
      "Epoch [198/500], Batch [40/110], Train Loss: 0.0034, Val Loss: 0.0069, LR: 0.0006657403\n",
      "Epoch [198/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006654729\n",
      "Epoch [198/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0006652054\n",
      "Epoch [198/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0006649378\n",
      "Epoch [198/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006646702\n",
      "Epoch [198/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0006644026\n",
      "Epoch [198/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006641348\n",
      "Epoch [198/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0006638671\n",
      "Epoch [199/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0006635993\n",
      "Epoch [199/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006633314\n",
      "Epoch [199/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0006630635\n",
      "Epoch [199/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006627955\n",
      "Epoch [199/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006625275\n",
      "Epoch [199/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0006622594\n",
      "Epoch [199/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0068, LR: 0.0006619913\n",
      "Epoch [199/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0068, LR: 0.0006617231\n",
      "Epoch [199/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006614549\n",
      "Epoch [199/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0066, LR: 0.0006611867\n",
      "Epoch [199/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0066, LR: 0.0006609183\n",
      "Epoch [200/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0064, LR: 0.0006606500\n",
      "Epoch [200/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0006603815\n",
      "Epoch [200/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006601130\n",
      "Epoch [200/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006598445\n",
      "Epoch [200/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0060, LR: 0.0006595759\n",
      "Epoch [200/500], Batch [60/110], Train Loss: 0.0067, Val Loss: 0.0060, LR: 0.0006593073\n",
      "Epoch [200/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0006590386\n",
      "Epoch [200/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0006587699\n",
      "Epoch [200/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0006585011\n",
      "Epoch [200/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0006582323\n",
      "Epoch [200/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006579634\n",
      "Epoch [201/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0006576945\n",
      "Epoch [201/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0070, LR: 0.0006574255\n",
      "Epoch [201/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0066, LR: 0.0006571565\n",
      "Epoch [201/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0058, LR: 0.0006568874\n",
      "Epoch [201/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0006566183\n",
      "Epoch [201/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0006563491\n",
      "Epoch [201/500], Batch [70/110], Train Loss: 0.0855, Val Loss: 0.0055, LR: 0.0006560799\n",
      "Epoch [201/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0006558106\n",
      "Epoch [201/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0006555413\n",
      "Epoch [201/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0006552719\n",
      "Epoch [201/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0006550025\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 201: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 198.76 MB\n",
      "Epoch [202/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0006547330\n",
      "Epoch [202/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0006544635\n",
      "Epoch [202/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006541939\n",
      "Epoch [202/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0006539243\n",
      "Epoch [202/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0006536546\n",
      "Epoch [202/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0006533849\n",
      "Epoch [202/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006531151\n",
      "Epoch [202/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006528453\n",
      "Epoch [202/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006525754\n",
      "Epoch [202/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0006523055\n",
      "Epoch [202/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0066, LR: 0.0006520356\n",
      "Epoch [203/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0006517656\n",
      "Epoch [203/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0070, LR: 0.0006514955\n",
      "Epoch [203/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0006512254\n",
      "Epoch [203/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006509553\n",
      "Epoch [203/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0006506851\n",
      "Epoch [203/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006504148\n",
      "Epoch [203/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0006501445\n",
      "Epoch [203/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0006498742\n",
      "Epoch [203/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006496038\n",
      "Epoch [203/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006493334\n",
      "Epoch [203/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006490629\n",
      "Epoch [204/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0006487924\n",
      "Epoch [204/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0006485218\n",
      "Epoch [204/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0006482512\n",
      "Epoch [204/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0006479805\n",
      "Epoch [204/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0006477098\n",
      "Epoch [204/500], Batch [60/110], Train Loss: 0.0341, Val Loss: 0.0048, LR: 0.0006474390\n",
      "Epoch [204/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0006471682\n",
      "Epoch [204/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0057, LR: 0.0006468974\n",
      "Epoch [204/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0006466265\n",
      "Epoch [204/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0006463555\n",
      "Epoch [204/500], Batch [110/110], Train Loss: 0.0029, Val Loss: 0.0068, LR: 0.0006460845\n",
      "Epoch [205/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0006458135\n",
      "Epoch [205/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006455424\n",
      "Epoch [205/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006452713\n",
      "Epoch [205/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0006450001\n",
      "Epoch [205/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006447289\n",
      "Epoch [205/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0066, LR: 0.0006444576\n",
      "Epoch [205/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006441863\n",
      "Epoch [205/500], Batch [80/110], Train Loss: 0.1225, Val Loss: 0.0060, LR: 0.0006439149\n",
      "Epoch [205/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006436435\n",
      "Epoch [205/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006433721\n",
      "Epoch [205/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006431006\n",
      "Epoch [206/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0006428291\n",
      "Epoch [206/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006425575\n",
      "Epoch [206/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006422858\n",
      "Epoch [206/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006420142\n",
      "Epoch [206/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006417425\n",
      "Epoch [206/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0006414707\n",
      "Epoch [206/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0006411989\n",
      "Epoch [206/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0060, LR: 0.0006409270\n",
      "Epoch [206/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006406551\n",
      "Epoch [206/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006403832\n",
      "Epoch [206/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0058, LR: 0.0006401112\n",
      "Epoch [207/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006398392\n",
      "Epoch [207/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006395671\n",
      "Epoch [207/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006392950\n",
      "Epoch [207/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0006390228\n",
      "Epoch [207/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0006387506\n",
      "Epoch [207/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0006384784\n",
      "Epoch [207/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0006382061\n",
      "Epoch [207/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006379337\n",
      "Epoch [207/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0006376614\n",
      "Epoch [207/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0006373889\n",
      "Epoch [207/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0006371165\n",
      "Epoch [208/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0006368440\n",
      "Epoch [208/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006365714\n",
      "Epoch [208/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006362988\n",
      "Epoch [208/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0006360262\n",
      "Epoch [208/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0006357535\n",
      "Epoch [208/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0043, LR: 0.0006354808\n",
      "Epoch [208/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0006352080\n",
      "Epoch [208/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0006349352\n",
      "Epoch [208/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0006346624\n",
      "Epoch [208/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0006343895\n",
      "Epoch [208/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0049, LR: 0.0006341165\n",
      "Epoch [209/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0050, LR: 0.0006338436\n",
      "Epoch [209/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0006335706\n",
      "Epoch [209/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0006332975\n",
      "Epoch [209/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0006330244\n",
      "Epoch [209/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006327512\n",
      "Epoch [209/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0006324781\n",
      "Epoch [209/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0006322048\n",
      "Epoch [209/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0006319316\n",
      "Epoch [209/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0006316583\n",
      "Epoch [209/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0053, LR: 0.0006313849\n",
      "Epoch [209/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0006311115\n",
      "Epoch [210/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0006308381\n",
      "Epoch [210/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0006305646\n",
      "Epoch [210/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006302911\n",
      "Epoch [210/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0006300175\n",
      "Epoch [210/500], Batch [50/110], Train Loss: 0.0464, Val Loss: 0.0061, LR: 0.0006297439\n",
      "Epoch [210/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0006294703\n",
      "Epoch [210/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0006291966\n",
      "Epoch [210/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0006289229\n",
      "Epoch [210/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0006286491\n",
      "Epoch [210/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0068, LR: 0.0006283753\n",
      "Epoch [210/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006281015\n",
      "Epoch [211/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0006278276\n",
      "Epoch [211/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0059, LR: 0.0006275537\n",
      "Epoch [211/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0006272797\n",
      "Epoch [211/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0006270057\n",
      "Epoch [211/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0006267317\n",
      "Epoch [211/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0006264576\n",
      "Epoch [211/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006261835\n",
      "Epoch [211/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006259093\n",
      "Epoch [211/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006256351\n",
      "Epoch [211/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006253609\n",
      "Epoch [211/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0006250866\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 211: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 198.79 MB\n",
      "Epoch [212/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0006248123\n",
      "Epoch [212/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0006245379\n",
      "Epoch [212/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006242636\n",
      "Epoch [212/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006239891\n",
      "Epoch [212/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006237146\n",
      "Epoch [212/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0006234401\n",
      "Epoch [212/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006231656\n",
      "Epoch [212/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0006228910\n",
      "Epoch [212/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0006226164\n",
      "Epoch [212/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0006223417\n",
      "Epoch [212/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0056, LR: 0.0006220670\n",
      "Epoch [213/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0006217923\n",
      "Epoch [213/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0006215175\n",
      "Epoch [213/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006212427\n",
      "Epoch [213/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006209678\n",
      "Epoch [213/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0056, LR: 0.0006206929\n",
      "Epoch [213/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0055, LR: 0.0006204180\n",
      "Epoch [213/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0054, LR: 0.0006201430\n",
      "Epoch [213/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0006198680\n",
      "Epoch [213/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0053, LR: 0.0006195930\n",
      "Epoch [213/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0006193179\n",
      "Epoch [213/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0006190428\n",
      "Epoch [214/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0006187676\n",
      "Epoch [214/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0006184924\n",
      "Epoch [214/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006182172\n",
      "Epoch [214/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006179419\n",
      "Epoch [214/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006176666\n",
      "Epoch [214/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0057, LR: 0.0006173913\n",
      "Epoch [214/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0006171159\n",
      "Epoch [214/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0006168405\n",
      "Epoch [214/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006165650\n",
      "Epoch [214/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006162895\n",
      "Epoch [214/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0006160140\n",
      "Epoch [215/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0006157385\n",
      "Epoch [215/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0006154629\n",
      "Epoch [215/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0006151872\n",
      "Epoch [215/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0006149116\n",
      "Epoch [215/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0068, LR: 0.0006146359\n",
      "Epoch [215/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0006143601\n",
      "Epoch [215/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0006140844\n",
      "Epoch [215/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0006138085\n",
      "Epoch [215/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0006135327\n",
      "Epoch [215/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0055, LR: 0.0006132568\n",
      "Epoch [215/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0006129809\n",
      "Epoch [216/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0055, LR: 0.0006127050\n",
      "Epoch [216/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0006124290\n",
      "Epoch [216/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006121529\n",
      "Epoch [216/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0006118769\n",
      "Epoch [216/500], Batch [50/110], Train Loss: 0.0060, Val Loss: 0.0049, LR: 0.0006116008\n",
      "Epoch [216/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0045, LR: 0.0006113247\n",
      "Epoch [216/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0006110485\n",
      "Epoch [216/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0006107723\n",
      "Epoch [216/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0006104961\n",
      "Epoch [216/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0050, LR: 0.0006102198\n",
      "Epoch [216/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0006099435\n",
      "Epoch [217/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0006096672\n",
      "Epoch [217/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0006093908\n",
      "Epoch [217/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0048, LR: 0.0006091144\n",
      "Epoch [217/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0006088380\n",
      "Epoch [217/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0006085615\n",
      "Epoch [217/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0006082850\n",
      "Epoch [217/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0006080085\n",
      "Epoch [217/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0006077319\n",
      "Epoch [217/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0006074553\n",
      "Epoch [217/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0052, LR: 0.0006071787\n",
      "Epoch [217/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0006069020\n",
      "Epoch [218/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0052, LR: 0.0006066253\n",
      "Epoch [218/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0006063486\n",
      "Epoch [218/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0006060718\n",
      "Epoch [218/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0006057950\n",
      "Epoch [218/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0006055181\n",
      "Epoch [218/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0006052413\n",
      "Epoch [218/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0006049644\n",
      "Epoch [218/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0056, LR: 0.0006046874\n",
      "Epoch [218/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0006044105\n",
      "Epoch [218/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0006041335\n",
      "Epoch [218/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0006038564\n",
      "Epoch [219/500], Batch [10/110], Train Loss: 0.0033, Val Loss: 0.0056, LR: 0.0006035794\n",
      "Epoch [219/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0006033023\n",
      "Epoch [219/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0055, LR: 0.0006030252\n",
      "Epoch [219/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0006027480\n",
      "Epoch [219/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0006024708\n",
      "Epoch [219/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006021936\n",
      "Epoch [219/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0056, LR: 0.0006019163\n",
      "Epoch [219/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006016390\n",
      "Epoch [219/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006013617\n",
      "Epoch [219/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0006010844\n",
      "Epoch [219/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0006008070\n",
      "Epoch [220/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0006005296\n",
      "Epoch [220/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0006002521\n",
      "Epoch [220/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0059, LR: 0.0005999747\n",
      "Epoch [220/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005996972\n",
      "Epoch [220/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0055, LR: 0.0005994196\n",
      "Epoch [220/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0005991420\n",
      "Epoch [220/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0061, LR: 0.0005988644\n",
      "Epoch [220/500], Batch [80/110], Train Loss: 0.0008, Val Loss: 0.0058, LR: 0.0005985868\n",
      "Epoch [220/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005983092\n",
      "Epoch [220/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0058, LR: 0.0005980315\n",
      "Epoch [220/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0067, LR: 0.0005977538\n",
      "Epoch [221/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005974760\n",
      "Epoch [221/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005971982\n",
      "Epoch [221/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005969204\n",
      "Epoch [221/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0005966426\n",
      "Epoch [221/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0005963647\n",
      "Epoch [221/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0005960868\n",
      "Epoch [221/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0005958089\n",
      "Epoch [221/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0052, LR: 0.0005955309\n",
      "Epoch [221/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0005952529\n",
      "Epoch [221/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0005949749\n",
      "Epoch [221/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0005946969\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 221: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.044 sec , Memory Usage: 198.79 MB\n",
      "Epoch [222/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0056, LR: 0.0005944188\n",
      "Epoch [222/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005941407\n",
      "Epoch [222/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0005938625\n",
      "Epoch [222/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005935844\n",
      "Epoch [222/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0005933062\n",
      "Epoch [222/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005930280\n",
      "Epoch [222/500], Batch [70/110], Train Loss: 0.0023, Val Loss: 0.0069, LR: 0.0005927497\n",
      "Epoch [222/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0005924714\n",
      "Epoch [222/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0005921931\n",
      "Epoch [222/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0064, LR: 0.0005919148\n",
      "Epoch [222/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0005916364\n",
      "Epoch [223/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0005913580\n",
      "Epoch [223/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005910796\n",
      "Epoch [223/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0005908012\n",
      "Epoch [223/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0005905227\n",
      "Epoch [223/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005902442\n",
      "Epoch [223/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0063, LR: 0.0005899656\n",
      "Epoch [223/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0005896871\n",
      "Epoch [223/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0005894085\n",
      "Epoch [223/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0005891299\n",
      "Epoch [223/500], Batch [100/110], Train Loss: 0.0620, Val Loss: 0.0059, LR: 0.0005888512\n",
      "Epoch [223/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005885726\n",
      "Epoch [224/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0068, LR: 0.0005882939\n",
      "Epoch [224/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0005880151\n",
      "Epoch [224/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005877364\n",
      "Epoch [224/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0055, LR: 0.0005874576\n",
      "Epoch [224/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005871788\n",
      "Epoch [224/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005869000\n",
      "Epoch [224/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0005866211\n",
      "Epoch [224/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0050, LR: 0.0005863422\n",
      "Epoch [224/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0005860633\n",
      "Epoch [224/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0005857844\n",
      "Epoch [224/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0051, LR: 0.0005855054\n",
      "Epoch [225/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0005852264\n",
      "Epoch [225/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0005849474\n",
      "Epoch [225/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0005846683\n",
      "Epoch [225/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0005843893\n",
      "Epoch [225/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0005841102\n",
      "Epoch [225/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005838311\n",
      "Epoch [225/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005835519\n",
      "Epoch [225/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0005832727\n",
      "Epoch [225/500], Batch [90/110], Train Loss: 0.0571, Val Loss: 0.0058, LR: 0.0005829935\n",
      "Epoch [225/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0060, LR: 0.0005827143\n",
      "Epoch [225/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005824351\n",
      "Epoch [226/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005821558\n",
      "Epoch [226/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0055, LR: 0.0005818765\n",
      "Epoch [226/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0005815972\n",
      "Epoch [226/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0005813178\n",
      "Epoch [226/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005810384\n",
      "Epoch [226/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0052, LR: 0.0005807590\n",
      "Epoch [226/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0005804796\n",
      "Epoch [226/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005802002\n",
      "Epoch [226/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0005799207\n",
      "Epoch [226/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005796412\n",
      "Epoch [226/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0005793617\n",
      "Epoch [227/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0005790821\n",
      "Epoch [227/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0005788025\n",
      "Epoch [227/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0005785230\n",
      "Epoch [227/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0005782433\n",
      "Epoch [227/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0005779637\n",
      "Epoch [227/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0005776840\n",
      "Epoch [227/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0005774043\n",
      "Epoch [227/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005771246\n",
      "Epoch [227/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0005768449\n",
      "Epoch [227/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0005765651\n",
      "Epoch [227/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0005762853\n",
      "Epoch [228/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0005760055\n",
      "Epoch [228/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0005757257\n",
      "Epoch [228/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0068, LR: 0.0005754458\n",
      "Epoch [228/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0005751660\n",
      "Epoch [228/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0005748861\n",
      "Epoch [228/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0005746061\n",
      "Epoch [228/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0005743262\n",
      "Epoch [228/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0005740462\n",
      "Epoch [228/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0005737662\n",
      "Epoch [228/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0005734862\n",
      "Epoch [228/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0005732062\n",
      "Epoch [229/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005729261\n",
      "Epoch [229/500], Batch [20/110], Train Loss: 0.0009, Val Loss: 0.0059, LR: 0.0005726461\n",
      "Epoch [229/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005723660\n",
      "Epoch [229/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0053, LR: 0.0005720858\n",
      "Epoch [229/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0005718057\n",
      "Epoch [229/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0005715255\n",
      "Epoch [229/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0005712453\n",
      "Epoch [229/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0005709651\n",
      "Epoch [229/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0005706849\n",
      "Epoch [229/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0005704046\n",
      "Epoch [229/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0005701244\n",
      "Epoch [230/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0005698441\n",
      "Epoch [230/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0049, LR: 0.0005695637\n",
      "Epoch [230/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0005692834\n",
      "Epoch [230/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005690030\n",
      "Epoch [230/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0005687227\n",
      "Epoch [230/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0056, LR: 0.0005684423\n",
      "Epoch [230/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005681618\n",
      "Epoch [230/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005678814\n",
      "Epoch [230/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005676009\n",
      "Epoch [230/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005673205\n",
      "Epoch [230/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0057, LR: 0.0005670400\n",
      "Epoch [231/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0005667594\n",
      "Epoch [231/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005664789\n",
      "Epoch [231/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0005661983\n",
      "Epoch [231/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005659177\n",
      "Epoch [231/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005656371\n",
      "Epoch [231/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0056, LR: 0.0005653565\n",
      "Epoch [231/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005650759\n",
      "Epoch [231/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005647952\n",
      "Epoch [231/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005645145\n",
      "Epoch [231/500], Batch [100/110], Train Loss: 0.1065, Val Loss: 0.0071, LR: 0.0005642338\n",
      "Epoch [231/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0005639531\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 231: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.037 sec , Memory Usage: 198.79 MB\n",
      "Epoch [232/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005636724\n",
      "Epoch [232/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005633916\n",
      "Epoch [232/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005631108\n",
      "Epoch [232/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005628300\n",
      "Epoch [232/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0005625492\n",
      "Epoch [232/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0005622684\n",
      "Epoch [232/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005619875\n",
      "Epoch [232/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0005617066\n",
      "Epoch [232/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0005614257\n",
      "Epoch [232/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005611448\n",
      "Epoch [232/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005608639\n",
      "Epoch [233/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0005605830\n",
      "Epoch [233/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005603020\n",
      "Epoch [233/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005600210\n",
      "Epoch [233/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005597400\n",
      "Epoch [233/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0005594590\n",
      "Epoch [233/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0005591780\n",
      "Epoch [233/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0005588969\n",
      "Epoch [233/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0005586158\n",
      "Epoch [233/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0005583348\n",
      "Epoch [233/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0005580536\n",
      "Epoch [233/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0070, LR: 0.0005577725\n",
      "Epoch [234/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0005574914\n",
      "Epoch [234/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0005572102\n",
      "Epoch [234/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0005569290\n",
      "Epoch [234/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0005566479\n",
      "Epoch [234/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005563666\n",
      "Epoch [234/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0005560854\n",
      "Epoch [234/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005558042\n",
      "Epoch [234/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0061, LR: 0.0005555229\n",
      "Epoch [234/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0005552416\n",
      "Epoch [234/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0005549604\n",
      "Epoch [234/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005546790\n",
      "Epoch [235/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0005543977\n",
      "Epoch [235/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0005541164\n",
      "Epoch [235/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0005538350\n",
      "Epoch [235/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005535537\n",
      "Epoch [235/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0066, LR: 0.0005532723\n",
      "Epoch [235/500], Batch [60/110], Train Loss: 0.0030, Val Loss: 0.0073, LR: 0.0005529909\n",
      "Epoch [235/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0005527094\n",
      "Epoch [235/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005524280\n",
      "Epoch [235/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0005521466\n",
      "Epoch [235/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0058, LR: 0.0005518651\n",
      "Epoch [235/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005515836\n",
      "Epoch [236/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0005513021\n",
      "Epoch [236/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005510206\n",
      "Epoch [236/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0005507391\n",
      "Epoch [236/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0005504575\n",
      "Epoch [236/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0005501760\n",
      "Epoch [236/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0005498944\n",
      "Epoch [236/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005496128\n",
      "Epoch [236/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005493312\n",
      "Epoch [236/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0005490496\n",
      "Epoch [236/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005487680\n",
      "Epoch [236/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005484863\n",
      "Epoch [237/500], Batch [10/110], Train Loss: 0.0411, Val Loss: 0.0060, LR: 0.0005482047\n",
      "Epoch [237/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0005479230\n",
      "Epoch [237/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0066, LR: 0.0005476413\n",
      "Epoch [237/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005473596\n",
      "Epoch [237/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005470779\n",
      "Epoch [237/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0005467962\n",
      "Epoch [237/500], Batch [70/110], Train Loss: 0.0440, Val Loss: 0.0062, LR: 0.0005465144\n",
      "Epoch [237/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0005462327\n",
      "Epoch [237/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005459509\n",
      "Epoch [237/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005456691\n",
      "Epoch [237/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0055, LR: 0.0005453874\n",
      "Epoch [238/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0005451055\n",
      "Epoch [238/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0005448237\n",
      "Epoch [238/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0005445419\n",
      "Epoch [238/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0005442600\n",
      "Epoch [238/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0005439782\n",
      "Epoch [238/500], Batch [60/110], Train Loss: 0.0016, Val Loss: 0.0053, LR: 0.0005436963\n",
      "Epoch [238/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0005434144\n",
      "Epoch [238/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0005431325\n",
      "Epoch [238/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0005428506\n",
      "Epoch [238/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005425687\n",
      "Epoch [238/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0056, LR: 0.0005422868\n",
      "Epoch [239/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0005420048\n",
      "Epoch [239/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0005417229\n",
      "Epoch [239/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0005414409\n",
      "Epoch [239/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0005411589\n",
      "Epoch [239/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0056, LR: 0.0005408769\n",
      "Epoch [239/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005405949\n",
      "Epoch [239/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0005403129\n",
      "Epoch [239/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0005400309\n",
      "Epoch [239/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005397488\n",
      "Epoch [239/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005394668\n",
      "Epoch [239/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0051, LR: 0.0005391847\n",
      "Epoch [240/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0005389026\n",
      "Epoch [240/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0005386206\n",
      "Epoch [240/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0051, LR: 0.0005383385\n",
      "Epoch [240/500], Batch [40/110], Train Loss: 0.0111, Val Loss: 0.0053, LR: 0.0005380564\n",
      "Epoch [240/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0005377742\n",
      "Epoch [240/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0005374921\n",
      "Epoch [240/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0005372100\n",
      "Epoch [240/500], Batch [80/110], Train Loss: 0.0134, Val Loss: 0.0052, LR: 0.0005369278\n",
      "Epoch [240/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0005366457\n",
      "Epoch [240/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0005363635\n",
      "Epoch [240/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005360813\n",
      "Epoch [241/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0005357991\n",
      "Epoch [241/500], Batch [20/110], Train Loss: 0.0010, Val Loss: 0.0071, LR: 0.0005355169\n",
      "Epoch [241/500], Batch [30/110], Train Loss: 0.0398, Val Loss: 0.0068, LR: 0.0005352347\n",
      "Epoch [241/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0005349525\n",
      "Epoch [241/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0069, LR: 0.0005346703\n",
      "Epoch [241/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0068, LR: 0.0005343880\n",
      "Epoch [241/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0005341058\n",
      "Epoch [241/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0005338235\n",
      "Epoch [241/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0005335412\n",
      "Epoch [241/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0005332590\n",
      "Epoch [241/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0005329767\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 241: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.036 sec , Memory Usage: 198.81 MB\n",
      "Epoch [242/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0064, LR: 0.0005326944\n",
      "Epoch [242/500], Batch [20/110], Train Loss: 0.0021, Val Loss: 0.0064, LR: 0.0005324121\n",
      "Epoch [242/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0005321298\n",
      "Epoch [242/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0005318474\n",
      "Epoch [242/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0005315651\n",
      "Epoch [242/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005312828\n",
      "Epoch [242/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005310004\n",
      "Epoch [242/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0066, LR: 0.0005307181\n",
      "Epoch [242/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0005304357\n",
      "Epoch [242/500], Batch [100/110], Train Loss: 0.0154, Val Loss: 0.0066, LR: 0.0005301533\n",
      "Epoch [242/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0005298709\n",
      "Epoch [243/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0005295885\n",
      "Epoch [243/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0005293061\n",
      "Epoch [243/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0005290237\n",
      "Epoch [243/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0005287413\n",
      "Epoch [243/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005284589\n",
      "Epoch [243/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0005281765\n",
      "Epoch [243/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0005278940\n",
      "Epoch [243/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0066, LR: 0.0005276116\n",
      "Epoch [243/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0005273291\n",
      "Epoch [243/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0064, LR: 0.0005270467\n",
      "Epoch [243/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0005267642\n",
      "Epoch [244/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0005264817\n",
      "Epoch [244/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0005261993\n",
      "Epoch [244/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0072, LR: 0.0005259168\n",
      "Epoch [244/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005256343\n",
      "Epoch [244/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0005253518\n",
      "Epoch [244/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0005250693\n",
      "Epoch [244/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0005247868\n",
      "Epoch [244/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0005245042\n",
      "Epoch [244/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0005242217\n",
      "Epoch [244/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0005239392\n",
      "Epoch [244/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0072, LR: 0.0005236566\n",
      "Epoch [245/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0073, LR: 0.0005233741\n",
      "Epoch [245/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0005230915\n",
      "Epoch [245/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0005228090\n",
      "Epoch [245/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005225264\n",
      "Epoch [245/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0005222439\n",
      "Epoch [245/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0005219613\n",
      "Epoch [245/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0051, LR: 0.0005216787\n",
      "Epoch [245/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0005213961\n",
      "Epoch [245/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0048, LR: 0.0005211135\n",
      "Epoch [245/500], Batch [100/110], Train Loss: 0.0021, Val Loss: 0.0048, LR: 0.0005208309\n",
      "Epoch [245/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0005205483\n",
      "Epoch [246/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0005202657\n",
      "Epoch [246/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0005199831\n",
      "Epoch [246/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0005197005\n",
      "Epoch [246/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0005194179\n",
      "Epoch [246/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0005191352\n",
      "Epoch [246/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0005188526\n",
      "Epoch [246/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005185700\n",
      "Epoch [246/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0005182873\n",
      "Epoch [246/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0071, LR: 0.0005180047\n",
      "Epoch [246/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0005177220\n",
      "Epoch [246/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0005174394\n",
      "Epoch [247/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0005171567\n",
      "Epoch [247/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0005168741\n",
      "Epoch [247/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005165914\n",
      "Epoch [247/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005163087\n",
      "Epoch [247/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0005160261\n",
      "Epoch [247/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0005157434\n",
      "Epoch [247/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0005154607\n",
      "Epoch [247/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0005151780\n",
      "Epoch [247/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0005148954\n",
      "Epoch [247/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0005146127\n",
      "Epoch [247/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005143300\n",
      "Epoch [248/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0071, LR: 0.0005140473\n",
      "Epoch [248/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0005137646\n",
      "Epoch [248/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005134819\n",
      "Epoch [248/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0005131992\n",
      "Epoch [248/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0005129165\n",
      "Epoch [248/500], Batch [60/110], Train Loss: 0.1096, Val Loss: 0.0065, LR: 0.0005126338\n",
      "Epoch [248/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005123511\n",
      "Epoch [248/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0005120683\n",
      "Epoch [248/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0005117856\n",
      "Epoch [248/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0005115029\n",
      "Epoch [248/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0005112202\n",
      "Epoch [249/500], Batch [10/110], Train Loss: 0.0070, Val Loss: 0.0086, LR: 0.0005109375\n",
      "Epoch [249/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0079, LR: 0.0005106547\n",
      "Epoch [249/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0076, LR: 0.0005103720\n",
      "Epoch [249/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0005100893\n",
      "Epoch [249/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0005098066\n",
      "Epoch [249/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0005095238\n",
      "Epoch [249/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0005092411\n",
      "Epoch [249/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0005089584\n",
      "Epoch [249/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005086756\n",
      "Epoch [249/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005083929\n",
      "Epoch [249/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0005081102\n",
      "Epoch [250/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0005078274\n",
      "Epoch [250/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005075447\n",
      "Epoch [250/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0005072619\n",
      "Epoch [250/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0005069792\n",
      "Epoch [250/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0005066965\n",
      "Epoch [250/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0005064137\n",
      "Epoch [250/500], Batch [70/110], Train Loss: 0.0123, Val Loss: 0.0072, LR: 0.0005061310\n",
      "Epoch [250/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0005058482\n",
      "Epoch [250/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005055655\n",
      "Epoch [250/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0005052827\n",
      "Epoch [250/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005050000\n",
      "Epoch [251/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0005047173\n",
      "Epoch [251/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005044345\n",
      "Epoch [251/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0005041518\n",
      "Epoch [251/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0005038690\n",
      "Epoch [251/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0005035863\n",
      "Epoch [251/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0005033035\n",
      "Epoch [251/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0005030208\n",
      "Epoch [251/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0005027381\n",
      "Epoch [251/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0005024553\n",
      "Epoch [251/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0005021726\n",
      "Epoch [251/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0005018898\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 251: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 197.07 MB\n",
      "Epoch [252/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0005016071\n",
      "Epoch [252/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0005013244\n",
      "Epoch [252/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0005010416\n",
      "Epoch [252/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0005007589\n",
      "Epoch [252/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0005004762\n",
      "Epoch [252/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0005001934\n",
      "Epoch [252/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004999107\n",
      "Epoch [252/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0067, LR: 0.0004996280\n",
      "Epoch [252/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004993453\n",
      "Epoch [252/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0004990625\n",
      "Epoch [252/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004987798\n",
      "Epoch [253/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004984971\n",
      "Epoch [253/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0080, LR: 0.0004982144\n",
      "Epoch [253/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0097, LR: 0.0004979317\n",
      "Epoch [253/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0004976489\n",
      "Epoch [253/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004973662\n",
      "Epoch [253/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0004970835\n",
      "Epoch [253/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0004968008\n",
      "Epoch [253/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0004965181\n",
      "Epoch [253/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0004962354\n",
      "Epoch [253/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0004959527\n",
      "Epoch [253/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0004956700\n",
      "Epoch [254/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004953873\n",
      "Epoch [254/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0004951046\n",
      "Epoch [254/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0004948220\n",
      "Epoch [254/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0079, LR: 0.0004945393\n",
      "Epoch [254/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004942566\n",
      "Epoch [254/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0004939739\n",
      "Epoch [254/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004936913\n",
      "Epoch [254/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0004934086\n",
      "Epoch [254/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0058, LR: 0.0004931259\n",
      "Epoch [254/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0058, LR: 0.0004928433\n",
      "Epoch [254/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0004925606\n",
      "Epoch [255/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004922780\n",
      "Epoch [255/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004919953\n",
      "Epoch [255/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0004917127\n",
      "Epoch [255/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004914300\n",
      "Epoch [255/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004911474\n",
      "Epoch [255/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0004908648\n",
      "Epoch [255/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0004905821\n",
      "Epoch [255/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0004902995\n",
      "Epoch [255/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004900169\n",
      "Epoch [255/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004897343\n",
      "Epoch [255/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004894517\n",
      "Epoch [256/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004891691\n",
      "Epoch [256/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0068, LR: 0.0004888865\n",
      "Epoch [256/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004886039\n",
      "Epoch [256/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004883213\n",
      "Epoch [256/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004880387\n",
      "Epoch [256/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004877561\n",
      "Epoch [256/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0004874736\n",
      "Epoch [256/500], Batch [80/110], Train Loss: 0.0019, Val Loss: 0.0065, LR: 0.0004871910\n",
      "Epoch [256/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004869085\n",
      "Epoch [256/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0004866259\n",
      "Epoch [256/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004863434\n",
      "Epoch [257/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004860608\n",
      "Epoch [257/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004857783\n",
      "Epoch [257/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004854958\n",
      "Epoch [257/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004852132\n",
      "Epoch [257/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004849307\n",
      "Epoch [257/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004846482\n",
      "Epoch [257/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004843657\n",
      "Epoch [257/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004840832\n",
      "Epoch [257/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0004838007\n",
      "Epoch [257/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0004835183\n",
      "Epoch [257/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0004832358\n",
      "Epoch [258/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0004829533\n",
      "Epoch [258/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0004826709\n",
      "Epoch [258/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0004823884\n",
      "Epoch [258/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0052, LR: 0.0004821060\n",
      "Epoch [258/500], Batch [50/110], Train Loss: 0.0057, Val Loss: 0.0053, LR: 0.0004818235\n",
      "Epoch [258/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0004815411\n",
      "Epoch [258/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0004812587\n",
      "Epoch [258/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0004809763\n",
      "Epoch [258/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0004806939\n",
      "Epoch [258/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0004804115\n",
      "Epoch [258/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0004801291\n",
      "Epoch [259/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0054, LR: 0.0004798467\n",
      "Epoch [259/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0004795643\n",
      "Epoch [259/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0055, LR: 0.0004792819\n",
      "Epoch [259/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0004789996\n",
      "Epoch [259/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004787172\n",
      "Epoch [259/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004784349\n",
      "Epoch [259/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0004781526\n",
      "Epoch [259/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0004778702\n",
      "Epoch [259/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0053, LR: 0.0004775879\n",
      "Epoch [259/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0004773056\n",
      "Epoch [259/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0004770233\n",
      "Epoch [260/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0053, LR: 0.0004767410\n",
      "Epoch [260/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0004764588\n",
      "Epoch [260/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0004761765\n",
      "Epoch [260/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0004758942\n",
      "Epoch [260/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0004756120\n",
      "Epoch [260/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0004753297\n",
      "Epoch [260/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0004750475\n",
      "Epoch [260/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0004747653\n",
      "Epoch [260/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0004744831\n",
      "Epoch [260/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0004742009\n",
      "Epoch [260/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0004739187\n",
      "Epoch [261/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0004736365\n",
      "Epoch [261/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004733543\n",
      "Epoch [261/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004730722\n",
      "Epoch [261/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0004727900\n",
      "Epoch [261/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0004725079\n",
      "Epoch [261/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004722258\n",
      "Epoch [261/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0064, LR: 0.0004719436\n",
      "Epoch [261/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0004716615\n",
      "Epoch [261/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0062, LR: 0.0004713794\n",
      "Epoch [261/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004710974\n",
      "Epoch [261/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0064, LR: 0.0004708153\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 261: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.041 sec , Memory Usage: 197.09 MB\n",
      "Epoch [262/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0004705332\n",
      "Epoch [262/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0004702512\n",
      "Epoch [262/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004699691\n",
      "Epoch [262/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0004696871\n",
      "Epoch [262/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0066, LR: 0.0004694051\n",
      "Epoch [262/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0071, LR: 0.0004691231\n",
      "Epoch [262/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004688411\n",
      "Epoch [262/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004685591\n",
      "Epoch [262/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004682771\n",
      "Epoch [262/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0004679952\n",
      "Epoch [262/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0004677132\n",
      "Epoch [263/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0004674313\n",
      "Epoch [263/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0073, LR: 0.0004671494\n",
      "Epoch [263/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0070, LR: 0.0004668675\n",
      "Epoch [263/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004665856\n",
      "Epoch [263/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004663037\n",
      "Epoch [263/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004660218\n",
      "Epoch [263/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004657400\n",
      "Epoch [263/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004654581\n",
      "Epoch [263/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004651763\n",
      "Epoch [263/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004648945\n",
      "Epoch [263/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004646126\n",
      "Epoch [264/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004643309\n",
      "Epoch [264/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004640491\n",
      "Epoch [264/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004637673\n",
      "Epoch [264/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0066, LR: 0.0004634856\n",
      "Epoch [264/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004632038\n",
      "Epoch [264/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004629221\n",
      "Epoch [264/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004626404\n",
      "Epoch [264/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0004623587\n",
      "Epoch [264/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0004620770\n",
      "Epoch [264/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004617953\n",
      "Epoch [264/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0004615137\n",
      "Epoch [265/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0004612320\n",
      "Epoch [265/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0004609504\n",
      "Epoch [265/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0004606688\n",
      "Epoch [265/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0004603872\n",
      "Epoch [265/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0004601056\n",
      "Epoch [265/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0004598240\n",
      "Epoch [265/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0004595425\n",
      "Epoch [265/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0070, LR: 0.0004592609\n",
      "Epoch [265/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0004589794\n",
      "Epoch [265/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0004586979\n",
      "Epoch [265/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0004584164\n",
      "Epoch [266/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0004581349\n",
      "Epoch [266/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0063, LR: 0.0004578534\n",
      "Epoch [266/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004575720\n",
      "Epoch [266/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004572906\n",
      "Epoch [266/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004570091\n",
      "Epoch [266/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004567277\n",
      "Epoch [266/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0062, LR: 0.0004564463\n",
      "Epoch [266/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0004561650\n",
      "Epoch [266/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004558836\n",
      "Epoch [266/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004556023\n",
      "Epoch [266/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004553210\n",
      "Epoch [267/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004550396\n",
      "Epoch [267/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004547584\n",
      "Epoch [267/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004544771\n",
      "Epoch [267/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004541958\n",
      "Epoch [267/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004539146\n",
      "Epoch [267/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0066, LR: 0.0004536334\n",
      "Epoch [267/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004533521\n",
      "Epoch [267/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004530710\n",
      "Epoch [267/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004527898\n",
      "Epoch [267/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004525086\n",
      "Epoch [267/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004522275\n",
      "Epoch [268/500], Batch [10/110], Train Loss: 0.0016, Val Loss: 0.0073, LR: 0.0004519464\n",
      "Epoch [268/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004516652\n",
      "Epoch [268/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004513842\n",
      "Epoch [268/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0004511031\n",
      "Epoch [268/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004508220\n",
      "Epoch [268/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004505410\n",
      "Epoch [268/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004502600\n",
      "Epoch [268/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0004499790\n",
      "Epoch [268/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0062, LR: 0.0004496980\n",
      "Epoch [268/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004494170\n",
      "Epoch [268/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004491361\n",
      "Epoch [269/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004488552\n",
      "Epoch [269/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004485743\n",
      "Epoch [269/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0004482934\n",
      "Epoch [269/500], Batch [40/110], Train Loss: 0.0017, Val Loss: 0.0055, LR: 0.0004480125\n",
      "Epoch [269/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0004477316\n",
      "Epoch [269/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0004474508\n",
      "Epoch [269/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0062, LR: 0.0004471700\n",
      "Epoch [269/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0062, LR: 0.0004468892\n",
      "Epoch [269/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0004466084\n",
      "Epoch [269/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004463276\n",
      "Epoch [269/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004460469\n",
      "Epoch [270/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0004457662\n",
      "Epoch [270/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004454855\n",
      "Epoch [270/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0070, LR: 0.0004452048\n",
      "Epoch [270/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004449241\n",
      "Epoch [270/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004446435\n",
      "Epoch [270/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004443629\n",
      "Epoch [270/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004440823\n",
      "Epoch [270/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004438017\n",
      "Epoch [270/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004435211\n",
      "Epoch [270/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004432406\n",
      "Epoch [270/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004429600\n",
      "Epoch [271/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004426795\n",
      "Epoch [271/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004423991\n",
      "Epoch [271/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004421186\n",
      "Epoch [271/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0004418382\n",
      "Epoch [271/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0004415577\n",
      "Epoch [271/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0004412773\n",
      "Epoch [271/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0004409970\n",
      "Epoch [271/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0004407166\n",
      "Epoch [271/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0004404363\n",
      "Epoch [271/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0004401559\n",
      "Epoch [271/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0004398756\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 271: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.041 sec , Memory Usage: 197.09 MB\n",
      "Epoch [272/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0004395954\n",
      "Epoch [272/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004393151\n",
      "Epoch [272/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0004390349\n",
      "Epoch [272/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0004387547\n",
      "Epoch [272/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0004384745\n",
      "Epoch [272/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0071, LR: 0.0004381943\n",
      "Epoch [272/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004379142\n",
      "Epoch [272/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004376340\n",
      "Epoch [272/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004373539\n",
      "Epoch [272/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0067, LR: 0.0004370739\n",
      "Epoch [272/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004367938\n",
      "Epoch [273/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004365138\n",
      "Epoch [273/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004362338\n",
      "Epoch [273/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004359538\n",
      "Epoch [273/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004356738\n",
      "Epoch [273/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004353939\n",
      "Epoch [273/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004351139\n",
      "Epoch [273/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004348340\n",
      "Epoch [273/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004345542\n",
      "Epoch [273/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004342743\n",
      "Epoch [273/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0004339945\n",
      "Epoch [273/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0004337147\n",
      "Epoch [274/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0004334349\n",
      "Epoch [274/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0004331551\n",
      "Epoch [274/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0060, LR: 0.0004328754\n",
      "Epoch [274/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0004325957\n",
      "Epoch [274/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004323160\n",
      "Epoch [274/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0004320363\n",
      "Epoch [274/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0004317567\n",
      "Epoch [274/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0004314770\n",
      "Epoch [274/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0004311975\n",
      "Epoch [274/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0004309179\n",
      "Epoch [274/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0004306383\n",
      "Epoch [275/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004303588\n",
      "Epoch [275/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004300793\n",
      "Epoch [275/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0061, LR: 0.0004297998\n",
      "Epoch [275/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0004295204\n",
      "Epoch [275/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0004292410\n",
      "Epoch [275/500], Batch [60/110], Train Loss: 0.0167, Val Loss: 0.0060, LR: 0.0004289616\n",
      "Epoch [275/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0004286822\n",
      "Epoch [275/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0004284028\n",
      "Epoch [275/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004281235\n",
      "Epoch [275/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004278442\n",
      "Epoch [275/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004275649\n",
      "Epoch [276/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004272857\n",
      "Epoch [276/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004270065\n",
      "Epoch [276/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0004267273\n",
      "Epoch [276/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0004264481\n",
      "Epoch [276/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0004261689\n",
      "Epoch [276/500], Batch [60/110], Train Loss: 0.0023, Val Loss: 0.0075, LR: 0.0004258898\n",
      "Epoch [276/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0004256107\n",
      "Epoch [276/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0004253317\n",
      "Epoch [276/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004250526\n",
      "Epoch [276/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0004247736\n",
      "Epoch [276/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0004244946\n",
      "Epoch [277/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0075, LR: 0.0004242156\n",
      "Epoch [277/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0004239367\n",
      "Epoch [277/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004236578\n",
      "Epoch [277/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0072, LR: 0.0004233789\n",
      "Epoch [277/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0004231000\n",
      "Epoch [277/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0075, LR: 0.0004228212\n",
      "Epoch [277/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0004225424\n",
      "Epoch [277/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0004222636\n",
      "Epoch [277/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004219849\n",
      "Epoch [277/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0004217061\n",
      "Epoch [277/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0004214274\n",
      "Epoch [278/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0004211488\n",
      "Epoch [278/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004208701\n",
      "Epoch [278/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004205915\n",
      "Epoch [278/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0004203129\n",
      "Epoch [278/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0004200344\n",
      "Epoch [278/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004197558\n",
      "Epoch [278/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004194773\n",
      "Epoch [278/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004191988\n",
      "Epoch [278/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004189204\n",
      "Epoch [278/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0004186420\n",
      "Epoch [278/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004183636\n",
      "Epoch [279/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0067, LR: 0.0004180852\n",
      "Epoch [279/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004178069\n",
      "Epoch [279/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004175286\n",
      "Epoch [279/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0004172503\n",
      "Epoch [279/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004169720\n",
      "Epoch [279/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0004166938\n",
      "Epoch [279/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0058, LR: 0.0004164156\n",
      "Epoch [279/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004161375\n",
      "Epoch [279/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0004158593\n",
      "Epoch [279/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004155812\n",
      "Epoch [279/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004153031\n",
      "Epoch [280/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004150251\n",
      "Epoch [280/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004147471\n",
      "Epoch [280/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004144691\n",
      "Epoch [280/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004141911\n",
      "Epoch [280/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0004139132\n",
      "Epoch [280/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0004136353\n",
      "Epoch [280/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004133574\n",
      "Epoch [280/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004130796\n",
      "Epoch [280/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004128018\n",
      "Epoch [280/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0004125240\n",
      "Epoch [280/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0004122462\n",
      "Epoch [281/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0004119685\n",
      "Epoch [281/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004116908\n",
      "Epoch [281/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0004114132\n",
      "Epoch [281/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0004111356\n",
      "Epoch [281/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0004108580\n",
      "Epoch [281/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004105804\n",
      "Epoch [281/500], Batch [70/110], Train Loss: 0.0207, Val Loss: 0.0074, LR: 0.0004103028\n",
      "Epoch [281/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0004100253\n",
      "Epoch [281/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0073, LR: 0.0004097479\n",
      "Epoch [281/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0004094704\n",
      "Epoch [281/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004091930\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 281: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.040 sec , Memory Usage: 197.11 MB\n",
      "Epoch [282/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004089156\n",
      "Epoch [282/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0004086383\n",
      "Epoch [282/500], Batch [30/110], Train Loss: 0.0747, Val Loss: 0.0085, LR: 0.0004083610\n",
      "Epoch [282/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0004080837\n",
      "Epoch [282/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0004078064\n",
      "Epoch [282/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0004075292\n",
      "Epoch [282/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0004072520\n",
      "Epoch [282/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0004069748\n",
      "Epoch [282/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004066977\n",
      "Epoch [282/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0004064206\n",
      "Epoch [282/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0004061436\n",
      "Epoch [283/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0004058665\n",
      "Epoch [283/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004055895\n",
      "Epoch [283/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004053126\n",
      "Epoch [283/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0004050356\n",
      "Epoch [283/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0004047587\n",
      "Epoch [283/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0004044819\n",
      "Epoch [283/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0004042050\n",
      "Epoch [283/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0004039282\n",
      "Epoch [283/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0004036514\n",
      "Epoch [283/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0004033747\n",
      "Epoch [283/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0004030980\n",
      "Epoch [284/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0004028213\n",
      "Epoch [284/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004025447\n",
      "Epoch [284/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004022681\n",
      "Epoch [284/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0004019915\n",
      "Epoch [284/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004017150\n",
      "Epoch [284/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0004014385\n",
      "Epoch [284/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0004011620\n",
      "Epoch [284/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0004008856\n",
      "Epoch [284/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0004006092\n",
      "Epoch [284/500], Batch [100/110], Train Loss: 0.0430, Val Loss: 0.0072, LR: 0.0004003328\n",
      "Epoch [284/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0004000565\n",
      "Epoch [285/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003997802\n",
      "Epoch [285/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0003995039\n",
      "Epoch [285/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0003992277\n",
      "Epoch [285/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003989515\n",
      "Epoch [285/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003986753\n",
      "Epoch [285/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0003983992\n",
      "Epoch [285/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0003981231\n",
      "Epoch [285/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003978471\n",
      "Epoch [285/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003975710\n",
      "Epoch [285/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003972950\n",
      "Epoch [285/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0003970191\n",
      "Epoch [286/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0003967432\n",
      "Epoch [286/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0003964673\n",
      "Epoch [286/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003961915\n",
      "Epoch [286/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0003959156\n",
      "Epoch [286/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0003956399\n",
      "Epoch [286/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0003953641\n",
      "Epoch [286/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0003950884\n",
      "Epoch [286/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0003948128\n",
      "Epoch [286/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0003945371\n",
      "Epoch [286/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0003942615\n",
      "Epoch [286/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0003939860\n",
      "Epoch [287/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003937105\n",
      "Epoch [287/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0003934350\n",
      "Epoch [287/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0058, LR: 0.0003931595\n",
      "Epoch [287/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003928841\n",
      "Epoch [287/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003926087\n",
      "Epoch [287/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003923334\n",
      "Epoch [287/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0003920581\n",
      "Epoch [287/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0003917828\n",
      "Epoch [287/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003915076\n",
      "Epoch [287/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0003912324\n",
      "Epoch [287/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0003909572\n",
      "Epoch [288/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0003906821\n",
      "Epoch [288/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0003904070\n",
      "Epoch [288/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003901320\n",
      "Epoch [288/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0003898570\n",
      "Epoch [288/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0047, LR: 0.0003895820\n",
      "Epoch [288/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003893071\n",
      "Epoch [288/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003890322\n",
      "Epoch [288/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0003887573\n",
      "Epoch [288/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003884825\n",
      "Epoch [288/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003882077\n",
      "Epoch [288/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003879330\n",
      "Epoch [289/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0003876583\n",
      "Epoch [289/500], Batch [20/110], Train Loss: 0.0018, Val Loss: 0.0060, LR: 0.0003873836\n",
      "Epoch [289/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003871090\n",
      "Epoch [289/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003868344\n",
      "Epoch [289/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0065, LR: 0.0003865599\n",
      "Epoch [289/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0003862854\n",
      "Epoch [289/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0003860109\n",
      "Epoch [289/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003857364\n",
      "Epoch [289/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003854621\n",
      "Epoch [289/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0003851877\n",
      "Epoch [289/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003849134\n",
      "Epoch [290/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0003846391\n",
      "Epoch [290/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003843649\n",
      "Epoch [290/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003840907\n",
      "Epoch [290/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003838165\n",
      "Epoch [290/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003835424\n",
      "Epoch [290/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003832683\n",
      "Epoch [290/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003829943\n",
      "Epoch [290/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003827203\n",
      "Epoch [290/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003824463\n",
      "Epoch [290/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003821724\n",
      "Epoch [290/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003818985\n",
      "Epoch [291/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003816247\n",
      "Epoch [291/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003813509\n",
      "Epoch [291/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003810771\n",
      "Epoch [291/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003808034\n",
      "Epoch [291/500], Batch [50/110], Train Loss: 0.0028, Val Loss: 0.0061, LR: 0.0003805297\n",
      "Epoch [291/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003802561\n",
      "Epoch [291/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0003799825\n",
      "Epoch [291/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0055, LR: 0.0003797089\n",
      "Epoch [291/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0003794354\n",
      "Epoch [291/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003791619\n",
      "Epoch [291/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003788885\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 291: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.036 sec , Memory Usage: 197.11 MB\n",
      "Epoch [292/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003786151\n",
      "Epoch [292/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003783417\n",
      "Epoch [292/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0066, LR: 0.0003780684\n",
      "Epoch [292/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003777952\n",
      "Epoch [292/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003775219\n",
      "Epoch [292/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003772488\n",
      "Epoch [292/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0003769756\n",
      "Epoch [292/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0003767025\n",
      "Epoch [292/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0055, LR: 0.0003764294\n",
      "Epoch [292/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003761564\n",
      "Epoch [292/500], Batch [110/110], Train Loss: 0.0394, Val Loss: 0.0055, LR: 0.0003758835\n",
      "Epoch [293/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0056, LR: 0.0003756105\n",
      "Epoch [293/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003753376\n",
      "Epoch [293/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003750648\n",
      "Epoch [293/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003747920\n",
      "Epoch [293/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003745192\n",
      "Epoch [293/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003742465\n",
      "Epoch [293/500], Batch [70/110], Train Loss: 0.0136, Val Loss: 0.0055, LR: 0.0003739738\n",
      "Epoch [293/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0003737012\n",
      "Epoch [293/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0003734286\n",
      "Epoch [293/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0003731560\n",
      "Epoch [293/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003728835\n",
      "Epoch [294/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003726111\n",
      "Epoch [294/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0003723386\n",
      "Epoch [294/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0003720663\n",
      "Epoch [294/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0003717939\n",
      "Epoch [294/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0071, LR: 0.0003715216\n",
      "Epoch [294/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0003712494\n",
      "Epoch [294/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0003709772\n",
      "Epoch [294/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003707050\n",
      "Epoch [294/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0003704329\n",
      "Epoch [294/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0003701608\n",
      "Epoch [294/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0003698888\n",
      "Epoch [295/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0003696168\n",
      "Epoch [295/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0003693449\n",
      "Epoch [295/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0056, LR: 0.0003690730\n",
      "Epoch [295/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003688011\n",
      "Epoch [295/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003685293\n",
      "Epoch [295/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0003682575\n",
      "Epoch [295/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0054, LR: 0.0003679858\n",
      "Epoch [295/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0003677142\n",
      "Epoch [295/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0003674425\n",
      "Epoch [295/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003671709\n",
      "Epoch [295/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003668994\n",
      "Epoch [296/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003666279\n",
      "Epoch [296/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0003663565\n",
      "Epoch [296/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003660851\n",
      "Epoch [296/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003658137\n",
      "Epoch [296/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003655424\n",
      "Epoch [296/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003652711\n",
      "Epoch [296/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003649999\n",
      "Epoch [296/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003647287\n",
      "Epoch [296/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003644576\n",
      "Epoch [296/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003641865\n",
      "Epoch [296/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0003639155\n",
      "Epoch [297/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0003636445\n",
      "Epoch [297/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0003633735\n",
      "Epoch [297/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0003631026\n",
      "Epoch [297/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0064, LR: 0.0003628318\n",
      "Epoch [297/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0064, LR: 0.0003625610\n",
      "Epoch [297/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0003622902\n",
      "Epoch [297/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0003620195\n",
      "Epoch [297/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003617488\n",
      "Epoch [297/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003614782\n",
      "Epoch [297/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003612076\n",
      "Epoch [297/500], Batch [110/110], Train Loss: 0.0291, Val Loss: 0.0059, LR: 0.0003609371\n",
      "Epoch [298/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003606666\n",
      "Epoch [298/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003603962\n",
      "Epoch [298/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003601258\n",
      "Epoch [298/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003598555\n",
      "Epoch [298/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003595852\n",
      "Epoch [298/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0003593149\n",
      "Epoch [298/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0003590447\n",
      "Epoch [298/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0003587746\n",
      "Epoch [298/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0003585045\n",
      "Epoch [298/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003582344\n",
      "Epoch [298/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003579644\n",
      "Epoch [299/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003576945\n",
      "Epoch [299/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003574246\n",
      "Epoch [299/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003571547\n",
      "Epoch [299/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0003568849\n",
      "Epoch [299/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0003566151\n",
      "Epoch [299/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0003563454\n",
      "Epoch [299/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003560757\n",
      "Epoch [299/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003558061\n",
      "Epoch [299/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003555365\n",
      "Epoch [299/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003552670\n",
      "Epoch [299/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003549975\n",
      "Epoch [300/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003547281\n",
      "Epoch [300/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003544587\n",
      "Epoch [300/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0003541894\n",
      "Epoch [300/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003539201\n",
      "Epoch [300/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003536509\n",
      "Epoch [300/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003533817\n",
      "Epoch [300/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0003531126\n",
      "Epoch [300/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003528435\n",
      "Epoch [300/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0057, LR: 0.0003525745\n",
      "Epoch [300/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003523055\n",
      "Epoch [300/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003520366\n",
      "Epoch [301/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003517677\n",
      "Epoch [301/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003514989\n",
      "Epoch [301/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003512301\n",
      "Epoch [301/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0003509614\n",
      "Epoch [301/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003506927\n",
      "Epoch [301/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003504241\n",
      "Epoch [301/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003501555\n",
      "Epoch [301/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0003498870\n",
      "Epoch [301/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0003496185\n",
      "Epoch [301/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0003493500\n",
      "Epoch [301/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0003490817\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 301: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 197.11 MB\n",
      "Epoch [302/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0003488133\n",
      "Epoch [302/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003485451\n",
      "Epoch [302/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003482769\n",
      "Epoch [302/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0049, LR: 0.0003480087\n",
      "Epoch [302/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003477406\n",
      "Epoch [302/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003474725\n",
      "Epoch [302/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003472045\n",
      "Epoch [302/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003469365\n",
      "Epoch [302/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0053, LR: 0.0003466686\n",
      "Epoch [302/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003464007\n",
      "Epoch [302/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0051, LR: 0.0003461329\n",
      "Epoch [303/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003458652\n",
      "Epoch [303/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003455974\n",
      "Epoch [303/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003453298\n",
      "Epoch [303/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003450622\n",
      "Epoch [303/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003447946\n",
      "Epoch [303/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003445271\n",
      "Epoch [303/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0003442597\n",
      "Epoch [303/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003439923\n",
      "Epoch [303/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0003437250\n",
      "Epoch [303/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003434577\n",
      "Epoch [303/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003431904\n",
      "Epoch [304/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0050, LR: 0.0003429232\n",
      "Epoch [304/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003426561\n",
      "Epoch [304/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003423890\n",
      "Epoch [304/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0003421220\n",
      "Epoch [304/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0003418550\n",
      "Epoch [304/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0052, LR: 0.0003415881\n",
      "Epoch [304/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003413213\n",
      "Epoch [304/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003410544\n",
      "Epoch [304/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003407877\n",
      "Epoch [304/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0003405210\n",
      "Epoch [304/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0003402543\n",
      "Epoch [305/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0003399877\n",
      "Epoch [305/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003397212\n",
      "Epoch [305/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003394547\n",
      "Epoch [305/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003391883\n",
      "Epoch [305/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0003389219\n",
      "Epoch [305/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003386556\n",
      "Epoch [305/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0003383893\n",
      "Epoch [305/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0003381231\n",
      "Epoch [305/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0003378569\n",
      "Epoch [305/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0066, LR: 0.0003375908\n",
      "Epoch [305/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0066, LR: 0.0003373247\n",
      "Epoch [306/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0003370587\n",
      "Epoch [306/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003367928\n",
      "Epoch [306/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003365269\n",
      "Epoch [306/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003362611\n",
      "Epoch [306/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0058, LR: 0.0003359953\n",
      "Epoch [306/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003357296\n",
      "Epoch [306/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003354639\n",
      "Epoch [306/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003351983\n",
      "Epoch [306/500], Batch [90/110], Train Loss: 0.0046, Val Loss: 0.0056, LR: 0.0003349327\n",
      "Epoch [306/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0003346672\n",
      "Epoch [306/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0003344018\n",
      "Epoch [307/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003341364\n",
      "Epoch [307/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0003338710\n",
      "Epoch [307/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0003336057\n",
      "Epoch [307/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0003333405\n",
      "Epoch [307/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003330753\n",
      "Epoch [307/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003328102\n",
      "Epoch [307/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003325452\n",
      "Epoch [307/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0003322802\n",
      "Epoch [307/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0003320152\n",
      "Epoch [307/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0003317503\n",
      "Epoch [307/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0003314855\n",
      "Epoch [308/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0063, LR: 0.0003312207\n",
      "Epoch [308/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0003309560\n",
      "Epoch [308/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0003306914\n",
      "Epoch [308/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003304268\n",
      "Epoch [308/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003301622\n",
      "Epoch [308/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003298977\n",
      "Epoch [308/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003296333\n",
      "Epoch [308/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0003293689\n",
      "Epoch [308/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0003291046\n",
      "Epoch [308/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003288403\n",
      "Epoch [308/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0003285761\n",
      "Epoch [309/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0003283120\n",
      "Epoch [309/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0003280479\n",
      "Epoch [309/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003277839\n",
      "Epoch [309/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003275199\n",
      "Epoch [309/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003272560\n",
      "Epoch [309/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003269921\n",
      "Epoch [309/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003267283\n",
      "Epoch [309/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003264646\n",
      "Epoch [309/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003262009\n",
      "Epoch [309/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003259373\n",
      "Epoch [309/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0003256737\n",
      "Epoch [310/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003254102\n",
      "Epoch [310/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0003251467\n",
      "Epoch [310/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0003248833\n",
      "Epoch [310/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003246200\n",
      "Epoch [310/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0003243567\n",
      "Epoch [310/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003240935\n",
      "Epoch [310/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0003238304\n",
      "Epoch [310/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003235673\n",
      "Epoch [310/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003233042\n",
      "Epoch [310/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0003230413\n",
      "Epoch [310/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0063, LR: 0.0003227783\n",
      "Epoch [311/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0003225155\n",
      "Epoch [311/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0003222527\n",
      "Epoch [311/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003219900\n",
      "Epoch [311/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003217273\n",
      "Epoch [311/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0003214647\n",
      "Epoch [311/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003212021\n",
      "Epoch [311/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003209396\n",
      "Epoch [311/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003206772\n",
      "Epoch [311/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003204148\n",
      "Epoch [311/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003201525\n",
      "Epoch [311/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003198902\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 311: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 197.12 MB\n",
      "Epoch [312/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003196280\n",
      "Epoch [312/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003193659\n",
      "Epoch [312/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003191038\n",
      "Epoch [312/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003188418\n",
      "Epoch [312/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003185798\n",
      "Epoch [312/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0052, LR: 0.0003183179\n",
      "Epoch [312/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003180561\n",
      "Epoch [312/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003177943\n",
      "Epoch [312/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003175326\n",
      "Epoch [312/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003172709\n",
      "Epoch [312/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003170093\n",
      "Epoch [313/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003167478\n",
      "Epoch [313/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003164864\n",
      "Epoch [313/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003162249\n",
      "Epoch [313/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003159636\n",
      "Epoch [313/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003157023\n",
      "Epoch [313/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003154411\n",
      "Epoch [313/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003151799\n",
      "Epoch [313/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003149188\n",
      "Epoch [313/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003146578\n",
      "Epoch [313/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003143968\n",
      "Epoch [313/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003141359\n",
      "Epoch [314/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003138751\n",
      "Epoch [314/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003136143\n",
      "Epoch [314/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003133536\n",
      "Epoch [314/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003130929\n",
      "Epoch [314/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003128323\n",
      "Epoch [314/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003125718\n",
      "Epoch [314/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003123113\n",
      "Epoch [314/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003120509\n",
      "Epoch [314/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003117905\n",
      "Epoch [314/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003115303\n",
      "Epoch [314/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003112700\n",
      "Epoch [315/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003110099\n",
      "Epoch [315/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003107498\n",
      "Epoch [315/500], Batch [30/110], Train Loss: 0.0055, Val Loss: 0.0051, LR: 0.0003104898\n",
      "Epoch [315/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003102298\n",
      "Epoch [315/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003099699\n",
      "Epoch [315/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0003097100\n",
      "Epoch [315/500], Batch [70/110], Train Loss: 0.0075, Val Loss: 0.0046, LR: 0.0003094503\n",
      "Epoch [315/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0003091905\n",
      "Epoch [315/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0003089309\n",
      "Epoch [315/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0003086713\n",
      "Epoch [315/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003084118\n",
      "Epoch [316/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003081523\n",
      "Epoch [316/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0050, LR: 0.0003078929\n",
      "Epoch [316/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003076336\n",
      "Epoch [316/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003073744\n",
      "Epoch [316/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003071152\n",
      "Epoch [316/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003068560\n",
      "Epoch [316/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003065969\n",
      "Epoch [316/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003063379\n",
      "Epoch [316/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0003060790\n",
      "Epoch [316/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0003058201\n",
      "Epoch [316/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0050, LR: 0.0003055613\n",
      "Epoch [317/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0003053026\n",
      "Epoch [317/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003050439\n",
      "Epoch [317/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003047853\n",
      "Epoch [317/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003045267\n",
      "Epoch [317/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003042682\n",
      "Epoch [317/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0003040098\n",
      "Epoch [317/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0003037515\n",
      "Epoch [317/500], Batch [80/110], Train Loss: 0.0008, Val Loss: 0.0056, LR: 0.0003034932\n",
      "Epoch [317/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003032350\n",
      "Epoch [317/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0003029768\n",
      "Epoch [317/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0003027187\n",
      "Epoch [318/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0003024607\n",
      "Epoch [318/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0003022027\n",
      "Epoch [318/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0003019448\n",
      "Epoch [318/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0003016870\n",
      "Epoch [318/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0003014292\n",
      "Epoch [318/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0003011716\n",
      "Epoch [318/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0003009139\n",
      "Epoch [318/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003006564\n",
      "Epoch [318/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003003989\n",
      "Epoch [318/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0003001414\n",
      "Epoch [318/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0002998841\n",
      "Epoch [319/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0002996268\n",
      "Epoch [319/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0002993696\n",
      "Epoch [319/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0002991124\n",
      "Epoch [319/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0002988553\n",
      "Epoch [319/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0002985983\n",
      "Epoch [319/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0002983413\n",
      "Epoch [319/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002980844\n",
      "Epoch [319/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002978276\n",
      "Epoch [319/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002975709\n",
      "Epoch [319/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002973142\n",
      "Epoch [319/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002970576\n",
      "Epoch [320/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002968010\n",
      "Epoch [320/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0002965445\n",
      "Epoch [320/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002962881\n",
      "Epoch [320/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0050, LR: 0.0002960318\n",
      "Epoch [320/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002957755\n",
      "Epoch [320/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002955193\n",
      "Epoch [320/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002952631\n",
      "Epoch [320/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002950071\n",
      "Epoch [320/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002947511\n",
      "Epoch [320/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002944951\n",
      "Epoch [320/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002942393\n",
      "Epoch [321/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002939835\n",
      "Epoch [321/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002937277\n",
      "Epoch [321/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0002934721\n",
      "Epoch [321/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0002932165\n",
      "Epoch [321/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002929609\n",
      "Epoch [321/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0002927055\n",
      "Epoch [321/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0002924501\n",
      "Epoch [321/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0002921948\n",
      "Epoch [321/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0002919395\n",
      "Epoch [321/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0002916844\n",
      "Epoch [321/500], Batch [110/110], Train Loss: 0.0100, Val Loss: 0.0044, LR: 0.0002914293\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 321: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 197.12 MB\n",
      "Epoch [322/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0002911742\n",
      "Epoch [322/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0002909193\n",
      "Epoch [322/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002906644\n",
      "Epoch [322/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002904095\n",
      "Epoch [322/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002901548\n",
      "Epoch [322/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002899001\n",
      "Epoch [322/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0002896455\n",
      "Epoch [322/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002893909\n",
      "Epoch [322/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002891364\n",
      "Epoch [322/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0002888820\n",
      "Epoch [322/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0050, LR: 0.0002886277\n",
      "Epoch [323/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002883734\n",
      "Epoch [323/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0002881192\n",
      "Epoch [323/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002878651\n",
      "Epoch [323/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002876111\n",
      "Epoch [323/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002873571\n",
      "Epoch [323/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0057, LR: 0.0002871032\n",
      "Epoch [323/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0002868493\n",
      "Epoch [323/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002865956\n",
      "Epoch [323/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002863419\n",
      "Epoch [323/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002860882\n",
      "Epoch [323/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002858347\n",
      "Epoch [324/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002855812\n",
      "Epoch [324/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002853278\n",
      "Epoch [324/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0002850744\n",
      "Epoch [324/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002848212\n",
      "Epoch [324/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002845680\n",
      "Epoch [324/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002843149\n",
      "Epoch [324/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0002840618\n",
      "Epoch [324/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002838088\n",
      "Epoch [324/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0002835559\n",
      "Epoch [324/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0002833031\n",
      "Epoch [324/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002830503\n",
      "Epoch [325/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002827976\n",
      "Epoch [325/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002825450\n",
      "Epoch [325/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002822925\n",
      "Epoch [325/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002820400\n",
      "Epoch [325/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002817876\n",
      "Epoch [325/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002815352\n",
      "Epoch [325/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002812830\n",
      "Epoch [325/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0002810308\n",
      "Epoch [325/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002807787\n",
      "Epoch [325/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0064, LR: 0.0002805267\n",
      "Epoch [325/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0002802747\n",
      "Epoch [326/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0002800228\n",
      "Epoch [326/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0002797710\n",
      "Epoch [326/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0002795193\n",
      "Epoch [326/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0002792676\n",
      "Epoch [326/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0002790160\n",
      "Epoch [326/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002787645\n",
      "Epoch [326/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002785130\n",
      "Epoch [326/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002782616\n",
      "Epoch [326/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002780103\n",
      "Epoch [326/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0002777591\n",
      "Epoch [326/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002775080\n",
      "Epoch [327/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002772569\n",
      "Epoch [327/500], Batch [20/110], Train Loss: 0.0327, Val Loss: 0.0069, LR: 0.0002770059\n",
      "Epoch [327/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0002767550\n",
      "Epoch [327/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0002765041\n",
      "Epoch [327/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0002762533\n",
      "Epoch [327/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0002760026\n",
      "Epoch [327/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0002757520\n",
      "Epoch [327/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0002755014\n",
      "Epoch [327/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002752510\n",
      "Epoch [327/500], Batch [100/110], Train Loss: 0.0112, Val Loss: 0.0056, LR: 0.0002750005\n",
      "Epoch [327/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002747502\n",
      "Epoch [328/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002745000\n",
      "Epoch [328/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002742498\n",
      "Epoch [328/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002739997\n",
      "Epoch [328/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002737496\n",
      "Epoch [328/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002734997\n",
      "Epoch [328/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0002732498\n",
      "Epoch [328/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0002730000\n",
      "Epoch [328/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002727503\n",
      "Epoch [328/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002725006\n",
      "Epoch [328/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002722511\n",
      "Epoch [328/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002720016\n",
      "Epoch [329/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002717521\n",
      "Epoch [329/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002715028\n",
      "Epoch [329/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002712535\n",
      "Epoch [329/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0002710043\n",
      "Epoch [329/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0002707552\n",
      "Epoch [329/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0002705062\n",
      "Epoch [329/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0002702572\n",
      "Epoch [329/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002700083\n",
      "Epoch [329/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002697595\n",
      "Epoch [329/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002695107\n",
      "Epoch [329/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002692621\n",
      "Epoch [330/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002690135\n",
      "Epoch [330/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0002687650\n",
      "Epoch [330/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0002685166\n",
      "Epoch [330/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0002682682\n",
      "Epoch [330/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0002680199\n",
      "Epoch [330/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002677718\n",
      "Epoch [330/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002675236\n",
      "Epoch [330/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0002672756\n",
      "Epoch [330/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002670276\n",
      "Epoch [330/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002667797\n",
      "Epoch [330/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002665319\n",
      "Epoch [331/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002662842\n",
      "Epoch [331/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002660365\n",
      "Epoch [331/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002657890\n",
      "Epoch [331/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002655415\n",
      "Epoch [331/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002652941\n",
      "Epoch [331/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002650467\n",
      "Epoch [331/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002647995\n",
      "Epoch [331/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002645523\n",
      "Epoch [331/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002643052\n",
      "Epoch [331/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002640581\n",
      "Epoch [331/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002638112\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 331: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 197.14 MB\n",
      "Epoch [332/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002635643\n",
      "Epoch [332/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002633175\n",
      "Epoch [332/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002630708\n",
      "Epoch [332/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002628242\n",
      "Epoch [332/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002625776\n",
      "Epoch [332/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002623312\n",
      "Epoch [332/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002620848\n",
      "Epoch [332/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002618384\n",
      "Epoch [332/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002615922\n",
      "Epoch [332/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002613460\n",
      "Epoch [332/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002611000\n",
      "Epoch [333/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002608540\n",
      "Epoch [333/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002606080\n",
      "Epoch [333/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002603622\n",
      "Epoch [333/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0002601164\n",
      "Epoch [333/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0002598708\n",
      "Epoch [333/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0002596252\n",
      "Epoch [333/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0065, LR: 0.0002593796\n",
      "Epoch [333/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0068, LR: 0.0002591342\n",
      "Epoch [333/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0002588889\n",
      "Epoch [333/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0002586436\n",
      "Epoch [333/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0002583984\n",
      "Epoch [334/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002581533\n",
      "Epoch [334/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002579082\n",
      "Epoch [334/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002576633\n",
      "Epoch [334/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002574184\n",
      "Epoch [334/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002571736\n",
      "Epoch [334/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002569289\n",
      "Epoch [334/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002566842\n",
      "Epoch [334/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002564397\n",
      "Epoch [334/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002561952\n",
      "Epoch [334/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002559508\n",
      "Epoch [334/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002557065\n",
      "Epoch [335/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002554623\n",
      "Epoch [335/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0002552181\n",
      "Epoch [335/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002549741\n",
      "Epoch [335/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002547301\n",
      "Epoch [335/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002544862\n",
      "Epoch [335/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002542424\n",
      "Epoch [335/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002539986\n",
      "Epoch [335/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002537550\n",
      "Epoch [335/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002535114\n",
      "Epoch [335/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002532679\n",
      "Epoch [335/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0002530245\n",
      "Epoch [336/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0002527812\n",
      "Epoch [336/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002525379\n",
      "Epoch [336/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0002522948\n",
      "Epoch [336/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0002520517\n",
      "Epoch [336/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0002518087\n",
      "Epoch [336/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0002515658\n",
      "Epoch [336/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0002513229\n",
      "Epoch [336/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0002510802\n",
      "Epoch [336/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0002508375\n",
      "Epoch [336/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0002505949\n",
      "Epoch [336/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0002503524\n",
      "Epoch [337/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0064, LR: 0.0002501100\n",
      "Epoch [337/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002498677\n",
      "Epoch [337/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002496254\n",
      "Epoch [337/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002493833\n",
      "Epoch [337/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002491412\n",
      "Epoch [337/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002488992\n",
      "Epoch [337/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002486573\n",
      "Epoch [337/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002484154\n",
      "Epoch [337/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002481737\n",
      "Epoch [337/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002479320\n",
      "Epoch [337/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002476904\n",
      "Epoch [338/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002474489\n",
      "Epoch [338/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002472075\n",
      "Epoch [338/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0002469662\n",
      "Epoch [338/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0002467249\n",
      "Epoch [338/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0002464838\n",
      "Epoch [338/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0002462427\n",
      "Epoch [338/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0002460017\n",
      "Epoch [338/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0002457608\n",
      "Epoch [338/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0002455200\n",
      "Epoch [338/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0002452792\n",
      "Epoch [338/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0002450386\n",
      "Epoch [339/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0067, LR: 0.0002447980\n",
      "Epoch [339/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0002445575\n",
      "Epoch [339/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0002443171\n",
      "Epoch [339/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0002440768\n",
      "Epoch [339/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0002438366\n",
      "Epoch [339/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0002435964\n",
      "Epoch [339/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0002433563\n",
      "Epoch [339/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0002431164\n",
      "Epoch [339/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002428765\n",
      "Epoch [339/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002426367\n",
      "Epoch [339/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002423970\n",
      "Epoch [340/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002421573\n",
      "Epoch [340/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002419178\n",
      "Epoch [340/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002416783\n",
      "Epoch [340/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002414389\n",
      "Epoch [340/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002411997\n",
      "Epoch [340/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002409605\n",
      "Epoch [340/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002407213\n",
      "Epoch [340/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002404823\n",
      "Epoch [340/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002402434\n",
      "Epoch [340/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0002400045\n",
      "Epoch [340/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002397657\n",
      "Epoch [341/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002395271\n",
      "Epoch [341/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002392885\n",
      "Epoch [341/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0002390499\n",
      "Epoch [341/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002388115\n",
      "Epoch [341/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002385732\n",
      "Epoch [341/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002383349\n",
      "Epoch [341/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002380968\n",
      "Epoch [341/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002378587\n",
      "Epoch [341/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002376207\n",
      "Epoch [341/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002373828\n",
      "Epoch [341/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002371450\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 341: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 197.20 MB\n",
      "Epoch [342/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002369073\n",
      "Epoch [342/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002366696\n",
      "Epoch [342/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002364321\n",
      "Epoch [342/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002361946\n",
      "Epoch [342/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002359572\n",
      "Epoch [342/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002357199\n",
      "Epoch [342/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002354827\n",
      "Epoch [342/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002352456\n",
      "Epoch [342/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0002350086\n",
      "Epoch [342/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0002347716\n",
      "Epoch [342/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0002345348\n",
      "Epoch [343/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0002342980\n",
      "Epoch [343/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0002340614\n",
      "Epoch [343/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0002338248\n",
      "Epoch [343/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002335883\n",
      "Epoch [343/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002333519\n",
      "Epoch [343/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002331156\n",
      "Epoch [343/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002328793\n",
      "Epoch [343/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002326432\n",
      "Epoch [343/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002324071\n",
      "Epoch [343/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002321712\n",
      "Epoch [343/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002319353\n",
      "Epoch [344/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002316995\n",
      "Epoch [344/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002314638\n",
      "Epoch [344/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002312282\n",
      "Epoch [344/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002309927\n",
      "Epoch [344/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0054, LR: 0.0002307573\n",
      "Epoch [344/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002305219\n",
      "Epoch [344/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002302867\n",
      "Epoch [344/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0002300515\n",
      "Epoch [344/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0002298164\n",
      "Epoch [344/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0002295815\n",
      "Epoch [344/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0002293466\n",
      "Epoch [345/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0002291118\n",
      "Epoch [345/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0002288771\n",
      "Epoch [345/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0002286424\n",
      "Epoch [345/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002284079\n",
      "Epoch [345/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002281735\n",
      "Epoch [345/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002279391\n",
      "Epoch [345/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002277049\n",
      "Epoch [345/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0002274707\n",
      "Epoch [345/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0002272366\n",
      "Epoch [345/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0002270026\n",
      "Epoch [345/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0039, LR: 0.0002267687\n",
      "Epoch [346/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0002265349\n",
      "Epoch [346/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0042, LR: 0.0002263012\n",
      "Epoch [346/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0002260676\n",
      "Epoch [346/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0002258340\n",
      "Epoch [346/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0002256006\n",
      "Epoch [346/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0002253673\n",
      "Epoch [346/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0002251340\n",
      "Epoch [346/500], Batch [80/110], Train Loss: 0.0118, Val Loss: 0.0040, LR: 0.0002249008\n",
      "Epoch [346/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0002246678\n",
      "Epoch [346/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0002244348\n",
      "Epoch [346/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0002242019\n",
      "Epoch [347/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0002239691\n",
      "Epoch [347/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0038, LR: 0.0002237364\n",
      "Epoch [347/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0038, LR: 0.0002235037\n",
      "Epoch [347/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0038, LR: 0.0002232712\n",
      "Epoch [347/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0038, LR: 0.0002230388\n",
      "Epoch [347/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0038, LR: 0.0002228064\n",
      "Epoch [347/500], Batch [70/110], Train Loss: 0.0168, Val Loss: 0.0042, LR: 0.0002225742\n",
      "Epoch [347/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0002223420\n",
      "Epoch [347/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0002221100\n",
      "Epoch [347/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0002218780\n",
      "Epoch [347/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0002216461\n",
      "Epoch [348/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0002214143\n",
      "Epoch [348/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0002211826\n",
      "Epoch [348/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0002209510\n",
      "Epoch [348/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0002207195\n",
      "Epoch [348/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0002204881\n",
      "Epoch [348/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0002202567\n",
      "Epoch [348/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0002200255\n",
      "Epoch [348/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0002197944\n",
      "Epoch [348/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0002195633\n",
      "Epoch [348/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0002193324\n",
      "Epoch [348/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0002191015\n",
      "Epoch [349/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0002188707\n",
      "Epoch [349/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0002186401\n",
      "Epoch [349/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0002184095\n",
      "Epoch [349/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0044, LR: 0.0002181790\n",
      "Epoch [349/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0002179486\n",
      "Epoch [349/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0002177183\n",
      "Epoch [349/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0002174881\n",
      "Epoch [349/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002172580\n",
      "Epoch [349/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002170280\n",
      "Epoch [349/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002167980\n",
      "Epoch [349/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0002165682\n",
      "Epoch [350/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002163385\n",
      "Epoch [350/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002161088\n",
      "Epoch [350/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002158793\n",
      "Epoch [350/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002156498\n",
      "Epoch [350/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0047, LR: 0.0002154205\n",
      "Epoch [350/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002151912\n",
      "Epoch [350/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002149620\n",
      "Epoch [350/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002147330\n",
      "Epoch [350/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0002145040\n",
      "Epoch [350/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002142751\n",
      "Epoch [350/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002140463\n",
      "Epoch [351/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002138176\n",
      "Epoch [351/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0002135890\n",
      "Epoch [351/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002133605\n",
      "Epoch [351/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002131321\n",
      "Epoch [351/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002129038\n",
      "Epoch [351/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002126755\n",
      "Epoch [351/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002124474\n",
      "Epoch [351/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002122194\n",
      "Epoch [351/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0002119915\n",
      "Epoch [351/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002117636\n",
      "Epoch [351/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0002115359\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 351: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.038 sec , Memory Usage: 197.21 MB\n",
      "Epoch [352/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0002113082\n",
      "Epoch [352/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002110807\n",
      "Epoch [352/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002108532\n",
      "Epoch [352/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0002106259\n",
      "Epoch [352/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0002103986\n",
      "Epoch [352/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0002101714\n",
      "Epoch [352/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0002099444\n",
      "Epoch [352/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002097174\n",
      "Epoch [352/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002094905\n",
      "Epoch [352/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002092637\n",
      "Epoch [352/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0002090370\n",
      "Epoch [353/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0002088104\n",
      "Epoch [353/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002085840\n",
      "Epoch [353/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0002083576\n",
      "Epoch [353/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002081313\n",
      "Epoch [353/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0002079051\n",
      "Epoch [353/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002076789\n",
      "Epoch [353/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0002074529\n",
      "Epoch [353/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002072270\n",
      "Epoch [353/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0002070012\n",
      "Epoch [353/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0002067755\n",
      "Epoch [353/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0051, LR: 0.0002065499\n",
      "Epoch [354/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0002063244\n",
      "Epoch [354/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002060989\n",
      "Epoch [354/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002058736\n",
      "Epoch [354/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002056484\n",
      "Epoch [354/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0002054232\n",
      "Epoch [354/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0002051982\n",
      "Epoch [354/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0002049733\n",
      "Epoch [354/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0002047484\n",
      "Epoch [354/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002045237\n",
      "Epoch [354/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002042990\n",
      "Epoch [354/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002040745\n",
      "Epoch [355/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002038501\n",
      "Epoch [355/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002036257\n",
      "Epoch [355/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002034015\n",
      "Epoch [355/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0002031773\n",
      "Epoch [355/500], Batch [50/110], Train Loss: 0.0035, Val Loss: 0.0051, LR: 0.0002029533\n",
      "Epoch [355/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002027293\n",
      "Epoch [355/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002025054\n",
      "Epoch [355/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002022817\n",
      "Epoch [355/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0002020580\n",
      "Epoch [355/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0002018345\n",
      "Epoch [355/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0002016110\n",
      "Epoch [356/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0002013876\n",
      "Epoch [356/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0002011644\n",
      "Epoch [356/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0002009412\n",
      "Epoch [356/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002007182\n",
      "Epoch [356/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002004952\n",
      "Epoch [356/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002002723\n",
      "Epoch [356/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0002000496\n",
      "Epoch [356/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001998269\n",
      "Epoch [356/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001996043\n",
      "Epoch [356/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001993819\n",
      "Epoch [356/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001991595\n",
      "Epoch [357/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0001989372\n",
      "Epoch [357/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001987151\n",
      "Epoch [357/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001984930\n",
      "Epoch [357/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001982710\n",
      "Epoch [357/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001980492\n",
      "Epoch [357/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001978274\n",
      "Epoch [357/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001976057\n",
      "Epoch [357/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001973841\n",
      "Epoch [357/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0001971627\n",
      "Epoch [357/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001969413\n",
      "Epoch [357/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001967200\n",
      "Epoch [358/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001964989\n",
      "Epoch [358/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001962778\n",
      "Epoch [358/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001960569\n",
      "Epoch [358/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001958360\n",
      "Epoch [358/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001956152\n",
      "Epoch [358/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001953946\n",
      "Epoch [358/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001951740\n",
      "Epoch [358/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001949535\n",
      "Epoch [358/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001947332\n",
      "Epoch [358/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001945129\n",
      "Epoch [358/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001942928\n",
      "Epoch [359/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001940727\n",
      "Epoch [359/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001938528\n",
      "Epoch [359/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001936329\n",
      "Epoch [359/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001934132\n",
      "Epoch [359/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0001931935\n",
      "Epoch [359/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001929740\n",
      "Epoch [359/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0001927545\n",
      "Epoch [359/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0001925352\n",
      "Epoch [359/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0001923159\n",
      "Epoch [359/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0001920968\n",
      "Epoch [359/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0001918778\n",
      "Epoch [360/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0048, LR: 0.0001916588\n",
      "Epoch [360/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0001914400\n",
      "Epoch [360/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0001912213\n",
      "Epoch [360/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001910026\n",
      "Epoch [360/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001907841\n",
      "Epoch [360/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001905657\n",
      "Epoch [360/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0051, LR: 0.0001903474\n",
      "Epoch [360/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0051, LR: 0.0001901292\n",
      "Epoch [360/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001899110\n",
      "Epoch [360/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001896930\n",
      "Epoch [360/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001894751\n",
      "Epoch [361/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001892573\n",
      "Epoch [361/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001890396\n",
      "Epoch [361/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001888220\n",
      "Epoch [361/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001886045\n",
      "Epoch [361/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001883871\n",
      "Epoch [361/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0051, LR: 0.0001881698\n",
      "Epoch [361/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001879526\n",
      "Epoch [361/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001877356\n",
      "Epoch [361/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001875186\n",
      "Epoch [361/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001873017\n",
      "Epoch [361/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001870849\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 361: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 197.21 MB\n",
      "Epoch [362/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0001868683\n",
      "Epoch [362/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001866517\n",
      "Epoch [362/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001864352\n",
      "Epoch [362/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0055, LR: 0.0001862189\n",
      "Epoch [362/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001860026\n",
      "Epoch [362/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001857865\n",
      "Epoch [362/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001855704\n",
      "Epoch [362/500], Batch [80/110], Train Loss: 0.0021, Val Loss: 0.0060, LR: 0.0001853545\n",
      "Epoch [362/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001851387\n",
      "Epoch [362/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001849229\n",
      "Epoch [362/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001847073\n",
      "Epoch [363/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001844918\n",
      "Epoch [363/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001842764\n",
      "Epoch [363/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0001840610\n",
      "Epoch [363/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001838458\n",
      "Epoch [363/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001836307\n",
      "Epoch [363/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0001834157\n",
      "Epoch [363/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001832008\n",
      "Epoch [363/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001829860\n",
      "Epoch [363/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001827714\n",
      "Epoch [363/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001825568\n",
      "Epoch [363/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001823423\n",
      "Epoch [364/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001821279\n",
      "Epoch [364/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001819137\n",
      "Epoch [364/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001816995\n",
      "Epoch [364/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001814855\n",
      "Epoch [364/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001812715\n",
      "Epoch [364/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0051, LR: 0.0001810577\n",
      "Epoch [364/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001808439\n",
      "Epoch [364/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001806303\n",
      "Epoch [364/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001804168\n",
      "Epoch [364/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001802034\n",
      "Epoch [364/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001799901\n",
      "Epoch [365/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001797768\n",
      "Epoch [365/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001795637\n",
      "Epoch [365/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001793507\n",
      "Epoch [365/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001791379\n",
      "Epoch [365/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001789251\n",
      "Epoch [365/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001787124\n",
      "Epoch [365/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001784998\n",
      "Epoch [365/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001782874\n",
      "Epoch [365/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001780750\n",
      "Epoch [365/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0001778628\n",
      "Epoch [365/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001776506\n",
      "Epoch [366/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001774386\n",
      "Epoch [366/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001772267\n",
      "Epoch [366/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001770148\n",
      "Epoch [366/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001768031\n",
      "Epoch [366/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001765915\n",
      "Epoch [366/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001763800\n",
      "Epoch [366/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0001761686\n",
      "Epoch [366/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001759573\n",
      "Epoch [366/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0001757462\n",
      "Epoch [366/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0001755351\n",
      "Epoch [366/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001753241\n",
      "Epoch [367/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001751133\n",
      "Epoch [367/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001749025\n",
      "Epoch [367/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001746919\n",
      "Epoch [367/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001744813\n",
      "Epoch [367/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001742709\n",
      "Epoch [367/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0001740606\n",
      "Epoch [367/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001738504\n",
      "Epoch [367/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001736403\n",
      "Epoch [367/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001734303\n",
      "Epoch [367/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001732204\n",
      "Epoch [367/500], Batch [110/110], Train Loss: 0.0041, Val Loss: 0.0056, LR: 0.0001730106\n",
      "Epoch [368/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001728010\n",
      "Epoch [368/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001725914\n",
      "Epoch [368/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001723820\n",
      "Epoch [368/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001721726\n",
      "Epoch [368/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001719634\n",
      "Epoch [368/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001717543\n",
      "Epoch [368/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001715452\n",
      "Epoch [368/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001713363\n",
      "Epoch [368/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001711275\n",
      "Epoch [368/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001709188\n",
      "Epoch [368/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001707103\n",
      "Epoch [369/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001705018\n",
      "Epoch [369/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0056, LR: 0.0001702934\n",
      "Epoch [369/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001700852\n",
      "Epoch [369/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001698770\n",
      "Epoch [369/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001696690\n",
      "Epoch [369/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001694611\n",
      "Epoch [369/500], Batch [70/110], Train Loss: 0.0078, Val Loss: 0.0053, LR: 0.0001692532\n",
      "Epoch [369/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001690455\n",
      "Epoch [369/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001688379\n",
      "Epoch [369/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001686305\n",
      "Epoch [369/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0049, LR: 0.0001684231\n",
      "Epoch [370/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0001682158\n",
      "Epoch [370/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0001680087\n",
      "Epoch [370/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0001678016\n",
      "Epoch [370/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0001675947\n",
      "Epoch [370/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0001673878\n",
      "Epoch [370/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0001671811\n",
      "Epoch [370/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0001669745\n",
      "Epoch [370/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0001667680\n",
      "Epoch [370/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0001665616\n",
      "Epoch [370/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0001663553\n",
      "Epoch [370/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0001661492\n",
      "Epoch [371/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0001659431\n",
      "Epoch [371/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0001657372\n",
      "Epoch [371/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0001655313\n",
      "Epoch [371/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0001653256\n",
      "Epoch [371/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0001651200\n",
      "Epoch [371/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0001649145\n",
      "Epoch [371/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0001647091\n",
      "Epoch [371/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001645038\n",
      "Epoch [371/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001642987\n",
      "Epoch [371/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001640936\n",
      "Epoch [371/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001638887\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 371: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 184.79 MB\n",
      "Epoch [372/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001636838\n",
      "Epoch [372/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001634791\n",
      "Epoch [372/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0001632745\n",
      "Epoch [372/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001630700\n",
      "Epoch [372/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001628656\n",
      "Epoch [372/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001626613\n",
      "Epoch [372/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001624572\n",
      "Epoch [372/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001622531\n",
      "Epoch [372/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001620492\n",
      "Epoch [372/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001618453\n",
      "Epoch [372/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0056, LR: 0.0001616416\n",
      "Epoch [373/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001614380\n",
      "Epoch [373/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001612345\n",
      "Epoch [373/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001610311\n",
      "Epoch [373/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0056, LR: 0.0001608279\n",
      "Epoch [373/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001606247\n",
      "Epoch [373/500], Batch [60/110], Train Loss: 0.0016, Val Loss: 0.0060, LR: 0.0001604217\n",
      "Epoch [373/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0001602187\n",
      "Epoch [373/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0001600159\n",
      "Epoch [373/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0001598132\n",
      "Epoch [373/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0001596106\n",
      "Epoch [373/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001594081\n",
      "Epoch [374/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001592057\n",
      "Epoch [374/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001590035\n",
      "Epoch [374/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001588013\n",
      "Epoch [374/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001585993\n",
      "Epoch [374/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001583974\n",
      "Epoch [374/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001581956\n",
      "Epoch [374/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001579939\n",
      "Epoch [374/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001577923\n",
      "Epoch [374/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001575909\n",
      "Epoch [374/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001573895\n",
      "Epoch [374/500], Batch [110/110], Train Loss: 0.0026, Val Loss: 0.0053, LR: 0.0001571883\n",
      "Epoch [375/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001569871\n",
      "Epoch [375/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0001567861\n",
      "Epoch [375/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001565852\n",
      "Epoch [375/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001563844\n",
      "Epoch [375/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001561838\n",
      "Epoch [375/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001559832\n",
      "Epoch [375/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001557828\n",
      "Epoch [375/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001555824\n",
      "Epoch [375/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001553822\n",
      "Epoch [375/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001551821\n",
      "Epoch [375/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001549821\n",
      "Epoch [376/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001547823\n",
      "Epoch [376/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001545825\n",
      "Epoch [376/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001543829\n",
      "Epoch [376/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001541833\n",
      "Epoch [376/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0001539839\n",
      "Epoch [376/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001537846\n",
      "Epoch [376/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001535854\n",
      "Epoch [376/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001533864\n",
      "Epoch [376/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001531874\n",
      "Epoch [376/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001529886\n",
      "Epoch [376/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001527898\n",
      "Epoch [377/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001525912\n",
      "Epoch [377/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001523927\n",
      "Epoch [377/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001521943\n",
      "Epoch [377/500], Batch [40/110], Train Loss: 0.0241, Val Loss: 0.0053, LR: 0.0001519961\n",
      "Epoch [377/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001517979\n",
      "Epoch [377/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001515999\n",
      "Epoch [377/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001514020\n",
      "Epoch [377/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0001512042\n",
      "Epoch [377/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001510065\n",
      "Epoch [377/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001508089\n",
      "Epoch [377/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001506114\n",
      "Epoch [378/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001504141\n",
      "Epoch [378/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0001502169\n",
      "Epoch [378/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0001500198\n",
      "Epoch [378/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0001498228\n",
      "Epoch [378/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0001496259\n",
      "Epoch [378/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001494291\n",
      "Epoch [378/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001492325\n",
      "Epoch [378/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001490359\n",
      "Epoch [378/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001488395\n",
      "Epoch [378/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001486432\n",
      "Epoch [378/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0001484470\n",
      "Epoch [379/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0001482510\n",
      "Epoch [379/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0001480550\n",
      "Epoch [379/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0001478592\n",
      "Epoch [379/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001476635\n",
      "Epoch [379/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001474679\n",
      "Epoch [379/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0001472724\n",
      "Epoch [379/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001470770\n",
      "Epoch [379/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001468818\n",
      "Epoch [379/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001466866\n",
      "Epoch [379/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001464916\n",
      "Epoch [379/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001462967\n",
      "Epoch [380/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001461019\n",
      "Epoch [380/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001459073\n",
      "Epoch [380/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001457127\n",
      "Epoch [380/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001455183\n",
      "Epoch [380/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001453240\n",
      "Epoch [380/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001451298\n",
      "Epoch [380/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001449357\n",
      "Epoch [380/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001447417\n",
      "Epoch [380/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0001445479\n",
      "Epoch [380/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0001443541\n",
      "Epoch [380/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001441605\n",
      "Epoch [381/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001439670\n",
      "Epoch [381/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001437737\n",
      "Epoch [381/500], Batch [30/110], Train Loss: 0.0047, Val Loss: 0.0051, LR: 0.0001435804\n",
      "Epoch [381/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001433873\n",
      "Epoch [381/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001431942\n",
      "Epoch [381/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001430013\n",
      "Epoch [381/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001428086\n",
      "Epoch [381/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001426159\n",
      "Epoch [381/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001424233\n",
      "Epoch [381/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0001422309\n",
      "Epoch [381/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0001420386\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 381: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 184.80 MB\n",
      "Epoch [382/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001418464\n",
      "Epoch [382/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001416543\n",
      "Epoch [382/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001414624\n",
      "Epoch [382/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001412705\n",
      "Epoch [382/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0001410788\n",
      "Epoch [382/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001408872\n",
      "Epoch [382/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0059, LR: 0.0001406957\n",
      "Epoch [382/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001405044\n",
      "Epoch [382/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001403131\n",
      "Epoch [382/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001401220\n",
      "Epoch [382/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001399310\n",
      "Epoch [383/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001397401\n",
      "Epoch [383/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001395494\n",
      "Epoch [383/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001393587\n",
      "Epoch [383/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001391682\n",
      "Epoch [383/500], Batch [50/110], Train Loss: 0.0015, Val Loss: 0.0057, LR: 0.0001389778\n",
      "Epoch [383/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001387875\n",
      "Epoch [383/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001385973\n",
      "Epoch [383/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001384073\n",
      "Epoch [383/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0001382173\n",
      "Epoch [383/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001380275\n",
      "Epoch [383/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0001378378\n",
      "Epoch [384/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001376483\n",
      "Epoch [384/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001374588\n",
      "Epoch [384/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001372695\n",
      "Epoch [384/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001370803\n",
      "Epoch [384/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001368912\n",
      "Epoch [384/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001367022\n",
      "Epoch [384/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001365133\n",
      "Epoch [384/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001363246\n",
      "Epoch [384/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001361360\n",
      "Epoch [384/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001359475\n",
      "Epoch [384/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0001357591\n",
      "Epoch [385/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001355709\n",
      "Epoch [385/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001353828\n",
      "Epoch [385/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001351947\n",
      "Epoch [385/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001350069\n",
      "Epoch [385/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001348191\n",
      "Epoch [385/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001346314\n",
      "Epoch [385/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001344439\n",
      "Epoch [385/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001342565\n",
      "Epoch [385/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0001340692\n",
      "Epoch [385/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001338821\n",
      "Epoch [385/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001336950\n",
      "Epoch [386/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001335081\n",
      "Epoch [386/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001333213\n",
      "Epoch [386/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001331346\n",
      "Epoch [386/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001329481\n",
      "Epoch [386/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001327616\n",
      "Epoch [386/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001325753\n",
      "Epoch [386/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001323891\n",
      "Epoch [386/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0001322031\n",
      "Epoch [386/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001320171\n",
      "Epoch [386/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001318313\n",
      "Epoch [386/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001316456\n",
      "Epoch [387/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001314600\n",
      "Epoch [387/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001312745\n",
      "Epoch [387/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001310892\n",
      "Epoch [387/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001309040\n",
      "Epoch [387/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001307189\n",
      "Epoch [387/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001305339\n",
      "Epoch [387/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001303490\n",
      "Epoch [387/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001301643\n",
      "Epoch [387/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001299797\n",
      "Epoch [387/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001297952\n",
      "Epoch [387/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001296109\n",
      "Epoch [388/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001294266\n",
      "Epoch [388/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001292425\n",
      "Epoch [388/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001290585\n",
      "Epoch [388/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001288746\n",
      "Epoch [388/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001286909\n",
      "Epoch [388/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001285072\n",
      "Epoch [388/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001283237\n",
      "Epoch [388/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001281404\n",
      "Epoch [388/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001279571\n",
      "Epoch [388/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0056, LR: 0.0001277740\n",
      "Epoch [388/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0056, LR: 0.0001275910\n",
      "Epoch [389/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001274081\n",
      "Epoch [389/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001272253\n",
      "Epoch [389/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001270427\n",
      "Epoch [389/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001268601\n",
      "Epoch [389/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001266777\n",
      "Epoch [389/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001264955\n",
      "Epoch [389/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001263133\n",
      "Epoch [389/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001261313\n",
      "Epoch [389/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001259494\n",
      "Epoch [389/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0001257676\n",
      "Epoch [389/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001255860\n",
      "Epoch [390/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001254044\n",
      "Epoch [390/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001252230\n",
      "Epoch [390/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001250417\n",
      "Epoch [390/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001248606\n",
      "Epoch [390/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001246795\n",
      "Epoch [390/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001244986\n",
      "Epoch [390/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0001243178\n",
      "Epoch [390/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001241372\n",
      "Epoch [390/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001239566\n",
      "Epoch [390/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0001237762\n",
      "Epoch [390/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001235959\n",
      "Epoch [391/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001234158\n",
      "Epoch [391/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001232357\n",
      "Epoch [391/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001230558\n",
      "Epoch [391/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001228760\n",
      "Epoch [391/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001226964\n",
      "Epoch [391/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0001225168\n",
      "Epoch [391/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0001223374\n",
      "Epoch [391/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0001221581\n",
      "Epoch [391/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0001219789\n",
      "Epoch [391/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001217999\n",
      "Epoch [391/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001216210\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 391: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.038 sec , Memory Usage: 184.81 MB\n",
      "Epoch [392/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001214422\n",
      "Epoch [392/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001212635\n",
      "Epoch [392/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001210850\n",
      "Epoch [392/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001209066\n",
      "Epoch [392/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0001207283\n",
      "Epoch [392/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0001205501\n",
      "Epoch [392/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0001203721\n",
      "Epoch [392/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001201942\n",
      "Epoch [392/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0001200164\n",
      "Epoch [392/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001198387\n",
      "Epoch [392/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001196612\n",
      "Epoch [393/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001194837\n",
      "Epoch [393/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001193065\n",
      "Epoch [393/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001191293\n",
      "Epoch [393/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001189523\n",
      "Epoch [393/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001187754\n",
      "Epoch [393/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001185986\n",
      "Epoch [393/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001184219\n",
      "Epoch [393/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001182454\n",
      "Epoch [393/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001180690\n",
      "Epoch [393/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001178927\n",
      "Epoch [393/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001177165\n",
      "Epoch [394/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001175405\n",
      "Epoch [394/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001173646\n",
      "Epoch [394/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001171889\n",
      "Epoch [394/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001170132\n",
      "Epoch [394/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001168377\n",
      "Epoch [394/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001166623\n",
      "Epoch [394/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001164870\n",
      "Epoch [394/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001163119\n",
      "Epoch [394/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0001161369\n",
      "Epoch [394/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001159620\n",
      "Epoch [394/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0001157872\n",
      "Epoch [395/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0068, LR: 0.0001156126\n",
      "Epoch [395/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0001154381\n",
      "Epoch [395/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001152637\n",
      "Epoch [395/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001150895\n",
      "Epoch [395/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0065, LR: 0.0001149153\n",
      "Epoch [395/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0065, LR: 0.0001147413\n",
      "Epoch [395/500], Batch [70/110], Train Loss: 0.0036, Val Loss: 0.0065, LR: 0.0001145675\n",
      "Epoch [395/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001143937\n",
      "Epoch [395/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001142201\n",
      "Epoch [395/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001140466\n",
      "Epoch [395/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001138733\n",
      "Epoch [396/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001137000\n",
      "Epoch [396/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001135269\n",
      "Epoch [396/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0001133540\n",
      "Epoch [396/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0001131811\n",
      "Epoch [396/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0001130084\n",
      "Epoch [396/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0001128358\n",
      "Epoch [396/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0001126633\n",
      "Epoch [396/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0001124910\n",
      "Epoch [396/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0001123188\n",
      "Epoch [396/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0001121467\n",
      "Epoch [396/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0001119748\n",
      "Epoch [397/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0069, LR: 0.0001118029\n",
      "Epoch [397/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0001116312\n",
      "Epoch [397/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0001114597\n",
      "Epoch [397/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0001112882\n",
      "Epoch [397/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0001111169\n",
      "Epoch [397/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0001109457\n",
      "Epoch [397/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0001107747\n",
      "Epoch [397/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0001106038\n",
      "Epoch [397/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0001104330\n",
      "Epoch [397/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0001102623\n",
      "Epoch [397/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001100918\n",
      "Epoch [398/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001099213\n",
      "Epoch [398/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001097511\n",
      "Epoch [398/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001095809\n",
      "Epoch [398/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001094109\n",
      "Epoch [398/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001092410\n",
      "Epoch [398/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001090712\n",
      "Epoch [398/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001089016\n",
      "Epoch [398/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001087321\n",
      "Epoch [398/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001085627\n",
      "Epoch [398/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001083935\n",
      "Epoch [398/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001082243\n",
      "Epoch [399/500], Batch [10/110], Train Loss: 0.0029, Val Loss: 0.0064, LR: 0.0001080554\n",
      "Epoch [399/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001078865\n",
      "Epoch [399/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001077178\n",
      "Epoch [399/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001075492\n",
      "Epoch [399/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001073807\n",
      "Epoch [399/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001072124\n",
      "Epoch [399/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001070441\n",
      "Epoch [399/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001068761\n",
      "Epoch [399/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001067081\n",
      "Epoch [399/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001065403\n",
      "Epoch [399/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001063726\n",
      "Epoch [400/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001062050\n",
      "Epoch [400/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001060376\n",
      "Epoch [400/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001058703\n",
      "Epoch [400/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001057031\n",
      "Epoch [400/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001055361\n",
      "Epoch [400/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001053692\n",
      "Epoch [400/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0001052024\n",
      "Epoch [400/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001050358\n",
      "Epoch [400/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001048692\n",
      "Epoch [400/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001047028\n",
      "Epoch [400/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001045366\n",
      "Epoch [401/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001043705\n",
      "Epoch [401/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001042045\n",
      "Epoch [401/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001040386\n",
      "Epoch [401/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001038729\n",
      "Epoch [401/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001037073\n",
      "Epoch [401/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001035418\n",
      "Epoch [401/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001033764\n",
      "Epoch [401/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001032112\n",
      "Epoch [401/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001030462\n",
      "Epoch [401/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0001028812\n",
      "Epoch [401/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001027164\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 401: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.046 sec , Memory Usage: 184.82 MB\n",
      "Epoch [402/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001025517\n",
      "Epoch [402/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001023871\n",
      "Epoch [402/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001022227\n",
      "Epoch [402/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001020584\n",
      "Epoch [402/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0001018943\n",
      "Epoch [402/500], Batch [60/110], Train Loss: 0.0037, Val Loss: 0.0063, LR: 0.0001017302\n",
      "Epoch [402/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001015663\n",
      "Epoch [402/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001014026\n",
      "Epoch [402/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001012389\n",
      "Epoch [402/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001010754\n",
      "Epoch [402/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001009121\n",
      "Epoch [403/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001007488\n",
      "Epoch [403/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001005857\n",
      "Epoch [403/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001004227\n",
      "Epoch [403/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001002599\n",
      "Epoch [403/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0001000972\n",
      "Epoch [403/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0000999346\n",
      "Epoch [403/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0000997722\n",
      "Epoch [403/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0000996099\n",
      "Epoch [403/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0000994477\n",
      "Epoch [403/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0000992856\n",
      "Epoch [403/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0000991237\n",
      "Epoch [404/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0000989619\n",
      "Epoch [404/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0000988003\n",
      "Epoch [404/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0000986387\n",
      "Epoch [404/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0000984774\n",
      "Epoch [404/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0000983161\n",
      "Epoch [404/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0000981550\n",
      "Epoch [404/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0000979940\n",
      "Epoch [404/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0000978331\n",
      "Epoch [404/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0000976724\n",
      "Epoch [404/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0000975118\n",
      "Epoch [404/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0000973514\n",
      "Epoch [405/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0000971910\n",
      "Epoch [405/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0000970308\n",
      "Epoch [405/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0000968708\n",
      "Epoch [405/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0000967109\n",
      "Epoch [405/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0067, LR: 0.0000965511\n",
      "Epoch [405/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0000963914\n",
      "Epoch [405/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0000962319\n",
      "Epoch [405/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000960725\n",
      "Epoch [405/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000959132\n",
      "Epoch [405/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000957541\n",
      "Epoch [405/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000955951\n",
      "Epoch [406/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000954363\n",
      "Epoch [406/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000952775\n",
      "Epoch [406/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000951189\n",
      "Epoch [406/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000949605\n",
      "Epoch [406/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000948022\n",
      "Epoch [406/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000946440\n",
      "Epoch [406/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000944859\n",
      "Epoch [406/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000943280\n",
      "Epoch [406/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000941702\n",
      "Epoch [406/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0000940125\n",
      "Epoch [406/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0000938550\n",
      "Epoch [407/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0000936976\n",
      "Epoch [407/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0000935404\n",
      "Epoch [407/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0000933833\n",
      "Epoch [407/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0000932263\n",
      "Epoch [407/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0070, LR: 0.0000930694\n",
      "Epoch [407/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0000929127\n",
      "Epoch [407/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0000927561\n",
      "Epoch [407/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000925997\n",
      "Epoch [407/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0000924434\n",
      "Epoch [407/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000922872\n",
      "Epoch [407/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000921312\n",
      "Epoch [408/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000919753\n",
      "Epoch [408/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000918195\n",
      "Epoch [408/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000916639\n",
      "Epoch [408/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000915084\n",
      "Epoch [408/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000913530\n",
      "Epoch [408/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000911978\n",
      "Epoch [408/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000910427\n",
      "Epoch [408/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000908877\n",
      "Epoch [408/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000907329\n",
      "Epoch [408/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000905782\n",
      "Epoch [408/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000904236\n",
      "Epoch [409/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000902692\n",
      "Epoch [409/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000901149\n",
      "Epoch [409/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000899608\n",
      "Epoch [409/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000898067\n",
      "Epoch [409/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000896529\n",
      "Epoch [409/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000894991\n",
      "Epoch [409/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000893455\n",
      "Epoch [409/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000891920\n",
      "Epoch [409/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000890387\n",
      "Epoch [409/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000888855\n",
      "Epoch [409/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000887324\n",
      "Epoch [410/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000885795\n",
      "Epoch [410/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000884267\n",
      "Epoch [410/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000882740\n",
      "Epoch [410/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000881215\n",
      "Epoch [410/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000879691\n",
      "Epoch [410/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000878169\n",
      "Epoch [410/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000876648\n",
      "Epoch [410/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000875128\n",
      "Epoch [410/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000873610\n",
      "Epoch [410/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000872092\n",
      "Epoch [410/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000870577\n",
      "Epoch [411/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000869062\n",
      "Epoch [411/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000867549\n",
      "Epoch [411/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000866038\n",
      "Epoch [411/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000864528\n",
      "Epoch [411/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000863019\n",
      "Epoch [411/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000861511\n",
      "Epoch [411/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000860005\n",
      "Epoch [411/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000858500\n",
      "Epoch [411/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000856997\n",
      "Epoch [411/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000855495\n",
      "Epoch [411/500], Batch [110/110], Train Loss: 0.0023, Val Loss: 0.0059, LR: 0.0000853994\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 411: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 184.82 MB\n",
      "Epoch [412/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000852495\n",
      "Epoch [412/500], Batch [20/110], Train Loss: 0.0031, Val Loss: 0.0059, LR: 0.0000850997\n",
      "Epoch [412/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000849500\n",
      "Epoch [412/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000848005\n",
      "Epoch [412/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0060, LR: 0.0000846511\n",
      "Epoch [412/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000845019\n",
      "Epoch [412/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0000843528\n",
      "Epoch [412/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0000842038\n",
      "Epoch [412/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0000840550\n",
      "Epoch [412/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0000839063\n",
      "Epoch [412/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0000837577\n",
      "Epoch [413/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0000836093\n",
      "Epoch [413/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000834610\n",
      "Epoch [413/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000833129\n",
      "Epoch [413/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000831649\n",
      "Epoch [413/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0000830170\n",
      "Epoch [413/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000828693\n",
      "Epoch [413/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000827217\n",
      "Epoch [413/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000825742\n",
      "Epoch [413/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000824269\n",
      "Epoch [413/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000822797\n",
      "Epoch [413/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000821327\n",
      "Epoch [414/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000819858\n",
      "Epoch [414/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000818390\n",
      "Epoch [414/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000816924\n",
      "Epoch [414/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000815459\n",
      "Epoch [414/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000813995\n",
      "Epoch [414/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000812533\n",
      "Epoch [414/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000811072\n",
      "Epoch [414/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000809613\n",
      "Epoch [414/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000808155\n",
      "Epoch [414/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000806698\n",
      "Epoch [414/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000805243\n",
      "Epoch [415/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000803789\n",
      "Epoch [415/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000802337\n",
      "Epoch [415/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000800886\n",
      "Epoch [415/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000799436\n",
      "Epoch [415/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000797988\n",
      "Epoch [415/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000796541\n",
      "Epoch [415/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000795095\n",
      "Epoch [415/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0000793651\n",
      "Epoch [415/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0000792208\n",
      "Epoch [415/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0000790767\n",
      "Epoch [415/500], Batch [110/110], Train Loss: 0.0109, Val Loss: 0.0055, LR: 0.0000789327\n",
      "Epoch [416/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0000787888\n",
      "Epoch [416/500], Batch [20/110], Train Loss: 0.0031, Val Loss: 0.0055, LR: 0.0000786451\n",
      "Epoch [416/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0000785015\n",
      "Epoch [416/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0000783581\n",
      "Epoch [416/500], Batch [50/110], Train Loss: 0.0021, Val Loss: 0.0054, LR: 0.0000782148\n",
      "Epoch [416/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0000780716\n",
      "Epoch [416/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0000779286\n",
      "Epoch [416/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0000777857\n",
      "Epoch [416/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000776430\n",
      "Epoch [416/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000775004\n",
      "Epoch [416/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000773579\n",
      "Epoch [417/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000772156\n",
      "Epoch [417/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000770734\n",
      "Epoch [417/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000769313\n",
      "Epoch [417/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000767894\n",
      "Epoch [417/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000766477\n",
      "Epoch [417/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000765060\n",
      "Epoch [417/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000763646\n",
      "Epoch [417/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000762232\n",
      "Epoch [417/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000760820\n",
      "Epoch [417/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000759409\n",
      "Epoch [417/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000758000\n",
      "Epoch [418/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000756592\n",
      "Epoch [418/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000755186\n",
      "Epoch [418/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000753781\n",
      "Epoch [418/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0000752377\n",
      "Epoch [418/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0000750975\n",
      "Epoch [418/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0000749574\n",
      "Epoch [418/500], Batch [70/110], Train Loss: 0.0037, Val Loss: 0.0056, LR: 0.0000748174\n",
      "Epoch [418/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0000746776\n",
      "Epoch [418/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0000745380\n",
      "Epoch [418/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000743984\n",
      "Epoch [418/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000742590\n",
      "Epoch [419/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000741198\n",
      "Epoch [419/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000739807\n",
      "Epoch [419/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000738417\n",
      "Epoch [419/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000737029\n",
      "Epoch [419/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000735642\n",
      "Epoch [419/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000734257\n",
      "Epoch [419/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000732873\n",
      "Epoch [419/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000731490\n",
      "Epoch [419/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000730109\n",
      "Epoch [419/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000728729\n",
      "Epoch [419/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000727351\n",
      "Epoch [420/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000725974\n",
      "Epoch [420/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000724598\n",
      "Epoch [420/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000723224\n",
      "Epoch [420/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000721851\n",
      "Epoch [420/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000720480\n",
      "Epoch [420/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000719110\n",
      "Epoch [420/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000717742\n",
      "Epoch [420/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0000716375\n",
      "Epoch [420/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000715009\n",
      "Epoch [420/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000713645\n",
      "Epoch [420/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000712282\n",
      "Epoch [421/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000710921\n",
      "Epoch [421/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000709561\n",
      "Epoch [421/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000708202\n",
      "Epoch [421/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000706845\n",
      "Epoch [421/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000705489\n",
      "Epoch [421/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000704135\n",
      "Epoch [421/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000702782\n",
      "Epoch [421/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000701430\n",
      "Epoch [421/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000700080\n",
      "Epoch [421/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000698732\n",
      "Epoch [421/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000697384\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 421: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 184.84 MB\n",
      "Epoch [422/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000696038\n",
      "Epoch [422/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000694694\n",
      "Epoch [422/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000693351\n",
      "Epoch [422/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000692010\n",
      "Epoch [422/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000690669\n",
      "Epoch [422/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000689331\n",
      "Epoch [422/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000687993\n",
      "Epoch [422/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000686657\n",
      "Epoch [422/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000685323\n",
      "Epoch [422/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000683990\n",
      "Epoch [422/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000682658\n",
      "Epoch [423/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000681328\n",
      "Epoch [423/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000680000\n",
      "Epoch [423/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000678672\n",
      "Epoch [423/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000677346\n",
      "Epoch [423/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000676022\n",
      "Epoch [423/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000674699\n",
      "Epoch [423/500], Batch [70/110], Train Loss: 0.0050, Val Loss: 0.0058, LR: 0.0000673377\n",
      "Epoch [423/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0058, LR: 0.0000672057\n",
      "Epoch [423/500], Batch [90/110], Train Loss: 0.0058, Val Loss: 0.0058, LR: 0.0000670738\n",
      "Epoch [423/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000669421\n",
      "Epoch [423/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000668105\n",
      "Epoch [424/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000666791\n",
      "Epoch [424/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000665478\n",
      "Epoch [424/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000664166\n",
      "Epoch [424/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000662856\n",
      "Epoch [424/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000661547\n",
      "Epoch [424/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0058, LR: 0.0000660240\n",
      "Epoch [424/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000658934\n",
      "Epoch [424/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000657629\n",
      "Epoch [424/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000656326\n",
      "Epoch [424/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000655025\n",
      "Epoch [424/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000653725\n",
      "Epoch [425/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000652426\n",
      "Epoch [425/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000651129\n",
      "Epoch [425/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000649833\n",
      "Epoch [425/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000648538\n",
      "Epoch [425/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000647245\n",
      "Epoch [425/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0059, LR: 0.0000645954\n",
      "Epoch [425/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000644664\n",
      "Epoch [425/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000643375\n",
      "Epoch [425/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000642088\n",
      "Epoch [425/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000640802\n",
      "Epoch [425/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000639518\n",
      "Epoch [426/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000638235\n",
      "Epoch [426/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000636953\n",
      "Epoch [426/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000635673\n",
      "Epoch [426/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0057, LR: 0.0000634395\n",
      "Epoch [426/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000633118\n",
      "Epoch [426/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000631842\n",
      "Epoch [426/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000630568\n",
      "Epoch [426/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000629295\n",
      "Epoch [426/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000628023\n",
      "Epoch [426/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000626753\n",
      "Epoch [426/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000625485\n",
      "Epoch [427/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000624218\n",
      "Epoch [427/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000622952\n",
      "Epoch [427/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000621688\n",
      "Epoch [427/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000620425\n",
      "Epoch [427/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000619164\n",
      "Epoch [427/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000617904\n",
      "Epoch [427/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000616646\n",
      "Epoch [427/500], Batch [80/110], Train Loss: 0.0060, Val Loss: 0.0060, LR: 0.0000615389\n",
      "Epoch [427/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000614134\n",
      "Epoch [427/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000612879\n",
      "Epoch [427/500], Batch [110/110], Train Loss: 0.0043, Val Loss: 0.0061, LR: 0.0000611627\n",
      "Epoch [428/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000610376\n",
      "Epoch [428/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000609126\n",
      "Epoch [428/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000607878\n",
      "Epoch [428/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000606631\n",
      "Epoch [428/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000605386\n",
      "Epoch [428/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000604142\n",
      "Epoch [428/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000602899\n",
      "Epoch [428/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000601658\n",
      "Epoch [428/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000600419\n",
      "Epoch [428/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000599181\n",
      "Epoch [428/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000597944\n",
      "Epoch [429/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000596709\n",
      "Epoch [429/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000595475\n",
      "Epoch [429/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000594243\n",
      "Epoch [429/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000593012\n",
      "Epoch [429/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000591783\n",
      "Epoch [429/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000590555\n",
      "Epoch [429/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000589328\n",
      "Epoch [429/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000588103\n",
      "Epoch [429/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000586880\n",
      "Epoch [429/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000585658\n",
      "Epoch [429/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000584437\n",
      "Epoch [430/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000583218\n",
      "Epoch [430/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000582000\n",
      "Epoch [430/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000580784\n",
      "Epoch [430/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000579569\n",
      "Epoch [430/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0000578356\n",
      "Epoch [430/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0000577144\n",
      "Epoch [430/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0000575933\n",
      "Epoch [430/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000574724\n",
      "Epoch [430/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000573517\n",
      "Epoch [430/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000572311\n",
      "Epoch [430/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0000571106\n",
      "Epoch [431/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000569903\n",
      "Epoch [431/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000568701\n",
      "Epoch [431/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000567501\n",
      "Epoch [431/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000566302\n",
      "Epoch [431/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000565105\n",
      "Epoch [431/500], Batch [60/110], Train Loss: 0.0019, Val Loss: 0.0059, LR: 0.0000563909\n",
      "Epoch [431/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000562715\n",
      "Epoch [431/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000561522\n",
      "Epoch [431/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000560331\n",
      "Epoch [431/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000559141\n",
      "Epoch [431/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000557952\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 431: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 181.95 MB\n",
      "Epoch [432/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000556765\n",
      "Epoch [432/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000555580\n",
      "Epoch [432/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0000554395\n",
      "Epoch [432/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0062, LR: 0.0000553213\n",
      "Epoch [432/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0000552032\n",
      "Epoch [432/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0000550852\n",
      "Epoch [432/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000549674\n",
      "Epoch [432/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000548497\n",
      "Epoch [432/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000547322\n",
      "Epoch [432/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0000546148\n",
      "Epoch [432/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0000544975\n",
      "Epoch [433/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0000543805\n",
      "Epoch [433/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0000542635\n",
      "Epoch [433/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0000541467\n",
      "Epoch [433/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000540301\n",
      "Epoch [433/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000539136\n",
      "Epoch [433/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000537972\n",
      "Epoch [433/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000536810\n",
      "Epoch [433/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000535650\n",
      "Epoch [433/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000534490\n",
      "Epoch [433/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000533333\n",
      "Epoch [433/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0000532177\n",
      "Epoch [434/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000531022\n",
      "Epoch [434/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000529869\n",
      "Epoch [434/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000528717\n",
      "Epoch [434/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000527567\n",
      "Epoch [434/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000526418\n",
      "Epoch [434/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000525271\n",
      "Epoch [434/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000524125\n",
      "Epoch [434/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000522980\n",
      "Epoch [434/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000521838\n",
      "Epoch [434/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000520696\n",
      "Epoch [434/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0000519556\n",
      "Epoch [435/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000518418\n",
      "Epoch [435/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000517281\n",
      "Epoch [435/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000516145\n",
      "Epoch [435/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000515011\n",
      "Epoch [435/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000513879\n",
      "Epoch [435/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000512748\n",
      "Epoch [435/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000511618\n",
      "Epoch [435/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000510490\n",
      "Epoch [435/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000509363\n",
      "Epoch [435/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000508238\n",
      "Epoch [435/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000507115\n",
      "Epoch [436/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000505992\n",
      "Epoch [436/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000504872\n",
      "Epoch [436/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000503753\n",
      "Epoch [436/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000502635\n",
      "Epoch [436/500], Batch [50/110], Train Loss: 0.0116, Val Loss: 0.0058, LR: 0.0000501519\n",
      "Epoch [436/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000500404\n",
      "Epoch [436/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000499291\n",
      "Epoch [436/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000498179\n",
      "Epoch [436/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000497068\n",
      "Epoch [436/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0000495960\n",
      "Epoch [436/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000494852\n",
      "Epoch [437/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000493747\n",
      "Epoch [437/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000492642\n",
      "Epoch [437/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000491539\n",
      "Epoch [437/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000490438\n",
      "Epoch [437/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0000489338\n",
      "Epoch [437/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0000488240\n",
      "Epoch [437/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0000487143\n",
      "Epoch [437/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0000486047\n",
      "Epoch [437/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0000484953\n",
      "Epoch [437/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0000483861\n",
      "Epoch [437/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0000482770\n",
      "Epoch [438/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0000481680\n",
      "Epoch [438/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0000480592\n",
      "Epoch [438/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0000479506\n",
      "Epoch [438/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0000478421\n",
      "Epoch [438/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0000477338\n",
      "Epoch [438/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0000476255\n",
      "Epoch [438/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0000475175\n",
      "Epoch [438/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0000474096\n",
      "Epoch [438/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0000473018\n",
      "Epoch [438/500], Batch [100/110], Train Loss: 0.0064, Val Loss: 0.0053, LR: 0.0000471942\n",
      "Epoch [438/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0000470868\n",
      "Epoch [439/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0000469795\n",
      "Epoch [439/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0000468723\n",
      "Epoch [439/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000467653\n",
      "Epoch [439/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000466585\n",
      "Epoch [439/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000465518\n",
      "Epoch [439/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000464452\n",
      "Epoch [439/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000463388\n",
      "Epoch [439/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000462325\n",
      "Epoch [439/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000461264\n",
      "Epoch [439/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000460205\n",
      "Epoch [439/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000459146\n",
      "Epoch [440/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000458090\n",
      "Epoch [440/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000457035\n",
      "Epoch [440/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000455981\n",
      "Epoch [440/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000454929\n",
      "Epoch [440/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000453878\n",
      "Epoch [440/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000452829\n",
      "Epoch [440/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000451782\n",
      "Epoch [440/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000450736\n",
      "Epoch [440/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000449691\n",
      "Epoch [440/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000448648\n",
      "Epoch [440/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000447606\n",
      "Epoch [441/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000446566\n",
      "Epoch [441/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000445528\n",
      "Epoch [441/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000444491\n",
      "Epoch [441/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000443455\n",
      "Epoch [441/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000442421\n",
      "Epoch [441/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000441388\n",
      "Epoch [441/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000440357\n",
      "Epoch [441/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000439328\n",
      "Epoch [441/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0000438300\n",
      "Epoch [441/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000437273\n",
      "Epoch [441/500], Batch [110/110], Train Loss: 0.0243, Val Loss: 0.0059, LR: 0.0000436248\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 441: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 181.97 MB\n",
      "Epoch [442/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000435224\n",
      "Epoch [442/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000434202\n",
      "Epoch [442/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000433182\n",
      "Epoch [442/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000432163\n",
      "Epoch [442/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000431145\n",
      "Epoch [442/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000430129\n",
      "Epoch [442/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000429115\n",
      "Epoch [442/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000428102\n",
      "Epoch [442/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000427090\n",
      "Epoch [442/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000426080\n",
      "Epoch [442/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000425072\n",
      "Epoch [443/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000424065\n",
      "Epoch [443/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000423059\n",
      "Epoch [443/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000422055\n",
      "Epoch [443/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000421053\n",
      "Epoch [443/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000420052\n",
      "Epoch [443/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000419053\n",
      "Epoch [443/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000418055\n",
      "Epoch [443/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000417058\n",
      "Epoch [443/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000416063\n",
      "Epoch [443/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0059, LR: 0.0000415070\n",
      "Epoch [443/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000414078\n",
      "Epoch [444/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000413088\n",
      "Epoch [444/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000412099\n",
      "Epoch [444/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000411112\n",
      "Epoch [444/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000410126\n",
      "Epoch [444/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000409141\n",
      "Epoch [444/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000408159\n",
      "Epoch [444/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000407177\n",
      "Epoch [444/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0000406198\n",
      "Epoch [444/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000405219\n",
      "Epoch [444/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0058, LR: 0.0000404243\n",
      "Epoch [444/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000403267\n",
      "Epoch [445/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000402294\n",
      "Epoch [445/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000401322\n",
      "Epoch [445/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000400351\n",
      "Epoch [445/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000399382\n",
      "Epoch [445/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000398414\n",
      "Epoch [445/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000397448\n",
      "Epoch [445/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000396483\n",
      "Epoch [445/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000395520\n",
      "Epoch [445/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000394559\n",
      "Epoch [445/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000393599\n",
      "Epoch [445/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000392640\n",
      "Epoch [446/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000391683\n",
      "Epoch [446/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000390728\n",
      "Epoch [446/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000389774\n",
      "Epoch [446/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000388821\n",
      "Epoch [446/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000387870\n",
      "Epoch [446/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000386921\n",
      "Epoch [446/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000385973\n",
      "Epoch [446/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000385027\n",
      "Epoch [446/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000384082\n",
      "Epoch [446/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000383139\n",
      "Epoch [446/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000382197\n",
      "Epoch [447/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000381257\n",
      "Epoch [447/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000380318\n",
      "Epoch [447/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000379381\n",
      "Epoch [447/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000378445\n",
      "Epoch [447/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000377511\n",
      "Epoch [447/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000376578\n",
      "Epoch [447/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000375647\n",
      "Epoch [447/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000374717\n",
      "Epoch [447/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000373789\n",
      "Epoch [447/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000372863\n",
      "Epoch [447/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000371938\n",
      "Epoch [448/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000371014\n",
      "Epoch [448/500], Batch [20/110], Train Loss: 0.0157, Val Loss: 0.0061, LR: 0.0000370092\n",
      "Epoch [448/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000369172\n",
      "Epoch [448/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000368253\n",
      "Epoch [448/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000367336\n",
      "Epoch [448/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000366420\n",
      "Epoch [448/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000365505\n",
      "Epoch [448/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000364593\n",
      "Epoch [448/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000363681\n",
      "Epoch [448/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000362772\n",
      "Epoch [448/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000361863\n",
      "Epoch [449/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000360957\n",
      "Epoch [449/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000360052\n",
      "Epoch [449/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000359148\n",
      "Epoch [449/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000358246\n",
      "Epoch [449/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0058, LR: 0.0000357345\n",
      "Epoch [449/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000356446\n",
      "Epoch [449/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000355549\n",
      "Epoch [449/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000354653\n",
      "Epoch [449/500], Batch [90/110], Train Loss: 0.0122, Val Loss: 0.0058, LR: 0.0000353758\n",
      "Epoch [449/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000352865\n",
      "Epoch [449/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000351974\n",
      "Epoch [450/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000351084\n",
      "Epoch [450/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000350196\n",
      "Epoch [450/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000349309\n",
      "Epoch [450/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0000348424\n",
      "Epoch [450/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0000347540\n",
      "Epoch [450/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0000346658\n",
      "Epoch [450/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000345777\n",
      "Epoch [450/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000344898\n",
      "Epoch [450/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000344021\n",
      "Epoch [450/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000343145\n",
      "Epoch [450/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000342270\n",
      "Epoch [451/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000341397\n",
      "Epoch [451/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000340526\n",
      "Epoch [451/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000339656\n",
      "Epoch [451/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000338788\n",
      "Epoch [451/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0000337921\n",
      "Epoch [451/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000337056\n",
      "Epoch [451/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000336192\n",
      "Epoch [451/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000335330\n",
      "Epoch [451/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000334469\n",
      "Epoch [451/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000333610\n",
      "Epoch [451/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000332752\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 451: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.039 sec , Memory Usage: 181.97 MB\n",
      "Epoch [452/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000331896\n",
      "Epoch [452/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0059, LR: 0.0000331042\n",
      "Epoch [452/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0059, LR: 0.0000330189\n",
      "Epoch [452/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000329337\n",
      "Epoch [452/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000328487\n",
      "Epoch [452/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000327639\n",
      "Epoch [452/500], Batch [70/110], Train Loss: 0.0035, Val Loss: 0.0059, LR: 0.0000326792\n",
      "Epoch [452/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000325947\n",
      "Epoch [452/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000325103\n",
      "Epoch [452/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000324261\n",
      "Epoch [452/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000323421\n",
      "Epoch [453/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000322581\n",
      "Epoch [453/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0000321744\n",
      "Epoch [453/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000320908\n",
      "Epoch [453/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0000320073\n",
      "Epoch [453/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000319240\n",
      "Epoch [453/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000318409\n",
      "Epoch [453/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000317579\n",
      "Epoch [453/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000316751\n",
      "Epoch [453/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000315924\n",
      "Epoch [453/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000315099\n",
      "Epoch [453/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000314275\n",
      "Epoch [454/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000313453\n",
      "Epoch [454/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000312633\n",
      "Epoch [454/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000311814\n",
      "Epoch [454/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000310996\n",
      "Epoch [454/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000310180\n",
      "Epoch [454/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000309366\n",
      "Epoch [454/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000308553\n",
      "Epoch [454/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000307742\n",
      "Epoch [454/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000306932\n",
      "Epoch [454/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000306124\n",
      "Epoch [454/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000305317\n",
      "Epoch [455/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000304512\n",
      "Epoch [455/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000303708\n",
      "Epoch [455/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000302906\n",
      "Epoch [455/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000302106\n",
      "Epoch [455/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000301307\n",
      "Epoch [455/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000300510\n",
      "Epoch [455/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000299714\n",
      "Epoch [455/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000298920\n",
      "Epoch [455/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000298127\n",
      "Epoch [455/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000297336\n",
      "Epoch [455/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000296546\n",
      "Epoch [456/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000295758\n",
      "Epoch [456/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000294972\n",
      "Epoch [456/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000294187\n",
      "Epoch [456/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000293403\n",
      "Epoch [456/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000292622\n",
      "Epoch [456/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000291841\n",
      "Epoch [456/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000291062\n",
      "Epoch [456/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000290285\n",
      "Epoch [456/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000289510\n",
      "Epoch [456/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000288736\n",
      "Epoch [456/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000287963\n",
      "Epoch [457/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000287192\n",
      "Epoch [457/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000286423\n",
      "Epoch [457/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000285655\n",
      "Epoch [457/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000284888\n",
      "Epoch [457/500], Batch [50/110], Train Loss: 0.0218, Val Loss: 0.0060, LR: 0.0000284124\n",
      "Epoch [457/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000283360\n",
      "Epoch [457/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000282599\n",
      "Epoch [457/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000281839\n",
      "Epoch [457/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000281080\n",
      "Epoch [457/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000280323\n",
      "Epoch [457/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000279568\n",
      "Epoch [458/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000278814\n",
      "Epoch [458/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000278062\n",
      "Epoch [458/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000277311\n",
      "Epoch [458/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000276562\n",
      "Epoch [458/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000275814\n",
      "Epoch [458/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000275068\n",
      "Epoch [458/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000274323\n",
      "Epoch [458/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000273580\n",
      "Epoch [458/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000272839\n",
      "Epoch [458/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000272099\n",
      "Epoch [458/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000271361\n",
      "Epoch [459/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000270624\n",
      "Epoch [459/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000269889\n",
      "Epoch [459/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000269155\n",
      "Epoch [459/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000268423\n",
      "Epoch [459/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000267693\n",
      "Epoch [459/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000266964\n",
      "Epoch [459/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000266237\n",
      "Epoch [459/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000265511\n",
      "Epoch [459/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000264786\n",
      "Epoch [459/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000264064\n",
      "Epoch [459/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000263343\n",
      "Epoch [460/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000262623\n",
      "Epoch [460/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000261905\n",
      "Epoch [460/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000261189\n",
      "Epoch [460/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000260474\n",
      "Epoch [460/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000259760\n",
      "Epoch [460/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000259049\n",
      "Epoch [460/500], Batch [70/110], Train Loss: 0.0021, Val Loss: 0.0062, LR: 0.0000258338\n",
      "Epoch [460/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000257630\n",
      "Epoch [460/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000256923\n",
      "Epoch [460/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000256217\n",
      "Epoch [460/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000255513\n",
      "Epoch [461/500], Batch [10/110], Train Loss: 0.0047, Val Loss: 0.0061, LR: 0.0000254811\n",
      "Epoch [461/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000254110\n",
      "Epoch [461/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000253411\n",
      "Epoch [461/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000252713\n",
      "Epoch [461/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000252017\n",
      "Epoch [461/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000251323\n",
      "Epoch [461/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000250630\n",
      "Epoch [461/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000249938\n",
      "Epoch [461/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000249248\n",
      "Epoch [461/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0000248560\n",
      "Epoch [461/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000247873\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 461: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.039 sec , Memory Usage: 181.97 MB\n",
      "Epoch [462/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000247188\n",
      "Epoch [462/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000246505\n",
      "Epoch [462/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000245823\n",
      "Epoch [462/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000245142\n",
      "Epoch [462/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000244463\n",
      "Epoch [462/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000243786\n",
      "Epoch [462/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000243110\n",
      "Epoch [462/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000242436\n",
      "Epoch [462/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000241763\n",
      "Epoch [462/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0059, LR: 0.0000241092\n",
      "Epoch [462/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000240423\n",
      "Epoch [463/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000239755\n",
      "Epoch [463/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000239089\n",
      "Epoch [463/500], Batch [30/110], Train Loss: 0.0046, Val Loss: 0.0058, LR: 0.0000238424\n",
      "Epoch [463/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000237761\n",
      "Epoch [463/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000237099\n",
      "Epoch [463/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000236439\n",
      "Epoch [463/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000235781\n",
      "Epoch [463/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000235124\n",
      "Epoch [463/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000234468\n",
      "Epoch [463/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000233815\n",
      "Epoch [463/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000233162\n",
      "Epoch [464/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000232512\n",
      "Epoch [464/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000231863\n",
      "Epoch [464/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000231215\n",
      "Epoch [464/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000230569\n",
      "Epoch [464/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000229925\n",
      "Epoch [464/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000229282\n",
      "Epoch [464/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000228641\n",
      "Epoch [464/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000228001\n",
      "Epoch [464/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0000227363\n",
      "Epoch [464/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000226727\n",
      "Epoch [464/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000226092\n",
      "Epoch [465/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000225459\n",
      "Epoch [465/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000224827\n",
      "Epoch [465/500], Batch [30/110], Train Loss: 0.0127, Val Loss: 0.0059, LR: 0.0000224197\n",
      "Epoch [465/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000223568\n",
      "Epoch [465/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000222941\n",
      "Epoch [465/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000222316\n",
      "Epoch [465/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000221692\n",
      "Epoch [465/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000221069\n",
      "Epoch [465/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000220449\n",
      "Epoch [465/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000219830\n",
      "Epoch [465/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000219212\n",
      "Epoch [466/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000218596\n",
      "Epoch [466/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0000217982\n",
      "Epoch [466/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000217369\n",
      "Epoch [466/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000216757\n",
      "Epoch [466/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000216148\n",
      "Epoch [466/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000215540\n",
      "Epoch [466/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000214933\n",
      "Epoch [466/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000214328\n",
      "Epoch [466/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000213725\n",
      "Epoch [466/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0000213123\n",
      "Epoch [466/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000212523\n",
      "Epoch [467/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000211924\n",
      "Epoch [467/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000211327\n",
      "Epoch [467/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000210732\n",
      "Epoch [467/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000210138\n",
      "Epoch [467/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000209545\n",
      "Epoch [467/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000208955\n",
      "Epoch [467/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000208365\n",
      "Epoch [467/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000207778\n",
      "Epoch [467/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000207192\n",
      "Epoch [467/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000206607\n",
      "Epoch [467/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0000206025\n",
      "Epoch [468/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000205443\n",
      "Epoch [468/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000204864\n",
      "Epoch [468/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000204285\n",
      "Epoch [468/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000203709\n",
      "Epoch [468/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000203134\n",
      "Epoch [468/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000202561\n",
      "Epoch [468/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000201989\n",
      "Epoch [468/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000201419\n",
      "Epoch [468/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000200850\n",
      "Epoch [468/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000200283\n",
      "Epoch [468/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0000199717\n",
      "Epoch [469/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000199154\n",
      "Epoch [469/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000198591\n",
      "Epoch [469/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000198031\n",
      "Epoch [469/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000197471\n",
      "Epoch [469/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000196914\n",
      "Epoch [469/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000196358\n",
      "Epoch [469/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0000195804\n",
      "Epoch [469/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000195251\n",
      "Epoch [469/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000194700\n",
      "Epoch [469/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000194150\n",
      "Epoch [469/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000193602\n",
      "Epoch [470/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000193055\n",
      "Epoch [470/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000192511\n",
      "Epoch [470/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000191967\n",
      "Epoch [470/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000191426\n",
      "Epoch [470/500], Batch [50/110], Train Loss: 0.0039, Val Loss: 0.0060, LR: 0.0000190886\n",
      "Epoch [470/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000190347\n",
      "Epoch [470/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0059, LR: 0.0000189810\n",
      "Epoch [470/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000189275\n",
      "Epoch [470/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000188741\n",
      "Epoch [470/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000188209\n",
      "Epoch [470/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0059, LR: 0.0000187678\n",
      "Epoch [471/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000187149\n",
      "Epoch [471/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000186622\n",
      "Epoch [471/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000186096\n",
      "Epoch [471/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000185572\n",
      "Epoch [471/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000185049\n",
      "Epoch [471/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000184528\n",
      "Epoch [471/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000184008\n",
      "Epoch [471/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000183490\n",
      "Epoch [471/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000182974\n",
      "Epoch [471/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000182459\n",
      "Epoch [471/500], Batch [110/110], Train Loss: 0.0127, Val Loss: 0.0060, LR: 0.0000181946\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 471: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 181.98 MB\n",
      "Epoch [472/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000181435\n",
      "Epoch [472/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000180925\n",
      "Epoch [472/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000180416\n",
      "Epoch [472/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000179910\n",
      "Epoch [472/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0000179404\n",
      "Epoch [472/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000178901\n",
      "Epoch [472/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000178399\n",
      "Epoch [472/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000177898\n",
      "Epoch [472/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000177399\n",
      "Epoch [472/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000176902\n",
      "Epoch [472/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000176407\n",
      "Epoch [473/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000175912\n",
      "Epoch [473/500], Batch [20/110], Train Loss: 0.0215, Val Loss: 0.0059, LR: 0.0000175420\n",
      "Epoch [473/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000174929\n",
      "Epoch [473/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000174440\n",
      "Epoch [473/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000173952\n",
      "Epoch [473/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000173466\n",
      "Epoch [473/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000172981\n",
      "Epoch [473/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000172499\n",
      "Epoch [473/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000172017\n",
      "Epoch [473/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000171537\n",
      "Epoch [473/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000171059\n",
      "Epoch [474/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000170583\n",
      "Epoch [474/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000170108\n",
      "Epoch [474/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000169634\n",
      "Epoch [474/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000169163\n",
      "Epoch [474/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000168692\n",
      "Epoch [474/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000168224\n",
      "Epoch [474/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000167757\n",
      "Epoch [474/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000167291\n",
      "Epoch [474/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000166827\n",
      "Epoch [474/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0059, LR: 0.0000166365\n",
      "Epoch [474/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000165905\n",
      "Epoch [475/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000165446\n",
      "Epoch [475/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000164988\n",
      "Epoch [475/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000164532\n",
      "Epoch [475/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000164078\n",
      "Epoch [475/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000163625\n",
      "Epoch [475/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000163174\n",
      "Epoch [475/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000162725\n",
      "Epoch [475/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000162277\n",
      "Epoch [475/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0059, LR: 0.0000161831\n",
      "Epoch [475/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000161386\n",
      "Epoch [475/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000160943\n",
      "Epoch [476/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000160501\n",
      "Epoch [476/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000160061\n",
      "Epoch [476/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000159623\n",
      "Epoch [476/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000159186\n",
      "Epoch [476/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000158751\n",
      "Epoch [476/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000158318\n",
      "Epoch [476/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000157886\n",
      "Epoch [476/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000157455\n",
      "Epoch [476/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000157027\n",
      "Epoch [476/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000156599\n",
      "Epoch [476/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000156174\n",
      "Epoch [477/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000155750\n",
      "Epoch [477/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000155328\n",
      "Epoch [477/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000154907\n",
      "Epoch [477/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000154488\n",
      "Epoch [477/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000154070\n",
      "Epoch [477/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000153654\n",
      "Epoch [477/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000153240\n",
      "Epoch [477/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000152827\n",
      "Epoch [477/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000152416\n",
      "Epoch [477/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000152006\n",
      "Epoch [477/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000151598\n",
      "Epoch [478/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000151192\n",
      "Epoch [478/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000150787\n",
      "Epoch [478/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000150384\n",
      "Epoch [478/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000149982\n",
      "Epoch [478/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000149582\n",
      "Epoch [478/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000149184\n",
      "Epoch [478/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000148787\n",
      "Epoch [478/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000148392\n",
      "Epoch [478/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000147998\n",
      "Epoch [478/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000147606\n",
      "Epoch [478/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000147216\n",
      "Epoch [479/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000146827\n",
      "Epoch [479/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000146440\n",
      "Epoch [479/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000146054\n",
      "Epoch [479/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000145670\n",
      "Epoch [479/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000145288\n",
      "Epoch [479/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000144907\n",
      "Epoch [479/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000144528\n",
      "Epoch [479/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000144150\n",
      "Epoch [479/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000143774\n",
      "Epoch [479/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000143400\n",
      "Epoch [479/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000143027\n",
      "Epoch [480/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000142656\n",
      "Epoch [480/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000142286\n",
      "Epoch [480/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000141918\n",
      "Epoch [480/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000141552\n",
      "Epoch [480/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0060, LR: 0.0000141187\n",
      "Epoch [480/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0060, LR: 0.0000140824\n",
      "Epoch [480/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000140463\n",
      "Epoch [480/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000140103\n",
      "Epoch [480/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0000139744\n",
      "Epoch [480/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000139387\n",
      "Epoch [480/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000139032\n",
      "Epoch [481/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000138679\n",
      "Epoch [481/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000138327\n",
      "Epoch [481/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000137976\n",
      "Epoch [481/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000137628\n",
      "Epoch [481/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000137280\n",
      "Epoch [481/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000136935\n",
      "Epoch [481/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000136591\n",
      "Epoch [481/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000136249\n",
      "Epoch [481/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000135908\n",
      "Epoch [481/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000135569\n",
      "Epoch [481/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000135231\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 481: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 181.99 MB\n",
      "Epoch [482/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000134895\n",
      "Epoch [482/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000134561\n",
      "Epoch [482/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000134228\n",
      "Epoch [482/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000133897\n",
      "Epoch [482/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000133567\n",
      "Epoch [482/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000133240\n",
      "Epoch [482/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000132913\n",
      "Epoch [482/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000132589\n",
      "Epoch [482/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000132265\n",
      "Epoch [482/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000131944\n",
      "Epoch [482/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000131624\n",
      "Epoch [483/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000131306\n",
      "Epoch [483/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000130989\n",
      "Epoch [483/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000130674\n",
      "Epoch [483/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000130360\n",
      "Epoch [483/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0000130049\n",
      "Epoch [483/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000129738\n",
      "Epoch [483/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000129430\n",
      "Epoch [483/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000129123\n",
      "Epoch [483/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000128817\n",
      "Epoch [483/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000128513\n",
      "Epoch [483/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000128211\n",
      "Epoch [484/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000127910\n",
      "Epoch [484/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0061, LR: 0.0000127611\n",
      "Epoch [484/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000127314\n",
      "Epoch [484/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000127018\n",
      "Epoch [484/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000126724\n",
      "Epoch [484/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000126431\n",
      "Epoch [484/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000126140\n",
      "Epoch [484/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000125851\n",
      "Epoch [484/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000125563\n",
      "Epoch [484/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000125277\n",
      "Epoch [484/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000124992\n",
      "Epoch [485/500], Batch [10/110], Train Loss: 0.0136, Val Loss: 0.0061, LR: 0.0000124710\n",
      "Epoch [485/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000124428\n",
      "Epoch [485/500], Batch [30/110], Train Loss: 0.0032, Val Loss: 0.0061, LR: 0.0000124148\n",
      "Epoch [485/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0060, LR: 0.0000123870\n",
      "Epoch [485/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000123594\n",
      "Epoch [485/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000123319\n",
      "Epoch [485/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000123045\n",
      "Epoch [485/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000122774\n",
      "Epoch [485/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000122504\n",
      "Epoch [485/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000122235\n",
      "Epoch [485/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000121968\n",
      "Epoch [486/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000121703\n",
      "Epoch [486/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000121439\n",
      "Epoch [486/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000121177\n",
      "Epoch [486/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000120917\n",
      "Epoch [486/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000120658\n",
      "Epoch [486/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000120401\n",
      "Epoch [486/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000120145\n",
      "Epoch [486/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000119891\n",
      "Epoch [486/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000119639\n",
      "Epoch [486/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000119388\n",
      "Epoch [486/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000119139\n",
      "Epoch [487/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000118891\n",
      "Epoch [487/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000118645\n",
      "Epoch [487/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000118401\n",
      "Epoch [487/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0000118158\n",
      "Epoch [487/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000117917\n",
      "Epoch [487/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000117677\n",
      "Epoch [487/500], Batch [70/110], Train Loss: 0.0241, Val Loss: 0.0060, LR: 0.0000117439\n",
      "Epoch [487/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000117203\n",
      "Epoch [487/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000116968\n",
      "Epoch [487/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000116735\n",
      "Epoch [487/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000116504\n",
      "Epoch [488/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000116274\n",
      "Epoch [488/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000116045\n",
      "Epoch [488/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0000115819\n",
      "Epoch [488/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000115594\n",
      "Epoch [488/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000115370\n",
      "Epoch [488/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000115148\n",
      "Epoch [488/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000114928\n",
      "Epoch [488/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000114710\n",
      "Epoch [488/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000114493\n",
      "Epoch [488/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000114277\n",
      "Epoch [488/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000114063\n",
      "Epoch [489/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000113851\n",
      "Epoch [489/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000113641\n",
      "Epoch [489/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000113432\n",
      "Epoch [489/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000113224\n",
      "Epoch [489/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000113019\n",
      "Epoch [489/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0062, LR: 0.0000112815\n",
      "Epoch [489/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000112612\n",
      "Epoch [489/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000112411\n",
      "Epoch [489/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0000112212\n",
      "Epoch [489/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000112014\n",
      "Epoch [489/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000111818\n",
      "Epoch [490/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000111624\n",
      "Epoch [490/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000111431\n",
      "Epoch [490/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000111240\n",
      "Epoch [490/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000111050\n",
      "Epoch [490/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000110862\n",
      "Epoch [490/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000110676\n",
      "Epoch [490/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000110491\n",
      "Epoch [490/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000110308\n",
      "Epoch [490/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000110126\n",
      "Epoch [490/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0000109946\n",
      "Epoch [490/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000109768\n",
      "Epoch [491/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000109591\n",
      "Epoch [491/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000109416\n",
      "Epoch [491/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000109242\n",
      "Epoch [491/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000109070\n",
      "Epoch [491/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000108900\n",
      "Epoch [491/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000108731\n",
      "Epoch [491/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000108564\n",
      "Epoch [491/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000108399\n",
      "Epoch [491/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000108235\n",
      "Epoch [491/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000108073\n",
      "Epoch [491/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000107912\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 491: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.037 sec , Memory Usage: 181.39 MB\n",
      "Epoch [492/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000107753\n",
      "Epoch [492/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000107596\n",
      "Epoch [492/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000107440\n",
      "Epoch [492/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0062, LR: 0.0000107286\n",
      "Epoch [492/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000107133\n",
      "Epoch [492/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000106983\n",
      "Epoch [492/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000106833\n",
      "Epoch [492/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000106686\n",
      "Epoch [492/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000106539\n",
      "Epoch [492/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0000106395\n",
      "Epoch [492/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000106252\n",
      "Epoch [493/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000106111\n",
      "Epoch [493/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000105971\n",
      "Epoch [493/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000105833\n",
      "Epoch [493/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0000105697\n",
      "Epoch [493/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000105562\n",
      "Epoch [493/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000105429\n",
      "Epoch [493/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000105297\n",
      "Epoch [493/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000105167\n",
      "Epoch [493/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000105039\n",
      "Epoch [493/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000104912\n",
      "Epoch [493/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000104787\n",
      "Epoch [494/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000104663\n",
      "Epoch [494/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000104542\n",
      "Epoch [494/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000104421\n",
      "Epoch [494/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000104303\n",
      "Epoch [494/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000104186\n",
      "Epoch [494/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000104070\n",
      "Epoch [494/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000103956\n",
      "Epoch [494/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000103844\n",
      "Epoch [494/500], Batch [90/110], Train Loss: 0.0036, Val Loss: 0.0062, LR: 0.0000103733\n",
      "Epoch [494/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000103624\n",
      "Epoch [494/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000103517\n",
      "Epoch [495/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0062, LR: 0.0000103411\n",
      "Epoch [495/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000103307\n",
      "Epoch [495/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000103205\n",
      "Epoch [495/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000103104\n",
      "Epoch [495/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000103004\n",
      "Epoch [495/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000102907\n",
      "Epoch [495/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000102811\n",
      "Epoch [495/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000102716\n",
      "Epoch [495/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000102623\n",
      "Epoch [495/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000102532\n",
      "Epoch [495/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000102443\n",
      "Epoch [496/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000102355\n",
      "Epoch [496/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000102268\n",
      "Epoch [496/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0000102183\n",
      "Epoch [496/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000102100\n",
      "Epoch [496/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000102019\n",
      "Epoch [496/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000101939\n",
      "Epoch [496/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000101860\n",
      "Epoch [496/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000101784\n",
      "Epoch [496/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000101709\n",
      "Epoch [496/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000101635\n",
      "Epoch [496/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000101563\n",
      "Epoch [497/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000101493\n",
      "Epoch [497/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000101424\n",
      "Epoch [497/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0000101357\n",
      "Epoch [497/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000101292\n",
      "Epoch [497/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000101228\n",
      "Epoch [497/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000101166\n",
      "Epoch [497/500], Batch [70/110], Train Loss: 0.0036, Val Loss: 0.0062, LR: 0.0000101105\n",
      "Epoch [497/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000101047\n",
      "Epoch [497/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100989\n",
      "Epoch [497/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100933\n",
      "Epoch [497/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100879\n",
      "Epoch [498/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100827\n",
      "Epoch [498/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100776\n",
      "Epoch [498/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100727\n",
      "Epoch [498/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100679\n",
      "Epoch [498/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100633\n",
      "Epoch [498/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100589\n",
      "Epoch [498/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000100546\n",
      "Epoch [498/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000100505\n",
      "Epoch [498/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000100465\n",
      "Epoch [498/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000100427\n",
      "Epoch [498/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100391\n",
      "Epoch [499/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100356\n",
      "Epoch [499/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100323\n",
      "Epoch [499/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100292\n",
      "Epoch [499/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100262\n",
      "Epoch [499/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100233\n",
      "Epoch [499/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100207\n",
      "Epoch [499/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100182\n",
      "Epoch [499/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100158\n",
      "Epoch [499/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100136\n",
      "Epoch [499/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100116\n",
      "Epoch [499/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100098\n",
      "Epoch [500/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100081\n",
      "Epoch [500/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100065\n",
      "Epoch [500/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100052\n",
      "Epoch [500/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100040\n",
      "Epoch [500/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0000100029\n",
      "Epoch [500/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000100020\n",
      "Epoch [500/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000100013\n",
      "Epoch [500/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000100007\n",
      "Epoch [500/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000100003\n",
      "Epoch [500/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0000100001\n",
      "Epoch [500/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0000100000\n",
      "Confusion Matrix:\n",
      "[[637   0]\n",
      " [  0 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       637\n",
      "           1    1.00000   1.00000   1.00000       863\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.036 sec , Memory Usage: 195.04 MB\n",
      "=========== SEED: 7 , FOLD: 2/4, D: cpu ===========\n",
      " Label\n",
      "1    5734\n",
      "0    4266\n",
      "Name: count, dtype: int64\n",
      "=========== TP: 2,879 ===========\n",
      "Epoch [1/500], Batch [10/110], Train Loss: 0.6827, Val Loss: 0.6794, LR: 0.0009999999\n",
      "Epoch [1/500], Batch [20/110], Train Loss: 0.6415, Val Loss: 0.6728, LR: 0.0009999997\n",
      "Epoch [1/500], Batch [30/110], Train Loss: 0.6503, Val Loss: 0.6634, LR: 0.0009999993\n",
      "Epoch [1/500], Batch [40/110], Train Loss: 0.6739, Val Loss: 0.6431, LR: 0.0009999987\n",
      "Epoch [1/500], Batch [50/110], Train Loss: 0.6389, Val Loss: 0.6053, LR: 0.0009999980\n",
      "Epoch [1/500], Batch [60/110], Train Loss: 0.5682, Val Loss: 0.5427, LR: 0.0009999971\n",
      "Epoch [1/500], Batch [70/110], Train Loss: 0.3886, Val Loss: 0.4672, LR: 0.0009999960\n",
      "Epoch [1/500], Batch [80/110], Train Loss: 0.3224, Val Loss: 0.3779, LR: 0.0009999948\n",
      "Epoch [1/500], Batch [90/110], Train Loss: 0.2136, Val Loss: 0.2867, LR: 0.0009999935\n",
      "Epoch [1/500], Batch [100/110], Train Loss: 0.2287, Val Loss: 0.2064, LR: 0.0009999919\n",
      "Epoch [1/500], Batch [110/110], Train Loss: 0.1416, Val Loss: 0.1533, LR: 0.0009999902\n",
      "Confusion Matrix:\n",
      "[[577  58]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.90866   0.95215       635\n",
      "           1    0.93716   1.00000   0.96756       865\n",
      "\n",
      "    accuracy                        0.96133      1500\n",
      "   macro avg    0.96858   0.95433   0.95985      1500\n",
      "weighted avg    0.96376   0.96133   0.96104      1500\n",
      "\n",
      "Total Errors: 58\n",
      "Index: 58, Predicted: 1, Actual: 0\n",
      "Index: 61, Predicted: 1, Actual: 0\n",
      "Index: 74, Predicted: 1, Actual: 0\n",
      "Index: 171, Predicted: 1, Actual: 0\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Epoch 1: OK- Accuracy: 0.96133, Precision: 0.93716, Recall: 1.00000, F1: 0.96756, ROC AUC: 0.95433, AUPR (PR-AUC): 0.93716, Sensitivity: 1.00000, Specificity: 0.90866, Far: 0.09133858267716535, False Positive Rate (FPR): 0.09134, False Negative Rate (FNR): 0.00000, Runtime: 0.038 sec , Memory Usage: 312.81 MB\n",
      "Epoch [2/500], Batch [10/110], Train Loss: 0.1164, Val Loss: 0.1211, LR: 0.0009999884\n",
      "Epoch [2/500], Batch [20/110], Train Loss: 0.1555, Val Loss: 0.1012, LR: 0.0009999864\n",
      "Epoch [2/500], Batch [30/110], Train Loss: 0.0657, Val Loss: 0.0883, LR: 0.0009999842\n",
      "Epoch [2/500], Batch [40/110], Train Loss: 0.2105, Val Loss: 0.0816, LR: 0.0009999818\n",
      "Epoch [2/500], Batch [50/110], Train Loss: 0.1224, Val Loss: 0.0809, LR: 0.0009999793\n",
      "Epoch [2/500], Batch [60/110], Train Loss: 0.0529, Val Loss: 0.0716, LR: 0.0009999767\n",
      "Epoch [2/500], Batch [70/110], Train Loss: 0.0670, Val Loss: 0.0699, LR: 0.0009999738\n",
      "Epoch [2/500], Batch [80/110], Train Loss: 0.1354, Val Loss: 0.0683, LR: 0.0009999708\n",
      "Epoch [2/500], Batch [90/110], Train Loss: 0.0390, Val Loss: 0.0711, LR: 0.0009999677\n",
      "Epoch [2/500], Batch [100/110], Train Loss: 0.1182, Val Loss: 0.0635, LR: 0.0009999644\n",
      "Epoch [2/500], Batch [110/110], Train Loss: 0.1709, Val Loss: 0.0644, LR: 0.0009999609\n",
      "Epoch [3/500], Batch [10/110], Train Loss: 0.1238, Val Loss: 0.0695, LR: 0.0009999573\n",
      "Epoch [3/500], Batch [20/110], Train Loss: 0.0320, Val Loss: 0.0609, LR: 0.0009999535\n",
      "Epoch [3/500], Batch [30/110], Train Loss: 0.0157, Val Loss: 0.0624, LR: 0.0009999495\n",
      "Epoch [3/500], Batch [40/110], Train Loss: 0.0144, Val Loss: 0.0607, LR: 0.0009999454\n",
      "Epoch [3/500], Batch [50/110], Train Loss: 0.0110, Val Loss: 0.0621, LR: 0.0009999411\n",
      "Epoch [3/500], Batch [60/110], Train Loss: 0.0118, Val Loss: 0.0646, LR: 0.0009999367\n",
      "Epoch [3/500], Batch [70/110], Train Loss: 0.1729, Val Loss: 0.0706, LR: 0.0009999321\n",
      "Epoch [3/500], Batch [80/110], Train Loss: 0.0222, Val Loss: 0.0568, LR: 0.0009999273\n",
      "Epoch [3/500], Batch [90/110], Train Loss: 0.0310, Val Loss: 0.0630, LR: 0.0009999224\n",
      "Epoch [3/500], Batch [100/110], Train Loss: 0.0114, Val Loss: 0.0588, LR: 0.0009999173\n",
      "Epoch [3/500], Batch [110/110], Train Loss: 0.0207, Val Loss: 0.0551, LR: 0.0009999121\n",
      "Epoch [4/500], Batch [10/110], Train Loss: 0.0521, Val Loss: 0.0699, LR: 0.0009999067\n",
      "Epoch [4/500], Batch [20/110], Train Loss: 0.0340, Val Loss: 0.0546, LR: 0.0009999011\n",
      "Epoch [4/500], Batch [30/110], Train Loss: 0.1627, Val Loss: 0.0564, LR: 0.0009998953\n",
      "Epoch [4/500], Batch [40/110], Train Loss: 0.0068, Val Loss: 0.0585, LR: 0.0009998895\n",
      "Epoch [4/500], Batch [50/110], Train Loss: 0.0076, Val Loss: 0.0558, LR: 0.0009998834\n",
      "Epoch [4/500], Batch [60/110], Train Loss: 0.0436, Val Loss: 0.0529, LR: 0.0009998772\n",
      "Epoch [4/500], Batch [70/110], Train Loss: 0.0351, Val Loss: 0.0550, LR: 0.0009998708\n",
      "Epoch [4/500], Batch [80/110], Train Loss: 0.0135, Val Loss: 0.0523, LR: 0.0009998643\n",
      "Epoch [4/500], Batch [90/110], Train Loss: 0.0803, Val Loss: 0.0527, LR: 0.0009998576\n",
      "Epoch [4/500], Batch [100/110], Train Loss: 0.0102, Val Loss: 0.0550, LR: 0.0009998507\n",
      "Epoch [4/500], Batch [110/110], Train Loss: 0.2694, Val Loss: 0.0667, LR: 0.0009998437\n",
      "Epoch [5/500], Batch [10/110], Train Loss: 0.0165, Val Loss: 0.0526, LR: 0.0009998365\n",
      "Epoch [5/500], Batch [20/110], Train Loss: 0.0111, Val Loss: 0.0506, LR: 0.0009998291\n",
      "Epoch [5/500], Batch [30/110], Train Loss: 0.0278, Val Loss: 0.0538, LR: 0.0009998216\n",
      "Epoch [5/500], Batch [40/110], Train Loss: 0.0227, Val Loss: 0.0540, LR: 0.0009998140\n",
      "Epoch [5/500], Batch [50/110], Train Loss: 0.0209, Val Loss: 0.0502, LR: 0.0009998061\n",
      "Epoch [5/500], Batch [60/110], Train Loss: 0.0245, Val Loss: 0.0578, LR: 0.0009997981\n",
      "Epoch [5/500], Batch [70/110], Train Loss: 0.1268, Val Loss: 0.0489, LR: 0.0009997900\n",
      "Epoch [5/500], Batch [80/110], Train Loss: 0.0068, Val Loss: 0.0569, LR: 0.0009997817\n",
      "Epoch [5/500], Batch [90/110], Train Loss: 0.1483, Val Loss: 0.0507, LR: 0.0009997732\n",
      "Epoch [5/500], Batch [100/110], Train Loss: 0.2572, Val Loss: 0.0481, LR: 0.0009997645\n",
      "Epoch [5/500], Batch [110/110], Train Loss: 0.1340, Val Loss: 0.0489, LR: 0.0009997557\n",
      "Epoch [6/500], Batch [10/110], Train Loss: 0.0077, Val Loss: 0.0482, LR: 0.0009997468\n",
      "Epoch [6/500], Batch [20/110], Train Loss: 0.0126, Val Loss: 0.0491, LR: 0.0009997377\n",
      "Epoch [6/500], Batch [30/110], Train Loss: 0.0088, Val Loss: 0.0489, LR: 0.0009997284\n",
      "Epoch [6/500], Batch [40/110], Train Loss: 0.0165, Val Loss: 0.0523, LR: 0.0009997189\n",
      "Epoch [6/500], Batch [50/110], Train Loss: 0.0108, Val Loss: 0.0472, LR: 0.0009997093\n",
      "Epoch [6/500], Batch [60/110], Train Loss: 0.1973, Val Loss: 0.0481, LR: 0.0009996996\n",
      "Epoch [6/500], Batch [70/110], Train Loss: 0.0118, Val Loss: 0.0591, LR: 0.0009996896\n",
      "Epoch [6/500], Batch [80/110], Train Loss: 0.0133, Val Loss: 0.0459, LR: 0.0009996795\n",
      "Epoch [6/500], Batch [90/110], Train Loss: 0.2930, Val Loss: 0.0517, LR: 0.0009996693\n",
      "Epoch [6/500], Batch [100/110], Train Loss: 0.0210, Val Loss: 0.0459, LR: 0.0009996589\n",
      "Epoch [6/500], Batch [110/110], Train Loss: 0.0043, Val Loss: 0.0594, LR: 0.0009996483\n",
      "Epoch [7/500], Batch [10/110], Train Loss: 0.0183, Val Loss: 0.0448, LR: 0.0009996376\n",
      "Epoch [7/500], Batch [20/110], Train Loss: 0.1881, Val Loss: 0.0539, LR: 0.0009996267\n",
      "Epoch [7/500], Batch [30/110], Train Loss: 0.0098, Val Loss: 0.0454, LR: 0.0009996156\n",
      "Epoch [7/500], Batch [40/110], Train Loss: 0.1458, Val Loss: 0.0494, LR: 0.0009996044\n",
      "Epoch [7/500], Batch [50/110], Train Loss: 0.1762, Val Loss: 0.0597, LR: 0.0009995930\n",
      "Epoch [7/500], Batch [60/110], Train Loss: 0.0071, Val Loss: 0.0452, LR: 0.0009995814\n",
      "Epoch [7/500], Batch [70/110], Train Loss: 0.1500, Val Loss: 0.0527, LR: 0.0009995697\n",
      "Epoch [7/500], Batch [80/110], Train Loss: 0.1404, Val Loss: 0.0518, LR: 0.0009995579\n",
      "Epoch [7/500], Batch [90/110], Train Loss: 0.1042, Val Loss: 0.0472, LR: 0.0009995458\n",
      "Epoch [7/500], Batch [100/110], Train Loss: 0.0059, Val Loss: 0.0615, LR: 0.0009995337\n",
      "Epoch [7/500], Batch [110/110], Train Loss: 0.0135, Val Loss: 0.0475, LR: 0.0009995213\n",
      "Epoch [8/500], Batch [10/110], Train Loss: 0.0105, Val Loss: 0.0445, LR: 0.0009995088\n",
      "Epoch [8/500], Batch [20/110], Train Loss: 0.0082, Val Loss: 0.0581, LR: 0.0009994961\n",
      "Epoch [8/500], Batch [30/110], Train Loss: 0.0150, Val Loss: 0.0451, LR: 0.0009994833\n",
      "Epoch [8/500], Batch [40/110], Train Loss: 0.0090, Val Loss: 0.0490, LR: 0.0009994703\n",
      "Epoch [8/500], Batch [50/110], Train Loss: 0.0093, Val Loss: 0.0446, LR: 0.0009994571\n",
      "Epoch [8/500], Batch [60/110], Train Loss: 0.0223, Val Loss: 0.0445, LR: 0.0009994438\n",
      "Epoch [8/500], Batch [70/110], Train Loss: 0.0052, Val Loss: 0.0482, LR: 0.0009994303\n",
      "Epoch [8/500], Batch [80/110], Train Loss: 0.0202, Val Loss: 0.0437, LR: 0.0009994167\n",
      "Epoch [8/500], Batch [90/110], Train Loss: 0.0117, Val Loss: 0.0467, LR: 0.0009994029\n",
      "Epoch [8/500], Batch [100/110], Train Loss: 0.3060, Val Loss: 0.0543, LR: 0.0009993889\n",
      "Epoch [8/500], Batch [110/110], Train Loss: 0.0044, Val Loss: 0.0434, LR: 0.0009993748\n",
      "Epoch [9/500], Batch [10/110], Train Loss: 0.1786, Val Loss: 0.0425, LR: 0.0009993605\n",
      "Epoch [9/500], Batch [20/110], Train Loss: 0.0242, Val Loss: 0.0433, LR: 0.0009993461\n",
      "Epoch [9/500], Batch [30/110], Train Loss: 0.1410, Val Loss: 0.0465, LR: 0.0009993314\n",
      "Epoch [9/500], Batch [40/110], Train Loss: 0.0242, Val Loss: 0.0431, LR: 0.0009993167\n",
      "Epoch [9/500], Batch [50/110], Train Loss: 0.0289, Val Loss: 0.0422, LR: 0.0009993017\n",
      "Epoch [9/500], Batch [60/110], Train Loss: 0.0073, Val Loss: 0.0529, LR: 0.0009992867\n",
      "Epoch [9/500], Batch [70/110], Train Loss: 0.0140, Val Loss: 0.0435, LR: 0.0009992714\n",
      "Epoch [9/500], Batch [80/110], Train Loss: 0.0051, Val Loss: 0.0442, LR: 0.0009992560\n",
      "Epoch [9/500], Batch [90/110], Train Loss: 0.0228, Val Loss: 0.0478, LR: 0.0009992404\n",
      "Epoch [9/500], Batch [100/110], Train Loss: 0.0142, Val Loss: 0.0416, LR: 0.0009992247\n",
      "Epoch [9/500], Batch [110/110], Train Loss: 0.0065, Val Loss: 0.0441, LR: 0.0009992088\n",
      "Epoch [10/500], Batch [10/110], Train Loss: 0.1157, Val Loss: 0.0412, LR: 0.0009991927\n",
      "Epoch [10/500], Batch [20/110], Train Loss: 0.0107, Val Loss: 0.0417, LR: 0.0009991765\n",
      "Epoch [10/500], Batch [30/110], Train Loss: 0.0072, Val Loss: 0.0431, LR: 0.0009991601\n",
      "Epoch [10/500], Batch [40/110], Train Loss: 0.0170, Val Loss: 0.0421, LR: 0.0009991436\n",
      "Epoch [10/500], Batch [50/110], Train Loss: 0.1530, Val Loss: 0.0482, LR: 0.0009991269\n",
      "Epoch [10/500], Batch [60/110], Train Loss: 0.0090, Val Loss: 0.0442, LR: 0.0009991100\n",
      "Epoch [10/500], Batch [70/110], Train Loss: 0.0042, Val Loss: 0.0445, LR: 0.0009990930\n",
      "Epoch [10/500], Batch [80/110], Train Loss: 0.0117, Val Loss: 0.0411, LR: 0.0009990758\n",
      "Epoch [10/500], Batch [90/110], Train Loss: 0.0046, Val Loss: 0.0435, LR: 0.0009990584\n",
      "Epoch [10/500], Batch [100/110], Train Loss: 0.1305, Val Loss: 0.0423, LR: 0.0009990409\n",
      "Epoch [10/500], Batch [110/110], Train Loss: 0.0051, Val Loss: 0.0397, LR: 0.0009990232\n",
      "Epoch [11/500], Batch [10/110], Train Loss: 0.0046, Val Loss: 0.0469, LR: 0.0009990054\n",
      "Epoch [11/500], Batch [20/110], Train Loss: 0.0083, Val Loss: 0.0385, LR: 0.0009989874\n",
      "Epoch [11/500], Batch [30/110], Train Loss: 0.0017, Val Loss: 0.0512, LR: 0.0009989692\n",
      "Epoch [11/500], Batch [40/110], Train Loss: 0.0081, Val Loss: 0.0420, LR: 0.0009989509\n",
      "Epoch [11/500], Batch [50/110], Train Loss: 0.1308, Val Loss: 0.0414, LR: 0.0009989324\n",
      "Epoch [11/500], Batch [60/110], Train Loss: 0.0040, Val Loss: 0.0417, LR: 0.0009989138\n",
      "Epoch [11/500], Batch [70/110], Train Loss: 0.0040, Val Loss: 0.0431, LR: 0.0009988950\n",
      "Epoch [11/500], Batch [80/110], Train Loss: 0.1040, Val Loss: 0.0393, LR: 0.0009988760\n",
      "Epoch [11/500], Batch [90/110], Train Loss: 0.0020, Val Loss: 0.0670, LR: 0.0009988569\n",
      "Epoch [11/500], Batch [100/110], Train Loss: 0.0067, Val Loss: 0.0403, LR: 0.0009988376\n",
      "Epoch [11/500], Batch [110/110], Train Loss: 0.0140, Val Loss: 0.0393, LR: 0.0009988182\n",
      "Confusion Matrix:\n",
      "[[612  23]\n",
      " [  2 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99674   0.96378   0.97998       635\n",
      "           1    0.97404   0.99769   0.98572       865\n",
      "\n",
      "    accuracy                        0.98333      1500\n",
      "   macro avg    0.98539   0.98073   0.98285      1500\n",
      "weighted avg    0.98365   0.98333   0.98329      1500\n",
      "\n",
      "Total Errors: 25\n",
      "Index: 58, Predicted: 1, Actual: 0\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 231, Predicted: 1, Actual: 0\n",
      "Index: 319, Predicted: 1, Actual: 0\n",
      "Index: 376, Predicted: 1, Actual: 0\n",
      "Epoch 11: OK- Accuracy: 0.98333, Precision: 0.97404, Recall: 0.99769, F1: 0.98572, ROC AUC: 0.98073, AUPR (PR-AUC): 0.97312, Sensitivity: 0.99769, Specificity: 0.96378, Far: 0.03622047244094488, False Positive Rate (FPR): 0.03622, False Negative Rate (FNR): 0.00231, Runtime: 0.039 sec , Memory Usage: 312.84 MB\n",
      "Epoch [12/500], Batch [10/110], Train Loss: 0.0085, Val Loss: 0.0451, LR: 0.0009987986\n",
      "Epoch [12/500], Batch [20/110], Train Loss: 0.0069, Val Loss: 0.0500, LR: 0.0009987788\n",
      "Epoch [12/500], Batch [30/110], Train Loss: 0.0818, Val Loss: 0.0447, LR: 0.0009987589\n",
      "Epoch [12/500], Batch [40/110], Train Loss: 0.1423, Val Loss: 0.0464, LR: 0.0009987388\n",
      "Epoch [12/500], Batch [50/110], Train Loss: 0.0099, Val Loss: 0.0443, LR: 0.0009987185\n",
      "Epoch [12/500], Batch [60/110], Train Loss: 0.2088, Val Loss: 0.0396, LR: 0.0009986981\n",
      "Epoch [12/500], Batch [70/110], Train Loss: 0.2435, Val Loss: 0.0499, LR: 0.0009986776\n",
      "Epoch [12/500], Batch [80/110], Train Loss: 0.0903, Val Loss: 0.0408, LR: 0.0009986568\n",
      "Epoch [12/500], Batch [90/110], Train Loss: 0.0023, Val Loss: 0.0406, LR: 0.0009986359\n",
      "Epoch [12/500], Batch [100/110], Train Loss: 0.0042, Val Loss: 0.0483, LR: 0.0009986149\n",
      "Epoch [12/500], Batch [110/110], Train Loss: 0.1232, Val Loss: 0.0401, LR: 0.0009985937\n",
      "Epoch [13/500], Batch [10/110], Train Loss: 0.1262, Val Loss: 0.0423, LR: 0.0009985723\n",
      "Epoch [13/500], Batch [20/110], Train Loss: 0.1283, Val Loss: 0.0423, LR: 0.0009985507\n",
      "Epoch [13/500], Batch [30/110], Train Loss: 0.0168, Val Loss: 0.0396, LR: 0.0009985290\n",
      "Epoch [13/500], Batch [40/110], Train Loss: 0.0071, Val Loss: 0.0450, LR: 0.0009985072\n",
      "Epoch [13/500], Batch [50/110], Train Loss: 0.0033, Val Loss: 0.0431, LR: 0.0009984852\n",
      "Epoch [13/500], Batch [60/110], Train Loss: 0.0135, Val Loss: 0.0395, LR: 0.0009984630\n",
      "Epoch [13/500], Batch [70/110], Train Loss: 0.0036, Val Loss: 0.0515, LR: 0.0009984406\n",
      "Epoch [13/500], Batch [80/110], Train Loss: 0.0069, Val Loss: 0.0408, LR: 0.0009984181\n",
      "Epoch [13/500], Batch [90/110], Train Loss: 0.0172, Val Loss: 0.0402, LR: 0.0009983955\n",
      "Epoch [13/500], Batch [100/110], Train Loss: 0.0053, Val Loss: 0.0457, LR: 0.0009983726\n",
      "Epoch [13/500], Batch [110/110], Train Loss: 0.0093, Val Loss: 0.0399, LR: 0.0009983496\n",
      "Epoch [14/500], Batch [10/110], Train Loss: 0.0228, Val Loss: 0.0384, LR: 0.0009983265\n",
      "Epoch [14/500], Batch [20/110], Train Loss: 0.0080, Val Loss: 0.0448, LR: 0.0009983032\n",
      "Epoch [14/500], Batch [30/110], Train Loss: 0.0339, Val Loss: 0.0409, LR: 0.0009982797\n",
      "Epoch [14/500], Batch [40/110], Train Loss: 0.1449, Val Loss: 0.0446, LR: 0.0009982561\n",
      "Epoch [14/500], Batch [50/110], Train Loss: 0.1423, Val Loss: 0.0407, LR: 0.0009982323\n",
      "Epoch [14/500], Batch [60/110], Train Loss: 0.0165, Val Loss: 0.0376, LR: 0.0009982083\n",
      "Epoch [14/500], Batch [70/110], Train Loss: 0.0037, Val Loss: 0.0498, LR: 0.0009981842\n",
      "Epoch [14/500], Batch [80/110], Train Loss: 0.0177, Val Loss: 0.0373, LR: 0.0009981599\n",
      "Epoch [14/500], Batch [90/110], Train Loss: 0.0138, Val Loss: 0.0378, LR: 0.0009981355\n",
      "Epoch [14/500], Batch [100/110], Train Loss: 0.0049, Val Loss: 0.0380, LR: 0.0009981109\n",
      "Epoch [14/500], Batch [110/110], Train Loss: 0.0187, Val Loss: 0.0424, LR: 0.0009980861\n",
      "Epoch [15/500], Batch [10/110], Train Loss: 0.1630, Val Loss: 0.0387, LR: 0.0009980612\n",
      "Epoch [15/500], Batch [20/110], Train Loss: 0.0133, Val Loss: 0.0380, LR: 0.0009980361\n",
      "Epoch [15/500], Batch [30/110], Train Loss: 0.0017, Val Loss: 0.0403, LR: 0.0009980109\n",
      "Epoch [15/500], Batch [40/110], Train Loss: 0.0113, Val Loss: 0.0358, LR: 0.0009979855\n",
      "Epoch [15/500], Batch [50/110], Train Loss: 0.0577, Val Loss: 0.0374, LR: 0.0009979599\n",
      "Epoch [15/500], Batch [60/110], Train Loss: 0.1226, Val Loss: 0.0377, LR: 0.0009979342\n",
      "Epoch [15/500], Batch [70/110], Train Loss: 0.0214, Val Loss: 0.0385, LR: 0.0009979083\n",
      "Epoch [15/500], Batch [80/110], Train Loss: 0.0029, Val Loss: 0.0514, LR: 0.0009978823\n",
      "Epoch [15/500], Batch [90/110], Train Loss: 0.0121, Val Loss: 0.0356, LR: 0.0009978561\n",
      "Epoch [15/500], Batch [100/110], Train Loss: 0.0170, Val Loss: 0.0355, LR: 0.0009978297\n",
      "Epoch [15/500], Batch [110/110], Train Loss: 0.1543, Val Loss: 0.0454, LR: 0.0009978032\n",
      "Epoch [16/500], Batch [10/110], Train Loss: 0.1287, Val Loss: 0.0387, LR: 0.0009977765\n",
      "Epoch [16/500], Batch [20/110], Train Loss: 0.0235, Val Loss: 0.0346, LR: 0.0009977496\n",
      "Epoch [16/500], Batch [30/110], Train Loss: 0.0085, Val Loss: 0.0389, LR: 0.0009977226\n",
      "Epoch [16/500], Batch [40/110], Train Loss: 0.0088, Val Loss: 0.0346, LR: 0.0009976955\n",
      "Epoch [16/500], Batch [50/110], Train Loss: 0.1794, Val Loss: 0.0472, LR: 0.0009976681\n",
      "Epoch [16/500], Batch [60/110], Train Loss: 0.1038, Val Loss: 0.0343, LR: 0.0009976406\n",
      "Epoch [16/500], Batch [70/110], Train Loss: 0.1101, Val Loss: 0.0345, LR: 0.0009976130\n",
      "Epoch [16/500], Batch [80/110], Train Loss: 0.0738, Val Loss: 0.0373, LR: 0.0009975852\n",
      "Epoch [16/500], Batch [90/110], Train Loss: 0.0037, Val Loss: 0.0435, LR: 0.0009975572\n",
      "Epoch [16/500], Batch [100/110], Train Loss: 0.1067, Val Loss: 0.0371, LR: 0.0009975290\n",
      "Epoch [16/500], Batch [110/110], Train Loss: 0.0871, Val Loss: 0.0370, LR: 0.0009975008\n",
      "Epoch [17/500], Batch [10/110], Train Loss: 0.0095, Val Loss: 0.0367, LR: 0.0009974723\n",
      "Epoch [17/500], Batch [20/110], Train Loss: 0.0040, Val Loss: 0.0348, LR: 0.0009974437\n",
      "Epoch [17/500], Batch [30/110], Train Loss: 0.0025, Val Loss: 0.0499, LR: 0.0009974149\n",
      "Epoch [17/500], Batch [40/110], Train Loss: 0.1823, Val Loss: 0.0360, LR: 0.0009973860\n",
      "Epoch [17/500], Batch [50/110], Train Loss: 0.0107, Val Loss: 0.0360, LR: 0.0009973569\n",
      "Epoch [17/500], Batch [60/110], Train Loss: 0.0045, Val Loss: 0.0422, LR: 0.0009973276\n",
      "Epoch [17/500], Batch [70/110], Train Loss: 0.0133, Val Loss: 0.0359, LR: 0.0009972982\n",
      "Epoch [17/500], Batch [80/110], Train Loss: 0.0123, Val Loss: 0.0356, LR: 0.0009972686\n",
      "Epoch [17/500], Batch [90/110], Train Loss: 0.0066, Val Loss: 0.0403, LR: 0.0009972389\n",
      "Epoch [17/500], Batch [100/110], Train Loss: 0.0397, Val Loss: 0.0388, LR: 0.0009972090\n",
      "Epoch [17/500], Batch [110/110], Train Loss: 0.1334, Val Loss: 0.0400, LR: 0.0009971789\n",
      "Epoch [18/500], Batch [10/110], Train Loss: 0.1155, Val Loss: 0.0338, LR: 0.0009971487\n",
      "Epoch [18/500], Batch [20/110], Train Loss: 0.0076, Val Loss: 0.0334, LR: 0.0009971183\n",
      "Epoch [18/500], Batch [30/110], Train Loss: 0.3366, Val Loss: 0.0358, LR: 0.0009970877\n",
      "Epoch [18/500], Batch [40/110], Train Loss: 0.1568, Val Loss: 0.0459, LR: 0.0009970570\n",
      "Epoch [18/500], Batch [50/110], Train Loss: 0.0070, Val Loss: 0.0332, LR: 0.0009970262\n",
      "Epoch [18/500], Batch [60/110], Train Loss: 0.0056, Val Loss: 0.0342, LR: 0.0009969951\n",
      "Epoch [18/500], Batch [70/110], Train Loss: 0.0155, Val Loss: 0.0344, LR: 0.0009969640\n",
      "Epoch [18/500], Batch [80/110], Train Loss: 0.1159, Val Loss: 0.0368, LR: 0.0009969326\n",
      "Epoch [18/500], Batch [90/110], Train Loss: 0.0072, Val Loss: 0.0369, LR: 0.0009969011\n",
      "Epoch [18/500], Batch [100/110], Train Loss: 0.0952, Val Loss: 0.0341, LR: 0.0009968694\n",
      "Epoch [18/500], Batch [110/110], Train Loss: 0.0052, Val Loss: 0.0379, LR: 0.0009968376\n",
      "Epoch [19/500], Batch [10/110], Train Loss: 0.0642, Val Loss: 0.0344, LR: 0.0009968056\n",
      "Epoch [19/500], Batch [20/110], Train Loss: 0.0020, Val Loss: 0.0436, LR: 0.0009967735\n",
      "Epoch [19/500], Batch [30/110], Train Loss: 0.2226, Val Loss: 0.0348, LR: 0.0009967411\n",
      "Epoch [19/500], Batch [40/110], Train Loss: 0.0726, Val Loss: 0.0334, LR: 0.0009967087\n",
      "Epoch [19/500], Batch [50/110], Train Loss: 0.0029, Val Loss: 0.0394, LR: 0.0009966760\n",
      "Epoch [19/500], Batch [60/110], Train Loss: 0.0030, Val Loss: 0.0355, LR: 0.0009966433\n",
      "Epoch [19/500], Batch [70/110], Train Loss: 0.0068, Val Loss: 0.0322, LR: 0.0009966103\n",
      "Epoch [19/500], Batch [80/110], Train Loss: 0.0048, Val Loss: 0.0396, LR: 0.0009965772\n",
      "Epoch [19/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0312, LR: 0.0009965439\n",
      "Epoch [19/500], Batch [100/110], Train Loss: 0.0197, Val Loss: 0.0312, LR: 0.0009965105\n",
      "Epoch [19/500], Batch [110/110], Train Loss: 0.0040, Val Loss: 0.0356, LR: 0.0009964769\n",
      "Epoch [20/500], Batch [10/110], Train Loss: 0.1173, Val Loss: 0.0351, LR: 0.0009964431\n",
      "Epoch [20/500], Batch [20/110], Train Loss: 0.0170, Val Loss: 0.0307, LR: 0.0009964092\n",
      "Epoch [20/500], Batch [30/110], Train Loss: 0.0140, Val Loss: 0.0318, LR: 0.0009963751\n",
      "Epoch [20/500], Batch [40/110], Train Loss: 0.0034, Val Loss: 0.0312, LR: 0.0009963409\n",
      "Epoch [20/500], Batch [50/110], Train Loss: 0.0037, Val Loss: 0.0316, LR: 0.0009963065\n",
      "Epoch [20/500], Batch [60/110], Train Loss: 0.0016, Val Loss: 0.0302, LR: 0.0009962720\n",
      "Epoch [20/500], Batch [70/110], Train Loss: 0.0892, Val Loss: 0.0299, LR: 0.0009962372\n",
      "Epoch [20/500], Batch [80/110], Train Loss: 0.0030, Val Loss: 0.0303, LR: 0.0009962024\n",
      "Epoch [20/500], Batch [90/110], Train Loss: 0.0258, Val Loss: 0.0293, LR: 0.0009961673\n",
      "Epoch [20/500], Batch [100/110], Train Loss: 0.0024, Val Loss: 0.0413, LR: 0.0009961321\n",
      "Epoch [20/500], Batch [110/110], Train Loss: 0.0747, Val Loss: 0.0309, LR: 0.0009960968\n",
      "Epoch [21/500], Batch [10/110], Train Loss: 0.0070, Val Loss: 0.0303, LR: 0.0009960613\n",
      "Epoch [21/500], Batch [20/110], Train Loss: 0.0046, Val Loss: 0.0324, LR: 0.0009960256\n",
      "Epoch [21/500], Batch [30/110], Train Loss: 0.0017, Val Loss: 0.0327, LR: 0.0009959897\n",
      "Epoch [21/500], Batch [40/110], Train Loss: 0.0113, Val Loss: 0.0291, LR: 0.0009959537\n",
      "Epoch [21/500], Batch [50/110], Train Loss: 0.0038, Val Loss: 0.0411, LR: 0.0009959176\n",
      "Epoch [21/500], Batch [60/110], Train Loss: 0.1455, Val Loss: 0.0413, LR: 0.0009958813\n",
      "Epoch [21/500], Batch [70/110], Train Loss: 0.0330, Val Loss: 0.0303, LR: 0.0009958448\n",
      "Epoch [21/500], Batch [80/110], Train Loss: 0.0063, Val Loss: 0.0298, LR: 0.0009958082\n",
      "Epoch [21/500], Batch [90/110], Train Loss: 0.0048, Val Loss: 0.0320, LR: 0.0009957714\n",
      "Epoch [21/500], Batch [100/110], Train Loss: 0.0047, Val Loss: 0.0358, LR: 0.0009957344\n",
      "Epoch [21/500], Batch [110/110], Train Loss: 0.0806, Val Loss: 0.0288, LR: 0.0009956973\n",
      "Confusion Matrix:\n",
      "[[614  21]\n",
      " [  2 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99675   0.96693   0.98161       635\n",
      "           1    0.97624   0.99769   0.98685       865\n",
      "\n",
      "    accuracy                        0.98467      1500\n",
      "   macro avg    0.98650   0.98231   0.98423      1500\n",
      "weighted avg    0.98493   0.98467   0.98463      1500\n",
      "\n",
      "Total Errors: 23\n",
      "Index: 58, Predicted: 1, Actual: 0\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 231, Predicted: 1, Actual: 0\n",
      "Index: 319, Predicted: 1, Actual: 0\n",
      "Index: 376, Predicted: 1, Actual: 0\n",
      "Epoch 21: OK- Accuracy: 0.98467, Precision: 0.97624, Recall: 0.99769, F1: 0.98685, ROC AUC: 0.98231, AUPR (PR-AUC): 0.97532, Sensitivity: 0.99769, Specificity: 0.96693, Far: 0.03307086614173228, False Positive Rate (FPR): 0.03307, False Negative Rate (FNR): 0.00231, Runtime: 0.032 sec , Memory Usage: 312.86 MB\n",
      "Epoch [22/500], Batch [10/110], Train Loss: 0.0073, Val Loss: 0.0299, LR: 0.0009956600\n",
      "Epoch [22/500], Batch [20/110], Train Loss: 0.0037, Val Loss: 0.0380, LR: 0.0009956226\n",
      "Epoch [22/500], Batch [30/110], Train Loss: 0.0142, Val Loss: 0.0295, LR: 0.0009955850\n",
      "Epoch [22/500], Batch [40/110], Train Loss: 0.0023, Val Loss: 0.0390, LR: 0.0009955472\n",
      "Epoch [22/500], Batch [50/110], Train Loss: 0.1146, Val Loss: 0.0336, LR: 0.0009955093\n",
      "Epoch [22/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0293, LR: 0.0009954712\n",
      "Epoch [22/500], Batch [70/110], Train Loss: 0.0081, Val Loss: 0.0291, LR: 0.0009954330\n",
      "Epoch [22/500], Batch [80/110], Train Loss: 0.0235, Val Loss: 0.0278, LR: 0.0009953946\n",
      "Epoch [22/500], Batch [90/110], Train Loss: 0.0033, Val Loss: 0.0293, LR: 0.0009953560\n",
      "Epoch [22/500], Batch [100/110], Train Loss: 0.1259, Val Loss: 0.0340, LR: 0.0009953173\n",
      "Epoch [22/500], Batch [110/110], Train Loss: 0.0618, Val Loss: 0.0284, LR: 0.0009952784\n",
      "Epoch [23/500], Batch [10/110], Train Loss: 0.0043, Val Loss: 0.0354, LR: 0.0009952394\n",
      "Epoch [23/500], Batch [20/110], Train Loss: 0.2135, Val Loss: 0.0312, LR: 0.0009952002\n",
      "Epoch [23/500], Batch [30/110], Train Loss: 0.0046, Val Loss: 0.0293, LR: 0.0009951608\n",
      "Epoch [23/500], Batch [40/110], Train Loss: 0.0062, Val Loss: 0.0291, LR: 0.0009951213\n",
      "Epoch [23/500], Batch [50/110], Train Loss: 0.0453, Val Loss: 0.0301, LR: 0.0009950816\n",
      "Epoch [23/500], Batch [60/110], Train Loss: 0.0030, Val Loss: 0.0294, LR: 0.0009950418\n",
      "Epoch [23/500], Batch [70/110], Train Loss: 0.0103, Val Loss: 0.0310, LR: 0.0009950018\n",
      "Epoch [23/500], Batch [80/110], Train Loss: 0.1219, Val Loss: 0.0333, LR: 0.0009949616\n",
      "Epoch [23/500], Batch [90/110], Train Loss: 0.0092, Val Loss: 0.0282, LR: 0.0009949213\n",
      "Epoch [23/500], Batch [100/110], Train Loss: 0.0124, Val Loss: 0.0270, LR: 0.0009948808\n",
      "Epoch [23/500], Batch [110/110], Train Loss: 0.0049, Val Loss: 0.0271, LR: 0.0009948402\n",
      "Epoch [24/500], Batch [10/110], Train Loss: 0.0109, Val Loss: 0.0263, LR: 0.0009947994\n",
      "Epoch [24/500], Batch [20/110], Train Loss: 0.0068, Val Loss: 0.0265, LR: 0.0009947584\n",
      "Epoch [24/500], Batch [30/110], Train Loss: 0.0014, Val Loss: 0.0392, LR: 0.0009947173\n",
      "Epoch [24/500], Batch [40/110], Train Loss: 0.0195, Val Loss: 0.0277, LR: 0.0009946760\n",
      "Epoch [24/500], Batch [50/110], Train Loss: 0.0536, Val Loss: 0.0292, LR: 0.0009946346\n",
      "Epoch [24/500], Batch [60/110], Train Loss: 0.0673, Val Loss: 0.0268, LR: 0.0009945930\n",
      "Epoch [24/500], Batch [70/110], Train Loss: 0.1114, Val Loss: 0.0316, LR: 0.0009945512\n",
      "Epoch [24/500], Batch [80/110], Train Loss: 0.0032, Val Loss: 0.0315, LR: 0.0009945093\n",
      "Epoch [24/500], Batch [90/110], Train Loss: 0.0692, Val Loss: 0.0261, LR: 0.0009944672\n",
      "Epoch [24/500], Batch [100/110], Train Loss: 0.1367, Val Loss: 0.0373, LR: 0.0009944250\n",
      "Epoch [24/500], Batch [110/110], Train Loss: 0.0014, Val Loss: 0.0278, LR: 0.0009943826\n",
      "Epoch [25/500], Batch [10/110], Train Loss: 0.0093, Val Loss: 0.0261, LR: 0.0009943401\n",
      "Epoch [25/500], Batch [20/110], Train Loss: 0.0055, Val Loss: 0.0332, LR: 0.0009942973\n",
      "Epoch [25/500], Batch [30/110], Train Loss: 0.0056, Val Loss: 0.0337, LR: 0.0009942545\n",
      "Epoch [25/500], Batch [40/110], Train Loss: 0.0116, Val Loss: 0.0252, LR: 0.0009942114\n",
      "Epoch [25/500], Batch [50/110], Train Loss: 0.0056, Val Loss: 0.0321, LR: 0.0009941682\n",
      "Epoch [25/500], Batch [60/110], Train Loss: 0.0157, Val Loss: 0.0344, LR: 0.0009941249\n",
      "Epoch [25/500], Batch [70/110], Train Loss: 0.0089, Val Loss: 0.0305, LR: 0.0009940814\n",
      "Epoch [25/500], Batch [80/110], Train Loss: 0.0054, Val Loss: 0.0303, LR: 0.0009940377\n",
      "Epoch [25/500], Batch [90/110], Train Loss: 0.0087, Val Loss: 0.0279, LR: 0.0009939939\n",
      "Epoch [25/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0352, LR: 0.0009939499\n",
      "Epoch [25/500], Batch [110/110], Train Loss: 0.0121, Val Loss: 0.0281, LR: 0.0009939057\n",
      "Epoch [26/500], Batch [10/110], Train Loss: 0.0064, Val Loss: 0.0278, LR: 0.0009938614\n",
      "Epoch [26/500], Batch [20/110], Train Loss: 0.0797, Val Loss: 0.0244, LR: 0.0009938169\n",
      "Epoch [26/500], Batch [30/110], Train Loss: 0.0142, Val Loss: 0.0247, LR: 0.0009937723\n",
      "Epoch [26/500], Batch [40/110], Train Loss: 0.0929, Val Loss: 0.0336, LR: 0.0009937275\n",
      "Epoch [26/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0242, LR: 0.0009936826\n",
      "Epoch [26/500], Batch [60/110], Train Loss: 0.0075, Val Loss: 0.0237, LR: 0.0009936375\n",
      "Epoch [26/500], Batch [70/110], Train Loss: 0.0049, Val Loss: 0.0256, LR: 0.0009935922\n",
      "Epoch [26/500], Batch [80/110], Train Loss: 0.0091, Val Loss: 0.0261, LR: 0.0009935468\n",
      "Epoch [26/500], Batch [90/110], Train Loss: 0.0897, Val Loss: 0.0274, LR: 0.0009935012\n",
      "Epoch [26/500], Batch [100/110], Train Loss: 0.0039, Val Loss: 0.0275, LR: 0.0009934554\n",
      "Epoch [26/500], Batch [110/110], Train Loss: 0.0017, Val Loss: 0.0304, LR: 0.0009934095\n",
      "Epoch [27/500], Batch [10/110], Train Loss: 0.0120, Val Loss: 0.0239, LR: 0.0009933635\n",
      "Epoch [27/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0254, LR: 0.0009933173\n",
      "Epoch [27/500], Batch [30/110], Train Loss: 0.0196, Val Loss: 0.0225, LR: 0.0009932709\n",
      "Epoch [27/500], Batch [40/110], Train Loss: 0.0737, Val Loss: 0.0278, LR: 0.0009932243\n",
      "Epoch [27/500], Batch [50/110], Train Loss: 0.0011, Val Loss: 0.0343, LR: 0.0009931776\n",
      "Epoch [27/500], Batch [60/110], Train Loss: 0.0045, Val Loss: 0.0281, LR: 0.0009931308\n",
      "Epoch [27/500], Batch [70/110], Train Loss: 0.0028, Val Loss: 0.0244, LR: 0.0009930837\n",
      "Epoch [27/500], Batch [80/110], Train Loss: 0.0747, Val Loss: 0.0233, LR: 0.0009930366\n",
      "Epoch [27/500], Batch [90/110], Train Loss: 0.1078, Val Loss: 0.0328, LR: 0.0009929892\n",
      "Epoch [27/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0226, LR: 0.0009929417\n",
      "Epoch [27/500], Batch [110/110], Train Loss: 0.0662, Val Loss: 0.0238, LR: 0.0009928941\n",
      "Epoch [28/500], Batch [10/110], Train Loss: 0.2082, Val Loss: 0.0296, LR: 0.0009928463\n",
      "Epoch [28/500], Batch [20/110], Train Loss: 0.0659, Val Loss: 0.0330, LR: 0.0009927983\n",
      "Epoch [28/500], Batch [30/110], Train Loss: 0.0081, Val Loss: 0.0227, LR: 0.0009927501\n",
      "Epoch [28/500], Batch [40/110], Train Loss: 0.1385, Val Loss: 0.0227, LR: 0.0009927019\n",
      "Epoch [28/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0266, LR: 0.0009926534\n",
      "Epoch [28/500], Batch [60/110], Train Loss: 0.0250, Val Loss: 0.0219, LR: 0.0009926048\n",
      "Epoch [28/500], Batch [70/110], Train Loss: 0.0028, Val Loss: 0.0215, LR: 0.0009925560\n",
      "Epoch [28/500], Batch [80/110], Train Loss: 0.0023, Val Loss: 0.0295, LR: 0.0009925071\n",
      "Epoch [28/500], Batch [90/110], Train Loss: 0.0137, Val Loss: 0.0234, LR: 0.0009924580\n",
      "Epoch [28/500], Batch [100/110], Train Loss: 0.0587, Val Loss: 0.0219, LR: 0.0009924088\n",
      "Epoch [28/500], Batch [110/110], Train Loss: 0.0062, Val Loss: 0.0362, LR: 0.0009923593\n",
      "Epoch [29/500], Batch [10/110], Train Loss: 0.0024, Val Loss: 0.0284, LR: 0.0009923098\n",
      "Epoch [29/500], Batch [20/110], Train Loss: 0.0881, Val Loss: 0.0327, LR: 0.0009922601\n",
      "Epoch [29/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0286, LR: 0.0009922102\n",
      "Epoch [29/500], Batch [40/110], Train Loss: 0.0624, Val Loss: 0.0216, LR: 0.0009921601\n",
      "Epoch [29/500], Batch [50/110], Train Loss: 0.0338, Val Loss: 0.0217, LR: 0.0009921099\n",
      "Epoch [29/500], Batch [60/110], Train Loss: 0.0017, Val Loss: 0.0240, LR: 0.0009920596\n",
      "Epoch [29/500], Batch [70/110], Train Loss: 0.0027, Val Loss: 0.0212, LR: 0.0009920090\n",
      "Epoch [29/500], Batch [80/110], Train Loss: 0.0039, Val Loss: 0.0357, LR: 0.0009919584\n",
      "Epoch [29/500], Batch [90/110], Train Loss: 0.0054, Val Loss: 0.0285, LR: 0.0009919075\n",
      "Epoch [29/500], Batch [100/110], Train Loss: 0.0069, Val Loss: 0.0223, LR: 0.0009918565\n",
      "Epoch [29/500], Batch [110/110], Train Loss: 0.0092, Val Loss: 0.0238, LR: 0.0009918054\n",
      "Epoch [30/500], Batch [10/110], Train Loss: 0.0039, Val Loss: 0.0227, LR: 0.0009917541\n",
      "Epoch [30/500], Batch [20/110], Train Loss: 0.0782, Val Loss: 0.0213, LR: 0.0009917026\n",
      "Epoch [30/500], Batch [30/110], Train Loss: 0.0063, Val Loss: 0.0203, LR: 0.0009916510\n",
      "Epoch [30/500], Batch [40/110], Train Loss: 0.0828, Val Loss: 0.0242, LR: 0.0009915992\n",
      "Epoch [30/500], Batch [50/110], Train Loss: 0.0063, Val Loss: 0.0262, LR: 0.0009915472\n",
      "Epoch [30/500], Batch [60/110], Train Loss: 0.0058, Val Loss: 0.0241, LR: 0.0009914951\n",
      "Epoch [30/500], Batch [70/110], Train Loss: 0.1010, Val Loss: 0.0289, LR: 0.0009914428\n",
      "Epoch [30/500], Batch [80/110], Train Loss: 0.0011, Val Loss: 0.0219, LR: 0.0009913904\n",
      "Epoch [30/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0269, LR: 0.0009913378\n",
      "Epoch [30/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0200, LR: 0.0009912851\n",
      "Epoch [30/500], Batch [110/110], Train Loss: 0.0063, Val Loss: 0.0254, LR: 0.0009912322\n",
      "Epoch [31/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0222, LR: 0.0009911791\n",
      "Epoch [31/500], Batch [20/110], Train Loss: 0.0695, Val Loss: 0.0224, LR: 0.0009911259\n",
      "Epoch [31/500], Batch [30/110], Train Loss: 0.0527, Val Loss: 0.0248, LR: 0.0009910725\n",
      "Epoch [31/500], Batch [40/110], Train Loss: 0.0322, Val Loss: 0.0292, LR: 0.0009910190\n",
      "Epoch [31/500], Batch [50/110], Train Loss: 0.0063, Val Loss: 0.0255, LR: 0.0009909653\n",
      "Epoch [31/500], Batch [60/110], Train Loss: 0.0221, Val Loss: 0.0214, LR: 0.0009909114\n",
      "Epoch [31/500], Batch [70/110], Train Loss: 0.0015, Val Loss: 0.0247, LR: 0.0009908574\n",
      "Epoch [31/500], Batch [80/110], Train Loss: 0.0780, Val Loss: 0.0203, LR: 0.0009908033\n",
      "Epoch [31/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0306, LR: 0.0009907489\n",
      "Epoch [31/500], Batch [100/110], Train Loss: 0.1744, Val Loss: 0.0237, LR: 0.0009906945\n",
      "Epoch [31/500], Batch [110/110], Train Loss: 0.0033, Val Loss: 0.0195, LR: 0.0009906398\n",
      "Confusion Matrix:\n",
      "[[615  20]\n",
      " [  2 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99676   0.96850   0.98243       635\n",
      "           1    0.97735   0.99769   0.98741       865\n",
      "\n",
      "    accuracy                        0.98533      1500\n",
      "   macro avg    0.98705   0.98310   0.98492      1500\n",
      "weighted avg    0.98557   0.98533   0.98530      1500\n",
      "\n",
      "Total Errors: 22\n",
      "Index: 58, Predicted: 1, Actual: 0\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 231, Predicted: 1, Actual: 0\n",
      "Index: 319, Predicted: 1, Actual: 0\n",
      "Index: 376, Predicted: 1, Actual: 0\n",
      "Epoch 31: OK- Accuracy: 0.98533, Precision: 0.97735, Recall: 0.99769, F1: 0.98741, ROC AUC: 0.98310, AUPR (PR-AUC): 0.97642, Sensitivity: 0.99769, Specificity: 0.96850, Far: 0.031496062992125984, False Positive Rate (FPR): 0.03150, False Negative Rate (FNR): 0.00231, Runtime: 0.050 sec , Memory Usage: 312.87 MB\n",
      "Epoch [32/500], Batch [10/110], Train Loss: 0.0891, Val Loss: 0.0218, LR: 0.0009905850\n",
      "Epoch [32/500], Batch [20/110], Train Loss: 0.0050, Val Loss: 0.0194, LR: 0.0009905300\n",
      "Epoch [32/500], Batch [30/110], Train Loss: 0.0028, Val Loss: 0.0193, LR: 0.0009904749\n",
      "Epoch [32/500], Batch [40/110], Train Loss: 0.0048, Val Loss: 0.0259, LR: 0.0009904196\n",
      "Epoch [32/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0220, LR: 0.0009903642\n",
      "Epoch [32/500], Batch [60/110], Train Loss: 0.0542, Val Loss: 0.0188, LR: 0.0009903086\n",
      "Epoch [32/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0195, LR: 0.0009902529\n",
      "Epoch [32/500], Batch [80/110], Train Loss: 0.0025, Val Loss: 0.0187, LR: 0.0009901969\n",
      "Epoch [32/500], Batch [90/110], Train Loss: 0.0308, Val Loss: 0.0197, LR: 0.0009901409\n",
      "Epoch [32/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0181, LR: 0.0009900846\n",
      "Epoch [32/500], Batch [110/110], Train Loss: 0.0096, Val Loss: 0.0177, LR: 0.0009900283\n",
      "Epoch [33/500], Batch [10/110], Train Loss: 0.0036, Val Loss: 0.0191, LR: 0.0009899717\n",
      "Epoch [33/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0200, LR: 0.0009899150\n",
      "Epoch [33/500], Batch [30/110], Train Loss: 0.0097, Val Loss: 0.0177, LR: 0.0009898581\n",
      "Epoch [33/500], Batch [40/110], Train Loss: 0.0368, Val Loss: 0.0184, LR: 0.0009898011\n",
      "Epoch [33/500], Batch [50/110], Train Loss: 0.0061, Val Loss: 0.0213, LR: 0.0009897439\n",
      "Epoch [33/500], Batch [60/110], Train Loss: 0.0011, Val Loss: 0.0225, LR: 0.0009896866\n",
      "Epoch [33/500], Batch [70/110], Train Loss: 0.0033, Val Loss: 0.0175, LR: 0.0009896291\n",
      "Epoch [33/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0191, LR: 0.0009895715\n",
      "Epoch [33/500], Batch [90/110], Train Loss: 0.0790, Val Loss: 0.0200, LR: 0.0009895136\n",
      "Epoch [33/500], Batch [100/110], Train Loss: 0.0701, Val Loss: 0.0187, LR: 0.0009894557\n",
      "Epoch [33/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0181, LR: 0.0009893975\n",
      "Epoch [34/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0212, LR: 0.0009893393\n",
      "Epoch [34/500], Batch [20/110], Train Loss: 0.0143, Val Loss: 0.0186, LR: 0.0009892808\n",
      "Epoch [34/500], Batch [30/110], Train Loss: 0.0618, Val Loss: 0.0206, LR: 0.0009892222\n",
      "Epoch [34/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0194, LR: 0.0009891635\n",
      "Epoch [34/500], Batch [50/110], Train Loss: 0.0018, Val Loss: 0.0228, LR: 0.0009891045\n",
      "Epoch [34/500], Batch [60/110], Train Loss: 0.0023, Val Loss: 0.0176, LR: 0.0009890455\n",
      "Epoch [34/500], Batch [70/110], Train Loss: 0.0020, Val Loss: 0.0225, LR: 0.0009889862\n",
      "Epoch [34/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0173, LR: 0.0009889268\n",
      "Epoch [34/500], Batch [90/110], Train Loss: 0.0856, Val Loss: 0.0182, LR: 0.0009888673\n",
      "Epoch [34/500], Batch [100/110], Train Loss: 0.0094, Val Loss: 0.0186, LR: 0.0009888076\n",
      "Epoch [34/500], Batch [110/110], Train Loss: 0.0018, Val Loss: 0.0173, LR: 0.0009887477\n",
      "Epoch [35/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0176, LR: 0.0009886877\n",
      "Epoch [35/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0307, LR: 0.0009886275\n",
      "Epoch [35/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0225, LR: 0.0009885672\n",
      "Epoch [35/500], Batch [40/110], Train Loss: 0.0034, Val Loss: 0.0190, LR: 0.0009885067\n",
      "Epoch [35/500], Batch [50/110], Train Loss: 0.0121, Val Loss: 0.0204, LR: 0.0009884460\n",
      "Epoch [35/500], Batch [60/110], Train Loss: 0.0063, Val Loss: 0.0202, LR: 0.0009883852\n",
      "Epoch [35/500], Batch [70/110], Train Loss: 0.0408, Val Loss: 0.0180, LR: 0.0009883243\n",
      "Epoch [35/500], Batch [80/110], Train Loss: 0.0010, Val Loss: 0.0211, LR: 0.0009882631\n",
      "Epoch [35/500], Batch [90/110], Train Loss: 0.0522, Val Loss: 0.0176, LR: 0.0009882018\n",
      "Epoch [35/500], Batch [100/110], Train Loss: 0.0051, Val Loss: 0.0176, LR: 0.0009881404\n",
      "Epoch [35/500], Batch [110/110], Train Loss: 0.0054, Val Loss: 0.0214, LR: 0.0009880788\n",
      "Epoch [36/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0214, LR: 0.0009880170\n",
      "Epoch [36/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0351, LR: 0.0009879551\n",
      "Epoch [36/500], Batch [30/110], Train Loss: 0.0483, Val Loss: 0.0171, LR: 0.0009878931\n",
      "Epoch [36/500], Batch [40/110], Train Loss: 0.0545, Val Loss: 0.0203, LR: 0.0009878308\n",
      "Epoch [36/500], Batch [50/110], Train Loss: 0.0018, Val Loss: 0.0176, LR: 0.0009877684\n",
      "Epoch [36/500], Batch [60/110], Train Loss: 0.0229, Val Loss: 0.0229, LR: 0.0009877059\n",
      "Epoch [36/500], Batch [70/110], Train Loss: 0.0078, Val Loss: 0.0261, LR: 0.0009876432\n",
      "Epoch [36/500], Batch [80/110], Train Loss: 0.0082, Val Loss: 0.0295, LR: 0.0009875803\n",
      "Epoch [36/500], Batch [90/110], Train Loss: 0.0139, Val Loss: 0.0178, LR: 0.0009875173\n",
      "Epoch [36/500], Batch [100/110], Train Loss: 0.0063, Val Loss: 0.0195, LR: 0.0009874541\n",
      "Epoch [36/500], Batch [110/110], Train Loss: 0.0068, Val Loss: 0.0172, LR: 0.0009873908\n",
      "Epoch [37/500], Batch [10/110], Train Loss: 0.0279, Val Loss: 0.0179, LR: 0.0009873273\n",
      "Epoch [37/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0182, LR: 0.0009872637\n",
      "Epoch [37/500], Batch [30/110], Train Loss: 0.0123, Val Loss: 0.0212, LR: 0.0009871999\n",
      "Epoch [37/500], Batch [40/110], Train Loss: 0.0271, Val Loss: 0.0175, LR: 0.0009871359\n",
      "Epoch [37/500], Batch [50/110], Train Loss: 0.0062, Val Loss: 0.0166, LR: 0.0009870718\n",
      "Epoch [37/500], Batch [60/110], Train Loss: 0.0099, Val Loss: 0.0232, LR: 0.0009870075\n",
      "Epoch [37/500], Batch [70/110], Train Loss: 0.0044, Val Loss: 0.0163, LR: 0.0009869431\n",
      "Epoch [37/500], Batch [80/110], Train Loss: 0.0011, Val Loss: 0.0242, LR: 0.0009868785\n",
      "Epoch [37/500], Batch [90/110], Train Loss: 0.0202, Val Loss: 0.0164, LR: 0.0009868137\n",
      "Epoch [37/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0206, LR: 0.0009867488\n",
      "Epoch [37/500], Batch [110/110], Train Loss: 0.1012, Val Loss: 0.0260, LR: 0.0009866838\n",
      "Epoch [38/500], Batch [10/110], Train Loss: 0.0757, Val Loss: 0.0171, LR: 0.0009866185\n",
      "Epoch [38/500], Batch [20/110], Train Loss: 0.0516, Val Loss: 0.0166, LR: 0.0009865532\n",
      "Epoch [38/500], Batch [30/110], Train Loss: 0.0038, Val Loss: 0.0267, LR: 0.0009864876\n",
      "Epoch [38/500], Batch [40/110], Train Loss: 0.0101, Val Loss: 0.0192, LR: 0.0009864219\n",
      "Epoch [38/500], Batch [50/110], Train Loss: 0.0318, Val Loss: 0.0222, LR: 0.0009863561\n",
      "Epoch [38/500], Batch [60/110], Train Loss: 0.0107, Val Loss: 0.0163, LR: 0.0009862901\n",
      "Epoch [38/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0185, LR: 0.0009862239\n",
      "Epoch [38/500], Batch [80/110], Train Loss: 0.0283, Val Loss: 0.0158, LR: 0.0009861576\n",
      "Epoch [38/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0164, LR: 0.0009860911\n",
      "Epoch [38/500], Batch [100/110], Train Loss: 0.0058, Val Loss: 0.0158, LR: 0.0009860245\n",
      "Epoch [38/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0153, LR: 0.0009859577\n",
      "Epoch [39/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0165, LR: 0.0009858908\n",
      "Epoch [39/500], Batch [20/110], Train Loss: 0.0031, Val Loss: 0.0168, LR: 0.0009858237\n",
      "Epoch [39/500], Batch [30/110], Train Loss: 0.0429, Val Loss: 0.0213, LR: 0.0009857564\n",
      "Epoch [39/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0343, LR: 0.0009856890\n",
      "Epoch [39/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0192, LR: 0.0009856214\n",
      "Epoch [39/500], Batch [60/110], Train Loss: 0.0178, Val Loss: 0.0186, LR: 0.0009855537\n",
      "Epoch [39/500], Batch [70/110], Train Loss: 0.0108, Val Loss: 0.0163, LR: 0.0009854858\n",
      "Epoch [39/500], Batch [80/110], Train Loss: 0.0377, Val Loss: 0.0161, LR: 0.0009854177\n",
      "Epoch [39/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0199, LR: 0.0009853495\n",
      "Epoch [39/500], Batch [100/110], Train Loss: 0.0164, Val Loss: 0.0155, LR: 0.0009852812\n",
      "Epoch [39/500], Batch [110/110], Train Loss: 0.0060, Val Loss: 0.0150, LR: 0.0009852127\n",
      "Epoch [40/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0150, LR: 0.0009851440\n",
      "Epoch [40/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0158, LR: 0.0009850752\n",
      "Epoch [40/500], Batch [30/110], Train Loss: 0.0077, Val Loss: 0.0150, LR: 0.0009850062\n",
      "Epoch [40/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0236, LR: 0.0009849370\n",
      "Epoch [40/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0179, LR: 0.0009848677\n",
      "Epoch [40/500], Batch [60/110], Train Loss: 0.0288, Val Loss: 0.0178, LR: 0.0009847983\n",
      "Epoch [40/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0153, LR: 0.0009847287\n",
      "Epoch [40/500], Batch [80/110], Train Loss: 0.0104, Val Loss: 0.0160, LR: 0.0009846589\n",
      "Epoch [40/500], Batch [90/110], Train Loss: 0.0017, Val Loss: 0.0153, LR: 0.0009845890\n",
      "Epoch [40/500], Batch [100/110], Train Loss: 0.0368, Val Loss: 0.0156, LR: 0.0009845189\n",
      "Epoch [40/500], Batch [110/110], Train Loss: 0.0049, Val Loss: 0.0234, LR: 0.0009844487\n",
      "Epoch [41/500], Batch [10/110], Train Loss: 0.0117, Val Loss: 0.0161, LR: 0.0009843783\n",
      "Epoch [41/500], Batch [20/110], Train Loss: 0.0247, Val Loss: 0.0174, LR: 0.0009843077\n",
      "Epoch [41/500], Batch [30/110], Train Loss: 0.0069, Val Loss: 0.0243, LR: 0.0009842370\n",
      "Epoch [41/500], Batch [40/110], Train Loss: 0.0381, Val Loss: 0.0152, LR: 0.0009841662\n",
      "Epoch [41/500], Batch [50/110], Train Loss: 0.0013, Val Loss: 0.0149, LR: 0.0009840951\n",
      "Epoch [41/500], Batch [60/110], Train Loss: 0.0058, Val Loss: 0.0157, LR: 0.0009840240\n",
      "Epoch [41/500], Batch [70/110], Train Loss: 0.0170, Val Loss: 0.0145, LR: 0.0009839526\n",
      "Epoch [41/500], Batch [80/110], Train Loss: 0.0098, Val Loss: 0.0150, LR: 0.0009838811\n",
      "Epoch [41/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0197, LR: 0.0009838095\n",
      "Epoch [41/500], Batch [100/110], Train Loss: 0.0073, Val Loss: 0.0229, LR: 0.0009837377\n",
      "Epoch [41/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0260, LR: 0.0009836657\n",
      "Confusion Matrix:\n",
      "[[613  22]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99837   0.96535   0.98159       635\n",
      "           1    0.97517   0.99884   0.98686       865\n",
      "\n",
      "    accuracy                        0.98467      1500\n",
      "   macro avg    0.98677   0.98210   0.98422      1500\n",
      "weighted avg    0.98499   0.98467   0.98463      1500\n",
      "\n",
      "Total Errors: 23\n",
      "Index: 58, Predicted: 1, Actual: 0\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 231, Predicted: 1, Actual: 0\n",
      "Index: 319, Predicted: 1, Actual: 0\n",
      "Index: 376, Predicted: 1, Actual: 0\n",
      "Epoch 41: OK- Accuracy: 0.98467, Precision: 0.97517, Recall: 0.99884, F1: 0.98686, ROC AUC: 0.98210, AUPR (PR-AUC): 0.97471, Sensitivity: 0.99884, Specificity: 0.96535, Far: 0.03464566929133858, False Positive Rate (FPR): 0.03465, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 312.87 MB\n",
      "Epoch [42/500], Batch [10/110], Train Loss: 0.0511, Val Loss: 0.0170, LR: 0.0009835936\n",
      "Epoch [42/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0197, LR: 0.0009835214\n",
      "Epoch [42/500], Batch [30/110], Train Loss: 0.0170, Val Loss: 0.0142, LR: 0.0009834489\n",
      "Epoch [42/500], Batch [40/110], Train Loss: 0.0507, Val Loss: 0.0143, LR: 0.0009833763\n",
      "Epoch [42/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0161, LR: 0.0009833036\n",
      "Epoch [42/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0157, LR: 0.0009832307\n",
      "Epoch [42/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0202, LR: 0.0009831577\n",
      "Epoch [42/500], Batch [80/110], Train Loss: 0.0021, Val Loss: 0.0245, LR: 0.0009830845\n",
      "Epoch [42/500], Batch [90/110], Train Loss: 0.0254, Val Loss: 0.0163, LR: 0.0009830111\n",
      "Epoch [42/500], Batch [100/110], Train Loss: 0.0366, Val Loss: 0.0141, LR: 0.0009829376\n",
      "Epoch [42/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0142, LR: 0.0009828639\n",
      "Epoch [43/500], Batch [10/110], Train Loss: 0.0439, Val Loss: 0.0198, LR: 0.0009827901\n",
      "Epoch [43/500], Batch [20/110], Train Loss: 0.0099, Val Loss: 0.0187, LR: 0.0009827161\n",
      "Epoch [43/500], Batch [30/110], Train Loss: 0.0038, Val Loss: 0.0136, LR: 0.0009826420\n",
      "Epoch [43/500], Batch [40/110], Train Loss: 0.0018, Val Loss: 0.0137, LR: 0.0009825677\n",
      "Epoch [43/500], Batch [50/110], Train Loss: 0.0163, Val Loss: 0.0138, LR: 0.0009824932\n",
      "Epoch [43/500], Batch [60/110], Train Loss: 0.0348, Val Loss: 0.0140, LR: 0.0009824186\n",
      "Epoch [43/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0146, LR: 0.0009823438\n",
      "Epoch [43/500], Batch [80/110], Train Loss: 0.0310, Val Loss: 0.0139, LR: 0.0009822689\n",
      "Epoch [43/500], Batch [90/110], Train Loss: 0.0297, Val Loss: 0.0142, LR: 0.0009821938\n",
      "Epoch [43/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0147, LR: 0.0009821186\n",
      "Epoch [43/500], Batch [110/110], Train Loss: 0.0082, Val Loss: 0.0155, LR: 0.0009820432\n",
      "Epoch [44/500], Batch [10/110], Train Loss: 0.0585, Val Loss: 0.0139, LR: 0.0009819677\n",
      "Epoch [44/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0211, LR: 0.0009818920\n",
      "Epoch [44/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0159, LR: 0.0009818161\n",
      "Epoch [44/500], Batch [40/110], Train Loss: 0.0151, Val Loss: 0.0135, LR: 0.0009817401\n",
      "Epoch [44/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0138, LR: 0.0009816640\n",
      "Epoch [44/500], Batch [60/110], Train Loss: 0.0539, Val Loss: 0.0257, LR: 0.0009815876\n",
      "Epoch [44/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0207, LR: 0.0009815112\n",
      "Epoch [44/500], Batch [80/110], Train Loss: 0.0053, Val Loss: 0.0157, LR: 0.0009814345\n",
      "Epoch [44/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0134, LR: 0.0009813577\n",
      "Epoch [44/500], Batch [100/110], Train Loss: 0.0077, Val Loss: 0.0142, LR: 0.0009812808\n",
      "Epoch [44/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0259, LR: 0.0009812037\n",
      "Epoch [45/500], Batch [10/110], Train Loss: 0.0243, Val Loss: 0.0140, LR: 0.0009811264\n",
      "Epoch [45/500], Batch [20/110], Train Loss: 0.0231, Val Loss: 0.0160, LR: 0.0009810490\n",
      "Epoch [45/500], Batch [30/110], Train Loss: 0.0041, Val Loss: 0.0225, LR: 0.0009809715\n",
      "Epoch [45/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0241, LR: 0.0009808938\n",
      "Epoch [45/500], Batch [50/110], Train Loss: 0.0131, Val Loss: 0.0180, LR: 0.0009808159\n",
      "Epoch [45/500], Batch [60/110], Train Loss: 0.0013, Val Loss: 0.0138, LR: 0.0009807378\n",
      "Epoch [45/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0203, LR: 0.0009806597\n",
      "Epoch [45/500], Batch [80/110], Train Loss: 0.0381, Val Loss: 0.0166, LR: 0.0009805813\n",
      "Epoch [45/500], Batch [90/110], Train Loss: 0.0390, Val Loss: 0.0139, LR: 0.0009805028\n",
      "Epoch [45/500], Batch [100/110], Train Loss: 0.0012, Val Loss: 0.0132, LR: 0.0009804242\n",
      "Epoch [45/500], Batch [110/110], Train Loss: 0.0749, Val Loss: 0.0137, LR: 0.0009803454\n",
      "Epoch [46/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0178, LR: 0.0009802664\n",
      "Epoch [46/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0130, LR: 0.0009801873\n",
      "Epoch [46/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0138, LR: 0.0009801080\n",
      "Epoch [46/500], Batch [40/110], Train Loss: 0.0024, Val Loss: 0.0235, LR: 0.0009800286\n",
      "Epoch [46/500], Batch [50/110], Train Loss: 0.0071, Val Loss: 0.0142, LR: 0.0009799490\n",
      "Epoch [46/500], Batch [60/110], Train Loss: 0.0278, Val Loss: 0.0155, LR: 0.0009798693\n",
      "Epoch [46/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0161, LR: 0.0009797894\n",
      "Epoch [46/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0153, LR: 0.0009797094\n",
      "Epoch [46/500], Batch [90/110], Train Loss: 0.0648, Val Loss: 0.0145, LR: 0.0009796292\n",
      "Epoch [46/500], Batch [100/110], Train Loss: 0.1217, Val Loss: 0.0175, LR: 0.0009795488\n",
      "Epoch [46/500], Batch [110/110], Train Loss: 0.0082, Val Loss: 0.0137, LR: 0.0009794683\n",
      "Epoch [47/500], Batch [10/110], Train Loss: 0.0234, Val Loss: 0.0179, LR: 0.0009793876\n",
      "Epoch [47/500], Batch [20/110], Train Loss: 0.0081, Val Loss: 0.0143, LR: 0.0009793068\n",
      "Epoch [47/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0133, LR: 0.0009792258\n",
      "Epoch [47/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0177, LR: 0.0009791447\n",
      "Epoch [47/500], Batch [50/110], Train Loss: 0.0168, Val Loss: 0.0127, LR: 0.0009790634\n",
      "Epoch [47/500], Batch [60/110], Train Loss: 0.0193, Val Loss: 0.0127, LR: 0.0009789820\n",
      "Epoch [47/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0128, LR: 0.0009789004\n",
      "Epoch [47/500], Batch [80/110], Train Loss: 0.0373, Val Loss: 0.0172, LR: 0.0009788186\n",
      "Epoch [47/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0126, LR: 0.0009787367\n",
      "Epoch [47/500], Batch [100/110], Train Loss: 0.1629, Val Loss: 0.0186, LR: 0.0009786547\n",
      "Epoch [47/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0148, LR: 0.0009785725\n",
      "Epoch [48/500], Batch [10/110], Train Loss: 0.0034, Val Loss: 0.0153, LR: 0.0009784901\n",
      "Epoch [48/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0170, LR: 0.0009784076\n",
      "Epoch [48/500], Batch [30/110], Train Loss: 0.0080, Val Loss: 0.0151, LR: 0.0009783249\n",
      "Epoch [48/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0199, LR: 0.0009782421\n",
      "Epoch [48/500], Batch [50/110], Train Loss: 0.0161, Val Loss: 0.0155, LR: 0.0009781591\n",
      "Epoch [48/500], Batch [60/110], Train Loss: 0.0554, Val Loss: 0.0130, LR: 0.0009780760\n",
      "Epoch [48/500], Batch [70/110], Train Loss: 0.0134, Val Loss: 0.0138, LR: 0.0009779927\n",
      "Epoch [48/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0133, LR: 0.0009779092\n",
      "Epoch [48/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0156, LR: 0.0009778256\n",
      "Epoch [48/500], Batch [100/110], Train Loss: 0.0424, Val Loss: 0.0160, LR: 0.0009777419\n",
      "Epoch [48/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0133, LR: 0.0009776579\n",
      "Epoch [49/500], Batch [10/110], Train Loss: 0.0098, Val Loss: 0.0125, LR: 0.0009775739\n",
      "Epoch [49/500], Batch [20/110], Train Loss: 0.0157, Val Loss: 0.0121, LR: 0.0009774897\n",
      "Epoch [49/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0153, LR: 0.0009774053\n",
      "Epoch [49/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0123, LR: 0.0009773208\n",
      "Epoch [49/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0142, LR: 0.0009772361\n",
      "Epoch [49/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0125, LR: 0.0009771513\n",
      "Epoch [49/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0170, LR: 0.0009770663\n",
      "Epoch [49/500], Batch [80/110], Train Loss: 0.0851, Val Loss: 0.0160, LR: 0.0009769811\n",
      "Epoch [49/500], Batch [90/110], Train Loss: 0.0014, Val Loss: 0.0164, LR: 0.0009768958\n",
      "Epoch [49/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0223, LR: 0.0009768104\n",
      "Epoch [49/500], Batch [110/110], Train Loss: 0.0504, Val Loss: 0.0148, LR: 0.0009767248\n",
      "Epoch [50/500], Batch [10/110], Train Loss: 0.0729, Val Loss: 0.0128, LR: 0.0009766390\n",
      "Epoch [50/500], Batch [20/110], Train Loss: 0.0315, Val Loss: 0.0126, LR: 0.0009765531\n",
      "Epoch [50/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0126, LR: 0.0009764670\n",
      "Epoch [50/500], Batch [40/110], Train Loss: 0.0229, Val Loss: 0.0132, LR: 0.0009763808\n",
      "Epoch [50/500], Batch [50/110], Train Loss: 0.0019, Val Loss: 0.0128, LR: 0.0009762944\n",
      "Epoch [50/500], Batch [60/110], Train Loss: 0.0022, Val Loss: 0.0136, LR: 0.0009762079\n",
      "Epoch [50/500], Batch [70/110], Train Loss: 0.0915, Val Loss: 0.0184, LR: 0.0009761212\n",
      "Epoch [50/500], Batch [80/110], Train Loss: 0.0024, Val Loss: 0.0136, LR: 0.0009760344\n",
      "Epoch [50/500], Batch [90/110], Train Loss: 0.0203, Val Loss: 0.0146, LR: 0.0009759474\n",
      "Epoch [50/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0138, LR: 0.0009758603\n",
      "Epoch [50/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0131, LR: 0.0009757730\n",
      "Epoch [51/500], Batch [10/110], Train Loss: 0.0023, Val Loss: 0.0124, LR: 0.0009756855\n",
      "Epoch [51/500], Batch [20/110], Train Loss: 0.0024, Val Loss: 0.0230, LR: 0.0009755979\n",
      "Epoch [51/500], Batch [30/110], Train Loss: 0.0023, Val Loss: 0.0239, LR: 0.0009755102\n",
      "Epoch [51/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0139, LR: 0.0009754223\n",
      "Epoch [51/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0128, LR: 0.0009753342\n",
      "Epoch [51/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0154, LR: 0.0009752460\n",
      "Epoch [51/500], Batch [70/110], Train Loss: 0.0086, Val Loss: 0.0195, LR: 0.0009751576\n",
      "Epoch [51/500], Batch [80/110], Train Loss: 0.0024, Val Loss: 0.0123, LR: 0.0009750691\n",
      "Epoch [51/500], Batch [90/110], Train Loss: 0.0254, Val Loss: 0.0274, LR: 0.0009749804\n",
      "Epoch [51/500], Batch [100/110], Train Loss: 0.0061, Val Loss: 0.0133, LR: 0.0009748916\n",
      "Epoch [51/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0236, LR: 0.0009748026\n",
      "Confusion Matrix:\n",
      "[[614  21]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99837   0.96693   0.98240       635\n",
      "           1    0.97627   0.99884   0.98743       865\n",
      "\n",
      "    accuracy                        0.98533      1500\n",
      "   macro avg    0.98732   0.98289   0.98491      1500\n",
      "weighted avg    0.98563   0.98533   0.98530      1500\n",
      "\n",
      "Total Errors: 22\n",
      "Index: 58, Predicted: 1, Actual: 0\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 231, Predicted: 1, Actual: 0\n",
      "Index: 319, Predicted: 1, Actual: 0\n",
      "Index: 376, Predicted: 1, Actual: 0\n",
      "Epoch 51: OK- Accuracy: 0.98533, Precision: 0.97627, Recall: 0.99884, F1: 0.98743, ROC AUC: 0.98289, AUPR (PR-AUC): 0.97581, Sensitivity: 0.99884, Specificity: 0.96693, Far: 0.03307086614173228, False Positive Rate (FPR): 0.03307, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 312.87 MB\n",
      "Epoch [52/500], Batch [10/110], Train Loss: 0.0917, Val Loss: 0.0156, LR: 0.0009747135\n",
      "Epoch [52/500], Batch [20/110], Train Loss: 0.0019, Val Loss: 0.0128, LR: 0.0009746242\n",
      "Epoch [52/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0222, LR: 0.0009745347\n",
      "Epoch [52/500], Batch [40/110], Train Loss: 0.0436, Val Loss: 0.0122, LR: 0.0009744451\n",
      "Epoch [52/500], Batch [50/110], Train Loss: 0.0187, Val Loss: 0.0122, LR: 0.0009743554\n",
      "Epoch [52/500], Batch [60/110], Train Loss: 0.0069, Val Loss: 0.0160, LR: 0.0009742655\n",
      "Epoch [52/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0128, LR: 0.0009741754\n",
      "Epoch [52/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0143, LR: 0.0009740852\n",
      "Epoch [52/500], Batch [90/110], Train Loss: 0.0162, Val Loss: 0.0129, LR: 0.0009739948\n",
      "Epoch [52/500], Batch [100/110], Train Loss: 0.0384, Val Loss: 0.0121, LR: 0.0009739043\n",
      "Epoch [52/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0148, LR: 0.0009738137\n",
      "Epoch [53/500], Batch [10/110], Train Loss: 0.1169, Val Loss: 0.0122, LR: 0.0009737228\n",
      "Epoch [53/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0120, LR: 0.0009736319\n",
      "Epoch [53/500], Batch [30/110], Train Loss: 0.0037, Val Loss: 0.0131, LR: 0.0009735407\n",
      "Epoch [53/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0133, LR: 0.0009734495\n",
      "Epoch [53/500], Batch [50/110], Train Loss: 0.0065, Val Loss: 0.0116, LR: 0.0009733580\n",
      "Epoch [53/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0126, LR: 0.0009732664\n",
      "Epoch [53/500], Batch [70/110], Train Loss: 0.0692, Val Loss: 0.0181, LR: 0.0009731747\n",
      "Epoch [53/500], Batch [80/110], Train Loss: 0.0283, Val Loss: 0.0118, LR: 0.0009730828\n",
      "Epoch [53/500], Batch [90/110], Train Loss: 0.0423, Val Loss: 0.0136, LR: 0.0009729908\n",
      "Epoch [53/500], Batch [100/110], Train Loss: 0.0029, Val Loss: 0.0132, LR: 0.0009728986\n",
      "Epoch [53/500], Batch [110/110], Train Loss: 0.0034, Val Loss: 0.0114, LR: 0.0009728062\n",
      "Epoch [54/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0118, LR: 0.0009727137\n",
      "Epoch [54/500], Batch [20/110], Train Loss: 0.0310, Val Loss: 0.0131, LR: 0.0009726211\n",
      "Epoch [54/500], Batch [30/110], Train Loss: 0.0084, Val Loss: 0.0133, LR: 0.0009725283\n",
      "Epoch [54/500], Batch [40/110], Train Loss: 0.0678, Val Loss: 0.0125, LR: 0.0009724353\n",
      "Epoch [54/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0117, LR: 0.0009723422\n",
      "Epoch [54/500], Batch [60/110], Train Loss: 0.0319, Val Loss: 0.0122, LR: 0.0009722489\n",
      "Epoch [54/500], Batch [70/110], Train Loss: 0.0497, Val Loss: 0.0140, LR: 0.0009721555\n",
      "Epoch [54/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0113, LR: 0.0009720619\n",
      "Epoch [54/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0123, LR: 0.0009719682\n",
      "Epoch [54/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0161, LR: 0.0009718743\n",
      "Epoch [54/500], Batch [110/110], Train Loss: 0.0406, Val Loss: 0.0120, LR: 0.0009717803\n",
      "Epoch [55/500], Batch [10/110], Train Loss: 0.0017, Val Loss: 0.0115, LR: 0.0009716861\n",
      "Epoch [55/500], Batch [20/110], Train Loss: 0.0115, Val Loss: 0.0118, LR: 0.0009715918\n",
      "Epoch [55/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0186, LR: 0.0009714973\n",
      "Epoch [55/500], Batch [40/110], Train Loss: 0.0054, Val Loss: 0.0108, LR: 0.0009714027\n",
      "Epoch [55/500], Batch [50/110], Train Loss: 0.0016, Val Loss: 0.0120, LR: 0.0009713079\n",
      "Epoch [55/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0112, LR: 0.0009712130\n",
      "Epoch [55/500], Batch [70/110], Train Loss: 0.0371, Val Loss: 0.0147, LR: 0.0009711179\n",
      "Epoch [55/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0115, LR: 0.0009710226\n",
      "Epoch [55/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0106, LR: 0.0009709272\n",
      "Epoch [55/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0108, LR: 0.0009708317\n",
      "Epoch [55/500], Batch [110/110], Train Loss: 0.0042, Val Loss: 0.0129, LR: 0.0009707360\n",
      "Epoch [56/500], Batch [10/110], Train Loss: 0.0359, Val Loss: 0.0104, LR: 0.0009706401\n",
      "Epoch [56/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0106, LR: 0.0009705441\n",
      "Epoch [56/500], Batch [30/110], Train Loss: 0.0012, Val Loss: 0.0126, LR: 0.0009704480\n",
      "Epoch [56/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0147, LR: 0.0009703517\n",
      "Epoch [56/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0009702552\n",
      "Epoch [56/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0107, LR: 0.0009701586\n",
      "Epoch [56/500], Batch [70/110], Train Loss: 0.0409, Val Loss: 0.0106, LR: 0.0009700618\n",
      "Epoch [56/500], Batch [80/110], Train Loss: 0.0153, Val Loss: 0.0105, LR: 0.0009699649\n",
      "Epoch [56/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0127, LR: 0.0009698678\n",
      "Epoch [56/500], Batch [100/110], Train Loss: 0.0593, Val Loss: 0.0113, LR: 0.0009697706\n",
      "Epoch [56/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0320, LR: 0.0009696733\n",
      "Epoch [57/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0118, LR: 0.0009695757\n",
      "Epoch [57/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0114, LR: 0.0009694781\n",
      "Epoch [57/500], Batch [30/110], Train Loss: 0.0072, Val Loss: 0.0113, LR: 0.0009693802\n",
      "Epoch [57/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0133, LR: 0.0009692823\n",
      "Epoch [57/500], Batch [50/110], Train Loss: 0.0176, Val Loss: 0.0128, LR: 0.0009691841\n",
      "Epoch [57/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0117, LR: 0.0009690859\n",
      "Epoch [57/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0109, LR: 0.0009689874\n",
      "Epoch [57/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0181, LR: 0.0009688888\n",
      "Epoch [57/500], Batch [90/110], Train Loss: 0.0052, Val Loss: 0.0114, LR: 0.0009687901\n",
      "Epoch [57/500], Batch [100/110], Train Loss: 0.0525, Val Loss: 0.0168, LR: 0.0009686912\n",
      "Epoch [57/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0140, LR: 0.0009685922\n",
      "Epoch [58/500], Batch [10/110], Train Loss: 0.0035, Val Loss: 0.0114, LR: 0.0009684930\n",
      "Epoch [58/500], Batch [20/110], Train Loss: 0.0775, Val Loss: 0.0139, LR: 0.0009683937\n",
      "Epoch [58/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0140, LR: 0.0009682942\n",
      "Epoch [58/500], Batch [40/110], Train Loss: 0.0218, Val Loss: 0.0110, LR: 0.0009681945\n",
      "Epoch [58/500], Batch [50/110], Train Loss: 0.0465, Val Loss: 0.0167, LR: 0.0009680947\n",
      "Epoch [58/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0236, LR: 0.0009679948\n",
      "Epoch [58/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0109, LR: 0.0009678947\n",
      "Epoch [58/500], Batch [80/110], Train Loss: 0.0132, Val Loss: 0.0110, LR: 0.0009677945\n",
      "Epoch [58/500], Batch [90/110], Train Loss: 0.0013, Val Loss: 0.0205, LR: 0.0009676941\n",
      "Epoch [58/500], Batch [100/110], Train Loss: 0.0319, Val Loss: 0.0117, LR: 0.0009675935\n",
      "Epoch [58/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0138, LR: 0.0009674928\n",
      "Epoch [59/500], Batch [10/110], Train Loss: 0.0015, Val Loss: 0.0112, LR: 0.0009673920\n",
      "Epoch [59/500], Batch [20/110], Train Loss: 0.0077, Val Loss: 0.0164, LR: 0.0009672910\n",
      "Epoch [59/500], Batch [30/110], Train Loss: 0.0288, Val Loss: 0.0123, LR: 0.0009671898\n",
      "Epoch [59/500], Batch [40/110], Train Loss: 0.0142, Val Loss: 0.0148, LR: 0.0009670885\n",
      "Epoch [59/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0119, LR: 0.0009669871\n",
      "Epoch [59/500], Batch [60/110], Train Loss: 0.0454, Val Loss: 0.0113, LR: 0.0009668855\n",
      "Epoch [59/500], Batch [70/110], Train Loss: 0.0430, Val Loss: 0.0130, LR: 0.0009667837\n",
      "Epoch [59/500], Batch [80/110], Train Loss: 0.0249, Val Loss: 0.0162, LR: 0.0009666818\n",
      "Epoch [59/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0185, LR: 0.0009665798\n",
      "Epoch [59/500], Batch [100/110], Train Loss: 0.0520, Val Loss: 0.0119, LR: 0.0009664776\n",
      "Epoch [59/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0112, LR: 0.0009663752\n",
      "Epoch [60/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0143, LR: 0.0009662727\n",
      "Epoch [60/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0133, LR: 0.0009661700\n",
      "Epoch [60/500], Batch [30/110], Train Loss: 0.0130, Val Loss: 0.0114, LR: 0.0009660672\n",
      "Epoch [60/500], Batch [40/110], Train Loss: 0.0036, Val Loss: 0.0128, LR: 0.0009659643\n",
      "Epoch [60/500], Batch [50/110], Train Loss: 0.0031, Val Loss: 0.0134, LR: 0.0009658612\n",
      "Epoch [60/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0120, LR: 0.0009657579\n",
      "Epoch [60/500], Batch [70/110], Train Loss: 0.0029, Val Loss: 0.0119, LR: 0.0009656545\n",
      "Epoch [60/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0105, LR: 0.0009655509\n",
      "Epoch [60/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0110, LR: 0.0009654472\n",
      "Epoch [60/500], Batch [100/110], Train Loss: 0.0117, Val Loss: 0.0110, LR: 0.0009653434\n",
      "Epoch [60/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0117, LR: 0.0009652394\n",
      "Epoch [61/500], Batch [10/110], Train Loss: 0.0592, Val Loss: 0.0103, LR: 0.0009651352\n",
      "Epoch [61/500], Batch [20/110], Train Loss: 0.0016, Val Loss: 0.0113, LR: 0.0009650309\n",
      "Epoch [61/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0242, LR: 0.0009649264\n",
      "Epoch [61/500], Batch [40/110], Train Loss: 0.0038, Val Loss: 0.0113, LR: 0.0009648218\n",
      "Epoch [61/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0104, LR: 0.0009647171\n",
      "Epoch [61/500], Batch [60/110], Train Loss: 0.0363, Val Loss: 0.0119, LR: 0.0009646122\n",
      "Epoch [61/500], Batch [70/110], Train Loss: 0.0024, Val Loss: 0.0102, LR: 0.0009645071\n",
      "Epoch [61/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0102, LR: 0.0009644019\n",
      "Epoch [61/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0105, LR: 0.0009642965\n",
      "Epoch [61/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0124, LR: 0.0009641910\n",
      "Epoch [61/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0111, LR: 0.0009640854\n",
      "Confusion Matrix:\n",
      "[[621  14]\n",
      " [  2 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99679   0.97795   0.98728       635\n",
      "           1    0.98404   0.99769   0.99082       865\n",
      "\n",
      "    accuracy                        0.98933      1500\n",
      "   macro avg    0.99041   0.98782   0.98905      1500\n",
      "weighted avg    0.98944   0.98933   0.98932      1500\n",
      "\n",
      "Total Errors: 16\n",
      "Index: 58, Predicted: 1, Actual: 0\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 319, Predicted: 1, Actual: 0\n",
      "Index: 576, Predicted: 1, Actual: 0\n",
      "Index: 693, Predicted: 1, Actual: 0\n",
      "Epoch 61: OK- Accuracy: 0.98933, Precision: 0.98404, Recall: 0.99769, F1: 0.99082, ROC AUC: 0.98782, AUPR (PR-AUC): 0.98309, Sensitivity: 0.99769, Specificity: 0.97795, Far: 0.02204724409448819, False Positive Rate (FPR): 0.02205, False Negative Rate (FNR): 0.00231, Runtime: 0.032 sec , Memory Usage: 312.87 MB\n",
      "Epoch [62/500], Batch [10/110], Train Loss: 0.0263, Val Loss: 0.0140, LR: 0.0009639795\n",
      "Epoch [62/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0232, LR: 0.0009638736\n",
      "Epoch [62/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0142, LR: 0.0009637675\n",
      "Epoch [62/500], Batch [40/110], Train Loss: 0.1149, Val Loss: 0.0116, LR: 0.0009636612\n",
      "Epoch [62/500], Batch [50/110], Train Loss: 0.0152, Val Loss: 0.0105, LR: 0.0009635548\n",
      "Epoch [62/500], Batch [60/110], Train Loss: 0.0087, Val Loss: 0.0104, LR: 0.0009634482\n",
      "Epoch [62/500], Batch [70/110], Train Loss: 0.0240, Val Loss: 0.0105, LR: 0.0009633415\n",
      "Epoch [62/500], Batch [80/110], Train Loss: 0.0012, Val Loss: 0.0120, LR: 0.0009632347\n",
      "Epoch [62/500], Batch [90/110], Train Loss: 0.0257, Val Loss: 0.0113, LR: 0.0009631277\n",
      "Epoch [62/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0126, LR: 0.0009630205\n",
      "Epoch [62/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0136, LR: 0.0009629132\n",
      "Epoch [63/500], Batch [10/110], Train Loss: 0.0090, Val Loss: 0.0195, LR: 0.0009628058\n",
      "Epoch [63/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0176, LR: 0.0009626982\n",
      "Epoch [63/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0132, LR: 0.0009625904\n",
      "Epoch [63/500], Batch [40/110], Train Loss: 0.0197, Val Loss: 0.0104, LR: 0.0009624825\n",
      "Epoch [63/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0103, LR: 0.0009623745\n",
      "Epoch [63/500], Batch [60/110], Train Loss: 0.0523, Val Loss: 0.0161, LR: 0.0009622662\n",
      "Epoch [63/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0222, LR: 0.0009621579\n",
      "Epoch [63/500], Batch [80/110], Train Loss: 0.0048, Val Loss: 0.0114, LR: 0.0009620494\n",
      "Epoch [63/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0114, LR: 0.0009619408\n",
      "Epoch [63/500], Batch [100/110], Train Loss: 0.0506, Val Loss: 0.0112, LR: 0.0009618320\n",
      "Epoch [63/500], Batch [110/110], Train Loss: 0.0196, Val Loss: 0.0106, LR: 0.0009617230\n",
      "Epoch [64/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0108, LR: 0.0009616139\n",
      "Epoch [64/500], Batch [20/110], Train Loss: 0.0027, Val Loss: 0.0106, LR: 0.0009615047\n",
      "Epoch [64/500], Batch [30/110], Train Loss: 0.0802, Val Loss: 0.0144, LR: 0.0009613953\n",
      "Epoch [64/500], Batch [40/110], Train Loss: 0.0229, Val Loss: 0.0109, LR: 0.0009612857\n",
      "Epoch [64/500], Batch [50/110], Train Loss: 0.1633, Val Loss: 0.0147, LR: 0.0009611760\n",
      "Epoch [64/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0188, LR: 0.0009610662\n",
      "Epoch [64/500], Batch [70/110], Train Loss: 0.0016, Val Loss: 0.0111, LR: 0.0009609562\n",
      "Epoch [64/500], Batch [80/110], Train Loss: 0.0302, Val Loss: 0.0105, LR: 0.0009608461\n",
      "Epoch [64/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0177, LR: 0.0009607358\n",
      "Epoch [64/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0105, LR: 0.0009606253\n",
      "Epoch [64/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0102, LR: 0.0009605148\n",
      "Epoch [65/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0108, LR: 0.0009604040\n",
      "Epoch [65/500], Batch [20/110], Train Loss: 0.0009, Val Loss: 0.0122, LR: 0.0009602932\n",
      "Epoch [65/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0128, LR: 0.0009601821\n",
      "Epoch [65/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0114, LR: 0.0009600709\n",
      "Epoch [65/500], Batch [50/110], Train Loss: 0.0702, Val Loss: 0.0109, LR: 0.0009599596\n",
      "Epoch [65/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0117, LR: 0.0009598481\n",
      "Epoch [65/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0245, LR: 0.0009597365\n",
      "Epoch [65/500], Batch [80/110], Train Loss: 0.0536, Val Loss: 0.0177, LR: 0.0009596247\n",
      "Epoch [65/500], Batch [90/110], Train Loss: 0.0014, Val Loss: 0.0119, LR: 0.0009595128\n",
      "Epoch [65/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0169, LR: 0.0009594008\n",
      "Epoch [65/500], Batch [110/110], Train Loss: 0.0149, Val Loss: 0.0159, LR: 0.0009592885\n",
      "Epoch [66/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0113, LR: 0.0009591762\n",
      "Epoch [66/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0130, LR: 0.0009590637\n",
      "Epoch [66/500], Batch [30/110], Train Loss: 0.0627, Val Loss: 0.0117, LR: 0.0009589510\n",
      "Epoch [66/500], Batch [40/110], Train Loss: 0.0110, Val Loss: 0.0125, LR: 0.0009588382\n",
      "Epoch [66/500], Batch [50/110], Train Loss: 0.0132, Val Loss: 0.0117, LR: 0.0009587252\n",
      "Epoch [66/500], Batch [60/110], Train Loss: 0.0171, Val Loss: 0.0109, LR: 0.0009586121\n",
      "Epoch [66/500], Batch [70/110], Train Loss: 0.0688, Val Loss: 0.0105, LR: 0.0009584989\n",
      "Epoch [66/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0111, LR: 0.0009583855\n",
      "Epoch [66/500], Batch [90/110], Train Loss: 0.0335, Val Loss: 0.0134, LR: 0.0009582719\n",
      "Epoch [66/500], Batch [100/110], Train Loss: 0.0165, Val Loss: 0.0102, LR: 0.0009581582\n",
      "Epoch [66/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0136, LR: 0.0009580444\n",
      "Epoch [67/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0009579304\n",
      "Epoch [67/500], Batch [20/110], Train Loss: 0.0242, Val Loss: 0.0097, LR: 0.0009578162\n",
      "Epoch [67/500], Batch [30/110], Train Loss: 0.0120, Val Loss: 0.0102, LR: 0.0009577020\n",
      "Epoch [67/500], Batch [40/110], Train Loss: 0.0258, Val Loss: 0.0097, LR: 0.0009575875\n",
      "Epoch [67/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0098, LR: 0.0009574729\n",
      "Epoch [67/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0102, LR: 0.0009573582\n",
      "Epoch [67/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0121, LR: 0.0009572433\n",
      "Epoch [67/500], Batch [80/110], Train Loss: 0.0128, Val Loss: 0.0098, LR: 0.0009571283\n",
      "Epoch [67/500], Batch [90/110], Train Loss: 0.0016, Val Loss: 0.0101, LR: 0.0009570131\n",
      "Epoch [67/500], Batch [100/110], Train Loss: 0.0301, Val Loss: 0.0096, LR: 0.0009568978\n",
      "Epoch [67/500], Batch [110/110], Train Loss: 0.0720, Val Loss: 0.0095, LR: 0.0009567823\n",
      "Epoch [68/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0107, LR: 0.0009566667\n",
      "Epoch [68/500], Batch [20/110], Train Loss: 0.0211, Val Loss: 0.0103, LR: 0.0009565510\n",
      "Epoch [68/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0121, LR: 0.0009564350\n",
      "Epoch [68/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0096, LR: 0.0009563190\n",
      "Epoch [68/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0009562028\n",
      "Epoch [68/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0167, LR: 0.0009560864\n",
      "Epoch [68/500], Batch [70/110], Train Loss: 0.0032, Val Loss: 0.0111, LR: 0.0009559699\n",
      "Epoch [68/500], Batch [80/110], Train Loss: 0.0138, Val Loss: 0.0121, LR: 0.0009558533\n",
      "Epoch [68/500], Batch [90/110], Train Loss: 0.0035, Val Loss: 0.0111, LR: 0.0009557365\n",
      "Epoch [68/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0104, LR: 0.0009556195\n",
      "Epoch [68/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0094, LR: 0.0009555025\n",
      "Epoch [69/500], Batch [10/110], Train Loss: 0.0013, Val Loss: 0.0096, LR: 0.0009553852\n",
      "Epoch [69/500], Batch [20/110], Train Loss: 0.0504, Val Loss: 0.0106, LR: 0.0009552678\n",
      "Epoch [69/500], Batch [30/110], Train Loss: 0.0135, Val Loss: 0.0093, LR: 0.0009551503\n",
      "Epoch [69/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0104, LR: 0.0009550326\n",
      "Epoch [69/500], Batch [50/110], Train Loss: 0.0243, Val Loss: 0.0093, LR: 0.0009549148\n",
      "Epoch [69/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0113, LR: 0.0009547968\n",
      "Epoch [69/500], Batch [70/110], Train Loss: 0.0888, Val Loss: 0.0221, LR: 0.0009546787\n",
      "Epoch [69/500], Batch [80/110], Train Loss: 0.0022, Val Loss: 0.0094, LR: 0.0009545605\n",
      "Epoch [69/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0009544420\n",
      "Epoch [69/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0132, LR: 0.0009543235\n",
      "Epoch [69/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0095, LR: 0.0009542048\n",
      "Epoch [70/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0106, LR: 0.0009540859\n",
      "Epoch [70/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0009539669\n",
      "Epoch [70/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0009538478\n",
      "Epoch [70/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0116, LR: 0.0009537285\n",
      "Epoch [70/500], Batch [50/110], Train Loss: 0.0392, Val Loss: 0.0103, LR: 0.0009536091\n",
      "Epoch [70/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0087, LR: 0.0009534895\n",
      "Epoch [70/500], Batch [70/110], Train Loss: 0.0233, Val Loss: 0.0180, LR: 0.0009533698\n",
      "Epoch [70/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0009532499\n",
      "Epoch [70/500], Batch [90/110], Train Loss: 0.0424, Val Loss: 0.0121, LR: 0.0009531299\n",
      "Epoch [70/500], Batch [100/110], Train Loss: 0.0940, Val Loss: 0.0136, LR: 0.0009530097\n",
      "Epoch [70/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0093, LR: 0.0009528894\n",
      "Epoch [71/500], Batch [10/110], Train Loss: 0.0553, Val Loss: 0.0094, LR: 0.0009527689\n",
      "Epoch [71/500], Batch [20/110], Train Loss: 0.0399, Val Loss: 0.0109, LR: 0.0009526483\n",
      "Epoch [71/500], Batch [30/110], Train Loss: 0.0123, Val Loss: 0.0094, LR: 0.0009525276\n",
      "Epoch [71/500], Batch [40/110], Train Loss: 0.0931, Val Loss: 0.0166, LR: 0.0009524067\n",
      "Epoch [71/500], Batch [50/110], Train Loss: 0.0087, Val Loss: 0.0115, LR: 0.0009522856\n",
      "Epoch [71/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0151, LR: 0.0009521644\n",
      "Epoch [71/500], Batch [70/110], Train Loss: 0.0379, Val Loss: 0.0095, LR: 0.0009520431\n",
      "Epoch [71/500], Batch [80/110], Train Loss: 0.0282, Val Loss: 0.0090, LR: 0.0009519216\n",
      "Epoch [71/500], Batch [90/110], Train Loss: 0.0059, Val Loss: 0.0098, LR: 0.0009518000\n",
      "Epoch [71/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0149, LR: 0.0009516782\n",
      "Epoch [71/500], Batch [110/110], Train Loss: 0.0410, Val Loss: 0.0105, LR: 0.0009515563\n",
      "Confusion Matrix:\n",
      "[[619  16]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99839   0.97480   0.98645       635\n",
      "           1    0.98182   0.99884   0.99026       865\n",
      "\n",
      "    accuracy                        0.98867      1500\n",
      "   macro avg    0.99010   0.98682   0.98836      1500\n",
      "weighted avg    0.98883   0.98867   0.98865      1500\n",
      "\n",
      "Total Errors: 17\n",
      "Index: 58, Predicted: 1, Actual: 0\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 319, Predicted: 1, Actual: 0\n",
      "Index: 576, Predicted: 1, Actual: 0\n",
      "Index: 693, Predicted: 1, Actual: 0\n",
      "Epoch 71: OK- Accuracy: 0.98867, Precision: 0.98182, Recall: 0.99884, F1: 0.99026, ROC AUC: 0.98682, AUPR (PR-AUC): 0.98135, Sensitivity: 0.99884, Specificity: 0.97480, Far: 0.025196850393700787, False Positive Rate (FPR): 0.02520, False Negative Rate (FNR): 0.00116, Runtime: 0.032 sec , Memory Usage: 312.88 MB\n",
      "Epoch [72/500], Batch [10/110], Train Loss: 0.0054, Val Loss: 0.0105, LR: 0.0009514342\n",
      "Epoch [72/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0087, LR: 0.0009513120\n",
      "Epoch [72/500], Batch [30/110], Train Loss: 0.0334, Val Loss: 0.0116, LR: 0.0009511897\n",
      "Epoch [72/500], Batch [40/110], Train Loss: 0.0636, Val Loss: 0.0104, LR: 0.0009510672\n",
      "Epoch [72/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0088, LR: 0.0009509445\n",
      "Epoch [72/500], Batch [60/110], Train Loss: 0.0729, Val Loss: 0.0095, LR: 0.0009508217\n",
      "Epoch [72/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0009506988\n",
      "Epoch [72/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0087, LR: 0.0009505757\n",
      "Epoch [72/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0088, LR: 0.0009504525\n",
      "Epoch [72/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0125, LR: 0.0009503291\n",
      "Epoch [72/500], Batch [110/110], Train Loss: 0.0053, Val Loss: 0.0119, LR: 0.0009502056\n",
      "Epoch [73/500], Batch [10/110], Train Loss: 0.0415, Val Loss: 0.0086, LR: 0.0009500819\n",
      "Epoch [73/500], Batch [20/110], Train Loss: 0.0232, Val Loss: 0.0153, LR: 0.0009499581\n",
      "Epoch [73/500], Batch [30/110], Train Loss: 0.0183, Val Loss: 0.0092, LR: 0.0009498342\n",
      "Epoch [73/500], Batch [40/110], Train Loss: 0.0025, Val Loss: 0.0099, LR: 0.0009497101\n",
      "Epoch [73/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0092, LR: 0.0009495858\n",
      "Epoch [73/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0120, LR: 0.0009494614\n",
      "Epoch [73/500], Batch [70/110], Train Loss: 0.0254, Val Loss: 0.0086, LR: 0.0009493369\n",
      "Epoch [73/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0009492122\n",
      "Epoch [73/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0084, LR: 0.0009490874\n",
      "Epoch [73/500], Batch [100/110], Train Loss: 0.0201, Val Loss: 0.0170, LR: 0.0009489624\n",
      "Epoch [73/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0009488373\n",
      "Epoch [74/500], Batch [10/110], Train Loss: 0.0416, Val Loss: 0.0097, LR: 0.0009487121\n",
      "Epoch [74/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0086, LR: 0.0009485866\n",
      "Epoch [74/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0236, LR: 0.0009484611\n",
      "Epoch [74/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0009483354\n",
      "Epoch [74/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0125, LR: 0.0009482096\n",
      "Epoch [74/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0009480836\n",
      "Epoch [74/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0088, LR: 0.0009479575\n",
      "Epoch [74/500], Batch [80/110], Train Loss: 0.0643, Val Loss: 0.0121, LR: 0.0009478312\n",
      "Epoch [74/500], Batch [90/110], Train Loss: 0.0595, Val Loss: 0.0137, LR: 0.0009477048\n",
      "Epoch [74/500], Batch [100/110], Train Loss: 0.0028, Val Loss: 0.0097, LR: 0.0009475782\n",
      "Epoch [74/500], Batch [110/110], Train Loss: 0.0227, Val Loss: 0.0161, LR: 0.0009474515\n",
      "Epoch [75/500], Batch [10/110], Train Loss: 0.0361, Val Loss: 0.0152, LR: 0.0009473247\n",
      "Epoch [75/500], Batch [20/110], Train Loss: 0.0780, Val Loss: 0.0227, LR: 0.0009471977\n",
      "Epoch [75/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0096, LR: 0.0009470705\n",
      "Epoch [75/500], Batch [40/110], Train Loss: 0.0033, Val Loss: 0.0155, LR: 0.0009469432\n",
      "Epoch [75/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0138, LR: 0.0009468158\n",
      "Epoch [75/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0009466882\n",
      "Epoch [75/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0128, LR: 0.0009465605\n",
      "Epoch [75/500], Batch [80/110], Train Loss: 0.0041, Val Loss: 0.0108, LR: 0.0009464327\n",
      "Epoch [75/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0097, LR: 0.0009463047\n",
      "Epoch [75/500], Batch [100/110], Train Loss: 0.0104, Val Loss: 0.0092, LR: 0.0009461765\n",
      "Epoch [75/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0093, LR: 0.0009460482\n",
      "Epoch [76/500], Batch [10/110], Train Loss: 0.0798, Val Loss: 0.0118, LR: 0.0009459198\n",
      "Epoch [76/500], Batch [20/110], Train Loss: 0.0152, Val Loss: 0.0089, LR: 0.0009457912\n",
      "Epoch [76/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0009456625\n",
      "Epoch [76/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0116, LR: 0.0009455336\n",
      "Epoch [76/500], Batch [50/110], Train Loss: 0.0153, Val Loss: 0.0093, LR: 0.0009454046\n",
      "Epoch [76/500], Batch [60/110], Train Loss: 0.0092, Val Loss: 0.0139, LR: 0.0009452755\n",
      "Epoch [76/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0142, LR: 0.0009451462\n",
      "Epoch [76/500], Batch [80/110], Train Loss: 0.0109, Val Loss: 0.0091, LR: 0.0009450167\n",
      "Epoch [76/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0009448871\n",
      "Epoch [76/500], Batch [100/110], Train Loss: 0.0543, Val Loss: 0.0144, LR: 0.0009447574\n",
      "Epoch [76/500], Batch [110/110], Train Loss: 0.0018, Val Loss: 0.0092, LR: 0.0009446275\n",
      "Epoch [77/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0103, LR: 0.0009444975\n",
      "Epoch [77/500], Batch [20/110], Train Loss: 0.0088, Val Loss: 0.0096, LR: 0.0009443674\n",
      "Epoch [77/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0116, LR: 0.0009442371\n",
      "Epoch [77/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0009441066\n",
      "Epoch [77/500], Batch [50/110], Train Loss: 0.0046, Val Loss: 0.0085, LR: 0.0009439760\n",
      "Epoch [77/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0119, LR: 0.0009438453\n",
      "Epoch [77/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0097, LR: 0.0009437144\n",
      "Epoch [77/500], Batch [80/110], Train Loss: 0.0977, Val Loss: 0.0106, LR: 0.0009435834\n",
      "Epoch [77/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0146, LR: 0.0009434522\n",
      "Epoch [77/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0085, LR: 0.0009433209\n",
      "Epoch [77/500], Batch [110/110], Train Loss: 0.0018, Val Loss: 0.0086, LR: 0.0009431895\n",
      "Epoch [78/500], Batch [10/110], Train Loss: 0.0325, Val Loss: 0.0085, LR: 0.0009430579\n",
      "Epoch [78/500], Batch [20/110], Train Loss: 0.0085, Val Loss: 0.0103, LR: 0.0009429262\n",
      "Epoch [78/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0108, LR: 0.0009427943\n",
      "Epoch [78/500], Batch [40/110], Train Loss: 0.0092, Val Loss: 0.0082, LR: 0.0009426623\n",
      "Epoch [78/500], Batch [50/110], Train Loss: 0.0201, Val Loss: 0.0091, LR: 0.0009425301\n",
      "Epoch [78/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0122, LR: 0.0009423978\n",
      "Epoch [78/500], Batch [70/110], Train Loss: 0.0072, Val Loss: 0.0102, LR: 0.0009422654\n",
      "Epoch [78/500], Batch [80/110], Train Loss: 0.0108, Val Loss: 0.0080, LR: 0.0009421328\n",
      "Epoch [78/500], Batch [90/110], Train Loss: 0.0061, Val Loss: 0.0110, LR: 0.0009420000\n",
      "Epoch [78/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0081, LR: 0.0009418672\n",
      "Epoch [78/500], Batch [110/110], Train Loss: 0.0714, Val Loss: 0.0106, LR: 0.0009417342\n",
      "Epoch [79/500], Batch [10/110], Train Loss: 0.0150, Val Loss: 0.0104, LR: 0.0009416010\n",
      "Epoch [79/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0190, LR: 0.0009414677\n",
      "Epoch [79/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0093, LR: 0.0009413343\n",
      "Epoch [79/500], Batch [40/110], Train Loss: 0.0487, Val Loss: 0.0166, LR: 0.0009412007\n",
      "Epoch [79/500], Batch [50/110], Train Loss: 0.0886, Val Loss: 0.0091, LR: 0.0009410669\n",
      "Epoch [79/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0088, LR: 0.0009409331\n",
      "Epoch [79/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0097, LR: 0.0009407990\n",
      "Epoch [79/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0009406649\n",
      "Epoch [79/500], Batch [90/110], Train Loss: 0.0020, Val Loss: 0.0081, LR: 0.0009405306\n",
      "Epoch [79/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0009403962\n",
      "Epoch [79/500], Batch [110/110], Train Loss: 0.0233, Val Loss: 0.0083, LR: 0.0009402616\n",
      "Epoch [80/500], Batch [10/110], Train Loss: 0.0312, Val Loss: 0.0077, LR: 0.0009401268\n",
      "Epoch [80/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0009399920\n",
      "Epoch [80/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0009398570\n",
      "Epoch [80/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0081, LR: 0.0009397218\n",
      "Epoch [80/500], Batch [50/110], Train Loss: 0.1112, Val Loss: 0.0134, LR: 0.0009395865\n",
      "Epoch [80/500], Batch [60/110], Train Loss: 0.0029, Val Loss: 0.0147, LR: 0.0009394511\n",
      "Epoch [80/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0009393155\n",
      "Epoch [80/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0009391798\n",
      "Epoch [80/500], Batch [90/110], Train Loss: 0.0254, Val Loss: 0.0084, LR: 0.0009390439\n",
      "Epoch [80/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0009389079\n",
      "Epoch [80/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0009387718\n",
      "Epoch [81/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0009386355\n",
      "Epoch [81/500], Batch [20/110], Train Loss: 0.0542, Val Loss: 0.0127, LR: 0.0009384991\n",
      "Epoch [81/500], Batch [30/110], Train Loss: 0.0305, Val Loss: 0.0099, LR: 0.0009383625\n",
      "Epoch [81/500], Batch [40/110], Train Loss: 0.0359, Val Loss: 0.0093, LR: 0.0009382258\n",
      "Epoch [81/500], Batch [50/110], Train Loss: 0.0205, Val Loss: 0.0077, LR: 0.0009380890\n",
      "Epoch [81/500], Batch [60/110], Train Loss: 0.0617, Val Loss: 0.0098, LR: 0.0009379520\n",
      "Epoch [81/500], Batch [70/110], Train Loss: 0.0181, Val Loss: 0.0082, LR: 0.0009378149\n",
      "Epoch [81/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0079, LR: 0.0009376776\n",
      "Epoch [81/500], Batch [90/110], Train Loss: 0.0741, Val Loss: 0.0126, LR: 0.0009375402\n",
      "Epoch [81/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0078, LR: 0.0009374026\n",
      "Epoch [81/500], Batch [110/110], Train Loss: 0.0099, Val Loss: 0.0111, LR: 0.0009372649\n",
      "Confusion Matrix:\n",
      "[[634   1]\n",
      " [ 12 853]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98142   0.99843   0.98985       635\n",
      "           1    0.99883   0.98613   0.99244       865\n",
      "\n",
      "    accuracy                        0.99133      1500\n",
      "   macro avg    0.99013   0.99228   0.99114      1500\n",
      "weighted avg    0.99146   0.99133   0.99134      1500\n",
      "\n",
      "Total Errors: 13\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 267, Predicted: 0, Actual: 1\n",
      "Index: 284, Predicted: 0, Actual: 1\n",
      "Index: 317, Predicted: 0, Actual: 1\n",
      "Index: 525, Predicted: 0, Actual: 1\n",
      "Epoch 81: OK- Accuracy: 0.99133, Precision: 0.99883, Recall: 0.98613, F1: 0.99244, ROC AUC: 0.99228, AUPR (PR-AUC): 0.99297, Sensitivity: 0.98613, Specificity: 0.99843, Far: 0.0015748031496062992, False Positive Rate (FPR): 0.00157, False Negative Rate (FNR): 0.01387, Runtime: 0.036 sec , Memory Usage: 312.90 MB\n",
      "Epoch [82/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0272, LR: 0.0009371271\n",
      "Epoch [82/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0327, LR: 0.0009369891\n",
      "Epoch [82/500], Batch [30/110], Train Loss: 0.0090, Val Loss: 0.0087, LR: 0.0009368510\n",
      "Epoch [82/500], Batch [40/110], Train Loss: 0.0145, Val Loss: 0.0118, LR: 0.0009367127\n",
      "Epoch [82/500], Batch [50/110], Train Loss: 0.0047, Val Loss: 0.0118, LR: 0.0009365743\n",
      "Epoch [82/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0082, LR: 0.0009364358\n",
      "Epoch [82/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0009362971\n",
      "Epoch [82/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0009361583\n",
      "Epoch [82/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0113, LR: 0.0009360193\n",
      "Epoch [82/500], Batch [100/110], Train Loss: 0.0091, Val Loss: 0.0080, LR: 0.0009358802\n",
      "Epoch [82/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0009357410\n",
      "Epoch [83/500], Batch [10/110], Train Loss: 0.0009, Val Loss: 0.0076, LR: 0.0009356016\n",
      "Epoch [83/500], Batch [20/110], Train Loss: 0.0143, Val Loss: 0.0074, LR: 0.0009354620\n",
      "Epoch [83/500], Batch [30/110], Train Loss: 0.0060, Val Loss: 0.0086, LR: 0.0009353224\n",
      "Epoch [83/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0243, LR: 0.0009351826\n",
      "Epoch [83/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0171, LR: 0.0009350426\n",
      "Epoch [83/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0089, LR: 0.0009349025\n",
      "Epoch [83/500], Batch [70/110], Train Loss: 0.0034, Val Loss: 0.0079, LR: 0.0009347623\n",
      "Epoch [83/500], Batch [80/110], Train Loss: 0.0311, Val Loss: 0.0077, LR: 0.0009346219\n",
      "Epoch [83/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0009344814\n",
      "Epoch [83/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0102, LR: 0.0009343408\n",
      "Epoch [83/500], Batch [110/110], Train Loss: 0.0188, Val Loss: 0.0079, LR: 0.0009342000\n",
      "Epoch [84/500], Batch [10/110], Train Loss: 0.0036, Val Loss: 0.0143, LR: 0.0009340591\n",
      "Epoch [84/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0078, LR: 0.0009339180\n",
      "Epoch [84/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0091, LR: 0.0009337768\n",
      "Epoch [84/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0009336354\n",
      "Epoch [84/500], Batch [50/110], Train Loss: 0.0269, Val Loss: 0.0090, LR: 0.0009334940\n",
      "Epoch [84/500], Batch [60/110], Train Loss: 0.0236, Val Loss: 0.0090, LR: 0.0009333523\n",
      "Epoch [84/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0009332106\n",
      "Epoch [84/500], Batch [80/110], Train Loss: 0.0052, Val Loss: 0.0097, LR: 0.0009330687\n",
      "Epoch [84/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0163, LR: 0.0009329266\n",
      "Epoch [84/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0121, LR: 0.0009327844\n",
      "Epoch [84/500], Batch [110/110], Train Loss: 0.0029, Val Loss: 0.0074, LR: 0.0009326421\n",
      "Epoch [85/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0084, LR: 0.0009324996\n",
      "Epoch [85/500], Batch [20/110], Train Loss: 0.0140, Val Loss: 0.0073, LR: 0.0009323570\n",
      "Epoch [85/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0083, LR: 0.0009322143\n",
      "Epoch [85/500], Batch [40/110], Train Loss: 0.0036, Val Loss: 0.0100, LR: 0.0009320714\n",
      "Epoch [85/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0084, LR: 0.0009319284\n",
      "Epoch [85/500], Batch [60/110], Train Loss: 0.0156, Val Loss: 0.0076, LR: 0.0009317852\n",
      "Epoch [85/500], Batch [70/110], Train Loss: 0.0183, Val Loss: 0.0083, LR: 0.0009316419\n",
      "Epoch [85/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0009314985\n",
      "Epoch [85/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0009313549\n",
      "Epoch [85/500], Batch [100/110], Train Loss: 0.0027, Val Loss: 0.0071, LR: 0.0009312112\n",
      "Epoch [85/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0113, LR: 0.0009310673\n",
      "Epoch [86/500], Batch [10/110], Train Loss: 0.0021, Val Loss: 0.0148, LR: 0.0009309233\n",
      "Epoch [86/500], Batch [20/110], Train Loss: 0.0251, Val Loss: 0.0081, LR: 0.0009307792\n",
      "Epoch [86/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0111, LR: 0.0009306349\n",
      "Epoch [86/500], Batch [40/110], Train Loss: 0.0772, Val Loss: 0.0078, LR: 0.0009304905\n",
      "Epoch [86/500], Batch [50/110], Train Loss: 0.0842, Val Loss: 0.0085, LR: 0.0009303459\n",
      "Epoch [86/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0092, LR: 0.0009302012\n",
      "Epoch [86/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0086, LR: 0.0009300564\n",
      "Epoch [86/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0009299114\n",
      "Epoch [86/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0139, LR: 0.0009297663\n",
      "Epoch [86/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0009296211\n",
      "Epoch [86/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0068, LR: 0.0009294757\n",
      "Epoch [87/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0009293302\n",
      "Epoch [87/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0075, LR: 0.0009291845\n",
      "Epoch [87/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0083, LR: 0.0009290387\n",
      "Epoch [87/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0201, LR: 0.0009288928\n",
      "Epoch [87/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0202, LR: 0.0009287467\n",
      "Epoch [87/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0198, LR: 0.0009286005\n",
      "Epoch [87/500], Batch [70/110], Train Loss: 0.0023, Val Loss: 0.0073, LR: 0.0009284541\n",
      "Epoch [87/500], Batch [80/110], Train Loss: 0.1278, Val Loss: 0.0149, LR: 0.0009283076\n",
      "Epoch [87/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0009281610\n",
      "Epoch [87/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0075, LR: 0.0009280142\n",
      "Epoch [87/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0075, LR: 0.0009278673\n",
      "Epoch [88/500], Batch [10/110], Train Loss: 0.0194, Val Loss: 0.0116, LR: 0.0009277203\n",
      "Epoch [88/500], Batch [20/110], Train Loss: 0.0082, Val Loss: 0.0074, LR: 0.0009275731\n",
      "Epoch [88/500], Batch [30/110], Train Loss: 0.0733, Val Loss: 0.0083, LR: 0.0009274258\n",
      "Epoch [88/500], Batch [40/110], Train Loss: 0.0032, Val Loss: 0.0071, LR: 0.0009272783\n",
      "Epoch [88/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0009271307\n",
      "Epoch [88/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0109, LR: 0.0009269830\n",
      "Epoch [88/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0009268351\n",
      "Epoch [88/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0009266871\n",
      "Epoch [88/500], Batch [90/110], Train Loss: 0.3079, Val Loss: 0.0121, LR: 0.0009265390\n",
      "Epoch [88/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0080, LR: 0.0009263907\n",
      "Epoch [88/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0009262423\n",
      "Epoch [89/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0009260937\n",
      "Epoch [89/500], Batch [20/110], Train Loss: 0.0038, Val Loss: 0.0081, LR: 0.0009259450\n",
      "Epoch [89/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0087, LR: 0.0009257962\n",
      "Epoch [89/500], Batch [40/110], Train Loss: 0.0387, Val Loss: 0.0072, LR: 0.0009256472\n",
      "Epoch [89/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0069, LR: 0.0009254981\n",
      "Epoch [89/500], Batch [60/110], Train Loss: 0.0020, Val Loss: 0.0152, LR: 0.0009253489\n",
      "Epoch [89/500], Batch [70/110], Train Loss: 0.0669, Val Loss: 0.0140, LR: 0.0009251995\n",
      "Epoch [89/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0009250500\n",
      "Epoch [89/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0071, LR: 0.0009249003\n",
      "Epoch [89/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0009247505\n",
      "Epoch [89/500], Batch [110/110], Train Loss: 0.0170, Val Loss: 0.0066, LR: 0.0009246006\n",
      "Epoch [90/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0009244505\n",
      "Epoch [90/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0077, LR: 0.0009243003\n",
      "Epoch [90/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0073, LR: 0.0009241500\n",
      "Epoch [90/500], Batch [40/110], Train Loss: 0.0114, Val Loss: 0.0071, LR: 0.0009239995\n",
      "Epoch [90/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0009238489\n",
      "Epoch [90/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0070, LR: 0.0009236981\n",
      "Epoch [90/500], Batch [70/110], Train Loss: 0.0010, Val Loss: 0.0087, LR: 0.0009235472\n",
      "Epoch [90/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0009233962\n",
      "Epoch [90/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0071, LR: 0.0009232451\n",
      "Epoch [90/500], Batch [100/110], Train Loss: 0.0279, Val Loss: 0.0067, LR: 0.0009230938\n",
      "Epoch [90/500], Batch [110/110], Train Loss: 0.0069, Val Loss: 0.0067, LR: 0.0009229423\n",
      "Epoch [91/500], Batch [10/110], Train Loss: 0.0730, Val Loss: 0.0083, LR: 0.0009227908\n",
      "Epoch [91/500], Batch [20/110], Train Loss: 0.0018, Val Loss: 0.0064, LR: 0.0009226390\n",
      "Epoch [91/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0067, LR: 0.0009224872\n",
      "Epoch [91/500], Batch [40/110], Train Loss: 0.0229, Val Loss: 0.0065, LR: 0.0009223352\n",
      "Epoch [91/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0066, LR: 0.0009221831\n",
      "Epoch [91/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0064, LR: 0.0009220309\n",
      "Epoch [91/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0115, LR: 0.0009218785\n",
      "Epoch [91/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0128, LR: 0.0009217260\n",
      "Epoch [91/500], Batch [90/110], Train Loss: 0.0023, Val Loss: 0.0190, LR: 0.0009215733\n",
      "Epoch [91/500], Batch [100/110], Train Loss: 0.0047, Val Loss: 0.0110, LR: 0.0009214205\n",
      "Epoch [91/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0089, LR: 0.0009212676\n",
      "Confusion Matrix:\n",
      "[[634   1]\n",
      " [  8 857]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98754   0.99843   0.99295       635\n",
      "           1    0.99883   0.99075   0.99478       865\n",
      "\n",
      "    accuracy                        0.99400      1500\n",
      "   macro avg    0.99319   0.99459   0.99386      1500\n",
      "weighted avg    0.99405   0.99400   0.99400      1500\n",
      "\n",
      "Total Errors: 9\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 267, Predicted: 0, Actual: 1\n",
      "Index: 284, Predicted: 0, Actual: 1\n",
      "Index: 525, Predicted: 0, Actual: 1\n",
      "Index: 626, Predicted: 0, Actual: 1\n",
      "Epoch 91: OK- Accuracy: 0.99400, Precision: 0.99883, Recall: 0.99075, F1: 0.99478, ROC AUC: 0.99459, AUPR (PR-AUC): 0.99493, Sensitivity: 0.99075, Specificity: 0.99843, Far: 0.0015748031496062992, False Positive Rate (FPR): 0.00157, False Negative Rate (FNR): 0.00925, Runtime: 0.033 sec , Memory Usage: 312.91 MB\n",
      "Epoch [92/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0076, LR: 0.0009211145\n",
      "Epoch [92/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0080, LR: 0.0009209613\n",
      "Epoch [92/500], Batch [30/110], Train Loss: 0.0284, Val Loss: 0.0080, LR: 0.0009208080\n",
      "Epoch [92/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0009206545\n",
      "Epoch [92/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0009205009\n",
      "Epoch [92/500], Batch [60/110], Train Loss: 0.0385, Val Loss: 0.0065, LR: 0.0009203471\n",
      "Epoch [92/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0064, LR: 0.0009201933\n",
      "Epoch [92/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0064, LR: 0.0009200392\n",
      "Epoch [92/500], Batch [90/110], Train Loss: 0.1224, Val Loss: 0.0113, LR: 0.0009198851\n",
      "Epoch [92/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0068, LR: 0.0009197308\n",
      "Epoch [92/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0096, LR: 0.0009195764\n",
      "Epoch [93/500], Batch [10/110], Train Loss: 0.0023, Val Loss: 0.0076, LR: 0.0009194218\n",
      "Epoch [93/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0073, LR: 0.0009192671\n",
      "Epoch [93/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0009191123\n",
      "Epoch [93/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0009189573\n",
      "Epoch [93/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0009188022\n",
      "Epoch [93/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0009186470\n",
      "Epoch [93/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0009184916\n",
      "Epoch [93/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0075, LR: 0.0009183361\n",
      "Epoch [93/500], Batch [90/110], Train Loss: 0.2085, Val Loss: 0.0071, LR: 0.0009181805\n",
      "Epoch [93/500], Batch [100/110], Train Loss: 0.0689, Val Loss: 0.0089, LR: 0.0009180247\n",
      "Epoch [93/500], Batch [110/110], Train Loss: 0.0030, Val Loss: 0.0060, LR: 0.0009178688\n",
      "Epoch [94/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0009177128\n",
      "Epoch [94/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0009175566\n",
      "Epoch [94/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0009174003\n",
      "Epoch [94/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0078, LR: 0.0009172439\n",
      "Epoch [94/500], Batch [50/110], Train Loss: 0.0057, Val Loss: 0.0069, LR: 0.0009170873\n",
      "Epoch [94/500], Batch [60/110], Train Loss: 0.0023, Val Loss: 0.0058, LR: 0.0009169306\n",
      "Epoch [94/500], Batch [70/110], Train Loss: 0.0311, Val Loss: 0.0079, LR: 0.0009167737\n",
      "Epoch [94/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0009166167\n",
      "Epoch [94/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0009164596\n",
      "Epoch [94/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0009163024\n",
      "Epoch [94/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0055, LR: 0.0009161450\n",
      "Epoch [95/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0009159875\n",
      "Epoch [95/500], Batch [20/110], Train Loss: 0.0009, Val Loss: 0.0094, LR: 0.0009158298\n",
      "Epoch [95/500], Batch [30/110], Train Loss: 0.0234, Val Loss: 0.0060, LR: 0.0009156720\n",
      "Epoch [95/500], Batch [40/110], Train Loss: 0.0415, Val Loss: 0.0113, LR: 0.0009155141\n",
      "Epoch [95/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0056, LR: 0.0009153560\n",
      "Epoch [95/500], Batch [60/110], Train Loss: 0.0314, Val Loss: 0.0111, LR: 0.0009151978\n",
      "Epoch [95/500], Batch [70/110], Train Loss: 0.0283, Val Loss: 0.0054, LR: 0.0009150395\n",
      "Epoch [95/500], Batch [80/110], Train Loss: 0.0178, Val Loss: 0.0054, LR: 0.0009148811\n",
      "Epoch [95/500], Batch [90/110], Train Loss: 0.0137, Val Loss: 0.0061, LR: 0.0009147225\n",
      "Epoch [95/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0009145637\n",
      "Epoch [95/500], Batch [110/110], Train Loss: 0.0094, Val Loss: 0.0053, LR: 0.0009144049\n",
      "Epoch [96/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0009142459\n",
      "Epoch [96/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0195, LR: 0.0009140868\n",
      "Epoch [96/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0009139275\n",
      "Epoch [96/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0068, LR: 0.0009137681\n",
      "Epoch [96/500], Batch [50/110], Train Loss: 0.0477, Val Loss: 0.0068, LR: 0.0009136086\n",
      "Epoch [96/500], Batch [60/110], Train Loss: 0.0059, Val Loss: 0.0056, LR: 0.0009134489\n",
      "Epoch [96/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0077, LR: 0.0009132891\n",
      "Epoch [96/500], Batch [80/110], Train Loss: 0.0337, Val Loss: 0.0058, LR: 0.0009131292\n",
      "Epoch [96/500], Batch [90/110], Train Loss: 0.0035, Val Loss: 0.0062, LR: 0.0009129692\n",
      "Epoch [96/500], Batch [100/110], Train Loss: 0.0218, Val Loss: 0.0054, LR: 0.0009128090\n",
      "Epoch [96/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0069, LR: 0.0009126486\n",
      "Epoch [97/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0145, LR: 0.0009124882\n",
      "Epoch [97/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0136, LR: 0.0009123276\n",
      "Epoch [97/500], Batch [30/110], Train Loss: 0.0060, Val Loss: 0.0120, LR: 0.0009121669\n",
      "Epoch [97/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0118, LR: 0.0009120060\n",
      "Epoch [97/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0090, LR: 0.0009118450\n",
      "Epoch [97/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0009116839\n",
      "Epoch [97/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0153, LR: 0.0009115226\n",
      "Epoch [97/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0061, LR: 0.0009113613\n",
      "Epoch [97/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0138, LR: 0.0009111997\n",
      "Epoch [97/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0009110381\n",
      "Epoch [97/500], Batch [110/110], Train Loss: 0.0092, Val Loss: 0.0057, LR: 0.0009108763\n",
      "Epoch [98/500], Batch [10/110], Train Loss: 0.0046, Val Loss: 0.0061, LR: 0.0009107144\n",
      "Epoch [98/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0182, LR: 0.0009105523\n",
      "Epoch [98/500], Batch [30/110], Train Loss: 0.0872, Val Loss: 0.0085, LR: 0.0009103901\n",
      "Epoch [98/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0009102278\n",
      "Epoch [98/500], Batch [50/110], Train Loss: 0.0061, Val Loss: 0.0068, LR: 0.0009100654\n",
      "Epoch [98/500], Batch [60/110], Train Loss: 0.0256, Val Loss: 0.0056, LR: 0.0009099028\n",
      "Epoch [98/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0055, LR: 0.0009097401\n",
      "Epoch [98/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0054, LR: 0.0009095773\n",
      "Epoch [98/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0058, LR: 0.0009094143\n",
      "Epoch [98/500], Batch [100/110], Train Loss: 0.0332, Val Loss: 0.0089, LR: 0.0009092512\n",
      "Epoch [98/500], Batch [110/110], Train Loss: 0.0028, Val Loss: 0.0053, LR: 0.0009090879\n",
      "Epoch [99/500], Batch [10/110], Train Loss: 0.0023, Val Loss: 0.0169, LR: 0.0009089246\n",
      "Epoch [99/500], Batch [20/110], Train Loss: 0.0015, Val Loss: 0.0053, LR: 0.0009087611\n",
      "Epoch [99/500], Batch [30/110], Train Loss: 0.0316, Val Loss: 0.0115, LR: 0.0009085974\n",
      "Epoch [99/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0051, LR: 0.0009084337\n",
      "Epoch [99/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0009082698\n",
      "Epoch [99/500], Batch [60/110], Train Loss: 0.0094, Val Loss: 0.0054, LR: 0.0009081057\n",
      "Epoch [99/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0009079416\n",
      "Epoch [99/500], Batch [80/110], Train Loss: 0.0024, Val Loss: 0.0081, LR: 0.0009077773\n",
      "Epoch [99/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0063, LR: 0.0009076129\n",
      "Epoch [99/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0009074483\n",
      "Epoch [99/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0081, LR: 0.0009072836\n",
      "Epoch [100/500], Batch [10/110], Train Loss: 0.0172, Val Loss: 0.0051, LR: 0.0009071188\n",
      "Epoch [100/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0009069538\n",
      "Epoch [100/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0158, LR: 0.0009067888\n",
      "Epoch [100/500], Batch [40/110], Train Loss: 0.0560, Val Loss: 0.0079, LR: 0.0009066236\n",
      "Epoch [100/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0009064582\n",
      "Epoch [100/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0169, LR: 0.0009062927\n",
      "Epoch [100/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0066, LR: 0.0009061271\n",
      "Epoch [100/500], Batch [80/110], Train Loss: 0.0279, Val Loss: 0.0056, LR: 0.0009059614\n",
      "Epoch [100/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0179, LR: 0.0009057955\n",
      "Epoch [100/500], Batch [100/110], Train Loss: 0.0074, Val Loss: 0.0064, LR: 0.0009056295\n",
      "Epoch [100/500], Batch [110/110], Train Loss: 0.0058, Val Loss: 0.0083, LR: 0.0009054634\n",
      "Epoch [101/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0062, LR: 0.0009052972\n",
      "Epoch [101/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0009051308\n",
      "Epoch [101/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0182, LR: 0.0009049642\n",
      "Epoch [101/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0009047976\n",
      "Epoch [101/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0055, LR: 0.0009046308\n",
      "Epoch [101/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0056, LR: 0.0009044639\n",
      "Epoch [101/500], Batch [70/110], Train Loss: 0.0066, Val Loss: 0.0059, LR: 0.0009042969\n",
      "Epoch [101/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0009041297\n",
      "Epoch [101/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0009039624\n",
      "Epoch [101/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0009037950\n",
      "Epoch [101/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0053, LR: 0.0009036274\n",
      "Confusion Matrix:\n",
      "[[631   4]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99842   0.99370   0.99605       635\n",
      "           1    0.99539   0.99884   0.99711       865\n",
      "\n",
      "    accuracy                        0.99667      1500\n",
      "   macro avg    0.99690   0.99627   0.99658      1500\n",
      "weighted avg    0.99667   0.99667   0.99667      1500\n",
      "\n",
      "Total Errors: 5\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 693, Predicted: 1, Actual: 0\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Index: 1067, Predicted: 1, Actual: 0\n",
      "Index: 1242, Predicted: 1, Actual: 0\n",
      "Epoch 101: OK- Accuracy: 0.99667, Precision: 0.99539, Recall: 0.99884, F1: 0.99711, ROC AUC: 0.99627, AUPR (PR-AUC): 0.99491, Sensitivity: 0.99884, Specificity: 0.99370, Far: 0.006299212598425197, False Positive Rate (FPR): 0.00630, False Negative Rate (FNR): 0.00116, Runtime: 0.032 sec , Memory Usage: 310.82 MB\n",
      "Epoch [102/500], Batch [10/110], Train Loss: 0.0487, Val Loss: 0.0068, LR: 0.0009034597\n",
      "Epoch [102/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0056, LR: 0.0009032919\n",
      "Epoch [102/500], Batch [30/110], Train Loss: 0.0197, Val Loss: 0.0054, LR: 0.0009031239\n",
      "Epoch [102/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0009029559\n",
      "Epoch [102/500], Batch [50/110], Train Loss: 0.0065, Val Loss: 0.0051, LR: 0.0009027876\n",
      "Epoch [102/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0052, LR: 0.0009026193\n",
      "Epoch [102/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0009024508\n",
      "Epoch [102/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0009022822\n",
      "Epoch [102/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0052, LR: 0.0009021135\n",
      "Epoch [102/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0108, LR: 0.0009019446\n",
      "Epoch [102/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0053, LR: 0.0009017757\n",
      "Epoch [103/500], Batch [10/110], Train Loss: 0.0330, Val Loss: 0.0052, LR: 0.0009016065\n",
      "Epoch [103/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0047, LR: 0.0009014373\n",
      "Epoch [103/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0056, LR: 0.0009012679\n",
      "Epoch [103/500], Batch [40/110], Train Loss: 0.0027, Val Loss: 0.0078, LR: 0.0009010984\n",
      "Epoch [103/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0009009288\n",
      "Epoch [103/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0047, LR: 0.0009007590\n",
      "Epoch [103/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0009005891\n",
      "Epoch [103/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0053, LR: 0.0009004191\n",
      "Epoch [103/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0064, LR: 0.0009002489\n",
      "Epoch [103/500], Batch [100/110], Train Loss: 0.0017, Val Loss: 0.0053, LR: 0.0009000787\n",
      "Epoch [103/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0056, LR: 0.0008999082\n",
      "Epoch [104/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0050, LR: 0.0008997377\n",
      "Epoch [104/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0008995670\n",
      "Epoch [104/500], Batch [30/110], Train Loss: 0.0224, Val Loss: 0.0046, LR: 0.0008993962\n",
      "Epoch [104/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0044, LR: 0.0008992253\n",
      "Epoch [104/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0008990543\n",
      "Epoch [104/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0052, LR: 0.0008988831\n",
      "Epoch [104/500], Batch [70/110], Train Loss: 0.0080, Val Loss: 0.0074, LR: 0.0008987118\n",
      "Epoch [104/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0090, LR: 0.0008985403\n",
      "Epoch [104/500], Batch [90/110], Train Loss: 0.0173, Val Loss: 0.0068, LR: 0.0008983688\n",
      "Epoch [104/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0051, LR: 0.0008981971\n",
      "Epoch [104/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0008980252\n",
      "Epoch [105/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0008978533\n",
      "Epoch [105/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0008976812\n",
      "Epoch [105/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0008975090\n",
      "Epoch [105/500], Batch [40/110], Train Loss: 0.0071, Val Loss: 0.0053, LR: 0.0008973367\n",
      "Epoch [105/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0056, LR: 0.0008971642\n",
      "Epoch [105/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0055, LR: 0.0008969916\n",
      "Epoch [105/500], Batch [70/110], Train Loss: 0.0281, Val Loss: 0.0065, LR: 0.0008968189\n",
      "Epoch [105/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0008966460\n",
      "Epoch [105/500], Batch [90/110], Train Loss: 0.0029, Val Loss: 0.0042, LR: 0.0008964731\n",
      "Epoch [105/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0008963000\n",
      "Epoch [105/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0008961267\n",
      "Epoch [106/500], Batch [10/110], Train Loss: 0.0036, Val Loss: 0.0048, LR: 0.0008959534\n",
      "Epoch [106/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0008957799\n",
      "Epoch [106/500], Batch [30/110], Train Loss: 0.0303, Val Loss: 0.0045, LR: 0.0008956063\n",
      "Epoch [106/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0044, LR: 0.0008954325\n",
      "Epoch [106/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0008952587\n",
      "Epoch [106/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0088, LR: 0.0008950847\n",
      "Epoch [106/500], Batch [70/110], Train Loss: 0.0456, Val Loss: 0.0084, LR: 0.0008949105\n",
      "Epoch [106/500], Batch [80/110], Train Loss: 0.0140, Val Loss: 0.0042, LR: 0.0008947363\n",
      "Epoch [106/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0008945619\n",
      "Epoch [106/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0078, LR: 0.0008943874\n",
      "Epoch [106/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0048, LR: 0.0008942128\n",
      "Epoch [107/500], Batch [10/110], Train Loss: 0.0132, Val Loss: 0.0062, LR: 0.0008940380\n",
      "Epoch [107/500], Batch [20/110], Train Loss: 0.1870, Val Loss: 0.0264, LR: 0.0008938631\n",
      "Epoch [107/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0043, LR: 0.0008936881\n",
      "Epoch [107/500], Batch [40/110], Train Loss: 0.0017, Val Loss: 0.0050, LR: 0.0008935130\n",
      "Epoch [107/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0179, LR: 0.0008933377\n",
      "Epoch [107/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0008931623\n",
      "Epoch [107/500], Batch [70/110], Train Loss: 0.0117, Val Loss: 0.0059, LR: 0.0008929868\n",
      "Epoch [107/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0125, LR: 0.0008928111\n",
      "Epoch [107/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0120, LR: 0.0008926354\n",
      "Epoch [107/500], Batch [100/110], Train Loss: 0.2396, Val Loss: 0.0078, LR: 0.0008924595\n",
      "Epoch [107/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0152, LR: 0.0008922835\n",
      "Epoch [108/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0008921073\n",
      "Epoch [108/500], Batch [20/110], Train Loss: 0.0186, Val Loss: 0.0053, LR: 0.0008919310\n",
      "Epoch [108/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0008917546\n",
      "Epoch [108/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0008915781\n",
      "Epoch [108/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0008914014\n",
      "Epoch [108/500], Batch [60/110], Train Loss: 0.0353, Val Loss: 0.0044, LR: 0.0008912246\n",
      "Epoch [108/500], Batch [70/110], Train Loss: 0.0029, Val Loss: 0.0069, LR: 0.0008910477\n",
      "Epoch [108/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0045, LR: 0.0008908707\n",
      "Epoch [108/500], Batch [90/110], Train Loss: 0.0034, Val Loss: 0.0050, LR: 0.0008906935\n",
      "Epoch [108/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0008905163\n",
      "Epoch [108/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0045, LR: 0.0008903388\n",
      "Epoch [109/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0008901613\n",
      "Epoch [109/500], Batch [20/110], Train Loss: 0.0016, Val Loss: 0.0067, LR: 0.0008899836\n",
      "Epoch [109/500], Batch [30/110], Train Loss: 0.0202, Val Loss: 0.0048, LR: 0.0008898058\n",
      "Epoch [109/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0008896279\n",
      "Epoch [109/500], Batch [50/110], Train Loss: 0.0158, Val Loss: 0.0060, LR: 0.0008894499\n",
      "Epoch [109/500], Batch [60/110], Train Loss: 0.0774, Val Loss: 0.0054, LR: 0.0008892717\n",
      "Epoch [109/500], Batch [70/110], Train Loss: 0.0372, Val Loss: 0.0066, LR: 0.0008890934\n",
      "Epoch [109/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0044, LR: 0.0008889150\n",
      "Epoch [109/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0042, LR: 0.0008887365\n",
      "Epoch [109/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0043, LR: 0.0008885578\n",
      "Epoch [109/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0008883790\n",
      "Epoch [110/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0041, LR: 0.0008882001\n",
      "Epoch [110/500], Batch [20/110], Train Loss: 0.0310, Val Loss: 0.0038, LR: 0.0008880211\n",
      "Epoch [110/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0039, LR: 0.0008878419\n",
      "Epoch [110/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0008876626\n",
      "Epoch [110/500], Batch [50/110], Train Loss: 0.0052, Val Loss: 0.0052, LR: 0.0008874832\n",
      "Epoch [110/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0042, LR: 0.0008873036\n",
      "Epoch [110/500], Batch [70/110], Train Loss: 0.0421, Val Loss: 0.0049, LR: 0.0008871240\n",
      "Epoch [110/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0008869442\n",
      "Epoch [110/500], Batch [90/110], Train Loss: 0.0104, Val Loss: 0.0040, LR: 0.0008867643\n",
      "Epoch [110/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0043, LR: 0.0008865842\n",
      "Epoch [110/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0040, LR: 0.0008864041\n",
      "Epoch [111/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0060, LR: 0.0008862238\n",
      "Epoch [111/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0050, LR: 0.0008860434\n",
      "Epoch [111/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0047, LR: 0.0008858628\n",
      "Epoch [111/500], Batch [40/110], Train Loss: 0.0459, Val Loss: 0.0076, LR: 0.0008856822\n",
      "Epoch [111/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0008855014\n",
      "Epoch [111/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0185, LR: 0.0008853205\n",
      "Epoch [111/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0008851394\n",
      "Epoch [111/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0047, LR: 0.0008849583\n",
      "Epoch [111/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0008847770\n",
      "Epoch [111/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0039, LR: 0.0008845956\n",
      "Epoch [111/500], Batch [110/110], Train Loss: 0.0021, Val Loss: 0.0038, LR: 0.0008844140\n",
      "Confusion Matrix:\n",
      "[[633   2]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99842   0.99685   0.99764       635\n",
      "           1    0.99769   0.99884   0.99827       865\n",
      "\n",
      "    accuracy                        0.99800      1500\n",
      "   macro avg    0.99806   0.99785   0.99795      1500\n",
      "weighted avg    0.99800   0.99800   0.99800      1500\n",
      "\n",
      "Total Errors: 3\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Index: 1242, Predicted: 1, Actual: 0\n",
      "Epoch 111: OK- Accuracy: 0.99800, Precision: 0.99769, Recall: 0.99884, F1: 0.99827, ROC AUC: 0.99785, AUPR (PR-AUC): 0.99720, Sensitivity: 0.99884, Specificity: 0.99685, Far: 0.0031496062992125984, False Positive Rate (FPR): 0.00315, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 310.82 MB\n",
      "Epoch [112/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0008842324\n",
      "Epoch [112/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0063, LR: 0.0008840506\n",
      "Epoch [112/500], Batch [30/110], Train Loss: 0.0137, Val Loss: 0.0045, LR: 0.0008838687\n",
      "Epoch [112/500], Batch [40/110], Train Loss: 0.0083, Val Loss: 0.0044, LR: 0.0008836867\n",
      "Epoch [112/500], Batch [50/110], Train Loss: 0.0502, Val Loss: 0.0142, LR: 0.0008835045\n",
      "Epoch [112/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0049, LR: 0.0008833223\n",
      "Epoch [112/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0008831399\n",
      "Epoch [112/500], Batch [80/110], Train Loss: 0.0043, Val Loss: 0.0043, LR: 0.0008829573\n",
      "Epoch [112/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0038, LR: 0.0008827747\n",
      "Epoch [112/500], Batch [100/110], Train Loss: 0.1120, Val Loss: 0.0038, LR: 0.0008825919\n",
      "Epoch [112/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0039, LR: 0.0008824090\n",
      "Epoch [113/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0008822260\n",
      "Epoch [113/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0043, LR: 0.0008820429\n",
      "Epoch [113/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0040, LR: 0.0008818596\n",
      "Epoch [113/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0008816763\n",
      "Epoch [113/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0037, LR: 0.0008814928\n",
      "Epoch [113/500], Batch [60/110], Train Loss: 0.0069, Val Loss: 0.0037, LR: 0.0008813091\n",
      "Epoch [113/500], Batch [70/110], Train Loss: 0.0262, Val Loss: 0.0042, LR: 0.0008811254\n",
      "Epoch [113/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0039, LR: 0.0008809415\n",
      "Epoch [113/500], Batch [90/110], Train Loss: 0.0037, Val Loss: 0.0054, LR: 0.0008807575\n",
      "Epoch [113/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0008805734\n",
      "Epoch [113/500], Batch [110/110], Train Loss: 0.0215, Val Loss: 0.0077, LR: 0.0008803891\n",
      "Epoch [114/500], Batch [10/110], Train Loss: 0.0029, Val Loss: 0.0048, LR: 0.0008802048\n",
      "Epoch [114/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0134, LR: 0.0008800203\n",
      "Epoch [114/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0124, LR: 0.0008798357\n",
      "Epoch [114/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0008796510\n",
      "Epoch [114/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0008794661\n",
      "Epoch [114/500], Batch [60/110], Train Loss: 0.0017, Val Loss: 0.0048, LR: 0.0008792811\n",
      "Epoch [114/500], Batch [70/110], Train Loss: 0.0018, Val Loss: 0.0065, LR: 0.0008790960\n",
      "Epoch [114/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0008789108\n",
      "Epoch [114/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0008787255\n",
      "Epoch [114/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0008785400\n",
      "Epoch [114/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0040, LR: 0.0008783544\n",
      "Epoch [115/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0008781687\n",
      "Epoch [115/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0008779829\n",
      "Epoch [115/500], Batch [30/110], Train Loss: 0.0143, Val Loss: 0.0051, LR: 0.0008777969\n",
      "Epoch [115/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0008776109\n",
      "Epoch [115/500], Batch [50/110], Train Loss: 0.0027, Val Loss: 0.0036, LR: 0.0008774247\n",
      "Epoch [115/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0037, LR: 0.0008772384\n",
      "Epoch [115/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0008770519\n",
      "Epoch [115/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0008768654\n",
      "Epoch [115/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0038, LR: 0.0008766787\n",
      "Epoch [115/500], Batch [100/110], Train Loss: 0.0640, Val Loss: 0.0055, LR: 0.0008764919\n",
      "Epoch [115/500], Batch [110/110], Train Loss: 0.0075, Val Loss: 0.0035, LR: 0.0008763050\n",
      "Epoch [116/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0008761179\n",
      "Epoch [116/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0008759308\n",
      "Epoch [116/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0053, LR: 0.0008757435\n",
      "Epoch [116/500], Batch [40/110], Train Loss: 0.0023, Val Loss: 0.0059, LR: 0.0008755561\n",
      "Epoch [116/500], Batch [50/110], Train Loss: 0.0247, Val Loss: 0.0037, LR: 0.0008753686\n",
      "Epoch [116/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0008751809\n",
      "Epoch [116/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0044, LR: 0.0008749931\n",
      "Epoch [116/500], Batch [80/110], Train Loss: 0.0272, Val Loss: 0.0040, LR: 0.0008748053\n",
      "Epoch [116/500], Batch [90/110], Train Loss: 0.0087, Val Loss: 0.0043, LR: 0.0008746172\n",
      "Epoch [116/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0008744291\n",
      "Epoch [116/500], Batch [110/110], Train Loss: 0.0457, Val Loss: 0.0034, LR: 0.0008742409\n",
      "Epoch [117/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0107, LR: 0.0008740525\n",
      "Epoch [117/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0008738640\n",
      "Epoch [117/500], Batch [30/110], Train Loss: 0.0013, Val Loss: 0.0079, LR: 0.0008736754\n",
      "Epoch [117/500], Batch [40/110], Train Loss: 0.0707, Val Loss: 0.0075, LR: 0.0008734867\n",
      "Epoch [117/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0168, LR: 0.0008732978\n",
      "Epoch [117/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0136, LR: 0.0008731088\n",
      "Epoch [117/500], Batch [70/110], Train Loss: 0.2084, Val Loss: 0.0133, LR: 0.0008729197\n",
      "Epoch [117/500], Batch [80/110], Train Loss: 0.0164, Val Loss: 0.0052, LR: 0.0008727305\n",
      "Epoch [117/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0008725412\n",
      "Epoch [117/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0008723517\n",
      "Epoch [117/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0052, LR: 0.0008721622\n",
      "Epoch [118/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0008719725\n",
      "Epoch [118/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0038, LR: 0.0008717827\n",
      "Epoch [118/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0049, LR: 0.0008715927\n",
      "Epoch [118/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0008714027\n",
      "Epoch [118/500], Batch [50/110], Train Loss: 0.0071, Val Loss: 0.0038, LR: 0.0008712125\n",
      "Epoch [118/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0008710222\n",
      "Epoch [118/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0008708318\n",
      "Epoch [118/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0037, LR: 0.0008706413\n",
      "Epoch [118/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0035, LR: 0.0008704506\n",
      "Epoch [118/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0036, LR: 0.0008702599\n",
      "Epoch [118/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0035, LR: 0.0008700690\n",
      "Epoch [119/500], Batch [10/110], Train Loss: 0.0259, Val Loss: 0.0042, LR: 0.0008698780\n",
      "Epoch [119/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0008696869\n",
      "Epoch [119/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0008694956\n",
      "Epoch [119/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0177, LR: 0.0008693043\n",
      "Epoch [119/500], Batch [50/110], Train Loss: 0.0547, Val Loss: 0.0043, LR: 0.0008691128\n",
      "Epoch [119/500], Batch [60/110], Train Loss: 0.0282, Val Loss: 0.0070, LR: 0.0008689212\n",
      "Epoch [119/500], Batch [70/110], Train Loss: 0.0658, Val Loss: 0.0060, LR: 0.0008687295\n",
      "Epoch [119/500], Batch [80/110], Train Loss: 0.0015, Val Loss: 0.0079, LR: 0.0008685376\n",
      "Epoch [119/500], Batch [90/110], Train Loss: 0.0061, Val Loss: 0.0041, LR: 0.0008683457\n",
      "Epoch [119/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0008681536\n",
      "Epoch [119/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0099, LR: 0.0008679614\n",
      "Epoch [120/500], Batch [10/110], Train Loss: 0.0022, Val Loss: 0.0041, LR: 0.0008677691\n",
      "Epoch [120/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0048, LR: 0.0008675767\n",
      "Epoch [120/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0045, LR: 0.0008673841\n",
      "Epoch [120/500], Batch [40/110], Train Loss: 0.0167, Val Loss: 0.0034, LR: 0.0008671914\n",
      "Epoch [120/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0008669987\n",
      "Epoch [120/500], Batch [60/110], Train Loss: 0.0203, Val Loss: 0.0042, LR: 0.0008668058\n",
      "Epoch [120/500], Batch [70/110], Train Loss: 0.0284, Val Loss: 0.0040, LR: 0.0008666127\n",
      "Epoch [120/500], Batch [80/110], Train Loss: 0.0112, Val Loss: 0.0033, LR: 0.0008664196\n",
      "Epoch [120/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0032, LR: 0.0008662263\n",
      "Epoch [120/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0008660330\n",
      "Epoch [120/500], Batch [110/110], Train Loss: 0.0172, Val Loss: 0.0033, LR: 0.0008658395\n",
      "Epoch [121/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0032, LR: 0.0008656459\n",
      "Epoch [121/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0008654521\n",
      "Epoch [121/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0042, LR: 0.0008652583\n",
      "Epoch [121/500], Batch [40/110], Train Loss: 0.0280, Val Loss: 0.0031, LR: 0.0008650643\n",
      "Epoch [121/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0033, LR: 0.0008648702\n",
      "Epoch [121/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0032, LR: 0.0008646760\n",
      "Epoch [121/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0061, LR: 0.0008644817\n",
      "Epoch [121/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0043, LR: 0.0008642873\n",
      "Epoch [121/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0008640927\n",
      "Epoch [121/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0029, LR: 0.0008638981\n",
      "Epoch [121/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0028, LR: 0.0008637033\n",
      "Confusion Matrix:\n",
      "[[634   1]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   0.99843   0.99843       635\n",
      "           1    0.99884   0.99884   0.99884       865\n",
      "\n",
      "    accuracy                        0.99867      1500\n",
      "   macro avg    0.99863   0.99863   0.99863      1500\n",
      "weighted avg    0.99867   0.99867   0.99867      1500\n",
      "\n",
      "Total Errors: 2\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Epoch 121: OK- Accuracy: 0.99867, Precision: 0.99884, Recall: 0.99884, F1: 0.99884, ROC AUC: 0.99863, AUPR (PR-AUC): 0.99836, Sensitivity: 0.99884, Specificity: 0.99843, Far: 0.0015748031496062992, False Positive Rate (FPR): 0.00157, False Negative Rate (FNR): 0.00116, Runtime: 0.037 sec , Memory Usage: 310.84 MB\n",
      "Epoch [122/500], Batch [10/110], Train Loss: 0.0041, Val Loss: 0.0028, LR: 0.0008635084\n",
      "Epoch [122/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0008633134\n",
      "Epoch [122/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0028, LR: 0.0008631182\n",
      "Epoch [122/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0008629230\n",
      "Epoch [122/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0045, LR: 0.0008627276\n",
      "Epoch [122/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0107, LR: 0.0008625321\n",
      "Epoch [122/500], Batch [70/110], Train Loss: 0.0608, Val Loss: 0.0115, LR: 0.0008623365\n",
      "Epoch [122/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0008621408\n",
      "Epoch [122/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0032, LR: 0.0008619450\n",
      "Epoch [122/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0008617490\n",
      "Epoch [122/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0032, LR: 0.0008615530\n",
      "Epoch [123/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0008613568\n",
      "Epoch [123/500], Batch [20/110], Train Loss: 0.1089, Val Loss: 0.0033, LR: 0.0008611605\n",
      "Epoch [123/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0008609641\n",
      "Epoch [123/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0008607675\n",
      "Epoch [123/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0048, LR: 0.0008605709\n",
      "Epoch [123/500], Batch [60/110], Train Loss: 0.0153, Val Loss: 0.0030, LR: 0.0008603741\n",
      "Epoch [123/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0032, LR: 0.0008601772\n",
      "Epoch [123/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0028, LR: 0.0008599802\n",
      "Epoch [123/500], Batch [90/110], Train Loss: 0.0143, Val Loss: 0.0052, LR: 0.0008597831\n",
      "Epoch [123/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0008595859\n",
      "Epoch [123/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0008593886\n",
      "Epoch [124/500], Batch [10/110], Train Loss: 0.0057, Val Loss: 0.0034, LR: 0.0008591911\n",
      "Epoch [124/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0008589935\n",
      "Epoch [124/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0008587958\n",
      "Epoch [124/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0139, LR: 0.0008585980\n",
      "Epoch [124/500], Batch [50/110], Train Loss: 0.0014, Val Loss: 0.0081, LR: 0.0008584001\n",
      "Epoch [124/500], Batch [60/110], Train Loss: 0.0435, Val Loss: 0.0032, LR: 0.0008582021\n",
      "Epoch [124/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0008580039\n",
      "Epoch [124/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0008578057\n",
      "Epoch [124/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0008576073\n",
      "Epoch [124/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0051, LR: 0.0008574088\n",
      "Epoch [124/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0176, LR: 0.0008572102\n",
      "Epoch [125/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0008570114\n",
      "Epoch [125/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0008568126\n",
      "Epoch [125/500], Batch [30/110], Train Loss: 0.0024, Val Loss: 0.0029, LR: 0.0008566136\n",
      "Epoch [125/500], Batch [40/110], Train Loss: 0.0147, Val Loss: 0.0034, LR: 0.0008564146\n",
      "Epoch [125/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0056, LR: 0.0008562154\n",
      "Epoch [125/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0078, LR: 0.0008560161\n",
      "Epoch [125/500], Batch [70/110], Train Loss: 0.0226, Val Loss: 0.0036, LR: 0.0008558167\n",
      "Epoch [125/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0008556171\n",
      "Epoch [125/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0039, LR: 0.0008554175\n",
      "Epoch [125/500], Batch [100/110], Train Loss: 0.0049, Val Loss: 0.0028, LR: 0.0008552177\n",
      "Epoch [125/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0048, LR: 0.0008550179\n",
      "Epoch [126/500], Batch [10/110], Train Loss: 0.0151, Val Loss: 0.0035, LR: 0.0008548179\n",
      "Epoch [126/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0036, LR: 0.0008546178\n",
      "Epoch [126/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0032, LR: 0.0008544176\n",
      "Epoch [126/500], Batch [40/110], Train Loss: 0.0141, Val Loss: 0.0053, LR: 0.0008542172\n",
      "Epoch [126/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0008540168\n",
      "Epoch [126/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0027, LR: 0.0008538162\n",
      "Epoch [126/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0037, LR: 0.0008536156\n",
      "Epoch [126/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0008534148\n",
      "Epoch [126/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0008532139\n",
      "Epoch [126/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0027, LR: 0.0008530129\n",
      "Epoch [126/500], Batch [110/110], Train Loss: 0.0088, Val Loss: 0.0029, LR: 0.0008528117\n",
      "Epoch [127/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0123, LR: 0.0008526105\n",
      "Epoch [127/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0029, LR: 0.0008524091\n",
      "Epoch [127/500], Batch [30/110], Train Loss: 0.0044, Val Loss: 0.0033, LR: 0.0008522077\n",
      "Epoch [127/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0008520061\n",
      "Epoch [127/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0030, LR: 0.0008518044\n",
      "Epoch [127/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0032, LR: 0.0008516026\n",
      "Epoch [127/500], Batch [70/110], Train Loss: 0.0105, Val Loss: 0.0027, LR: 0.0008514007\n",
      "Epoch [127/500], Batch [80/110], Train Loss: 0.0013, Val Loss: 0.0047, LR: 0.0008511987\n",
      "Epoch [127/500], Batch [90/110], Train Loss: 0.0095, Val Loss: 0.0026, LR: 0.0008509965\n",
      "Epoch [127/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0049, LR: 0.0008507943\n",
      "Epoch [127/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0008505919\n",
      "Epoch [128/500], Batch [10/110], Train Loss: 0.0085, Val Loss: 0.0025, LR: 0.0008503894\n",
      "Epoch [128/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0008501868\n",
      "Epoch [128/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0008499841\n",
      "Epoch [128/500], Batch [40/110], Train Loss: 0.0037, Val Loss: 0.0065, LR: 0.0008497813\n",
      "Epoch [128/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0008495783\n",
      "Epoch [128/500], Batch [60/110], Train Loss: 0.0245, Val Loss: 0.0064, LR: 0.0008493753\n",
      "Epoch [128/500], Batch [70/110], Train Loss: 0.0084, Val Loss: 0.0051, LR: 0.0008491721\n",
      "Epoch [128/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008489689\n",
      "Epoch [128/500], Batch [90/110], Train Loss: 0.0064, Val Loss: 0.0072, LR: 0.0008487655\n",
      "Epoch [128/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0035, LR: 0.0008485620\n",
      "Epoch [128/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0008483584\n",
      "Epoch [129/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0038, LR: 0.0008481547\n",
      "Epoch [129/500], Batch [20/110], Train Loss: 0.0038, Val Loss: 0.0038, LR: 0.0008479508\n",
      "Epoch [129/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0008477469\n",
      "Epoch [129/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0008475428\n",
      "Epoch [129/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0008473387\n",
      "Epoch [129/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0025, LR: 0.0008471344\n",
      "Epoch [129/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0045, LR: 0.0008469300\n",
      "Epoch [129/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0008467255\n",
      "Epoch [129/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0022, LR: 0.0008465209\n",
      "Epoch [129/500], Batch [100/110], Train Loss: 0.0421, Val Loss: 0.0040, LR: 0.0008463162\n",
      "Epoch [129/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0008461113\n",
      "Epoch [130/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0008459064\n",
      "Epoch [130/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0008457013\n",
      "Epoch [130/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0030, LR: 0.0008454962\n",
      "Epoch [130/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0023, LR: 0.0008452909\n",
      "Epoch [130/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0039, LR: 0.0008450855\n",
      "Epoch [130/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0008448800\n",
      "Epoch [130/500], Batch [70/110], Train Loss: 0.0351, Val Loss: 0.0048, LR: 0.0008446744\n",
      "Epoch [130/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0008444687\n",
      "Epoch [130/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0008442628\n",
      "Epoch [130/500], Batch [100/110], Train Loss: 0.0154, Val Loss: 0.0022, LR: 0.0008440569\n",
      "Epoch [130/500], Batch [110/110], Train Loss: 0.0034, Val Loss: 0.0033, LR: 0.0008438508\n",
      "Epoch [131/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0022, LR: 0.0008436447\n",
      "Epoch [131/500], Batch [20/110], Train Loss: 0.0048, Val Loss: 0.0021, LR: 0.0008434384\n",
      "Epoch [131/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0008432320\n",
      "Epoch [131/500], Batch [40/110], Train Loss: 0.0042, Val Loss: 0.0022, LR: 0.0008430255\n",
      "Epoch [131/500], Batch [50/110], Train Loss: 0.0020, Val Loss: 0.0038, LR: 0.0008428189\n",
      "Epoch [131/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0142, LR: 0.0008426122\n",
      "Epoch [131/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0008424053\n",
      "Epoch [131/500], Batch [80/110], Train Loss: 0.0120, Val Loss: 0.0020, LR: 0.0008421984\n",
      "Epoch [131/500], Batch [90/110], Train Loss: 0.0189, Val Loss: 0.0021, LR: 0.0008419913\n",
      "Epoch [131/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0031, LR: 0.0008417842\n",
      "Epoch [131/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0109, LR: 0.0008415769\n",
      "Confusion Matrix:\n",
      "[[620  15]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99839   0.97638   0.98726       635\n",
      "           1    0.98294   0.99884   0.99083       865\n",
      "\n",
      "    accuracy                        0.98933      1500\n",
      "   macro avg    0.99066   0.98761   0.98904      1500\n",
      "weighted avg    0.98948   0.98933   0.98932      1500\n",
      "\n",
      "Total Errors: 16\n",
      "Index: 58, Predicted: 1, Actual: 0\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 319, Predicted: 1, Actual: 0\n",
      "Index: 576, Predicted: 1, Actual: 0\n",
      "Index: 693, Predicted: 1, Actual: 0\n",
      "Epoch 131: OK- Accuracy: 0.98933, Precision: 0.98294, Recall: 0.99884, F1: 0.99083, ROC AUC: 0.98761, AUPR (PR-AUC): 0.98247, Sensitivity: 0.99884, Specificity: 0.97638, Far: 0.023622047244094488, False Positive Rate (FPR): 0.02362, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 310.87 MB\n",
      "Epoch [132/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0039, LR: 0.0008413695\n",
      "Epoch [132/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0008411621\n",
      "Epoch [132/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0141, LR: 0.0008409545\n",
      "Epoch [132/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0008407468\n",
      "Epoch [132/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0008405389\n",
      "Epoch [132/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0008403310\n",
      "Epoch [132/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0033, LR: 0.0008401230\n",
      "Epoch [132/500], Batch [80/110], Train Loss: 0.0033, Val Loss: 0.0050, LR: 0.0008399148\n",
      "Epoch [132/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0103, LR: 0.0008397066\n",
      "Epoch [132/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0008394982\n",
      "Epoch [132/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0008392897\n",
      "Epoch [133/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0008390812\n",
      "Epoch [133/500], Batch [20/110], Train Loss: 0.0044, Val Loss: 0.0021, LR: 0.0008388725\n",
      "Epoch [133/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0042, LR: 0.0008386637\n",
      "Epoch [133/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0020, LR: 0.0008384548\n",
      "Epoch [133/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0008382457\n",
      "Epoch [133/500], Batch [60/110], Train Loss: 0.0079, Val Loss: 0.0021, LR: 0.0008380366\n",
      "Epoch [133/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0008378274\n",
      "Epoch [133/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0008376180\n",
      "Epoch [133/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0029, LR: 0.0008374086\n",
      "Epoch [133/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0008371990\n",
      "Epoch [133/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0084, LR: 0.0008369894\n",
      "Epoch [134/500], Batch [10/110], Train Loss: 0.0436, Val Loss: 0.0025, LR: 0.0008367796\n",
      "Epoch [134/500], Batch [20/110], Train Loss: 0.0040, Val Loss: 0.0027, LR: 0.0008365697\n",
      "Epoch [134/500], Batch [30/110], Train Loss: 0.0023, Val Loss: 0.0027, LR: 0.0008363597\n",
      "Epoch [134/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0008361496\n",
      "Epoch [134/500], Batch [50/110], Train Loss: 0.0197, Val Loss: 0.0030, LR: 0.0008359394\n",
      "Epoch [134/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0008357291\n",
      "Epoch [134/500], Batch [70/110], Train Loss: 0.0092, Val Loss: 0.0018, LR: 0.0008355187\n",
      "Epoch [134/500], Batch [80/110], Train Loss: 0.0049, Val Loss: 0.0046, LR: 0.0008353081\n",
      "Epoch [134/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0008350975\n",
      "Epoch [134/500], Batch [100/110], Train Loss: 0.0036, Val Loss: 0.0020, LR: 0.0008348867\n",
      "Epoch [134/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0008346759\n",
      "Epoch [135/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0008344649\n",
      "Epoch [135/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0018, LR: 0.0008342538\n",
      "Epoch [135/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0008340427\n",
      "Epoch [135/500], Batch [40/110], Train Loss: 0.0048, Val Loss: 0.0017, LR: 0.0008338314\n",
      "Epoch [135/500], Batch [50/110], Train Loss: 0.0060, Val Loss: 0.0035, LR: 0.0008336200\n",
      "Epoch [135/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0008334085\n",
      "Epoch [135/500], Batch [70/110], Train Loss: 0.0087, Val Loss: 0.0017, LR: 0.0008331969\n",
      "Epoch [135/500], Batch [80/110], Train Loss: 0.0381, Val Loss: 0.0057, LR: 0.0008329852\n",
      "Epoch [135/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0123, LR: 0.0008327733\n",
      "Epoch [135/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0037, LR: 0.0008325614\n",
      "Epoch [135/500], Batch [110/110], Train Loss: 0.0091, Val Loss: 0.0018, LR: 0.0008323494\n",
      "Epoch [136/500], Batch [10/110], Train Loss: 0.0043, Val Loss: 0.0031, LR: 0.0008321372\n",
      "Epoch [136/500], Batch [20/110], Train Loss: 0.0132, Val Loss: 0.0017, LR: 0.0008319250\n",
      "Epoch [136/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0052, LR: 0.0008317126\n",
      "Epoch [136/500], Batch [40/110], Train Loss: 0.0052, Val Loss: 0.0018, LR: 0.0008315002\n",
      "Epoch [136/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0008312876\n",
      "Epoch [136/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0008310749\n",
      "Epoch [136/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0008308621\n",
      "Epoch [136/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0037, LR: 0.0008306493\n",
      "Epoch [136/500], Batch [90/110], Train Loss: 0.0086, Val Loss: 0.0020, LR: 0.0008304363\n",
      "Epoch [136/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0008302232\n",
      "Epoch [136/500], Batch [110/110], Train Loss: 0.0014, Val Loss: 0.0043, LR: 0.0008300099\n",
      "Epoch [137/500], Batch [10/110], Train Loss: 0.0022, Val Loss: 0.0018, LR: 0.0008297966\n",
      "Epoch [137/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0083, LR: 0.0008295832\n",
      "Epoch [137/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0016, LR: 0.0008293697\n",
      "Epoch [137/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0008291561\n",
      "Epoch [137/500], Batch [50/110], Train Loss: 0.0074, Val Loss: 0.0024, LR: 0.0008289423\n",
      "Epoch [137/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0008287285\n",
      "Epoch [137/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0008285145\n",
      "Epoch [137/500], Batch [80/110], Train Loss: 0.0077, Val Loss: 0.0017, LR: 0.0008283005\n",
      "Epoch [137/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0008280863\n",
      "Epoch [137/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0023, LR: 0.0008278721\n",
      "Epoch [137/500], Batch [110/110], Train Loss: 0.0113, Val Loss: 0.0022, LR: 0.0008276577\n",
      "Epoch [138/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0008274432\n",
      "Epoch [138/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0008272286\n",
      "Epoch [138/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0008270140\n",
      "Epoch [138/500], Batch [40/110], Train Loss: 0.0074, Val Loss: 0.0016, LR: 0.0008267992\n",
      "Epoch [138/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0008265843\n",
      "Epoch [138/500], Batch [60/110], Train Loss: 0.0108, Val Loss: 0.0037, LR: 0.0008263693\n",
      "Epoch [138/500], Batch [70/110], Train Loss: 0.0040, Val Loss: 0.0020, LR: 0.0008261542\n",
      "Epoch [138/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0092, LR: 0.0008259390\n",
      "Epoch [138/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0008257236\n",
      "Epoch [138/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0018, LR: 0.0008255082\n",
      "Epoch [138/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0008252927\n",
      "Epoch [139/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0008250771\n",
      "Epoch [139/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0008248613\n",
      "Epoch [139/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0008246455\n",
      "Epoch [139/500], Batch [40/110], Train Loss: 0.0061, Val Loss: 0.0016, LR: 0.0008244296\n",
      "Epoch [139/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0032, LR: 0.0008242135\n",
      "Epoch [139/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0008239974\n",
      "Epoch [139/500], Batch [70/110], Train Loss: 0.0044, Val Loss: 0.0018, LR: 0.0008237811\n",
      "Epoch [139/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0008235648\n",
      "Epoch [139/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0038, LR: 0.0008233483\n",
      "Epoch [139/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0105, LR: 0.0008231317\n",
      "Epoch [139/500], Batch [110/110], Train Loss: 0.0173, Val Loss: 0.0028, LR: 0.0008229151\n",
      "Epoch [140/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0008226983\n",
      "Epoch [140/500], Batch [20/110], Train Loss: 0.0889, Val Loss: 0.0018, LR: 0.0008224814\n",
      "Epoch [140/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0025, LR: 0.0008222644\n",
      "Epoch [140/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0008220474\n",
      "Epoch [140/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0008218302\n",
      "Epoch [140/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0008216129\n",
      "Epoch [140/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0008213955\n",
      "Epoch [140/500], Batch [80/110], Train Loss: 0.0008, Val Loss: 0.0014, LR: 0.0008211780\n",
      "Epoch [140/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0008209604\n",
      "Epoch [140/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0008207427\n",
      "Epoch [140/500], Batch [110/110], Train Loss: 0.0574, Val Loss: 0.0078, LR: 0.0008205249\n",
      "Epoch [141/500], Batch [10/110], Train Loss: 0.0055, Val Loss: 0.0052, LR: 0.0008203070\n",
      "Epoch [141/500], Batch [20/110], Train Loss: 0.0054, Val Loss: 0.0036, LR: 0.0008200890\n",
      "Epoch [141/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0102, LR: 0.0008198708\n",
      "Epoch [141/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0008196526\n",
      "Epoch [141/500], Batch [50/110], Train Loss: 0.0047, Val Loss: 0.0014, LR: 0.0008194343\n",
      "Epoch [141/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0016, LR: 0.0008192159\n",
      "Epoch [141/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0008189974\n",
      "Epoch [141/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0008187787\n",
      "Epoch [141/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0049, LR: 0.0008185600\n",
      "Epoch [141/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0008183412\n",
      "Epoch [141/500], Batch [110/110], Train Loss: 0.0020, Val Loss: 0.0017, LR: 0.0008181222\n",
      "Confusion Matrix:\n",
      "[[633   2]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99842   0.99685   0.99764       635\n",
      "           1    0.99769   0.99884   0.99827       865\n",
      "\n",
      "    accuracy                        0.99800      1500\n",
      "   macro avg    0.99806   0.99785   0.99795      1500\n",
      "weighted avg    0.99800   0.99800   0.99800      1500\n",
      "\n",
      "Total Errors: 3\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 767, Predicted: 1, Actual: 0\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Epoch 141: OK- Accuracy: 0.99800, Precision: 0.99769, Recall: 0.99884, F1: 0.99827, ROC AUC: 0.99785, AUPR (PR-AUC): 0.99720, Sensitivity: 0.99884, Specificity: 0.99685, Far: 0.0031496062992125984, False Positive Rate (FPR): 0.00315, False Negative Rate (FNR): 0.00116, Runtime: 0.036 sec , Memory Usage: 310.88 MB\n",
      "Epoch [142/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0008179032\n",
      "Epoch [142/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0008176841\n",
      "Epoch [142/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0008174648\n",
      "Epoch [142/500], Batch [40/110], Train Loss: 0.0013, Val Loss: 0.0013, LR: 0.0008172455\n",
      "Epoch [142/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0008170260\n",
      "Epoch [142/500], Batch [60/110], Train Loss: 0.0014, Val Loss: 0.0016, LR: 0.0008168065\n",
      "Epoch [142/500], Batch [70/110], Train Loss: 0.0044, Val Loss: 0.0013, LR: 0.0008165868\n",
      "Epoch [142/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0008163671\n",
      "Epoch [142/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0008161472\n",
      "Epoch [142/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0101, LR: 0.0008159273\n",
      "Epoch [142/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0037, LR: 0.0008157072\n",
      "Epoch [143/500], Batch [10/110], Train Loss: 0.0277, Val Loss: 0.0036, LR: 0.0008154871\n",
      "Epoch [143/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0071, LR: 0.0008152668\n",
      "Epoch [143/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0019, LR: 0.0008150465\n",
      "Epoch [143/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0008148260\n",
      "Epoch [143/500], Batch [50/110], Train Loss: 0.0562, Val Loss: 0.0132, LR: 0.0008146054\n",
      "Epoch [143/500], Batch [60/110], Train Loss: 0.0017, Val Loss: 0.0026, LR: 0.0008143848\n",
      "Epoch [143/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0008141640\n",
      "Epoch [143/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0018, LR: 0.0008139431\n",
      "Epoch [143/500], Batch [90/110], Train Loss: 0.0037, Val Loss: 0.0017, LR: 0.0008137222\n",
      "Epoch [143/500], Batch [100/110], Train Loss: 0.0521, Val Loss: 0.0037, LR: 0.0008135011\n",
      "Epoch [143/500], Batch [110/110], Train Loss: 0.0080, Val Loss: 0.0024, LR: 0.0008132800\n",
      "Epoch [144/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0008130587\n",
      "Epoch [144/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0008128373\n",
      "Epoch [144/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0022, LR: 0.0008126159\n",
      "Epoch [144/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0008123943\n",
      "Epoch [144/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0008121726\n",
      "Epoch [144/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0016, LR: 0.0008119508\n",
      "Epoch [144/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0008117290\n",
      "Epoch [144/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0023, LR: 0.0008115070\n",
      "Epoch [144/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0008112849\n",
      "Epoch [144/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0085, LR: 0.0008110628\n",
      "Epoch [144/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0049, LR: 0.0008108405\n",
      "Epoch [145/500], Batch [10/110], Train Loss: 0.0051, Val Loss: 0.0013, LR: 0.0008106181\n",
      "Epoch [145/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0008103957\n",
      "Epoch [145/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0008101731\n",
      "Epoch [145/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0008099504\n",
      "Epoch [145/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0023, LR: 0.0008097277\n",
      "Epoch [145/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0016, LR: 0.0008095048\n",
      "Epoch [145/500], Batch [70/110], Train Loss: 0.0045, Val Loss: 0.0033, LR: 0.0008092818\n",
      "Epoch [145/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0008090588\n",
      "Epoch [145/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0008088356\n",
      "Epoch [145/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0008086124\n",
      "Epoch [145/500], Batch [110/110], Train Loss: 0.0297, Val Loss: 0.0034, LR: 0.0008083890\n",
      "Epoch [146/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0013, LR: 0.0008081655\n",
      "Epoch [146/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0008079420\n",
      "Epoch [146/500], Batch [30/110], Train Loss: 0.0070, Val Loss: 0.0015, LR: 0.0008077183\n",
      "Epoch [146/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0008074946\n",
      "Epoch [146/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0016, LR: 0.0008072707\n",
      "Epoch [146/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0008070467\n",
      "Epoch [146/500], Batch [70/110], Train Loss: 0.0012, Val Loss: 0.0017, LR: 0.0008068227\n",
      "Epoch [146/500], Batch [80/110], Train Loss: 0.0028, Val Loss: 0.0018, LR: 0.0008065985\n",
      "Epoch [146/500], Batch [90/110], Train Loss: 0.0030, Val Loss: 0.0013, LR: 0.0008063743\n",
      "Epoch [146/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0008061499\n",
      "Epoch [146/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0012, LR: 0.0008059255\n",
      "Epoch [147/500], Batch [10/110], Train Loss: 0.0039, Val Loss: 0.0011, LR: 0.0008057010\n",
      "Epoch [147/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0008054763\n",
      "Epoch [147/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0013, LR: 0.0008052516\n",
      "Epoch [147/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0008050267\n",
      "Epoch [147/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0008048018\n",
      "Epoch [147/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0008045768\n",
      "Epoch [147/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0008043516\n",
      "Epoch [147/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0008041264\n",
      "Epoch [147/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0008039011\n",
      "Epoch [147/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0094, LR: 0.0008036756\n",
      "Epoch [147/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0008034501\n",
      "Epoch [148/500], Batch [10/110], Train Loss: 0.0133, Val Loss: 0.0014, LR: 0.0008032245\n",
      "Epoch [148/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0008029988\n",
      "Epoch [148/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0042, LR: 0.0008027730\n",
      "Epoch [148/500], Batch [40/110], Train Loss: 0.0042, Val Loss: 0.0020, LR: 0.0008025471\n",
      "Epoch [148/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0008023211\n",
      "Epoch [148/500], Batch [60/110], Train Loss: 0.0015, Val Loss: 0.0014, LR: 0.0008020949\n",
      "Epoch [148/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0011, LR: 0.0008018687\n",
      "Epoch [148/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0008016424\n",
      "Epoch [148/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0008014160\n",
      "Epoch [148/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0070, LR: 0.0008011896\n",
      "Epoch [148/500], Batch [110/110], Train Loss: 0.0047, Val Loss: 0.0012, LR: 0.0008009630\n",
      "Epoch [149/500], Batch [10/110], Train Loss: 0.0105, Val Loss: 0.0016, LR: 0.0008007363\n",
      "Epoch [149/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0008005095\n",
      "Epoch [149/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0008002826\n",
      "Epoch [149/500], Batch [40/110], Train Loss: 0.0044, Val Loss: 0.0021, LR: 0.0008000556\n",
      "Epoch [149/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007998286\n",
      "Epoch [149/500], Batch [60/110], Train Loss: 0.0100, Val Loss: 0.0309, LR: 0.0007996014\n",
      "Epoch [149/500], Batch [70/110], Train Loss: 0.0201, Val Loss: 0.0042, LR: 0.0007993741\n",
      "Epoch [149/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0007991468\n",
      "Epoch [149/500], Batch [90/110], Train Loss: 0.0025, Val Loss: 0.0011, LR: 0.0007989193\n",
      "Epoch [149/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007986918\n",
      "Epoch [149/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0007984641\n",
      "Epoch [150/500], Batch [10/110], Train Loss: 0.0016, Val Loss: 0.0012, LR: 0.0007982364\n",
      "Epoch [150/500], Batch [20/110], Train Loss: 0.0010, Val Loss: 0.0031, LR: 0.0007980085\n",
      "Epoch [150/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007977806\n",
      "Epoch [150/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0016, LR: 0.0007975526\n",
      "Epoch [150/500], Batch [50/110], Train Loss: 0.0023, Val Loss: 0.0017, LR: 0.0007973245\n",
      "Epoch [150/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007970962\n",
      "Epoch [150/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007968679\n",
      "Epoch [150/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0007966395\n",
      "Epoch [150/500], Batch [90/110], Train Loss: 0.0602, Val Loss: 0.0082, LR: 0.0007964110\n",
      "Epoch [150/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007961824\n",
      "Epoch [150/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0007959537\n",
      "Epoch [151/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0007957249\n",
      "Epoch [151/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007954960\n",
      "Epoch [151/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0007952670\n",
      "Epoch [151/500], Batch [40/110], Train Loss: 0.0023, Val Loss: 0.0014, LR: 0.0007950380\n",
      "Epoch [151/500], Batch [50/110], Train Loss: 0.0054, Val Loss: 0.0020, LR: 0.0007948088\n",
      "Epoch [151/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007945795\n",
      "Epoch [151/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007943502\n",
      "Epoch [151/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007941207\n",
      "Epoch [151/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007938912\n",
      "Epoch [151/500], Batch [100/110], Train Loss: 0.0019, Val Loss: 0.0010, LR: 0.0007936615\n",
      "Epoch [151/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007934318\n",
      "Confusion Matrix:\n",
      "[[634   1]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   0.99843   0.99843       635\n",
      "           1    0.99884   0.99884   0.99884       865\n",
      "\n",
      "    accuracy                        0.99867      1500\n",
      "   macro avg    0.99863   0.99863   0.99863      1500\n",
      "weighted avg    0.99867   0.99867   0.99867      1500\n",
      "\n",
      "Total Errors: 2\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Epoch 151: OK- Accuracy: 0.99867, Precision: 0.99884, Recall: 0.99884, F1: 0.99884, ROC AUC: 0.99863, AUPR (PR-AUC): 0.99836, Sensitivity: 0.99884, Specificity: 0.99843, Far: 0.0015748031496062992, False Positive Rate (FPR): 0.00157, False Negative Rate (FNR): 0.00116, Runtime: 0.043 sec , Memory Usage: 310.89 MB\n",
      "Epoch [152/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007932020\n",
      "Epoch [152/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007929720\n",
      "Epoch [152/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0007927420\n",
      "Epoch [152/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0007925119\n",
      "Epoch [152/500], Batch [50/110], Train Loss: 0.0026, Val Loss: 0.0010, LR: 0.0007922817\n",
      "Epoch [152/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007920514\n",
      "Epoch [152/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0007918210\n",
      "Epoch [152/500], Batch [80/110], Train Loss: 0.0075, Val Loss: 0.0021, LR: 0.0007915905\n",
      "Epoch [152/500], Batch [90/110], Train Loss: 0.0237, Val Loss: 0.0025, LR: 0.0007913599\n",
      "Epoch [152/500], Batch [100/110], Train Loss: 0.0031, Val Loss: 0.0015, LR: 0.0007911293\n",
      "Epoch [152/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0007908985\n",
      "Epoch [153/500], Batch [10/110], Train Loss: 0.0086, Val Loss: 0.0010, LR: 0.0007906676\n",
      "Epoch [153/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007904367\n",
      "Epoch [153/500], Batch [30/110], Train Loss: 0.0774, Val Loss: 0.0048, LR: 0.0007902056\n",
      "Epoch [153/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0007899745\n",
      "Epoch [153/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0007897433\n",
      "Epoch [153/500], Batch [60/110], Train Loss: 0.0035, Val Loss: 0.0011, LR: 0.0007895119\n",
      "Epoch [153/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007892805\n",
      "Epoch [153/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007890490\n",
      "Epoch [153/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007888174\n",
      "Epoch [153/500], Batch [100/110], Train Loss: 0.0012, Val Loss: 0.0016, LR: 0.0007885857\n",
      "Epoch [153/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007883539\n",
      "Epoch [154/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0007881220\n",
      "Epoch [154/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007878900\n",
      "Epoch [154/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0007876580\n",
      "Epoch [154/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0206, LR: 0.0007874258\n",
      "Epoch [154/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0007871936\n",
      "Epoch [154/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0007869612\n",
      "Epoch [154/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0007867288\n",
      "Epoch [154/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0011, LR: 0.0007864963\n",
      "Epoch [154/500], Batch [90/110], Train Loss: 0.0034, Val Loss: 0.0010, LR: 0.0007862636\n",
      "Epoch [154/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007860309\n",
      "Epoch [154/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0007857981\n",
      "Epoch [155/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0026, LR: 0.0007855652\n",
      "Epoch [155/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007853322\n",
      "Epoch [155/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007850992\n",
      "Epoch [155/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0007848660\n",
      "Epoch [155/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0007846327\n",
      "Epoch [155/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007843994\n",
      "Epoch [155/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0014, LR: 0.0007841660\n",
      "Epoch [155/500], Batch [80/110], Train Loss: 0.0103, Val Loss: 0.0020, LR: 0.0007839324\n",
      "Epoch [155/500], Batch [90/110], Train Loss: 0.0045, Val Loss: 0.0010, LR: 0.0007836988\n",
      "Epoch [155/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007834651\n",
      "Epoch [155/500], Batch [110/110], Train Loss: 0.0371, Val Loss: 0.0019, LR: 0.0007832313\n",
      "Epoch [156/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007829974\n",
      "Epoch [156/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0011, LR: 0.0007827634\n",
      "Epoch [156/500], Batch [30/110], Train Loss: 0.0216, Val Loss: 0.0016, LR: 0.0007825293\n",
      "Epoch [156/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007822951\n",
      "Epoch [156/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0012, LR: 0.0007820609\n",
      "Epoch [156/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007818265\n",
      "Epoch [156/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0007815921\n",
      "Epoch [156/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007813576\n",
      "Epoch [156/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007811229\n",
      "Epoch [156/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0010, LR: 0.0007808882\n",
      "Epoch [156/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0011, LR: 0.0007806534\n",
      "Epoch [157/500], Batch [10/110], Train Loss: 0.0043, Val Loss: 0.0023, LR: 0.0007804185\n",
      "Epoch [157/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0007801836\n",
      "Epoch [157/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0007799485\n",
      "Epoch [157/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0210, LR: 0.0007797133\n",
      "Epoch [157/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0007794781\n",
      "Epoch [157/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0007792427\n",
      "Epoch [157/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0007790073\n",
      "Epoch [157/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0013, LR: 0.0007787718\n",
      "Epoch [157/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0007785362\n",
      "Epoch [157/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0007783005\n",
      "Epoch [157/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0007780647\n",
      "Epoch [158/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007778288\n",
      "Epoch [158/500], Batch [20/110], Train Loss: 0.0046, Val Loss: 0.0022, LR: 0.0007775929\n",
      "Epoch [158/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007773568\n",
      "Epoch [158/500], Batch [40/110], Train Loss: 0.0029, Val Loss: 0.0020, LR: 0.0007771207\n",
      "Epoch [158/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007768844\n",
      "Epoch [158/500], Batch [60/110], Train Loss: 0.0508, Val Loss: 0.0067, LR: 0.0007766481\n",
      "Epoch [158/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0014, LR: 0.0007764117\n",
      "Epoch [158/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0007761752\n",
      "Epoch [158/500], Batch [90/110], Train Loss: 0.0213, Val Loss: 0.0039, LR: 0.0007759386\n",
      "Epoch [158/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007757020\n",
      "Epoch [158/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007754652\n",
      "Epoch [159/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007752284\n",
      "Epoch [159/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007749914\n",
      "Epoch [159/500], Batch [30/110], Train Loss: 0.0061, Val Loss: 0.0025, LR: 0.0007747544\n",
      "Epoch [159/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007745173\n",
      "Epoch [159/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0038, LR: 0.0007742801\n",
      "Epoch [159/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0007740428\n",
      "Epoch [159/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0012, LR: 0.0007738054\n",
      "Epoch [159/500], Batch [80/110], Train Loss: 0.0175, Val Loss: 0.0009, LR: 0.0007735679\n",
      "Epoch [159/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007733304\n",
      "Epoch [159/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007730927\n",
      "Epoch [159/500], Batch [110/110], Train Loss: 0.0078, Val Loss: 0.0016, LR: 0.0007728550\n",
      "Epoch [160/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0007726172\n",
      "Epoch [160/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007723793\n",
      "Epoch [160/500], Batch [30/110], Train Loss: 0.0492, Val Loss: 0.0075, LR: 0.0007721413\n",
      "Epoch [160/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007719032\n",
      "Epoch [160/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0031, LR: 0.0007716651\n",
      "Epoch [160/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0007714268\n",
      "Epoch [160/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0015, LR: 0.0007711885\n",
      "Epoch [160/500], Batch [80/110], Train Loss: 0.0022, Val Loss: 0.0008, LR: 0.0007709501\n",
      "Epoch [160/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0010, LR: 0.0007707115\n",
      "Epoch [160/500], Batch [100/110], Train Loss: 0.0038, Val Loss: 0.0009, LR: 0.0007704729\n",
      "Epoch [160/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0007702343\n",
      "Epoch [161/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0007699955\n",
      "Epoch [161/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007697566\n",
      "Epoch [161/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0015, LR: 0.0007695177\n",
      "Epoch [161/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0032, LR: 0.0007692787\n",
      "Epoch [161/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0036, LR: 0.0007690395\n",
      "Epoch [161/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0007688003\n",
      "Epoch [161/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0007685611\n",
      "Epoch [161/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007683217\n",
      "Epoch [161/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0010, LR: 0.0007680822\n",
      "Epoch [161/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007678427\n",
      "Epoch [161/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007676030\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99921       635\n",
      "           1    1.00000   0.99884   0.99942       865\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99921   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Epoch 161: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.040 sec , Memory Usage: 310.91 MB\n",
      "Epoch [162/500], Batch [10/110], Train Loss: 0.0017, Val Loss: 0.0009, LR: 0.0007673633\n",
      "Epoch [162/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007671235\n",
      "Epoch [162/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0031, LR: 0.0007668836\n",
      "Epoch [162/500], Batch [40/110], Train Loss: 0.0337, Val Loss: 0.0047, LR: 0.0007666437\n",
      "Epoch [162/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007664036\n",
      "Epoch [162/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0007661634\n",
      "Epoch [162/500], Batch [70/110], Train Loss: 0.0246, Val Loss: 0.0036, LR: 0.0007659232\n",
      "Epoch [162/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0007656829\n",
      "Epoch [162/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007654425\n",
      "Epoch [162/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007652020\n",
      "Epoch [162/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007649614\n",
      "Epoch [163/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007647208\n",
      "Epoch [163/500], Batch [20/110], Train Loss: 0.0019, Val Loss: 0.0007, LR: 0.0007644800\n",
      "Epoch [163/500], Batch [30/110], Train Loss: 0.0012, Val Loss: 0.0009, LR: 0.0007642392\n",
      "Epoch [163/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007639983\n",
      "Epoch [163/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007637573\n",
      "Epoch [163/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007635162\n",
      "Epoch [163/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007632751\n",
      "Epoch [163/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007630338\n",
      "Epoch [163/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007627925\n",
      "Epoch [163/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007625511\n",
      "Epoch [163/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0019, LR: 0.0007623096\n",
      "Epoch [164/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007620680\n",
      "Epoch [164/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007618263\n",
      "Epoch [164/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007615846\n",
      "Epoch [164/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0009, LR: 0.0007613427\n",
      "Epoch [164/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007611008\n",
      "Epoch [164/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0007608588\n",
      "Epoch [164/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007606167\n",
      "Epoch [164/500], Batch [80/110], Train Loss: 0.0023, Val Loss: 0.0010, LR: 0.0007603746\n",
      "Epoch [164/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0007, LR: 0.0007601323\n",
      "Epoch [164/500], Batch [100/110], Train Loss: 0.0029, Val Loss: 0.0015, LR: 0.0007598900\n",
      "Epoch [164/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007596476\n",
      "Epoch [165/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007594051\n",
      "Epoch [165/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007591625\n",
      "Epoch [165/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0007, LR: 0.0007589198\n",
      "Epoch [165/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007586771\n",
      "Epoch [165/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0011, LR: 0.0007584342\n",
      "Epoch [165/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007581913\n",
      "Epoch [165/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0007, LR: 0.0007579483\n",
      "Epoch [165/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0039, LR: 0.0007577052\n",
      "Epoch [165/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0009, LR: 0.0007574621\n",
      "Epoch [165/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007572188\n",
      "Epoch [165/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007569755\n",
      "Epoch [166/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007567321\n",
      "Epoch [166/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0007564886\n",
      "Epoch [166/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0017, LR: 0.0007562450\n",
      "Epoch [166/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0007560014\n",
      "Epoch [166/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007557576\n",
      "Epoch [166/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007555138\n",
      "Epoch [166/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0007552699\n",
      "Epoch [166/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007550259\n",
      "Epoch [166/500], Batch [90/110], Train Loss: 0.0038, Val Loss: 0.0008, LR: 0.0007547819\n",
      "Epoch [166/500], Batch [100/110], Train Loss: 0.0014, Val Loss: 0.0006, LR: 0.0007545377\n",
      "Epoch [166/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007542935\n",
      "Epoch [167/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0007540492\n",
      "Epoch [167/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007538048\n",
      "Epoch [167/500], Batch [30/110], Train Loss: 0.0122, Val Loss: 0.0023, LR: 0.0007535603\n",
      "Epoch [167/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0007533158\n",
      "Epoch [167/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007530711\n",
      "Epoch [167/500], Batch [60/110], Train Loss: 0.0041, Val Loss: 0.0006, LR: 0.0007528264\n",
      "Epoch [167/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007525816\n",
      "Epoch [167/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0007523367\n",
      "Epoch [167/500], Batch [90/110], Train Loss: 0.0194, Val Loss: 0.0010, LR: 0.0007520918\n",
      "Epoch [167/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007518467\n",
      "Epoch [167/500], Batch [110/110], Train Loss: 0.0122, Val Loss: 0.0006, LR: 0.0007516016\n",
      "Epoch [168/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0007513564\n",
      "Epoch [168/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0007511111\n",
      "Epoch [168/500], Batch [30/110], Train Loss: 0.0021, Val Loss: 0.0007, LR: 0.0007508658\n",
      "Epoch [168/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007506204\n",
      "Epoch [168/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007503748\n",
      "Epoch [168/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007501292\n",
      "Epoch [168/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007498836\n",
      "Epoch [168/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007496378\n",
      "Epoch [168/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007493920\n",
      "Epoch [168/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007491460\n",
      "Epoch [168/500], Batch [110/110], Train Loss: 0.0131, Val Loss: 0.0006, LR: 0.0007489000\n",
      "Epoch [169/500], Batch [10/110], Train Loss: 0.0020, Val Loss: 0.0009, LR: 0.0007486540\n",
      "Epoch [169/500], Batch [20/110], Train Loss: 0.0023, Val Loss: 0.0007, LR: 0.0007484078\n",
      "Epoch [169/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007481616\n",
      "Epoch [169/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007479152\n",
      "Epoch [169/500], Batch [50/110], Train Loss: 0.0015, Val Loss: 0.0005, LR: 0.0007476688\n",
      "Epoch [169/500], Batch [60/110], Train Loss: 0.0051, Val Loss: 0.0063, LR: 0.0007474224\n",
      "Epoch [169/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007471758\n",
      "Epoch [169/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007469292\n",
      "Epoch [169/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007466825\n",
      "Epoch [169/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0007464357\n",
      "Epoch [169/500], Batch [110/110], Train Loss: 0.0064, Val Loss: 0.0013, LR: 0.0007461888\n",
      "Epoch [170/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007459419\n",
      "Epoch [170/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007456948\n",
      "Epoch [170/500], Batch [30/110], Train Loss: 0.1956, Val Loss: 0.0006, LR: 0.0007454477\n",
      "Epoch [170/500], Batch [40/110], Train Loss: 0.0018, Val Loss: 0.0011, LR: 0.0007452005\n",
      "Epoch [170/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007449533\n",
      "Epoch [170/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0005, LR: 0.0007447059\n",
      "Epoch [170/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007444585\n",
      "Epoch [170/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007442110\n",
      "Epoch [170/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0016, LR: 0.0007439635\n",
      "Epoch [170/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007437158\n",
      "Epoch [170/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0009, LR: 0.0007434681\n",
      "Epoch [171/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007432203\n",
      "Epoch [171/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0007429724\n",
      "Epoch [171/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007427244\n",
      "Epoch [171/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007424764\n",
      "Epoch [171/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007422282\n",
      "Epoch [171/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007419801\n",
      "Epoch [171/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0007417318\n",
      "Epoch [171/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007414834\n",
      "Epoch [171/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007412350\n",
      "Epoch [171/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0007409865\n",
      "Epoch [171/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0007407379\n",
      "Confusion Matrix:\n",
      "[[629   6]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99841   0.99055   0.99447       635\n",
      "           1    0.99310   0.99884   0.99597       865\n",
      "\n",
      "    accuracy                        0.99533      1500\n",
      "   macro avg    0.99576   0.99470   0.99522      1500\n",
      "weighted avg    0.99535   0.99533   0.99533      1500\n",
      "\n",
      "Total Errors: 7\n",
      "Index: 172, Predicted: 1, Actual: 0\n",
      "Index: 693, Predicted: 1, Actual: 0\n",
      "Index: 767, Predicted: 1, Actual: 0\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Index: 1067, Predicted: 1, Actual: 0\n",
      "Epoch 171: OK- Accuracy: 0.99533, Precision: 0.99310, Recall: 0.99884, F1: 0.99597, ROC AUC: 0.99470, AUPR (PR-AUC): 0.99262, Sensitivity: 0.99884, Specificity: 0.99055, Far: 0.009448818897637795, False Positive Rate (FPR): 0.00945, False Negative Rate (FNR): 0.00116, Runtime: 0.041 sec , Memory Usage: 310.91 MB\n",
      "Epoch [172/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0007404893\n",
      "Epoch [172/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007402405\n",
      "Epoch [172/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0032, LR: 0.0007399917\n",
      "Epoch [172/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007397428\n",
      "Epoch [172/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0007394938\n",
      "Epoch [172/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007392448\n",
      "Epoch [172/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007389957\n",
      "Epoch [172/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007387465\n",
      "Epoch [172/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007384972\n",
      "Epoch [172/500], Batch [100/110], Train Loss: 0.0019, Val Loss: 0.0005, LR: 0.0007382479\n",
      "Epoch [172/500], Batch [110/110], Train Loss: 0.0026, Val Loss: 0.0007, LR: 0.0007379984\n",
      "Epoch [173/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007377489\n",
      "Epoch [173/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0007374994\n",
      "Epoch [173/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0005, LR: 0.0007372497\n",
      "Epoch [173/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007370000\n",
      "Epoch [173/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007367502\n",
      "Epoch [173/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007365003\n",
      "Epoch [173/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007362504\n",
      "Epoch [173/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0007360003\n",
      "Epoch [173/500], Batch [90/110], Train Loss: 0.0082, Val Loss: 0.0004, LR: 0.0007357502\n",
      "Epoch [173/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007355000\n",
      "Epoch [173/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007352498\n",
      "Epoch [174/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007349995\n",
      "Epoch [174/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007347490\n",
      "Epoch [174/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007344986\n",
      "Epoch [174/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0011, LR: 0.0007342480\n",
      "Epoch [174/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007339974\n",
      "Epoch [174/500], Batch [60/110], Train Loss: 0.0033, Val Loss: 0.0004, LR: 0.0007337467\n",
      "Epoch [174/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0007334959\n",
      "Epoch [174/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007332450\n",
      "Epoch [174/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0007329941\n",
      "Epoch [174/500], Batch [100/110], Train Loss: 0.0056, Val Loss: 0.0010, LR: 0.0007327431\n",
      "Epoch [174/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0007324920\n",
      "Epoch [175/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0007322409\n",
      "Epoch [175/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007319897\n",
      "Epoch [175/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007317384\n",
      "Epoch [175/500], Batch [40/110], Train Loss: 0.0010, Val Loss: 0.0005, LR: 0.0007314870\n",
      "Epoch [175/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0007312355\n",
      "Epoch [175/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0007309840\n",
      "Epoch [175/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007307324\n",
      "Epoch [175/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007304807\n",
      "Epoch [175/500], Batch [90/110], Train Loss: 0.0038, Val Loss: 0.0011, LR: 0.0007302290\n",
      "Epoch [175/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0007299772\n",
      "Epoch [175/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0007297253\n",
      "Epoch [176/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0007294733\n",
      "Epoch [176/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007292213\n",
      "Epoch [176/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007289692\n",
      "Epoch [176/500], Batch [40/110], Train Loss: 0.0028, Val Loss: 0.0007, LR: 0.0007287170\n",
      "Epoch [176/500], Batch [50/110], Train Loss: 0.0218, Val Loss: 0.0009, LR: 0.0007284648\n",
      "Epoch [176/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0007282124\n",
      "Epoch [176/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0004, LR: 0.0007279600\n",
      "Epoch [176/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007277075\n",
      "Epoch [176/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0007274550\n",
      "Epoch [176/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0007272024\n",
      "Epoch [176/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0007269497\n",
      "Epoch [177/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007266969\n",
      "Epoch [177/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007264441\n",
      "Epoch [177/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007261912\n",
      "Epoch [177/500], Batch [40/110], Train Loss: 0.0102, Val Loss: 0.0009, LR: 0.0007259382\n",
      "Epoch [177/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007256851\n",
      "Epoch [177/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007254320\n",
      "Epoch [177/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007251788\n",
      "Epoch [177/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0007249256\n",
      "Epoch [177/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0007246722\n",
      "Epoch [177/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0011, LR: 0.0007244188\n",
      "Epoch [177/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007241653\n",
      "Epoch [178/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0007239118\n",
      "Epoch [178/500], Batch [20/110], Train Loss: 0.0021, Val Loss: 0.0003, LR: 0.0007236581\n",
      "Epoch [178/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0004, LR: 0.0007234044\n",
      "Epoch [178/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007231507\n",
      "Epoch [178/500], Batch [50/110], Train Loss: 0.1011, Val Loss: 0.0042, LR: 0.0007228968\n",
      "Epoch [178/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0129, LR: 0.0007226429\n",
      "Epoch [178/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007223889\n",
      "Epoch [178/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0018, LR: 0.0007221349\n",
      "Epoch [178/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007218808\n",
      "Epoch [178/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0007216266\n",
      "Epoch [178/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007213723\n",
      "Epoch [179/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007211180\n",
      "Epoch [179/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007208636\n",
      "Epoch [179/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0007206091\n",
      "Epoch [179/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007203545\n",
      "Epoch [179/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0007200999\n",
      "Epoch [179/500], Batch [60/110], Train Loss: 0.0026, Val Loss: 0.0023, LR: 0.0007198452\n",
      "Epoch [179/500], Batch [70/110], Train Loss: 0.1017, Val Loss: 0.0148, LR: 0.0007195905\n",
      "Epoch [179/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0007193356\n",
      "Epoch [179/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007190807\n",
      "Epoch [179/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007188258\n",
      "Epoch [179/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0007185707\n",
      "Epoch [180/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007183156\n",
      "Epoch [180/500], Batch [20/110], Train Loss: 0.1667, Val Loss: 0.0004, LR: 0.0007180605\n",
      "Epoch [180/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007178052\n",
      "Epoch [180/500], Batch [40/110], Train Loss: 0.0010, Val Loss: 0.0004, LR: 0.0007175499\n",
      "Epoch [180/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0007172945\n",
      "Epoch [180/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007170391\n",
      "Epoch [180/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0017, LR: 0.0007167835\n",
      "Epoch [180/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0007165279\n",
      "Epoch [180/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007162723\n",
      "Epoch [180/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0007160165\n",
      "Epoch [180/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007157607\n",
      "Epoch [181/500], Batch [10/110], Train Loss: 0.0033, Val Loss: 0.0005, LR: 0.0007155049\n",
      "Epoch [181/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007152489\n",
      "Epoch [181/500], Batch [30/110], Train Loss: 0.0035, Val Loss: 0.0004, LR: 0.0007149929\n",
      "Epoch [181/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007147369\n",
      "Epoch [181/500], Batch [50/110], Train Loss: 0.0022, Val Loss: 0.0006, LR: 0.0007144807\n",
      "Epoch [181/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007142245\n",
      "Epoch [181/500], Batch [70/110], Train Loss: 0.0020, Val Loss: 0.0004, LR: 0.0007139682\n",
      "Epoch [181/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0007137119\n",
      "Epoch [181/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0007134555\n",
      "Epoch [181/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007131990\n",
      "Epoch [181/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007129424\n",
      "Confusion Matrix:\n",
      "[[633   2]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99842   0.99685   0.99764       635\n",
      "           1    0.99769   0.99884   0.99827       865\n",
      "\n",
      "    accuracy                        0.99800      1500\n",
      "   macro avg    0.99806   0.99785   0.99795      1500\n",
      "weighted avg    0.99800   0.99800   0.99800      1500\n",
      "\n",
      "Total Errors: 3\n",
      "Index: 767, Predicted: 1, Actual: 0\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Index: 1067, Predicted: 1, Actual: 0\n",
      "Epoch 181: OK- Accuracy: 0.99800, Precision: 0.99769, Recall: 0.99884, F1: 0.99827, ROC AUC: 0.99785, AUPR (PR-AUC): 0.99720, Sensitivity: 0.99884, Specificity: 0.99685, Far: 0.0031496062992125984, False Positive Rate (FPR): 0.00315, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 310.91 MB\n",
      "Epoch [182/500], Batch [10/110], Train Loss: 0.0024, Val Loss: 0.0004, LR: 0.0007126858\n",
      "Epoch [182/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007124291\n",
      "Epoch [182/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007121724\n",
      "Epoch [182/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0007119156\n",
      "Epoch [182/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0023, LR: 0.0007116587\n",
      "Epoch [182/500], Batch [60/110], Train Loss: 0.0078, Val Loss: 0.0004, LR: 0.0007114017\n",
      "Epoch [182/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007111447\n",
      "Epoch [182/500], Batch [80/110], Train Loss: 0.0137, Val Loss: 0.0020, LR: 0.0007108876\n",
      "Epoch [182/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007106304\n",
      "Epoch [182/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007103732\n",
      "Epoch [182/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007101159\n",
      "Epoch [183/500], Batch [10/110], Train Loss: 0.0231, Val Loss: 0.0057, LR: 0.0007098586\n",
      "Epoch [183/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007096011\n",
      "Epoch [183/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0007093436\n",
      "Epoch [183/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007090861\n",
      "Epoch [183/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007088284\n",
      "Epoch [183/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007085708\n",
      "Epoch [183/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0005, LR: 0.0007083130\n",
      "Epoch [183/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0007080552\n",
      "Epoch [183/500], Batch [90/110], Train Loss: 0.0047, Val Loss: 0.0011, LR: 0.0007077973\n",
      "Epoch [183/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007075393\n",
      "Epoch [183/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007072813\n",
      "Epoch [184/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0007070232\n",
      "Epoch [184/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007067650\n",
      "Epoch [184/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007065068\n",
      "Epoch [184/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007062485\n",
      "Epoch [184/500], Batch [50/110], Train Loss: 0.0492, Val Loss: 0.0006, LR: 0.0007059902\n",
      "Epoch [184/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007057318\n",
      "Epoch [184/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007054733\n",
      "Epoch [184/500], Batch [80/110], Train Loss: 0.0013, Val Loss: 0.0007, LR: 0.0007052147\n",
      "Epoch [184/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0007049561\n",
      "Epoch [184/500], Batch [100/110], Train Loss: 0.0062, Val Loss: 0.0012, LR: 0.0007046974\n",
      "Epoch [184/500], Batch [110/110], Train Loss: 0.0025, Val Loss: 0.0009, LR: 0.0007044387\n",
      "Epoch [185/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007041799\n",
      "Epoch [185/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0007039210\n",
      "Epoch [185/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0010, LR: 0.0007036621\n",
      "Epoch [185/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007034031\n",
      "Epoch [185/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007031440\n",
      "Epoch [185/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007028848\n",
      "Epoch [185/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0005, LR: 0.0007026256\n",
      "Epoch [185/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007023664\n",
      "Epoch [185/500], Batch [90/110], Train Loss: 0.0350, Val Loss: 0.0013, LR: 0.0007021071\n",
      "Epoch [185/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007018477\n",
      "Epoch [185/500], Batch [110/110], Train Loss: 0.0031, Val Loss: 0.0004, LR: 0.0007015882\n",
      "Epoch [186/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007013287\n",
      "Epoch [186/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0007010691\n",
      "Epoch [186/500], Batch [30/110], Train Loss: 0.0016, Val Loss: 0.0005, LR: 0.0007008095\n",
      "Epoch [186/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0006, LR: 0.0007005497\n",
      "Epoch [186/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0007002900\n",
      "Epoch [186/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0007000301\n",
      "Epoch [186/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006997702\n",
      "Epoch [186/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006995102\n",
      "Epoch [186/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006992502\n",
      "Epoch [186/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006989901\n",
      "Epoch [186/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0006987300\n",
      "Epoch [187/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0006, LR: 0.0006984697\n",
      "Epoch [187/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006982095\n",
      "Epoch [187/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0006979491\n",
      "Epoch [187/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0004, LR: 0.0006976887\n",
      "Epoch [187/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006974282\n",
      "Epoch [187/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006971677\n",
      "Epoch [187/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006969071\n",
      "Epoch [187/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0006966464\n",
      "Epoch [187/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0006963857\n",
      "Epoch [187/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006961249\n",
      "Epoch [187/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0006958641\n",
      "Epoch [188/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006956032\n",
      "Epoch [188/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006953422\n",
      "Epoch [188/500], Batch [30/110], Train Loss: 0.0133, Val Loss: 0.0042, LR: 0.0006950812\n",
      "Epoch [188/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0005, LR: 0.0006948201\n",
      "Epoch [188/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006945589\n",
      "Epoch [188/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0007, LR: 0.0006942977\n",
      "Epoch [188/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0006940364\n",
      "Epoch [188/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006937751\n",
      "Epoch [188/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006935136\n",
      "Epoch [188/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006932522\n",
      "Epoch [188/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0003, LR: 0.0006929907\n",
      "Epoch [189/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0032, LR: 0.0006927291\n",
      "Epoch [189/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006924674\n",
      "Epoch [189/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0006922057\n",
      "Epoch [189/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006919439\n",
      "Epoch [189/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006916821\n",
      "Epoch [189/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006914202\n",
      "Epoch [189/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0006911582\n",
      "Epoch [189/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0006908962\n",
      "Epoch [189/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0006906341\n",
      "Epoch [189/500], Batch [100/110], Train Loss: 0.0059, Val Loss: 0.0004, LR: 0.0006903720\n",
      "Epoch [189/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006901098\n",
      "Epoch [190/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006898475\n",
      "Epoch [190/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006895852\n",
      "Epoch [190/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006893228\n",
      "Epoch [190/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0006890604\n",
      "Epoch [190/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006887979\n",
      "Epoch [190/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0006885353\n",
      "Epoch [190/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0006882727\n",
      "Epoch [190/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0006880100\n",
      "Epoch [190/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0006, LR: 0.0006877473\n",
      "Epoch [190/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0004, LR: 0.0006874845\n",
      "Epoch [190/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0004, LR: 0.0006872217\n",
      "Epoch [191/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0006869587\n",
      "Epoch [191/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0006866958\n",
      "Epoch [191/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006864327\n",
      "Epoch [191/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006861696\n",
      "Epoch [191/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006859065\n",
      "Epoch [191/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0006856433\n",
      "Epoch [191/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006853800\n",
      "Epoch [191/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0006851167\n",
      "Epoch [191/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0007, LR: 0.0006848533\n",
      "Epoch [191/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006845898\n",
      "Epoch [191/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0006843263\n",
      "Confusion Matrix:\n",
      "[[634   1]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   0.99843   0.99843       635\n",
      "           1    0.99884   0.99884   0.99884       865\n",
      "\n",
      "    accuracy                        0.99867      1500\n",
      "   macro avg    0.99863   0.99863   0.99863      1500\n",
      "weighted avg    0.99867   0.99867   0.99867      1500\n",
      "\n",
      "Total Errors: 2\n",
      "Index: 767, Predicted: 1, Actual: 0\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Epoch 191: OK- Accuracy: 0.99867, Precision: 0.99884, Recall: 0.99884, F1: 0.99884, ROC AUC: 0.99863, AUPR (PR-AUC): 0.99836, Sensitivity: 0.99884, Specificity: 0.99843, Far: 0.0015748031496062992, False Positive Rate (FPR): 0.00157, False Negative Rate (FNR): 0.00116, Runtime: 0.039 sec , Memory Usage: 310.92 MB\n",
      "Epoch [192/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0006840627\n",
      "Epoch [192/500], Batch [20/110], Train Loss: 0.0042, Val Loss: 0.0003, LR: 0.0006837991\n",
      "Epoch [192/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0004, LR: 0.0006835354\n",
      "Epoch [192/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006832717\n",
      "Epoch [192/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006830079\n",
      "Epoch [192/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0006827440\n",
      "Epoch [192/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0059, LR: 0.0006824801\n",
      "Epoch [192/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0075, LR: 0.0006822161\n",
      "Epoch [192/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0003, LR: 0.0006819521\n",
      "Epoch [192/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0006816880\n",
      "Epoch [192/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006814239\n",
      "Epoch [193/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006811597\n",
      "Epoch [193/500], Batch [20/110], Train Loss: 0.0053, Val Loss: 0.0008, LR: 0.0006808954\n",
      "Epoch [193/500], Batch [30/110], Train Loss: 0.0031, Val Loss: 0.0005, LR: 0.0006806311\n",
      "Epoch [193/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006803667\n",
      "Epoch [193/500], Batch [50/110], Train Loss: 0.0014, Val Loss: 0.0004, LR: 0.0006801023\n",
      "Epoch [193/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0006798378\n",
      "Epoch [193/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0027, LR: 0.0006795732\n",
      "Epoch [193/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006793086\n",
      "Epoch [193/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0004, LR: 0.0006790440\n",
      "Epoch [193/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0007, LR: 0.0006787793\n",
      "Epoch [193/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006785145\n",
      "Epoch [194/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006782497\n",
      "Epoch [194/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006779848\n",
      "Epoch [194/500], Batch [30/110], Train Loss: 0.0396, Val Loss: 0.0015, LR: 0.0006777198\n",
      "Epoch [194/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0006774548\n",
      "Epoch [194/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0006771898\n",
      "Epoch [194/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006769247\n",
      "Epoch [194/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0004, LR: 0.0006766595\n",
      "Epoch [194/500], Batch [80/110], Train Loss: 0.0012, Val Loss: 0.0004, LR: 0.0006763943\n",
      "Epoch [194/500], Batch [90/110], Train Loss: 0.0037, Val Loss: 0.0010, LR: 0.0006761290\n",
      "Epoch [194/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0006758636\n",
      "Epoch [194/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006755982\n",
      "Epoch [195/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0006753328\n",
      "Epoch [195/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006750673\n",
      "Epoch [195/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006748017\n",
      "Epoch [195/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006745361\n",
      "Epoch [195/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006742704\n",
      "Epoch [195/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006740047\n",
      "Epoch [195/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006737389\n",
      "Epoch [195/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006734731\n",
      "Epoch [195/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006732072\n",
      "Epoch [195/500], Batch [100/110], Train Loss: 0.0015, Val Loss: 0.0005, LR: 0.0006729413\n",
      "Epoch [195/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006726753\n",
      "Epoch [196/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0006724092\n",
      "Epoch [196/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006721431\n",
      "Epoch [196/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0006718769\n",
      "Epoch [196/500], Batch [40/110], Train Loss: 0.0036, Val Loss: 0.0003, LR: 0.0006716107\n",
      "Epoch [196/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006713444\n",
      "Epoch [196/500], Batch [60/110], Train Loss: 0.0021, Val Loss: 0.0005, LR: 0.0006710781\n",
      "Epoch [196/500], Batch [70/110], Train Loss: 0.0046, Val Loss: 0.0008, LR: 0.0006708117\n",
      "Epoch [196/500], Batch [80/110], Train Loss: 0.0133, Val Loss: 0.0003, LR: 0.0006705453\n",
      "Epoch [196/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006702788\n",
      "Epoch [196/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006700123\n",
      "Epoch [196/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006697457\n",
      "Epoch [197/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006694790\n",
      "Epoch [197/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0006692123\n",
      "Epoch [197/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006689456\n",
      "Epoch [197/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006686787\n",
      "Epoch [197/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006684119\n",
      "Epoch [197/500], Batch [60/110], Train Loss: 0.0021, Val Loss: 0.0003, LR: 0.0006681450\n",
      "Epoch [197/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0006678780\n",
      "Epoch [197/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006676110\n",
      "Epoch [197/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006673439\n",
      "Epoch [197/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0006670768\n",
      "Epoch [197/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006668096\n",
      "Epoch [198/500], Batch [10/110], Train Loss: 0.0040, Val Loss: 0.0004, LR: 0.0006665423\n",
      "Epoch [198/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006662750\n",
      "Epoch [198/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006660077\n",
      "Epoch [198/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0004, LR: 0.0006657403\n",
      "Epoch [198/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006654729\n",
      "Epoch [198/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006652054\n",
      "Epoch [198/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006649378\n",
      "Epoch [198/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0006646702\n",
      "Epoch [198/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006644026\n",
      "Epoch [198/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0006641348\n",
      "Epoch [198/500], Batch [110/110], Train Loss: 0.0009, Val Loss: 0.0003, LR: 0.0006638671\n",
      "Epoch [199/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006635993\n",
      "Epoch [199/500], Batch [20/110], Train Loss: 0.0044, Val Loss: 0.0011, LR: 0.0006633314\n",
      "Epoch [199/500], Batch [30/110], Train Loss: 0.0027, Val Loss: 0.0003, LR: 0.0006630635\n",
      "Epoch [199/500], Batch [40/110], Train Loss: 0.0085, Val Loss: 0.0005, LR: 0.0006627955\n",
      "Epoch [199/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006625275\n",
      "Epoch [199/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006622594\n",
      "Epoch [199/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006619913\n",
      "Epoch [199/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006617231\n",
      "Epoch [199/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0006614549\n",
      "Epoch [199/500], Batch [100/110], Train Loss: 0.0083, Val Loss: 0.0005, LR: 0.0006611867\n",
      "Epoch [199/500], Batch [110/110], Train Loss: 0.0021, Val Loss: 0.0003, LR: 0.0006609183\n",
      "Epoch [200/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006606500\n",
      "Epoch [200/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006603815\n",
      "Epoch [200/500], Batch [30/110], Train Loss: 0.0324, Val Loss: 0.0004, LR: 0.0006601130\n",
      "Epoch [200/500], Batch [40/110], Train Loss: 0.0092, Val Loss: 0.0023, LR: 0.0006598445\n",
      "Epoch [200/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0006595759\n",
      "Epoch [200/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0006593073\n",
      "Epoch [200/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006590386\n",
      "Epoch [200/500], Batch [80/110], Train Loss: 0.0010, Val Loss: 0.0004, LR: 0.0006587699\n",
      "Epoch [200/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006585011\n",
      "Epoch [200/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006582323\n",
      "Epoch [200/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0004, LR: 0.0006579634\n",
      "Epoch [201/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006576945\n",
      "Epoch [201/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006574255\n",
      "Epoch [201/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0006571565\n",
      "Epoch [201/500], Batch [40/110], Train Loss: 0.0021, Val Loss: 0.0006, LR: 0.0006568874\n",
      "Epoch [201/500], Batch [50/110], Train Loss: 0.0024, Val Loss: 0.0003, LR: 0.0006566183\n",
      "Epoch [201/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0006563491\n",
      "Epoch [201/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0006560799\n",
      "Epoch [201/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0005, LR: 0.0006558106\n",
      "Epoch [201/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006555413\n",
      "Epoch [201/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006552719\n",
      "Epoch [201/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006550025\n",
      "Confusion Matrix:\n",
      "[[634   1]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   0.99843   0.99843       635\n",
      "           1    0.99884   0.99884   0.99884       865\n",
      "\n",
      "    accuracy                        0.99867      1500\n",
      "   macro avg    0.99863   0.99863   0.99863      1500\n",
      "weighted avg    0.99867   0.99867   0.99867      1500\n",
      "\n",
      "Total Errors: 2\n",
      "Index: 767, Predicted: 1, Actual: 0\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Epoch 201: OK- Accuracy: 0.99867, Precision: 0.99884, Recall: 0.99884, F1: 0.99884, ROC AUC: 0.99863, AUPR (PR-AUC): 0.99836, Sensitivity: 0.99884, Specificity: 0.99843, Far: 0.0015748031496062992, False Positive Rate (FPR): 0.00157, False Negative Rate (FNR): 0.00116, Runtime: 0.033 sec , Memory Usage: 310.93 MB\n",
      "Epoch [202/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006547330\n",
      "Epoch [202/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006544635\n",
      "Epoch [202/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006541939\n",
      "Epoch [202/500], Batch [40/110], Train Loss: 0.0187, Val Loss: 0.0020, LR: 0.0006539243\n",
      "Epoch [202/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0006536546\n",
      "Epoch [202/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006533849\n",
      "Epoch [202/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006531151\n",
      "Epoch [202/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006528453\n",
      "Epoch [202/500], Batch [90/110], Train Loss: 0.0572, Val Loss: 0.0003, LR: 0.0006525754\n",
      "Epoch [202/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006523055\n",
      "Epoch [202/500], Batch [110/110], Train Loss: 0.0018, Val Loss: 0.0003, LR: 0.0006520356\n",
      "Epoch [203/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006517656\n",
      "Epoch [203/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006514955\n",
      "Epoch [203/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0006512254\n",
      "Epoch [203/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006509553\n",
      "Epoch [203/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0006506851\n",
      "Epoch [203/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006504148\n",
      "Epoch [203/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006501445\n",
      "Epoch [203/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006498742\n",
      "Epoch [203/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0006496038\n",
      "Epoch [203/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006493334\n",
      "Epoch [203/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0006490629\n",
      "Epoch [204/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0029, LR: 0.0006487924\n",
      "Epoch [204/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006485218\n",
      "Epoch [204/500], Batch [30/110], Train Loss: 0.0072, Val Loss: 0.0011, LR: 0.0006482512\n",
      "Epoch [204/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006479805\n",
      "Epoch [204/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0006477098\n",
      "Epoch [204/500], Batch [60/110], Train Loss: 0.0031, Val Loss: 0.0007, LR: 0.0006474390\n",
      "Epoch [204/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006471682\n",
      "Epoch [204/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006468974\n",
      "Epoch [204/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006466265\n",
      "Epoch [204/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006463555\n",
      "Epoch [204/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0003, LR: 0.0006460845\n",
      "Epoch [205/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006458135\n",
      "Epoch [205/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0006455424\n",
      "Epoch [205/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0006452713\n",
      "Epoch [205/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006450001\n",
      "Epoch [205/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006447289\n",
      "Epoch [205/500], Batch [60/110], Train Loss: 0.0017, Val Loss: 0.0003, LR: 0.0006444576\n",
      "Epoch [205/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006441863\n",
      "Epoch [205/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006439149\n",
      "Epoch [205/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0004, LR: 0.0006436435\n",
      "Epoch [205/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006433721\n",
      "Epoch [205/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0006431006\n",
      "Epoch [206/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0006428291\n",
      "Epoch [206/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006425575\n",
      "Epoch [206/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0006422858\n",
      "Epoch [206/500], Batch [40/110], Train Loss: 0.0020, Val Loss: 0.0003, LR: 0.0006420142\n",
      "Epoch [206/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006417425\n",
      "Epoch [206/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006414707\n",
      "Epoch [206/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0123, LR: 0.0006411989\n",
      "Epoch [206/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006409270\n",
      "Epoch [206/500], Batch [90/110], Train Loss: 0.0131, Val Loss: 0.0013, LR: 0.0006406551\n",
      "Epoch [206/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0038, LR: 0.0006403832\n",
      "Epoch [206/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0004, LR: 0.0006401112\n",
      "Epoch [207/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0006398392\n",
      "Epoch [207/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006395671\n",
      "Epoch [207/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006392950\n",
      "Epoch [207/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0004, LR: 0.0006390228\n",
      "Epoch [207/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006387506\n",
      "Epoch [207/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006384784\n",
      "Epoch [207/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006382061\n",
      "Epoch [207/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006379337\n",
      "Epoch [207/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0006376614\n",
      "Epoch [207/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006373889\n",
      "Epoch [207/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006371165\n",
      "Epoch [208/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006368440\n",
      "Epoch [208/500], Batch [20/110], Train Loss: 0.0056, Val Loss: 0.0015, LR: 0.0006365714\n",
      "Epoch [208/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0006362988\n",
      "Epoch [208/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0006360262\n",
      "Epoch [208/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006357535\n",
      "Epoch [208/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0006354808\n",
      "Epoch [208/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0006352080\n",
      "Epoch [208/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006349352\n",
      "Epoch [208/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006346624\n",
      "Epoch [208/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006343895\n",
      "Epoch [208/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006341165\n",
      "Epoch [209/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006338436\n",
      "Epoch [209/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006335706\n",
      "Epoch [209/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006332975\n",
      "Epoch [209/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006330244\n",
      "Epoch [209/500], Batch [50/110], Train Loss: 0.0328, Val Loss: 0.0012, LR: 0.0006327512\n",
      "Epoch [209/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006324781\n",
      "Epoch [209/500], Batch [70/110], Train Loss: 0.1914, Val Loss: 0.0002, LR: 0.0006322048\n",
      "Epoch [209/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006319316\n",
      "Epoch [209/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006316583\n",
      "Epoch [209/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0006313849\n",
      "Epoch [209/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006311115\n",
      "Epoch [210/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006308381\n",
      "Epoch [210/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006305646\n",
      "Epoch [210/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0006302911\n",
      "Epoch [210/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0006300175\n",
      "Epoch [210/500], Batch [50/110], Train Loss: 0.0029, Val Loss: 0.0006, LR: 0.0006297439\n",
      "Epoch [210/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006294703\n",
      "Epoch [210/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0006291966\n",
      "Epoch [210/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0006289229\n",
      "Epoch [210/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0006286491\n",
      "Epoch [210/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006283753\n",
      "Epoch [210/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006281015\n",
      "Epoch [211/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0006278276\n",
      "Epoch [211/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006275537\n",
      "Epoch [211/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006272797\n",
      "Epoch [211/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006270057\n",
      "Epoch [211/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006267317\n",
      "Epoch [211/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006264576\n",
      "Epoch [211/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006261835\n",
      "Epoch [211/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006259093\n",
      "Epoch [211/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006256351\n",
      "Epoch [211/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006253609\n",
      "Epoch [211/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0006250866\n",
      "Confusion Matrix:\n",
      "[[634   1]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   0.99843   0.99843       635\n",
      "           1    0.99884   0.99884   0.99884       865\n",
      "\n",
      "    accuracy                        0.99867      1500\n",
      "   macro avg    0.99863   0.99863   0.99863      1500\n",
      "weighted avg    0.99867   0.99867   0.99867      1500\n",
      "\n",
      "Total Errors: 2\n",
      "Index: 767, Predicted: 1, Actual: 0\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Epoch 211: OK- Accuracy: 0.99867, Precision: 0.99884, Recall: 0.99884, F1: 0.99884, ROC AUC: 0.99863, AUPR (PR-AUC): 0.99836, Sensitivity: 0.99884, Specificity: 0.99843, Far: 0.0015748031496062992, False Positive Rate (FPR): 0.00157, False Negative Rate (FNR): 0.00116, Runtime: 0.035 sec , Memory Usage: 310.93 MB\n",
      "Epoch [212/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0006248123\n",
      "Epoch [212/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006245379\n",
      "Epoch [212/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0006242636\n",
      "Epoch [212/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006239891\n",
      "Epoch [212/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006237146\n",
      "Epoch [212/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006234401\n",
      "Epoch [212/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006231656\n",
      "Epoch [212/500], Batch [80/110], Train Loss: 0.0041, Val Loss: 0.0010, LR: 0.0006228910\n",
      "Epoch [212/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006226164\n",
      "Epoch [212/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0006223417\n",
      "Epoch [212/500], Batch [110/110], Train Loss: 0.0021, Val Loss: 0.0002, LR: 0.0006220670\n",
      "Epoch [213/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006217923\n",
      "Epoch [213/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006215175\n",
      "Epoch [213/500], Batch [30/110], Train Loss: 0.0037, Val Loss: 0.0052, LR: 0.0006212427\n",
      "Epoch [213/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006209678\n",
      "Epoch [213/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006206929\n",
      "Epoch [213/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006204180\n",
      "Epoch [213/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006201430\n",
      "Epoch [213/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0006198680\n",
      "Epoch [213/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006195930\n",
      "Epoch [213/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0006193179\n",
      "Epoch [213/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006190428\n",
      "Epoch [214/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006187676\n",
      "Epoch [214/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0002, LR: 0.0006184924\n",
      "Epoch [214/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006182172\n",
      "Epoch [214/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006179419\n",
      "Epoch [214/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0006176666\n",
      "Epoch [214/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006173913\n",
      "Epoch [214/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006171159\n",
      "Epoch [214/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006168405\n",
      "Epoch [214/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006165650\n",
      "Epoch [214/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006162895\n",
      "Epoch [214/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0006160140\n",
      "Epoch [215/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0006157385\n",
      "Epoch [215/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006154629\n",
      "Epoch [215/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006151872\n",
      "Epoch [215/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0004, LR: 0.0006149116\n",
      "Epoch [215/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006146359\n",
      "Epoch [215/500], Batch [60/110], Train Loss: 0.0137, Val Loss: 0.0005, LR: 0.0006143601\n",
      "Epoch [215/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0006140844\n",
      "Epoch [215/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006138085\n",
      "Epoch [215/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0006135327\n",
      "Epoch [215/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006132568\n",
      "Epoch [215/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006129809\n",
      "Epoch [216/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0006127050\n",
      "Epoch [216/500], Batch [20/110], Train Loss: 0.0111, Val Loss: 0.0004, LR: 0.0006124290\n",
      "Epoch [216/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006121529\n",
      "Epoch [216/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006118769\n",
      "Epoch [216/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006116008\n",
      "Epoch [216/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006113247\n",
      "Epoch [216/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006110485\n",
      "Epoch [216/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006107723\n",
      "Epoch [216/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0006104961\n",
      "Epoch [216/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0006102198\n",
      "Epoch [216/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006099435\n",
      "Epoch [217/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0006096672\n",
      "Epoch [217/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006093908\n",
      "Epoch [217/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0006091144\n",
      "Epoch [217/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006088380\n",
      "Epoch [217/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0006085615\n",
      "Epoch [217/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006082850\n",
      "Epoch [217/500], Batch [70/110], Train Loss: 0.0018, Val Loss: 0.0002, LR: 0.0006080085\n",
      "Epoch [217/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006077319\n",
      "Epoch [217/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006074553\n",
      "Epoch [217/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006071787\n",
      "Epoch [217/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006069020\n",
      "Epoch [218/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006066253\n",
      "Epoch [218/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006063486\n",
      "Epoch [218/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006060718\n",
      "Epoch [218/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0006057950\n",
      "Epoch [218/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006055181\n",
      "Epoch [218/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006052413\n",
      "Epoch [218/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006049644\n",
      "Epoch [218/500], Batch [80/110], Train Loss: 0.0015, Val Loss: 0.0006, LR: 0.0006046874\n",
      "Epoch [218/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006044105\n",
      "Epoch [218/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0006041335\n",
      "Epoch [218/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006038564\n",
      "Epoch [219/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006035794\n",
      "Epoch [219/500], Batch [20/110], Train Loss: 0.0023, Val Loss: 0.0006, LR: 0.0006033023\n",
      "Epoch [219/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006030252\n",
      "Epoch [219/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006027480\n",
      "Epoch [219/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006024708\n",
      "Epoch [219/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006021936\n",
      "Epoch [219/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006019163\n",
      "Epoch [219/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006016390\n",
      "Epoch [219/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0002, LR: 0.0006013617\n",
      "Epoch [219/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006010844\n",
      "Epoch [219/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0006008070\n",
      "Epoch [220/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0006005296\n",
      "Epoch [220/500], Batch [20/110], Train Loss: 0.0021, Val Loss: 0.0002, LR: 0.0006002521\n",
      "Epoch [220/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005999747\n",
      "Epoch [220/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005996972\n",
      "Epoch [220/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005994196\n",
      "Epoch [220/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005991420\n",
      "Epoch [220/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005988644\n",
      "Epoch [220/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005985868\n",
      "Epoch [220/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005983092\n",
      "Epoch [220/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005980315\n",
      "Epoch [220/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005977538\n",
      "Epoch [221/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005974760\n",
      "Epoch [221/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005971982\n",
      "Epoch [221/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005969204\n",
      "Epoch [221/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005966426\n",
      "Epoch [221/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005963647\n",
      "Epoch [221/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005960868\n",
      "Epoch [221/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005958089\n",
      "Epoch [221/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005955309\n",
      "Epoch [221/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0005952529\n",
      "Epoch [221/500], Batch [100/110], Train Loss: 0.0089, Val Loss: 0.0022, LR: 0.0005949749\n",
      "Epoch [221/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0002, LR: 0.0005946969\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99921       635\n",
      "           1    1.00000   0.99884   0.99942       865\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99921   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Epoch 221: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.033 sec , Memory Usage: 307.49 MB\n",
      "Epoch [222/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0005944188\n",
      "Epoch [222/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005941407\n",
      "Epoch [222/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0002, LR: 0.0005938625\n",
      "Epoch [222/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005935844\n",
      "Epoch [222/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005933062\n",
      "Epoch [222/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005930280\n",
      "Epoch [222/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005927497\n",
      "Epoch [222/500], Batch [80/110], Train Loss: 0.0030, Val Loss: 0.0005, LR: 0.0005924714\n",
      "Epoch [222/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0005921931\n",
      "Epoch [222/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0005919148\n",
      "Epoch [222/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005916364\n",
      "Epoch [223/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005913580\n",
      "Epoch [223/500], Batch [20/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0005910796\n",
      "Epoch [223/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005908012\n",
      "Epoch [223/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005905227\n",
      "Epoch [223/500], Batch [50/110], Train Loss: 0.1180, Val Loss: 0.0003, LR: 0.0005902442\n",
      "Epoch [223/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005899656\n",
      "Epoch [223/500], Batch [70/110], Train Loss: 0.0677, Val Loss: 0.0008, LR: 0.0005896871\n",
      "Epoch [223/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005894085\n",
      "Epoch [223/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0005891299\n",
      "Epoch [223/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005888512\n",
      "Epoch [223/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0005885726\n",
      "Epoch [224/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0005882939\n",
      "Epoch [224/500], Batch [20/110], Train Loss: 0.0013, Val Loss: 0.0007, LR: 0.0005880151\n",
      "Epoch [224/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0005877364\n",
      "Epoch [224/500], Batch [40/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0005874576\n",
      "Epoch [224/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005871788\n",
      "Epoch [224/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005869000\n",
      "Epoch [224/500], Batch [70/110], Train Loss: 0.0198, Val Loss: 0.0005, LR: 0.0005866211\n",
      "Epoch [224/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005863422\n",
      "Epoch [224/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005860633\n",
      "Epoch [224/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005857844\n",
      "Epoch [224/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005855054\n",
      "Epoch [225/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005852264\n",
      "Epoch [225/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0005849474\n",
      "Epoch [225/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005846683\n",
      "Epoch [225/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005843893\n",
      "Epoch [225/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0004, LR: 0.0005841102\n",
      "Epoch [225/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005838311\n",
      "Epoch [225/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005835519\n",
      "Epoch [225/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0005832727\n",
      "Epoch [225/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005829935\n",
      "Epoch [225/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005827143\n",
      "Epoch [225/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005824351\n",
      "Epoch [226/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0009, LR: 0.0005821558\n",
      "Epoch [226/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0009, LR: 0.0005818765\n",
      "Epoch [226/500], Batch [30/110], Train Loss: 0.0028, Val Loss: 0.0002, LR: 0.0005815972\n",
      "Epoch [226/500], Batch [40/110], Train Loss: 0.0278, Val Loss: 0.0017, LR: 0.0005813178\n",
      "Epoch [226/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0041, LR: 0.0005810384\n",
      "Epoch [226/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0005807590\n",
      "Epoch [226/500], Batch [70/110], Train Loss: 0.0018, Val Loss: 0.0002, LR: 0.0005804796\n",
      "Epoch [226/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005802002\n",
      "Epoch [226/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0005799207\n",
      "Epoch [226/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005796412\n",
      "Epoch [226/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005793617\n",
      "Epoch [227/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0005790821\n",
      "Epoch [227/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005788025\n",
      "Epoch [227/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005785230\n",
      "Epoch [227/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005782433\n",
      "Epoch [227/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005779637\n",
      "Epoch [227/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005776840\n",
      "Epoch [227/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005774043\n",
      "Epoch [227/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0005771246\n",
      "Epoch [227/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005768449\n",
      "Epoch [227/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005765651\n",
      "Epoch [227/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005762853\n",
      "Epoch [228/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005760055\n",
      "Epoch [228/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0005757257\n",
      "Epoch [228/500], Batch [30/110], Train Loss: 0.0082, Val Loss: 0.0003, LR: 0.0005754458\n",
      "Epoch [228/500], Batch [40/110], Train Loss: 0.0145, Val Loss: 0.0005, LR: 0.0005751660\n",
      "Epoch [228/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005748861\n",
      "Epoch [228/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005746061\n",
      "Epoch [228/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005743262\n",
      "Epoch [228/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0005740462\n",
      "Epoch [228/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005737662\n",
      "Epoch [228/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005734862\n",
      "Epoch [228/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005732062\n",
      "Epoch [229/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0005729261\n",
      "Epoch [229/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005726461\n",
      "Epoch [229/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005723660\n",
      "Epoch [229/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005720858\n",
      "Epoch [229/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005718057\n",
      "Epoch [229/500], Batch [60/110], Train Loss: 0.0021, Val Loss: 0.0002, LR: 0.0005715255\n",
      "Epoch [229/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005712453\n",
      "Epoch [229/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0005709651\n",
      "Epoch [229/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0005706849\n",
      "Epoch [229/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005704046\n",
      "Epoch [229/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0005701244\n",
      "Epoch [230/500], Batch [10/110], Train Loss: 0.0020, Val Loss: 0.0008, LR: 0.0005698441\n",
      "Epoch [230/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005695637\n",
      "Epoch [230/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005692834\n",
      "Epoch [230/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005690030\n",
      "Epoch [230/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0005687227\n",
      "Epoch [230/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005684423\n",
      "Epoch [230/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005681618\n",
      "Epoch [230/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005678814\n",
      "Epoch [230/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005676009\n",
      "Epoch [230/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005673205\n",
      "Epoch [230/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005670400\n",
      "Epoch [231/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005667594\n",
      "Epoch [231/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005664789\n",
      "Epoch [231/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0005661983\n",
      "Epoch [231/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0002, LR: 0.0005659177\n",
      "Epoch [231/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005656371\n",
      "Epoch [231/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0005653565\n",
      "Epoch [231/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005650759\n",
      "Epoch [231/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0005647952\n",
      "Epoch [231/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005645145\n",
      "Epoch [231/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005642338\n",
      "Epoch [231/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005639531\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99921       635\n",
      "           1    1.00000   0.99884   0.99942       865\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99921   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Epoch 231: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.033 sec , Memory Usage: 307.49 MB\n",
      "Epoch [232/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005636724\n",
      "Epoch [232/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005633916\n",
      "Epoch [232/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005631108\n",
      "Epoch [232/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005628300\n",
      "Epoch [232/500], Batch [50/110], Train Loss: 0.0013, Val Loss: 0.0003, LR: 0.0005625492\n",
      "Epoch [232/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005622684\n",
      "Epoch [232/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0005619875\n",
      "Epoch [232/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005617066\n",
      "Epoch [232/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005614257\n",
      "Epoch [232/500], Batch [100/110], Train Loss: 0.0023, Val Loss: 0.0003, LR: 0.0005611448\n",
      "Epoch [232/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0005608639\n",
      "Epoch [233/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005605830\n",
      "Epoch [233/500], Batch [20/110], Train Loss: 0.0101, Val Loss: 0.0003, LR: 0.0005603020\n",
      "Epoch [233/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005600210\n",
      "Epoch [233/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005597400\n",
      "Epoch [233/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005594590\n",
      "Epoch [233/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005591780\n",
      "Epoch [233/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0002, LR: 0.0005588969\n",
      "Epoch [233/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005586158\n",
      "Epoch [233/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005583348\n",
      "Epoch [233/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0002, LR: 0.0005580536\n",
      "Epoch [233/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005577725\n",
      "Epoch [234/500], Batch [10/110], Train Loss: 0.0025, Val Loss: 0.0002, LR: 0.0005574914\n",
      "Epoch [234/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005572102\n",
      "Epoch [234/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005569290\n",
      "Epoch [234/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005566479\n",
      "Epoch [234/500], Batch [50/110], Train Loss: 0.0022, Val Loss: 0.0002, LR: 0.0005563666\n",
      "Epoch [234/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005560854\n",
      "Epoch [234/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005558042\n",
      "Epoch [234/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0005555229\n",
      "Epoch [234/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0005552416\n",
      "Epoch [234/500], Batch [100/110], Train Loss: 0.0030, Val Loss: 0.0003, LR: 0.0005549604\n",
      "Epoch [234/500], Batch [110/110], Train Loss: 0.0137, Val Loss: 0.0002, LR: 0.0005546790\n",
      "Epoch [235/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0001, LR: 0.0005543977\n",
      "Epoch [235/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005541164\n",
      "Epoch [235/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0005538350\n",
      "Epoch [235/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005535537\n",
      "Epoch [235/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005532723\n",
      "Epoch [235/500], Batch [60/110], Train Loss: 0.0067, Val Loss: 0.0002, LR: 0.0005529909\n",
      "Epoch [235/500], Batch [70/110], Train Loss: 0.0012, Val Loss: 0.0004, LR: 0.0005527094\n",
      "Epoch [235/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0005524280\n",
      "Epoch [235/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005521466\n",
      "Epoch [235/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005518651\n",
      "Epoch [235/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0005515836\n",
      "Epoch [236/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005513021\n",
      "Epoch [236/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005510206\n",
      "Epoch [236/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005507391\n",
      "Epoch [236/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0005504575\n",
      "Epoch [236/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005501760\n",
      "Epoch [236/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005498944\n",
      "Epoch [236/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005496128\n",
      "Epoch [236/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005493312\n",
      "Epoch [236/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005490496\n",
      "Epoch [236/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005487680\n",
      "Epoch [236/500], Batch [110/110], Train Loss: 0.0018, Val Loss: 0.0014, LR: 0.0005484863\n",
      "Epoch [237/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005482047\n",
      "Epoch [237/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0005479230\n",
      "Epoch [237/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005476413\n",
      "Epoch [237/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0005473596\n",
      "Epoch [237/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005470779\n",
      "Epoch [237/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005467962\n",
      "Epoch [237/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0005465144\n",
      "Epoch [237/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0005462327\n",
      "Epoch [237/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005459509\n",
      "Epoch [237/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0005456691\n",
      "Epoch [237/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005453874\n",
      "Epoch [238/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005451055\n",
      "Epoch [238/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005448237\n",
      "Epoch [238/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005445419\n",
      "Epoch [238/500], Batch [40/110], Train Loss: 0.0322, Val Loss: 0.0028, LR: 0.0005442600\n",
      "Epoch [238/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005439782\n",
      "Epoch [238/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0005436963\n",
      "Epoch [238/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0005434144\n",
      "Epoch [238/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0005431325\n",
      "Epoch [238/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0005428506\n",
      "Epoch [238/500], Batch [100/110], Train Loss: 0.0037, Val Loss: 0.0052, LR: 0.0005425687\n",
      "Epoch [238/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0005422868\n",
      "Epoch [239/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0005420048\n",
      "Epoch [239/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005417229\n",
      "Epoch [239/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005414409\n",
      "Epoch [239/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005411589\n",
      "Epoch [239/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005408769\n",
      "Epoch [239/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0005405949\n",
      "Epoch [239/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005403129\n",
      "Epoch [239/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005400309\n",
      "Epoch [239/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0005397488\n",
      "Epoch [239/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005394668\n",
      "Epoch [239/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0005391847\n",
      "Epoch [240/500], Batch [10/110], Train Loss: 0.0527, Val Loss: 0.0014, LR: 0.0005389026\n",
      "Epoch [240/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005386206\n",
      "Epoch [240/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005383385\n",
      "Epoch [240/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0005380564\n",
      "Epoch [240/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005377742\n",
      "Epoch [240/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005374921\n",
      "Epoch [240/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005372100\n",
      "Epoch [240/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005369278\n",
      "Epoch [240/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005366457\n",
      "Epoch [240/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0005363635\n",
      "Epoch [240/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005360813\n",
      "Epoch [241/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005357991\n",
      "Epoch [241/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005355169\n",
      "Epoch [241/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005352347\n",
      "Epoch [241/500], Batch [40/110], Train Loss: 0.0326, Val Loss: 0.0019, LR: 0.0005349525\n",
      "Epoch [241/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0005346703\n",
      "Epoch [241/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005343880\n",
      "Epoch [241/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005341058\n",
      "Epoch [241/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005338235\n",
      "Epoch [241/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0002, LR: 0.0005335412\n",
      "Epoch [241/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0005332590\n",
      "Epoch [241/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0005329767\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99921       635\n",
      "           1    1.00000   0.99884   0.99942       865\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99921   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Epoch 241: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.040 sec , Memory Usage: 307.49 MB\n",
      "Epoch [242/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005326944\n",
      "Epoch [242/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005324121\n",
      "Epoch [242/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0005321298\n",
      "Epoch [242/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005318474\n",
      "Epoch [242/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005315651\n",
      "Epoch [242/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005312828\n",
      "Epoch [242/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0005310004\n",
      "Epoch [242/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005307181\n",
      "Epoch [242/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005304357\n",
      "Epoch [242/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005301533\n",
      "Epoch [242/500], Batch [110/110], Train Loss: 0.0030, Val Loss: 0.0009, LR: 0.0005298709\n",
      "Epoch [243/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005295885\n",
      "Epoch [243/500], Batch [20/110], Train Loss: 0.0038, Val Loss: 0.0003, LR: 0.0005293061\n",
      "Epoch [243/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005290237\n",
      "Epoch [243/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0006, LR: 0.0005287413\n",
      "Epoch [243/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005284589\n",
      "Epoch [243/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005281765\n",
      "Epoch [243/500], Batch [70/110], Train Loss: 0.0034, Val Loss: 0.0003, LR: 0.0005278940\n",
      "Epoch [243/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005276116\n",
      "Epoch [243/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005273291\n",
      "Epoch [243/500], Batch [100/110], Train Loss: 0.0020, Val Loss: 0.0004, LR: 0.0005270467\n",
      "Epoch [243/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005267642\n",
      "Epoch [244/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005264817\n",
      "Epoch [244/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005261993\n",
      "Epoch [244/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0005259168\n",
      "Epoch [244/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0005256343\n",
      "Epoch [244/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005253518\n",
      "Epoch [244/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005250693\n",
      "Epoch [244/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0002, LR: 0.0005247868\n",
      "Epoch [244/500], Batch [80/110], Train Loss: 0.0068, Val Loss: 0.0002, LR: 0.0005245042\n",
      "Epoch [244/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005242217\n",
      "Epoch [244/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0005239392\n",
      "Epoch [244/500], Batch [110/110], Train Loss: 0.0163, Val Loss: 0.0004, LR: 0.0005236566\n",
      "Epoch [245/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005233741\n",
      "Epoch [245/500], Batch [20/110], Train Loss: 0.0030, Val Loss: 0.0002, LR: 0.0005230915\n",
      "Epoch [245/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005228090\n",
      "Epoch [245/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005225264\n",
      "Epoch [245/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005222439\n",
      "Epoch [245/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0005219613\n",
      "Epoch [245/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0009, LR: 0.0005216787\n",
      "Epoch [245/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0005213961\n",
      "Epoch [245/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005211135\n",
      "Epoch [245/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005208309\n",
      "Epoch [245/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0005205483\n",
      "Epoch [246/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005202657\n",
      "Epoch [246/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005199831\n",
      "Epoch [246/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005197005\n",
      "Epoch [246/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005194179\n",
      "Epoch [246/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005191352\n",
      "Epoch [246/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005188526\n",
      "Epoch [246/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005185700\n",
      "Epoch [246/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0005182873\n",
      "Epoch [246/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005180047\n",
      "Epoch [246/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005177220\n",
      "Epoch [246/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005174394\n",
      "Epoch [247/500], Batch [10/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0005171567\n",
      "Epoch [247/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005168741\n",
      "Epoch [247/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005165914\n",
      "Epoch [247/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005163087\n",
      "Epoch [247/500], Batch [50/110], Train Loss: 0.0067, Val Loss: 0.0005, LR: 0.0005160261\n",
      "Epoch [247/500], Batch [60/110], Train Loss: 0.0065, Val Loss: 0.0004, LR: 0.0005157434\n",
      "Epoch [247/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005154607\n",
      "Epoch [247/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005151780\n",
      "Epoch [247/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005148954\n",
      "Epoch [247/500], Batch [100/110], Train Loss: 0.0024, Val Loss: 0.0008, LR: 0.0005146127\n",
      "Epoch [247/500], Batch [110/110], Train Loss: 0.0017, Val Loss: 0.0006, LR: 0.0005143300\n",
      "Epoch [248/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005140473\n",
      "Epoch [248/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0005137646\n",
      "Epoch [248/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005134819\n",
      "Epoch [248/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0005131992\n",
      "Epoch [248/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005129165\n",
      "Epoch [248/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005126338\n",
      "Epoch [248/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0008, LR: 0.0005123511\n",
      "Epoch [248/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005120683\n",
      "Epoch [248/500], Batch [90/110], Train Loss: 0.0191, Val Loss: 0.0019, LR: 0.0005117856\n",
      "Epoch [248/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0005115029\n",
      "Epoch [248/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005112202\n",
      "Epoch [249/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005109375\n",
      "Epoch [249/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0005106547\n",
      "Epoch [249/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005103720\n",
      "Epoch [249/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005100893\n",
      "Epoch [249/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005098066\n",
      "Epoch [249/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005095238\n",
      "Epoch [249/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005092411\n",
      "Epoch [249/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005089584\n",
      "Epoch [249/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005086756\n",
      "Epoch [249/500], Batch [100/110], Train Loss: 0.0044, Val Loss: 0.0003, LR: 0.0005083929\n",
      "Epoch [249/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005081102\n",
      "Epoch [250/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005078274\n",
      "Epoch [250/500], Batch [20/110], Train Loss: 0.0152, Val Loss: 0.0003, LR: 0.0005075447\n",
      "Epoch [250/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0005072619\n",
      "Epoch [250/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005069792\n",
      "Epoch [250/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005066965\n",
      "Epoch [250/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0005064137\n",
      "Epoch [250/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005061310\n",
      "Epoch [250/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005058482\n",
      "Epoch [250/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005055655\n",
      "Epoch [250/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005052827\n",
      "Epoch [250/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005050000\n",
      "Epoch [251/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0005047173\n",
      "Epoch [251/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005044345\n",
      "Epoch [251/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005041518\n",
      "Epoch [251/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005038690\n",
      "Epoch [251/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005035863\n",
      "Epoch [251/500], Batch [60/110], Train Loss: 0.0023, Val Loss: 0.0001, LR: 0.0005033035\n",
      "Epoch [251/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005030208\n",
      "Epoch [251/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005027381\n",
      "Epoch [251/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0005024553\n",
      "Epoch [251/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005021726\n",
      "Epoch [251/500], Batch [110/110], Train Loss: 0.1765, Val Loss: 0.0001, LR: 0.0005018898\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99921       635\n",
      "           1    1.00000   0.99884   0.99942       865\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99921   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Epoch 251: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 307.49 MB\n",
      "Epoch [252/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005016071\n",
      "Epoch [252/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0005013244\n",
      "Epoch [252/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0003, LR: 0.0005010416\n",
      "Epoch [252/500], Batch [40/110], Train Loss: 0.0026, Val Loss: 0.0002, LR: 0.0005007589\n",
      "Epoch [252/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005004762\n",
      "Epoch [252/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005001934\n",
      "Epoch [252/500], Batch [70/110], Train Loss: 0.0492, Val Loss: 0.0027, LR: 0.0004999107\n",
      "Epoch [252/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004996280\n",
      "Epoch [252/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0004993453\n",
      "Epoch [252/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0004990625\n",
      "Epoch [252/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004987798\n",
      "Epoch [253/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0004984971\n",
      "Epoch [253/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004982144\n",
      "Epoch [253/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004979317\n",
      "Epoch [253/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004976489\n",
      "Epoch [253/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004973662\n",
      "Epoch [253/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004970835\n",
      "Epoch [253/500], Batch [70/110], Train Loss: 0.0015, Val Loss: 0.0001, LR: 0.0004968008\n",
      "Epoch [253/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004965181\n",
      "Epoch [253/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004962354\n",
      "Epoch [253/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0004959527\n",
      "Epoch [253/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004956700\n",
      "Epoch [254/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0004953873\n",
      "Epoch [254/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004951046\n",
      "Epoch [254/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0004948220\n",
      "Epoch [254/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0004945393\n",
      "Epoch [254/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0004942566\n",
      "Epoch [254/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0004939739\n",
      "Epoch [254/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004936913\n",
      "Epoch [254/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004934086\n",
      "Epoch [254/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004931259\n",
      "Epoch [254/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0004928433\n",
      "Epoch [254/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0004925606\n",
      "Epoch [255/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004922780\n",
      "Epoch [255/500], Batch [20/110], Train Loss: 0.0020, Val Loss: 0.0001, LR: 0.0004919953\n",
      "Epoch [255/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0004917127\n",
      "Epoch [255/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004914300\n",
      "Epoch [255/500], Batch [50/110], Train Loss: 0.0969, Val Loss: 0.0002, LR: 0.0004911474\n",
      "Epoch [255/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0004908648\n",
      "Epoch [255/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004905821\n",
      "Epoch [255/500], Batch [80/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0004902995\n",
      "Epoch [255/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004900169\n",
      "Epoch [255/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004897343\n",
      "Epoch [255/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004894517\n",
      "Epoch [256/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004891691\n",
      "Epoch [256/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0004888865\n",
      "Epoch [256/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004886039\n",
      "Epoch [256/500], Batch [40/110], Train Loss: 0.0142, Val Loss: 0.0004, LR: 0.0004883213\n",
      "Epoch [256/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004880387\n",
      "Epoch [256/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004877561\n",
      "Epoch [256/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004874736\n",
      "Epoch [256/500], Batch [80/110], Train Loss: 0.0022, Val Loss: 0.0003, LR: 0.0004871910\n",
      "Epoch [256/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004869085\n",
      "Epoch [256/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0004866259\n",
      "Epoch [256/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004863434\n",
      "Epoch [257/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004860608\n",
      "Epoch [257/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0004857783\n",
      "Epoch [257/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004854958\n",
      "Epoch [257/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004852132\n",
      "Epoch [257/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004849307\n",
      "Epoch [257/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004846482\n",
      "Epoch [257/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004843657\n",
      "Epoch [257/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004840832\n",
      "Epoch [257/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004838007\n",
      "Epoch [257/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004835183\n",
      "Epoch [257/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0004832358\n",
      "Epoch [258/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004829533\n",
      "Epoch [258/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0004826709\n",
      "Epoch [258/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004823884\n",
      "Epoch [258/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0004821060\n",
      "Epoch [258/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0004818235\n",
      "Epoch [258/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004815411\n",
      "Epoch [258/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0004812587\n",
      "Epoch [258/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004809763\n",
      "Epoch [258/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004806939\n",
      "Epoch [258/500], Batch [100/110], Train Loss: 0.0061, Val Loss: 0.0002, LR: 0.0004804115\n",
      "Epoch [258/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0004801291\n",
      "Epoch [259/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0004798467\n",
      "Epoch [259/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004795643\n",
      "Epoch [259/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0004792819\n",
      "Epoch [259/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0004789996\n",
      "Epoch [259/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0002, LR: 0.0004787172\n",
      "Epoch [259/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0004784349\n",
      "Epoch [259/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004781526\n",
      "Epoch [259/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004778702\n",
      "Epoch [259/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004775879\n",
      "Epoch [259/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0002, LR: 0.0004773056\n",
      "Epoch [259/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004770233\n",
      "Epoch [260/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004767410\n",
      "Epoch [260/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004764588\n",
      "Epoch [260/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0004761765\n",
      "Epoch [260/500], Batch [40/110], Train Loss: 0.0014, Val Loss: 0.0026, LR: 0.0004758942\n",
      "Epoch [260/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0004756120\n",
      "Epoch [260/500], Batch [60/110], Train Loss: 0.0018, Val Loss: 0.0002, LR: 0.0004753297\n",
      "Epoch [260/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0004750475\n",
      "Epoch [260/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004747653\n",
      "Epoch [260/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004744831\n",
      "Epoch [260/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004742009\n",
      "Epoch [260/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004739187\n",
      "Epoch [261/500], Batch [10/110], Train Loss: 0.0560, Val Loss: 0.0050, LR: 0.0004736365\n",
      "Epoch [261/500], Batch [20/110], Train Loss: 0.0020, Val Loss: 0.0010, LR: 0.0004733543\n",
      "Epoch [261/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004730722\n",
      "Epoch [261/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004727900\n",
      "Epoch [261/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004725079\n",
      "Epoch [261/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004722258\n",
      "Epoch [261/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004719436\n",
      "Epoch [261/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0004716615\n",
      "Epoch [261/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004713794\n",
      "Epoch [261/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004710974\n",
      "Epoch [261/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0004708153\n",
      "Confusion Matrix:\n",
      "[[631   4]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99842   0.99370   0.99605       635\n",
      "           1    0.99539   0.99884   0.99711       865\n",
      "\n",
      "    accuracy                        0.99667      1500\n",
      "   macro avg    0.99690   0.99627   0.99658      1500\n",
      "weighted avg    0.99667   0.99667   0.99667      1500\n",
      "\n",
      "Total Errors: 5\n",
      "Index: 693, Predicted: 1, Actual: 0\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Index: 1067, Predicted: 1, Actual: 0\n",
      "Index: 1256, Predicted: 1, Actual: 0\n",
      "Index: 1351, Predicted: 1, Actual: 0\n",
      "Epoch 261: OK- Accuracy: 0.99667, Precision: 0.99539, Recall: 0.99884, F1: 0.99711, ROC AUC: 0.99627, AUPR (PR-AUC): 0.99491, Sensitivity: 0.99884, Specificity: 0.99370, Far: 0.006299212598425197, False Positive Rate (FPR): 0.00630, False Negative Rate (FNR): 0.00116, Runtime: 0.034 sec , Memory Usage: 307.50 MB\n",
      "Epoch [262/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004705332\n",
      "Epoch [262/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004702512\n",
      "Epoch [262/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0004699691\n",
      "Epoch [262/500], Batch [40/110], Train Loss: 0.0690, Val Loss: 0.0051, LR: 0.0004696871\n",
      "Epoch [262/500], Batch [50/110], Train Loss: 0.0031, Val Loss: 0.0007, LR: 0.0004694051\n",
      "Epoch [262/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0001, LR: 0.0004691231\n",
      "Epoch [262/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004688411\n",
      "Epoch [262/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0010, LR: 0.0004685591\n",
      "Epoch [262/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004682771\n",
      "Epoch [262/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004679952\n",
      "Epoch [262/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004677132\n",
      "Epoch [263/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0004674313\n",
      "Epoch [263/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004671494\n",
      "Epoch [263/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0004668675\n",
      "Epoch [263/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004665856\n",
      "Epoch [263/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004663037\n",
      "Epoch [263/500], Batch [60/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0004660218\n",
      "Epoch [263/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004657400\n",
      "Epoch [263/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004654581\n",
      "Epoch [263/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004651763\n",
      "Epoch [263/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004648945\n",
      "Epoch [263/500], Batch [110/110], Train Loss: 0.0009, Val Loss: 0.0001, LR: 0.0004646126\n",
      "Epoch [264/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004643309\n",
      "Epoch [264/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0004640491\n",
      "Epoch [264/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004637673\n",
      "Epoch [264/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0004634856\n",
      "Epoch [264/500], Batch [50/110], Train Loss: 0.0015, Val Loss: 0.0002, LR: 0.0004632038\n",
      "Epoch [264/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0004629221\n",
      "Epoch [264/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004626404\n",
      "Epoch [264/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004623587\n",
      "Epoch [264/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0004620770\n",
      "Epoch [264/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004617953\n",
      "Epoch [264/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004615137\n",
      "Epoch [265/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004612320\n",
      "Epoch [265/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004609504\n",
      "Epoch [265/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004606688\n",
      "Epoch [265/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004603872\n",
      "Epoch [265/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004601056\n",
      "Epoch [265/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004598240\n",
      "Epoch [265/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004595425\n",
      "Epoch [265/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004592609\n",
      "Epoch [265/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0004589794\n",
      "Epoch [265/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004586979\n",
      "Epoch [265/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004584164\n",
      "Epoch [266/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0004581349\n",
      "Epoch [266/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004578534\n",
      "Epoch [266/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004575720\n",
      "Epoch [266/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004572906\n",
      "Epoch [266/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0004570091\n",
      "Epoch [266/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0004567277\n",
      "Epoch [266/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004564463\n",
      "Epoch [266/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004561650\n",
      "Epoch [266/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0004558836\n",
      "Epoch [266/500], Batch [100/110], Train Loss: 0.0446, Val Loss: 0.0005, LR: 0.0004556023\n",
      "Epoch [266/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004553210\n",
      "Epoch [267/500], Batch [10/110], Train Loss: 0.0020, Val Loss: 0.0002, LR: 0.0004550396\n",
      "Epoch [267/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004547584\n",
      "Epoch [267/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004544771\n",
      "Epoch [267/500], Batch [40/110], Train Loss: 0.0164, Val Loss: 0.0030, LR: 0.0004541958\n",
      "Epoch [267/500], Batch [50/110], Train Loss: 0.0034, Val Loss: 0.0002, LR: 0.0004539146\n",
      "Epoch [267/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004536334\n",
      "Epoch [267/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0004533521\n",
      "Epoch [267/500], Batch [80/110], Train Loss: 0.0015, Val Loss: 0.0001, LR: 0.0004530710\n",
      "Epoch [267/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004527898\n",
      "Epoch [267/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0004525086\n",
      "Epoch [267/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004522275\n",
      "Epoch [268/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0004519464\n",
      "Epoch [268/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004516652\n",
      "Epoch [268/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004513842\n",
      "Epoch [268/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004511031\n",
      "Epoch [268/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004508220\n",
      "Epoch [268/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004505410\n",
      "Epoch [268/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004502600\n",
      "Epoch [268/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004499790\n",
      "Epoch [268/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0004496980\n",
      "Epoch [268/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004494170\n",
      "Epoch [268/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0004491361\n",
      "Epoch [269/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004488552\n",
      "Epoch [269/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004485743\n",
      "Epoch [269/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004482934\n",
      "Epoch [269/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004480125\n",
      "Epoch [269/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004477316\n",
      "Epoch [269/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004474508\n",
      "Epoch [269/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004471700\n",
      "Epoch [269/500], Batch [80/110], Train Loss: 0.0040, Val Loss: 0.0001, LR: 0.0004468892\n",
      "Epoch [269/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0004466084\n",
      "Epoch [269/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004463276\n",
      "Epoch [269/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004460469\n",
      "Epoch [270/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004457662\n",
      "Epoch [270/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0004454855\n",
      "Epoch [270/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0004452048\n",
      "Epoch [270/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0004449241\n",
      "Epoch [270/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004446435\n",
      "Epoch [270/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004443629\n",
      "Epoch [270/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0004440823\n",
      "Epoch [270/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004438017\n",
      "Epoch [270/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004435211\n",
      "Epoch [270/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004432406\n",
      "Epoch [270/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004429600\n",
      "Epoch [271/500], Batch [10/110], Train Loss: 0.1042, Val Loss: 0.0001, LR: 0.0004426795\n",
      "Epoch [271/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004423991\n",
      "Epoch [271/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0004421186\n",
      "Epoch [271/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004418382\n",
      "Epoch [271/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004415577\n",
      "Epoch [271/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004412773\n",
      "Epoch [271/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004409970\n",
      "Epoch [271/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004407166\n",
      "Epoch [271/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004404363\n",
      "Epoch [271/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004401559\n",
      "Epoch [271/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004398756\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  1 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99843   1.00000   0.99921       635\n",
      "           1    1.00000   0.99884   0.99942       865\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99921   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 798, Predicted: 0, Actual: 1\n",
      "Epoch 271: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99884, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99951, Sensitivity: 0.99884, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00116, Runtime: 0.045 sec , Memory Usage: 307.52 MB\n",
      "Epoch [272/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004395954\n",
      "Epoch [272/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004393151\n",
      "Epoch [272/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004390349\n",
      "Epoch [272/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004387547\n",
      "Epoch [272/500], Batch [50/110], Train Loss: 0.0033, Val Loss: 0.0002, LR: 0.0004384745\n",
      "Epoch [272/500], Batch [60/110], Train Loss: 0.1849, Val Loss: 0.0001, LR: 0.0004381943\n",
      "Epoch [272/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004379142\n",
      "Epoch [272/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004376340\n",
      "Epoch [272/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0004373539\n",
      "Epoch [272/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0004370739\n",
      "Epoch [272/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004367938\n",
      "Epoch [273/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004365138\n",
      "Epoch [273/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004362338\n",
      "Epoch [273/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004359538\n",
      "Epoch [273/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004356738\n",
      "Epoch [273/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004353939\n",
      "Epoch [273/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004351139\n",
      "Epoch [273/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004348340\n",
      "Epoch [273/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004345542\n",
      "Epoch [273/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0004342743\n",
      "Epoch [273/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004339945\n",
      "Epoch [273/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004337147\n",
      "Epoch [274/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004334349\n",
      "Epoch [274/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004331551\n",
      "Epoch [274/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004328754\n",
      "Epoch [274/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004325957\n",
      "Epoch [274/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004323160\n",
      "Epoch [274/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004320363\n",
      "Epoch [274/500], Batch [70/110], Train Loss: 0.0016, Val Loss: 0.0002, LR: 0.0004317567\n",
      "Epoch [274/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004314770\n",
      "Epoch [274/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004311975\n",
      "Epoch [274/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004309179\n",
      "Epoch [274/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004306383\n",
      "Epoch [275/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004303588\n",
      "Epoch [275/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004300793\n",
      "Epoch [275/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004297998\n",
      "Epoch [275/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0004295204\n",
      "Epoch [275/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0004292410\n",
      "Epoch [275/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004289616\n",
      "Epoch [275/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004286822\n",
      "Epoch [275/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004284028\n",
      "Epoch [275/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004281235\n",
      "Epoch [275/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004278442\n",
      "Epoch [275/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0002, LR: 0.0004275649\n",
      "Epoch [276/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004272857\n",
      "Epoch [276/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0004270065\n",
      "Epoch [276/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004267273\n",
      "Epoch [276/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004264481\n",
      "Epoch [276/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004261689\n",
      "Epoch [276/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0004258898\n",
      "Epoch [276/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004256107\n",
      "Epoch [276/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004253317\n",
      "Epoch [276/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004250526\n",
      "Epoch [276/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004247736\n",
      "Epoch [276/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004244946\n",
      "Epoch [277/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0004242156\n",
      "Epoch [277/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004239367\n",
      "Epoch [277/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0004236578\n",
      "Epoch [277/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004233789\n",
      "Epoch [277/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004231000\n",
      "Epoch [277/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0004228212\n",
      "Epoch [277/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004225424\n",
      "Epoch [277/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004222636\n",
      "Epoch [277/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004219849\n",
      "Epoch [277/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004217061\n",
      "Epoch [277/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004214274\n",
      "Epoch [278/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004211488\n",
      "Epoch [278/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004208701\n",
      "Epoch [278/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004205915\n",
      "Epoch [278/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004203129\n",
      "Epoch [278/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004200344\n",
      "Epoch [278/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004197558\n",
      "Epoch [278/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004194773\n",
      "Epoch [278/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004191988\n",
      "Epoch [278/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0004189204\n",
      "Epoch [278/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004186420\n",
      "Epoch [278/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004183636\n",
      "Epoch [279/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004180852\n",
      "Epoch [279/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004178069\n",
      "Epoch [279/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004175286\n",
      "Epoch [279/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004172503\n",
      "Epoch [279/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004169720\n",
      "Epoch [279/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004166938\n",
      "Epoch [279/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0004164156\n",
      "Epoch [279/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004161375\n",
      "Epoch [279/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004158593\n",
      "Epoch [279/500], Batch [100/110], Train Loss: 0.0143, Val Loss: 0.0016, LR: 0.0004155812\n",
      "Epoch [279/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004153031\n",
      "Epoch [280/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004150251\n",
      "Epoch [280/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004147471\n",
      "Epoch [280/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004144691\n",
      "Epoch [280/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004141911\n",
      "Epoch [280/500], Batch [50/110], Train Loss: 0.0065, Val Loss: 0.0002, LR: 0.0004139132\n",
      "Epoch [280/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004136353\n",
      "Epoch [280/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004133574\n",
      "Epoch [280/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004130796\n",
      "Epoch [280/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0004128018\n",
      "Epoch [280/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004125240\n",
      "Epoch [280/500], Batch [110/110], Train Loss: 0.0091, Val Loss: 0.0001, LR: 0.0004122462\n",
      "Epoch [281/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004119685\n",
      "Epoch [281/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0004116908\n",
      "Epoch [281/500], Batch [30/110], Train Loss: 0.0035, Val Loss: 0.0004, LR: 0.0004114132\n",
      "Epoch [281/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004111356\n",
      "Epoch [281/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004108580\n",
      "Epoch [281/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0004105804\n",
      "Epoch [281/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0004103028\n",
      "Epoch [281/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004100253\n",
      "Epoch [281/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004097479\n",
      "Epoch [281/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004094704\n",
      "Epoch [281/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004091930\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 281: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 307.52 MB\n",
      "Epoch [282/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004089156\n",
      "Epoch [282/500], Batch [20/110], Train Loss: 0.0182, Val Loss: 0.0019, LR: 0.0004086383\n",
      "Epoch [282/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0004083610\n",
      "Epoch [282/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004080837\n",
      "Epoch [282/500], Batch [50/110], Train Loss: 0.0012, Val Loss: 0.0006, LR: 0.0004078064\n",
      "Epoch [282/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004075292\n",
      "Epoch [282/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004072520\n",
      "Epoch [282/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004069748\n",
      "Epoch [282/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0001, LR: 0.0004066977\n",
      "Epoch [282/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0004064206\n",
      "Epoch [282/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004061436\n",
      "Epoch [283/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004058665\n",
      "Epoch [283/500], Batch [20/110], Train Loss: 0.0034, Val Loss: 0.0001, LR: 0.0004055895\n",
      "Epoch [283/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004053126\n",
      "Epoch [283/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004050356\n",
      "Epoch [283/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0004047587\n",
      "Epoch [283/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004044819\n",
      "Epoch [283/500], Batch [70/110], Train Loss: 0.1605, Val Loss: 0.0149, LR: 0.0004042050\n",
      "Epoch [283/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0004039282\n",
      "Epoch [283/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0004036514\n",
      "Epoch [283/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0004033747\n",
      "Epoch [283/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0004030980\n",
      "Epoch [284/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0004028213\n",
      "Epoch [284/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004025447\n",
      "Epoch [284/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004022681\n",
      "Epoch [284/500], Batch [40/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0004019915\n",
      "Epoch [284/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004017150\n",
      "Epoch [284/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004014385\n",
      "Epoch [284/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004011620\n",
      "Epoch [284/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004008856\n",
      "Epoch [284/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004006092\n",
      "Epoch [284/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0004003328\n",
      "Epoch [284/500], Batch [110/110], Train Loss: 0.0043, Val Loss: 0.0001, LR: 0.0004000565\n",
      "Epoch [285/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003997802\n",
      "Epoch [285/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003995039\n",
      "Epoch [285/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003992277\n",
      "Epoch [285/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0003989515\n",
      "Epoch [285/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003986753\n",
      "Epoch [285/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0003983992\n",
      "Epoch [285/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003981231\n",
      "Epoch [285/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003978471\n",
      "Epoch [285/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0003975710\n",
      "Epoch [285/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003972950\n",
      "Epoch [285/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0003970191\n",
      "Epoch [286/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0003967432\n",
      "Epoch [286/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003964673\n",
      "Epoch [286/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003961915\n",
      "Epoch [286/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003959156\n",
      "Epoch [286/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003956399\n",
      "Epoch [286/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0003953641\n",
      "Epoch [286/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0003950884\n",
      "Epoch [286/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003948128\n",
      "Epoch [286/500], Batch [90/110], Train Loss: 0.0585, Val Loss: 0.0002, LR: 0.0003945371\n",
      "Epoch [286/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003942615\n",
      "Epoch [286/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0003939860\n",
      "Epoch [287/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003937105\n",
      "Epoch [287/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003934350\n",
      "Epoch [287/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003931595\n",
      "Epoch [287/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003928841\n",
      "Epoch [287/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003926087\n",
      "Epoch [287/500], Batch [60/110], Train Loss: 0.0043, Val Loss: 0.0002, LR: 0.0003923334\n",
      "Epoch [287/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003920581\n",
      "Epoch [287/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003917828\n",
      "Epoch [287/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003915076\n",
      "Epoch [287/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003912324\n",
      "Epoch [287/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003909572\n",
      "Epoch [288/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0003906821\n",
      "Epoch [288/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003904070\n",
      "Epoch [288/500], Batch [30/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0003901320\n",
      "Epoch [288/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003898570\n",
      "Epoch [288/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0003895820\n",
      "Epoch [288/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003893071\n",
      "Epoch [288/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003890322\n",
      "Epoch [288/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003887573\n",
      "Epoch [288/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003884825\n",
      "Epoch [288/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0003882077\n",
      "Epoch [288/500], Batch [110/110], Train Loss: 0.2463, Val Loss: 0.0007, LR: 0.0003879330\n",
      "Epoch [289/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0021, LR: 0.0003876583\n",
      "Epoch [289/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003873836\n",
      "Epoch [289/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0003871090\n",
      "Epoch [289/500], Batch [40/110], Train Loss: 0.0018, Val Loss: 0.0002, LR: 0.0003868344\n",
      "Epoch [289/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0003865599\n",
      "Epoch [289/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003862854\n",
      "Epoch [289/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003860109\n",
      "Epoch [289/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003857364\n",
      "Epoch [289/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003854621\n",
      "Epoch [289/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003851877\n",
      "Epoch [289/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003849134\n",
      "Epoch [290/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003846391\n",
      "Epoch [290/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003843649\n",
      "Epoch [290/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003840907\n",
      "Epoch [290/500], Batch [40/110], Train Loss: 0.0031, Val Loss: 0.0005, LR: 0.0003838165\n",
      "Epoch [290/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003835424\n",
      "Epoch [290/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003832683\n",
      "Epoch [290/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003829943\n",
      "Epoch [290/500], Batch [80/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0003827203\n",
      "Epoch [290/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0001, LR: 0.0003824463\n",
      "Epoch [290/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003821724\n",
      "Epoch [290/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003818985\n",
      "Epoch [291/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003816247\n",
      "Epoch [291/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003813509\n",
      "Epoch [291/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003810771\n",
      "Epoch [291/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003808034\n",
      "Epoch [291/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003805297\n",
      "Epoch [291/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0003802561\n",
      "Epoch [291/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003799825\n",
      "Epoch [291/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003797089\n",
      "Epoch [291/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003794354\n",
      "Epoch [291/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003791619\n",
      "Epoch [291/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003788885\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 291: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.038 sec , Memory Usage: 307.52 MB\n",
      "Epoch [292/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003786151\n",
      "Epoch [292/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003783417\n",
      "Epoch [292/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0003780684\n",
      "Epoch [292/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003777952\n",
      "Epoch [292/500], Batch [50/110], Train Loss: 0.1083, Val Loss: 0.0001, LR: 0.0003775219\n",
      "Epoch [292/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003772488\n",
      "Epoch [292/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003769756\n",
      "Epoch [292/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003767025\n",
      "Epoch [292/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003764294\n",
      "Epoch [292/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003761564\n",
      "Epoch [292/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003758835\n",
      "Epoch [293/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0010, LR: 0.0003756105\n",
      "Epoch [293/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003753376\n",
      "Epoch [293/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003750648\n",
      "Epoch [293/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0003747920\n",
      "Epoch [293/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003745192\n",
      "Epoch [293/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003742465\n",
      "Epoch [293/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003739738\n",
      "Epoch [293/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0003737012\n",
      "Epoch [293/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0003734286\n",
      "Epoch [293/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003731560\n",
      "Epoch [293/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003728835\n",
      "Epoch [294/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003726111\n",
      "Epoch [294/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003723386\n",
      "Epoch [294/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003720663\n",
      "Epoch [294/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003717939\n",
      "Epoch [294/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003715216\n",
      "Epoch [294/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003712494\n",
      "Epoch [294/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003709772\n",
      "Epoch [294/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003707050\n",
      "Epoch [294/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003704329\n",
      "Epoch [294/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003701608\n",
      "Epoch [294/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003698888\n",
      "Epoch [295/500], Batch [10/110], Train Loss: 0.0020, Val Loss: 0.0003, LR: 0.0003696168\n",
      "Epoch [295/500], Batch [20/110], Train Loss: 0.0029, Val Loss: 0.0002, LR: 0.0003693449\n",
      "Epoch [295/500], Batch [30/110], Train Loss: 0.0065, Val Loss: 0.0004, LR: 0.0003690730\n",
      "Epoch [295/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003688011\n",
      "Epoch [295/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003685293\n",
      "Epoch [295/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0003682575\n",
      "Epoch [295/500], Batch [70/110], Train Loss: 0.0026, Val Loss: 0.0001, LR: 0.0003679858\n",
      "Epoch [295/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003677142\n",
      "Epoch [295/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003674425\n",
      "Epoch [295/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003671709\n",
      "Epoch [295/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0003668994\n",
      "Epoch [296/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003666279\n",
      "Epoch [296/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0004, LR: 0.0003663565\n",
      "Epoch [296/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003660851\n",
      "Epoch [296/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003658137\n",
      "Epoch [296/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003655424\n",
      "Epoch [296/500], Batch [60/110], Train Loss: 0.0144, Val Loss: 0.0005, LR: 0.0003652711\n",
      "Epoch [296/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0003649999\n",
      "Epoch [296/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003647287\n",
      "Epoch [296/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0003644576\n",
      "Epoch [296/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003641865\n",
      "Epoch [296/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003639155\n",
      "Epoch [297/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003636445\n",
      "Epoch [297/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003633735\n",
      "Epoch [297/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003631026\n",
      "Epoch [297/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003628318\n",
      "Epoch [297/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003625610\n",
      "Epoch [297/500], Batch [60/110], Train Loss: 0.0011, Val Loss: 0.0008, LR: 0.0003622902\n",
      "Epoch [297/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003620195\n",
      "Epoch [297/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003617488\n",
      "Epoch [297/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003614782\n",
      "Epoch [297/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003612076\n",
      "Epoch [297/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003609371\n",
      "Epoch [298/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003606666\n",
      "Epoch [298/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003603962\n",
      "Epoch [298/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003601258\n",
      "Epoch [298/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003598555\n",
      "Epoch [298/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003595852\n",
      "Epoch [298/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003593149\n",
      "Epoch [298/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003590447\n",
      "Epoch [298/500], Batch [80/110], Train Loss: 0.0028, Val Loss: 0.0003, LR: 0.0003587746\n",
      "Epoch [298/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003585045\n",
      "Epoch [298/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003582344\n",
      "Epoch [298/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003579644\n",
      "Epoch [299/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003576945\n",
      "Epoch [299/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003574246\n",
      "Epoch [299/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003571547\n",
      "Epoch [299/500], Batch [40/110], Train Loss: 0.0033, Val Loss: 0.0001, LR: 0.0003568849\n",
      "Epoch [299/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003566151\n",
      "Epoch [299/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0003563454\n",
      "Epoch [299/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0003560757\n",
      "Epoch [299/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003558061\n",
      "Epoch [299/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003555365\n",
      "Epoch [299/500], Batch [100/110], Train Loss: 0.0067, Val Loss: 0.0002, LR: 0.0003552670\n",
      "Epoch [299/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003549975\n",
      "Epoch [300/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003547281\n",
      "Epoch [300/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003544587\n",
      "Epoch [300/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003541894\n",
      "Epoch [300/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0001, LR: 0.0003539201\n",
      "Epoch [300/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003536509\n",
      "Epoch [300/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003533817\n",
      "Epoch [300/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003531126\n",
      "Epoch [300/500], Batch [80/110], Train Loss: 0.0010, Val Loss: 0.0001, LR: 0.0003528435\n",
      "Epoch [300/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0007, LR: 0.0003525745\n",
      "Epoch [300/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003523055\n",
      "Epoch [300/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003520366\n",
      "Epoch [301/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003517677\n",
      "Epoch [301/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003514989\n",
      "Epoch [301/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003512301\n",
      "Epoch [301/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003509614\n",
      "Epoch [301/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003506927\n",
      "Epoch [301/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003504241\n",
      "Epoch [301/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003501555\n",
      "Epoch [301/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0003498870\n",
      "Epoch [301/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003496185\n",
      "Epoch [301/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003493500\n",
      "Epoch [301/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003490817\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 301: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.042 sec , Memory Usage: 307.52 MB\n",
      "Epoch [302/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003488133\n",
      "Epoch [302/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003485451\n",
      "Epoch [302/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003482769\n",
      "Epoch [302/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003480087\n",
      "Epoch [302/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003477406\n",
      "Epoch [302/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003474725\n",
      "Epoch [302/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003472045\n",
      "Epoch [302/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003469365\n",
      "Epoch [302/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003466686\n",
      "Epoch [302/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0003, LR: 0.0003464007\n",
      "Epoch [302/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003461329\n",
      "Epoch [303/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003458652\n",
      "Epoch [303/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003455974\n",
      "Epoch [303/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003453298\n",
      "Epoch [303/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003450622\n",
      "Epoch [303/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003447946\n",
      "Epoch [303/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0003445271\n",
      "Epoch [303/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003442597\n",
      "Epoch [303/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0003439923\n",
      "Epoch [303/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003437250\n",
      "Epoch [303/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003434577\n",
      "Epoch [303/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003431904\n",
      "Epoch [304/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003429232\n",
      "Epoch [304/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003426561\n",
      "Epoch [304/500], Batch [30/110], Train Loss: 0.0223, Val Loss: 0.0032, LR: 0.0003423890\n",
      "Epoch [304/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003421220\n",
      "Epoch [304/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003418550\n",
      "Epoch [304/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0003415881\n",
      "Epoch [304/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003413213\n",
      "Epoch [304/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003410544\n",
      "Epoch [304/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0003407877\n",
      "Epoch [304/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003405210\n",
      "Epoch [304/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003402543\n",
      "Epoch [305/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003399877\n",
      "Epoch [305/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0003397212\n",
      "Epoch [305/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003394547\n",
      "Epoch [305/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003391883\n",
      "Epoch [305/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003389219\n",
      "Epoch [305/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0001, LR: 0.0003386556\n",
      "Epoch [305/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003383893\n",
      "Epoch [305/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003381231\n",
      "Epoch [305/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003378569\n",
      "Epoch [305/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003375908\n",
      "Epoch [305/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003373247\n",
      "Epoch [306/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003370587\n",
      "Epoch [306/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003367928\n",
      "Epoch [306/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003365269\n",
      "Epoch [306/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003362611\n",
      "Epoch [306/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003359953\n",
      "Epoch [306/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003357296\n",
      "Epoch [306/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003354639\n",
      "Epoch [306/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003351983\n",
      "Epoch [306/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003349327\n",
      "Epoch [306/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003346672\n",
      "Epoch [306/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003344018\n",
      "Epoch [307/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003341364\n",
      "Epoch [307/500], Batch [20/110], Train Loss: 0.0029, Val Loss: 0.0001, LR: 0.0003338710\n",
      "Epoch [307/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003336057\n",
      "Epoch [307/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003333405\n",
      "Epoch [307/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003330753\n",
      "Epoch [307/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0003328102\n",
      "Epoch [307/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003325452\n",
      "Epoch [307/500], Batch [80/110], Train Loss: 0.0409, Val Loss: 0.0007, LR: 0.0003322802\n",
      "Epoch [307/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0003320152\n",
      "Epoch [307/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003317503\n",
      "Epoch [307/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003314855\n",
      "Epoch [308/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0001, LR: 0.0003312207\n",
      "Epoch [308/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003309560\n",
      "Epoch [308/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003306914\n",
      "Epoch [308/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003304268\n",
      "Epoch [308/500], Batch [50/110], Train Loss: 0.0024, Val Loss: 0.0003, LR: 0.0003301622\n",
      "Epoch [308/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003298977\n",
      "Epoch [308/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003296333\n",
      "Epoch [308/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003293689\n",
      "Epoch [308/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003291046\n",
      "Epoch [308/500], Batch [100/110], Train Loss: 0.0021, Val Loss: 0.0001, LR: 0.0003288403\n",
      "Epoch [308/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003285761\n",
      "Epoch [309/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003283120\n",
      "Epoch [309/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003280479\n",
      "Epoch [309/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003277839\n",
      "Epoch [309/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003275199\n",
      "Epoch [309/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003272560\n",
      "Epoch [309/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003269921\n",
      "Epoch [309/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0003267283\n",
      "Epoch [309/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003264646\n",
      "Epoch [309/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003262009\n",
      "Epoch [309/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003259373\n",
      "Epoch [309/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003256737\n",
      "Epoch [310/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003254102\n",
      "Epoch [310/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003251467\n",
      "Epoch [310/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003248833\n",
      "Epoch [310/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003246200\n",
      "Epoch [310/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003243567\n",
      "Epoch [310/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003240935\n",
      "Epoch [310/500], Batch [70/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0003238304\n",
      "Epoch [310/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003235673\n",
      "Epoch [310/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0003233042\n",
      "Epoch [310/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0003230413\n",
      "Epoch [310/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003227783\n",
      "Epoch [311/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003225155\n",
      "Epoch [311/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003222527\n",
      "Epoch [311/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003219900\n",
      "Epoch [311/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003217273\n",
      "Epoch [311/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0003214647\n",
      "Epoch [311/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003212021\n",
      "Epoch [311/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003209396\n",
      "Epoch [311/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003206772\n",
      "Epoch [311/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003204148\n",
      "Epoch [311/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003201525\n",
      "Epoch [311/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003198902\n",
      "Confusion Matrix:\n",
      "[[634   1]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99843   0.99921       635\n",
      "           1    0.99885   1.00000   0.99942       865\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99942   0.99921   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 1067, Predicted: 1, Actual: 0\n",
      "Epoch 311: OK- Accuracy: 0.99933, Precision: 0.99885, Recall: 1.00000, F1: 0.99942, ROC AUC: 0.99921, AUPR (PR-AUC): 0.99885, Sensitivity: 1.00000, Specificity: 0.99843, Far: 0.0015748031496062992, False Positive Rate (FPR): 0.00157, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 307.52 MB\n",
      "Epoch [312/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003196280\n",
      "Epoch [312/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003193659\n",
      "Epoch [312/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003191038\n",
      "Epoch [312/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003188418\n",
      "Epoch [312/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003185798\n",
      "Epoch [312/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003183179\n",
      "Epoch [312/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003180561\n",
      "Epoch [312/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003177943\n",
      "Epoch [312/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003175326\n",
      "Epoch [312/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003172709\n",
      "Epoch [312/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003170093\n",
      "Epoch [313/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003167478\n",
      "Epoch [313/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003164864\n",
      "Epoch [313/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003162249\n",
      "Epoch [313/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0003159636\n",
      "Epoch [313/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003157023\n",
      "Epoch [313/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003154411\n",
      "Epoch [313/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003151799\n",
      "Epoch [313/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003149188\n",
      "Epoch [313/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003146578\n",
      "Epoch [313/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0003143968\n",
      "Epoch [313/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003141359\n",
      "Epoch [314/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003138751\n",
      "Epoch [314/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003136143\n",
      "Epoch [314/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003133536\n",
      "Epoch [314/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0003130929\n",
      "Epoch [314/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003128323\n",
      "Epoch [314/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003125718\n",
      "Epoch [314/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003123113\n",
      "Epoch [314/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003120509\n",
      "Epoch [314/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0003117905\n",
      "Epoch [314/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003115303\n",
      "Epoch [314/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003112700\n",
      "Epoch [315/500], Batch [10/110], Train Loss: 0.0010, Val Loss: 0.0001, LR: 0.0003110099\n",
      "Epoch [315/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003107498\n",
      "Epoch [315/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003104898\n",
      "Epoch [315/500], Batch [40/110], Train Loss: 0.0009, Val Loss: 0.0001, LR: 0.0003102298\n",
      "Epoch [315/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0003099699\n",
      "Epoch [315/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0003097100\n",
      "Epoch [315/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003094503\n",
      "Epoch [315/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0003091905\n",
      "Epoch [315/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003089309\n",
      "Epoch [315/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003086713\n",
      "Epoch [315/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0003084118\n",
      "Epoch [316/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003081523\n",
      "Epoch [316/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003078929\n",
      "Epoch [316/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003076336\n",
      "Epoch [316/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003073744\n",
      "Epoch [316/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003071152\n",
      "Epoch [316/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0003068560\n",
      "Epoch [316/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003065969\n",
      "Epoch [316/500], Batch [80/110], Train Loss: 0.0030, Val Loss: 0.0002, LR: 0.0003063379\n",
      "Epoch [316/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0003060790\n",
      "Epoch [316/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003058201\n",
      "Epoch [316/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003055613\n",
      "Epoch [317/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003053026\n",
      "Epoch [317/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003050439\n",
      "Epoch [317/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003047853\n",
      "Epoch [317/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003045267\n",
      "Epoch [317/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003042682\n",
      "Epoch [317/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003040098\n",
      "Epoch [317/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003037515\n",
      "Epoch [317/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0003034932\n",
      "Epoch [317/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003032350\n",
      "Epoch [317/500], Batch [100/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0003029768\n",
      "Epoch [317/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003027187\n",
      "Epoch [318/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003024607\n",
      "Epoch [318/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003022027\n",
      "Epoch [318/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003019448\n",
      "Epoch [318/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003016870\n",
      "Epoch [318/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003014292\n",
      "Epoch [318/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0003011716\n",
      "Epoch [318/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003009139\n",
      "Epoch [318/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0003006564\n",
      "Epoch [318/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0003003989\n",
      "Epoch [318/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0003001414\n",
      "Epoch [318/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0009, LR: 0.0002998841\n",
      "Epoch [319/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002996268\n",
      "Epoch [319/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002993696\n",
      "Epoch [319/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002991124\n",
      "Epoch [319/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002988553\n",
      "Epoch [319/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002985983\n",
      "Epoch [319/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002983413\n",
      "Epoch [319/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002980844\n",
      "Epoch [319/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002978276\n",
      "Epoch [319/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002975709\n",
      "Epoch [319/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002973142\n",
      "Epoch [319/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0002970576\n",
      "Epoch [320/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002968010\n",
      "Epoch [320/500], Batch [20/110], Train Loss: 0.0025, Val Loss: 0.0001, LR: 0.0002965445\n",
      "Epoch [320/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002962881\n",
      "Epoch [320/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002960318\n",
      "Epoch [320/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002957755\n",
      "Epoch [320/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0002955193\n",
      "Epoch [320/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0002952631\n",
      "Epoch [320/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002950071\n",
      "Epoch [320/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002947511\n",
      "Epoch [320/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0002944951\n",
      "Epoch [320/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002942393\n",
      "Epoch [321/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002939835\n",
      "Epoch [321/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002937277\n",
      "Epoch [321/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002934721\n",
      "Epoch [321/500], Batch [40/110], Train Loss: 0.0016, Val Loss: 0.0001, LR: 0.0002932165\n",
      "Epoch [321/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002929609\n",
      "Epoch [321/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002927055\n",
      "Epoch [321/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002924501\n",
      "Epoch [321/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002921948\n",
      "Epoch [321/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002919395\n",
      "Epoch [321/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002916844\n",
      "Epoch [321/500], Batch [110/110], Train Loss: 0.0224, Val Loss: 0.0007, LR: 0.0002914293\n",
      "Confusion Matrix:\n",
      "[[634   1]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99843   0.99921       635\n",
      "           1    0.99885   1.00000   0.99942       865\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99942   0.99921   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 1067, Predicted: 1, Actual: 0\n",
      "Epoch 321: OK- Accuracy: 0.99933, Precision: 0.99885, Recall: 1.00000, F1: 0.99942, ROC AUC: 0.99921, AUPR (PR-AUC): 0.99885, Sensitivity: 1.00000, Specificity: 0.99843, Far: 0.0015748031496062992, False Positive Rate (FPR): 0.00157, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 307.52 MB\n",
      "Epoch [322/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0002911742\n",
      "Epoch [322/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0002909193\n",
      "Epoch [322/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002906644\n",
      "Epoch [322/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002904095\n",
      "Epoch [322/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0002901548\n",
      "Epoch [322/500], Batch [60/110], Train Loss: 0.0428, Val Loss: 0.0007, LR: 0.0002899001\n",
      "Epoch [322/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0002896455\n",
      "Epoch [322/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002893909\n",
      "Epoch [322/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002891364\n",
      "Epoch [322/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002888820\n",
      "Epoch [322/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002886277\n",
      "Epoch [323/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002883734\n",
      "Epoch [323/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002881192\n",
      "Epoch [323/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002878651\n",
      "Epoch [323/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002876111\n",
      "Epoch [323/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002873571\n",
      "Epoch [323/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002871032\n",
      "Epoch [323/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002868493\n",
      "Epoch [323/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002865956\n",
      "Epoch [323/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002863419\n",
      "Epoch [323/500], Batch [100/110], Train Loss: 0.0020, Val Loss: 0.0001, LR: 0.0002860882\n",
      "Epoch [323/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002858347\n",
      "Epoch [324/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002855812\n",
      "Epoch [324/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0002853278\n",
      "Epoch [324/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002850744\n",
      "Epoch [324/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002848212\n",
      "Epoch [324/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002845680\n",
      "Epoch [324/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002843149\n",
      "Epoch [324/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0002840618\n",
      "Epoch [324/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002838088\n",
      "Epoch [324/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002835559\n",
      "Epoch [324/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002833031\n",
      "Epoch [324/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0002830503\n",
      "Epoch [325/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002827976\n",
      "Epoch [325/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002825450\n",
      "Epoch [325/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002822925\n",
      "Epoch [325/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002820400\n",
      "Epoch [325/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002817876\n",
      "Epoch [325/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002815352\n",
      "Epoch [325/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002812830\n",
      "Epoch [325/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002810308\n",
      "Epoch [325/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0002807787\n",
      "Epoch [325/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002805267\n",
      "Epoch [325/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002802747\n",
      "Epoch [326/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002800228\n",
      "Epoch [326/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002797710\n",
      "Epoch [326/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002795193\n",
      "Epoch [326/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002792676\n",
      "Epoch [326/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002790160\n",
      "Epoch [326/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002787645\n",
      "Epoch [326/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002785130\n",
      "Epoch [326/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0002782616\n",
      "Epoch [326/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002780103\n",
      "Epoch [326/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002777591\n",
      "Epoch [326/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002775080\n",
      "Epoch [327/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002772569\n",
      "Epoch [327/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0002770059\n",
      "Epoch [327/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002767550\n",
      "Epoch [327/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002765041\n",
      "Epoch [327/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002762533\n",
      "Epoch [327/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002760026\n",
      "Epoch [327/500], Batch [70/110], Train Loss: 0.0131, Val Loss: 0.0011, LR: 0.0002757520\n",
      "Epoch [327/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0002755014\n",
      "Epoch [327/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002752510\n",
      "Epoch [327/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002750005\n",
      "Epoch [327/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0002747502\n",
      "Epoch [328/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002745000\n",
      "Epoch [328/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002742498\n",
      "Epoch [328/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002739997\n",
      "Epoch [328/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002737496\n",
      "Epoch [328/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002734997\n",
      "Epoch [328/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002732498\n",
      "Epoch [328/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002730000\n",
      "Epoch [328/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002727503\n",
      "Epoch [328/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002725006\n",
      "Epoch [328/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0001, LR: 0.0002722511\n",
      "Epoch [328/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002720016\n",
      "Epoch [329/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002717521\n",
      "Epoch [329/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002715028\n",
      "Epoch [329/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002712535\n",
      "Epoch [329/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0002710043\n",
      "Epoch [329/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002707552\n",
      "Epoch [329/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002705062\n",
      "Epoch [329/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002702572\n",
      "Epoch [329/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002700083\n",
      "Epoch [329/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002697595\n",
      "Epoch [329/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002695107\n",
      "Epoch [329/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002692621\n",
      "Epoch [330/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0002690135\n",
      "Epoch [330/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002687650\n",
      "Epoch [330/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002685166\n",
      "Epoch [330/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002682682\n",
      "Epoch [330/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0002680199\n",
      "Epoch [330/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002677718\n",
      "Epoch [330/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002675236\n",
      "Epoch [330/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0002672756\n",
      "Epoch [330/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0002670276\n",
      "Epoch [330/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002667797\n",
      "Epoch [330/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002665319\n",
      "Epoch [331/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002662842\n",
      "Epoch [331/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0001, LR: 0.0002660365\n",
      "Epoch [331/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002657890\n",
      "Epoch [331/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002655415\n",
      "Epoch [331/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002652941\n",
      "Epoch [331/500], Batch [60/110], Train Loss: 0.0025, Val Loss: 0.0001, LR: 0.0002650467\n",
      "Epoch [331/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0002647995\n",
      "Epoch [331/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002645523\n",
      "Epoch [331/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002643052\n",
      "Epoch [331/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002640581\n",
      "Epoch [331/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002638112\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 331: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.042 sec , Memory Usage: 307.52 MB\n",
      "Epoch [332/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002635643\n",
      "Epoch [332/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002633175\n",
      "Epoch [332/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002630708\n",
      "Epoch [332/500], Batch [40/110], Train Loss: 0.0045, Val Loss: 0.0001, LR: 0.0002628242\n",
      "Epoch [332/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002625776\n",
      "Epoch [332/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0002623312\n",
      "Epoch [332/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002620848\n",
      "Epoch [332/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002618384\n",
      "Epoch [332/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002615922\n",
      "Epoch [332/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002613460\n",
      "Epoch [332/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002611000\n",
      "Epoch [333/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002608540\n",
      "Epoch [333/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002606080\n",
      "Epoch [333/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002603622\n",
      "Epoch [333/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002601164\n",
      "Epoch [333/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002598708\n",
      "Epoch [333/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002596252\n",
      "Epoch [333/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0002593796\n",
      "Epoch [333/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002591342\n",
      "Epoch [333/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002588889\n",
      "Epoch [333/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0002586436\n",
      "Epoch [333/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002583984\n",
      "Epoch [334/500], Batch [10/110], Train Loss: 0.0052, Val Loss: 0.0002, LR: 0.0002581533\n",
      "Epoch [334/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002579082\n",
      "Epoch [334/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0002576633\n",
      "Epoch [334/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002574184\n",
      "Epoch [334/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002571736\n",
      "Epoch [334/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002569289\n",
      "Epoch [334/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0002566842\n",
      "Epoch [334/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002564397\n",
      "Epoch [334/500], Batch [90/110], Train Loss: 0.0017, Val Loss: 0.0002, LR: 0.0002561952\n",
      "Epoch [334/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002559508\n",
      "Epoch [334/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002557065\n",
      "Epoch [335/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002554623\n",
      "Epoch [335/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002552181\n",
      "Epoch [335/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002549741\n",
      "Epoch [335/500], Batch [40/110], Train Loss: 0.1447, Val Loss: 0.0001, LR: 0.0002547301\n",
      "Epoch [335/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002544862\n",
      "Epoch [335/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002542424\n",
      "Epoch [335/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002539986\n",
      "Epoch [335/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0002537550\n",
      "Epoch [335/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0002535114\n",
      "Epoch [335/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002532679\n",
      "Epoch [335/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002530245\n",
      "Epoch [336/500], Batch [10/110], Train Loss: 0.0018, Val Loss: 0.0001, LR: 0.0002527812\n",
      "Epoch [336/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002525379\n",
      "Epoch [336/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002522948\n",
      "Epoch [336/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002520517\n",
      "Epoch [336/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002518087\n",
      "Epoch [336/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002515658\n",
      "Epoch [336/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002513229\n",
      "Epoch [336/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002510802\n",
      "Epoch [336/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002508375\n",
      "Epoch [336/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002505949\n",
      "Epoch [336/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002503524\n",
      "Epoch [337/500], Batch [10/110], Train Loss: 0.0027, Val Loss: 0.0003, LR: 0.0002501100\n",
      "Epoch [337/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002498677\n",
      "Epoch [337/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002496254\n",
      "Epoch [337/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002493833\n",
      "Epoch [337/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002491412\n",
      "Epoch [337/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002488992\n",
      "Epoch [337/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002486573\n",
      "Epoch [337/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002484154\n",
      "Epoch [337/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002481737\n",
      "Epoch [337/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002479320\n",
      "Epoch [337/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002476904\n",
      "Epoch [338/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002474489\n",
      "Epoch [338/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002472075\n",
      "Epoch [338/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002469662\n",
      "Epoch [338/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002467249\n",
      "Epoch [338/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002464838\n",
      "Epoch [338/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002462427\n",
      "Epoch [338/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0002460017\n",
      "Epoch [338/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002457608\n",
      "Epoch [338/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002455200\n",
      "Epoch [338/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002452792\n",
      "Epoch [338/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002450386\n",
      "Epoch [339/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002447980\n",
      "Epoch [339/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002445575\n",
      "Epoch [339/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002443171\n",
      "Epoch [339/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002440768\n",
      "Epoch [339/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002438366\n",
      "Epoch [339/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002435964\n",
      "Epoch [339/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0002433563\n",
      "Epoch [339/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002431164\n",
      "Epoch [339/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0002428765\n",
      "Epoch [339/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002426367\n",
      "Epoch [339/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002423970\n",
      "Epoch [340/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002421573\n",
      "Epoch [340/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0002419178\n",
      "Epoch [340/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002416783\n",
      "Epoch [340/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002414389\n",
      "Epoch [340/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002411997\n",
      "Epoch [340/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002409605\n",
      "Epoch [340/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002407213\n",
      "Epoch [340/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002404823\n",
      "Epoch [340/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002402434\n",
      "Epoch [340/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002400045\n",
      "Epoch [340/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0002397657\n",
      "Epoch [341/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002395271\n",
      "Epoch [341/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002392885\n",
      "Epoch [341/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002390499\n",
      "Epoch [341/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0002388115\n",
      "Epoch [341/500], Batch [50/110], Train Loss: 0.0019, Val Loss: 0.0001, LR: 0.0002385732\n",
      "Epoch [341/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002383349\n",
      "Epoch [341/500], Batch [70/110], Train Loss: 0.0088, Val Loss: 0.0001, LR: 0.0002380968\n",
      "Epoch [341/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002378587\n",
      "Epoch [341/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002376207\n",
      "Epoch [341/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002373828\n",
      "Epoch [341/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002371450\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 341: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 307.46 MB\n",
      "Epoch [342/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002369073\n",
      "Epoch [342/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002366696\n",
      "Epoch [342/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002364321\n",
      "Epoch [342/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002361946\n",
      "Epoch [342/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002359572\n",
      "Epoch [342/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002357199\n",
      "Epoch [342/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002354827\n",
      "Epoch [342/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0002352456\n",
      "Epoch [342/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002350086\n",
      "Epoch [342/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002347716\n",
      "Epoch [342/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0002345348\n",
      "Epoch [343/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002342980\n",
      "Epoch [343/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0002340614\n",
      "Epoch [343/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002338248\n",
      "Epoch [343/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002335883\n",
      "Epoch [343/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002333519\n",
      "Epoch [343/500], Batch [60/110], Train Loss: 0.0029, Val Loss: 0.0001, LR: 0.0002331156\n",
      "Epoch [343/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0002328793\n",
      "Epoch [343/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002326432\n",
      "Epoch [343/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002324071\n",
      "Epoch [343/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002321712\n",
      "Epoch [343/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002319353\n",
      "Epoch [344/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0002316995\n",
      "Epoch [344/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0002314638\n",
      "Epoch [344/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002312282\n",
      "Epoch [344/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002309927\n",
      "Epoch [344/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0002307573\n",
      "Epoch [344/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002305219\n",
      "Epoch [344/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002302867\n",
      "Epoch [344/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002300515\n",
      "Epoch [344/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0002298164\n",
      "Epoch [344/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002295815\n",
      "Epoch [344/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002293466\n",
      "Epoch [345/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002291118\n",
      "Epoch [345/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0002288771\n",
      "Epoch [345/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002286424\n",
      "Epoch [345/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002284079\n",
      "Epoch [345/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002281735\n",
      "Epoch [345/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002279391\n",
      "Epoch [345/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002277049\n",
      "Epoch [345/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002274707\n",
      "Epoch [345/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002272366\n",
      "Epoch [345/500], Batch [100/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0002270026\n",
      "Epoch [345/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002267687\n",
      "Epoch [346/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002265349\n",
      "Epoch [346/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002263012\n",
      "Epoch [346/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002260676\n",
      "Epoch [346/500], Batch [40/110], Train Loss: 0.0018, Val Loss: 0.0002, LR: 0.0002258340\n",
      "Epoch [346/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0002256006\n",
      "Epoch [346/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002253673\n",
      "Epoch [346/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002251340\n",
      "Epoch [346/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002249008\n",
      "Epoch [346/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002246678\n",
      "Epoch [346/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002244348\n",
      "Epoch [346/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002242019\n",
      "Epoch [347/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0010, LR: 0.0002239691\n",
      "Epoch [347/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0002237364\n",
      "Epoch [347/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002235037\n",
      "Epoch [347/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002232712\n",
      "Epoch [347/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002230388\n",
      "Epoch [347/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002228064\n",
      "Epoch [347/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002225742\n",
      "Epoch [347/500], Batch [80/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0002223420\n",
      "Epoch [347/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0002221100\n",
      "Epoch [347/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0001, LR: 0.0002218780\n",
      "Epoch [347/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002216461\n",
      "Epoch [348/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002214143\n",
      "Epoch [348/500], Batch [20/110], Train Loss: 0.0018, Val Loss: 0.0002, LR: 0.0002211826\n",
      "Epoch [348/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002209510\n",
      "Epoch [348/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002207195\n",
      "Epoch [348/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002204881\n",
      "Epoch [348/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002202567\n",
      "Epoch [348/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002200255\n",
      "Epoch [348/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002197944\n",
      "Epoch [348/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002195633\n",
      "Epoch [348/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002193324\n",
      "Epoch [348/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002191015\n",
      "Epoch [349/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002188707\n",
      "Epoch [349/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002186401\n",
      "Epoch [349/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002184095\n",
      "Epoch [349/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002181790\n",
      "Epoch [349/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002179486\n",
      "Epoch [349/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0002177183\n",
      "Epoch [349/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002174881\n",
      "Epoch [349/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002172580\n",
      "Epoch [349/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002170280\n",
      "Epoch [349/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002167980\n",
      "Epoch [349/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002165682\n",
      "Epoch [350/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002163385\n",
      "Epoch [350/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002161088\n",
      "Epoch [350/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002158793\n",
      "Epoch [350/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002156498\n",
      "Epoch [350/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002154205\n",
      "Epoch [350/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002151912\n",
      "Epoch [350/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002149620\n",
      "Epoch [350/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002147330\n",
      "Epoch [350/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0002145040\n",
      "Epoch [350/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002142751\n",
      "Epoch [350/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002140463\n",
      "Epoch [351/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002138176\n",
      "Epoch [351/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002135890\n",
      "Epoch [351/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002133605\n",
      "Epoch [351/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002131321\n",
      "Epoch [351/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002129038\n",
      "Epoch [351/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002126755\n",
      "Epoch [351/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002124474\n",
      "Epoch [351/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002122194\n",
      "Epoch [351/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002119915\n",
      "Epoch [351/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002117636\n",
      "Epoch [351/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002115359\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 351: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.039 sec , Memory Usage: 307.46 MB\n",
      "Epoch [352/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002113082\n",
      "Epoch [352/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002110807\n",
      "Epoch [352/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002108532\n",
      "Epoch [352/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002106259\n",
      "Epoch [352/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002103986\n",
      "Epoch [352/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002101714\n",
      "Epoch [352/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002099444\n",
      "Epoch [352/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002097174\n",
      "Epoch [352/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002094905\n",
      "Epoch [352/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002092637\n",
      "Epoch [352/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002090370\n",
      "Epoch [353/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002088104\n",
      "Epoch [353/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002085840\n",
      "Epoch [353/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002083576\n",
      "Epoch [353/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002081313\n",
      "Epoch [353/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002079051\n",
      "Epoch [353/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002076789\n",
      "Epoch [353/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002074529\n",
      "Epoch [353/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002072270\n",
      "Epoch [353/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002070012\n",
      "Epoch [353/500], Batch [100/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0002067755\n",
      "Epoch [353/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002065499\n",
      "Epoch [354/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002063244\n",
      "Epoch [354/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002060989\n",
      "Epoch [354/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002058736\n",
      "Epoch [354/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002056484\n",
      "Epoch [354/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002054232\n",
      "Epoch [354/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002051982\n",
      "Epoch [354/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0002049733\n",
      "Epoch [354/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002047484\n",
      "Epoch [354/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002045237\n",
      "Epoch [354/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002042990\n",
      "Epoch [354/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002040745\n",
      "Epoch [355/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002038501\n",
      "Epoch [355/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002036257\n",
      "Epoch [355/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002034015\n",
      "Epoch [355/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002031773\n",
      "Epoch [355/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002029533\n",
      "Epoch [355/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002027293\n",
      "Epoch [355/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002025054\n",
      "Epoch [355/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002022817\n",
      "Epoch [355/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002020580\n",
      "Epoch [355/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002018345\n",
      "Epoch [355/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002016110\n",
      "Epoch [356/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002013876\n",
      "Epoch [356/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002011644\n",
      "Epoch [356/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002009412\n",
      "Epoch [356/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002007182\n",
      "Epoch [356/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002004952\n",
      "Epoch [356/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002002723\n",
      "Epoch [356/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002000496\n",
      "Epoch [356/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001998269\n",
      "Epoch [356/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0001996043\n",
      "Epoch [356/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001993819\n",
      "Epoch [356/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001991595\n",
      "Epoch [357/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001989372\n",
      "Epoch [357/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001987151\n",
      "Epoch [357/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001984930\n",
      "Epoch [357/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001982710\n",
      "Epoch [357/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001980492\n",
      "Epoch [357/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001978274\n",
      "Epoch [357/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0001976057\n",
      "Epoch [357/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001973841\n",
      "Epoch [357/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0001971627\n",
      "Epoch [357/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001969413\n",
      "Epoch [357/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001967200\n",
      "Epoch [358/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001964989\n",
      "Epoch [358/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001962778\n",
      "Epoch [358/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001960569\n",
      "Epoch [358/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001958360\n",
      "Epoch [358/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001956152\n",
      "Epoch [358/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001953946\n",
      "Epoch [358/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001951740\n",
      "Epoch [358/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001949535\n",
      "Epoch [358/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001947332\n",
      "Epoch [358/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001945129\n",
      "Epoch [358/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001942928\n",
      "Epoch [359/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001940727\n",
      "Epoch [359/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001938528\n",
      "Epoch [359/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001936329\n",
      "Epoch [359/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0001934132\n",
      "Epoch [359/500], Batch [50/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0001931935\n",
      "Epoch [359/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001929740\n",
      "Epoch [359/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001927545\n",
      "Epoch [359/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001925352\n",
      "Epoch [359/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001923159\n",
      "Epoch [359/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001920968\n",
      "Epoch [359/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001918778\n",
      "Epoch [360/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001916588\n",
      "Epoch [360/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001914400\n",
      "Epoch [360/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001912213\n",
      "Epoch [360/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001910026\n",
      "Epoch [360/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001907841\n",
      "Epoch [360/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001905657\n",
      "Epoch [360/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001903474\n",
      "Epoch [360/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001901292\n",
      "Epoch [360/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001899110\n",
      "Epoch [360/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001896930\n",
      "Epoch [360/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001894751\n",
      "Epoch [361/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001892573\n",
      "Epoch [361/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001890396\n",
      "Epoch [361/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001888220\n",
      "Epoch [361/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0001886045\n",
      "Epoch [361/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001883871\n",
      "Epoch [361/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001881698\n",
      "Epoch [361/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001879526\n",
      "Epoch [361/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001877356\n",
      "Epoch [361/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001875186\n",
      "Epoch [361/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001873017\n",
      "Epoch [361/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001870849\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 361: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.035 sec , Memory Usage: 307.46 MB\n",
      "Epoch [362/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001868683\n",
      "Epoch [362/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001866517\n",
      "Epoch [362/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001864352\n",
      "Epoch [362/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0001862189\n",
      "Epoch [362/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001860026\n",
      "Epoch [362/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001857865\n",
      "Epoch [362/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001855704\n",
      "Epoch [362/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001853545\n",
      "Epoch [362/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001851387\n",
      "Epoch [362/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0001849229\n",
      "Epoch [362/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0001, LR: 0.0001847073\n",
      "Epoch [363/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0001844918\n",
      "Epoch [363/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001842764\n",
      "Epoch [363/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001840610\n",
      "Epoch [363/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0001838458\n",
      "Epoch [363/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001836307\n",
      "Epoch [363/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001834157\n",
      "Epoch [363/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001832008\n",
      "Epoch [363/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001829860\n",
      "Epoch [363/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001827714\n",
      "Epoch [363/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001825568\n",
      "Epoch [363/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001823423\n",
      "Epoch [364/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001821279\n",
      "Epoch [364/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001819137\n",
      "Epoch [364/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001816995\n",
      "Epoch [364/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001814855\n",
      "Epoch [364/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001812715\n",
      "Epoch [364/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0001810577\n",
      "Epoch [364/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001808439\n",
      "Epoch [364/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001806303\n",
      "Epoch [364/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001804168\n",
      "Epoch [364/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001802034\n",
      "Epoch [364/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001799901\n",
      "Epoch [365/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001797768\n",
      "Epoch [365/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001795637\n",
      "Epoch [365/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001793507\n",
      "Epoch [365/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001791379\n",
      "Epoch [365/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001789251\n",
      "Epoch [365/500], Batch [60/110], Train Loss: 0.0013, Val Loss: 0.0002, LR: 0.0001787124\n",
      "Epoch [365/500], Batch [70/110], Train Loss: 0.0019, Val Loss: 0.0001, LR: 0.0001784998\n",
      "Epoch [365/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001782874\n",
      "Epoch [365/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001780750\n",
      "Epoch [365/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001778628\n",
      "Epoch [365/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001776506\n",
      "Epoch [366/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001774386\n",
      "Epoch [366/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001772267\n",
      "Epoch [366/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001770148\n",
      "Epoch [366/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001768031\n",
      "Epoch [366/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001765915\n",
      "Epoch [366/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001763800\n",
      "Epoch [366/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001761686\n",
      "Epoch [366/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001759573\n",
      "Epoch [366/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001757462\n",
      "Epoch [366/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001755351\n",
      "Epoch [366/500], Batch [110/110], Train Loss: 0.0017, Val Loss: 0.0001, LR: 0.0001753241\n",
      "Epoch [367/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001751133\n",
      "Epoch [367/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001749025\n",
      "Epoch [367/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0001746919\n",
      "Epoch [367/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001744813\n",
      "Epoch [367/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001742709\n",
      "Epoch [367/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0001740606\n",
      "Epoch [367/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001738504\n",
      "Epoch [367/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001736403\n",
      "Epoch [367/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001734303\n",
      "Epoch [367/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001732204\n",
      "Epoch [367/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001730106\n",
      "Epoch [368/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001728010\n",
      "Epoch [368/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001725914\n",
      "Epoch [368/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001723820\n",
      "Epoch [368/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001721726\n",
      "Epoch [368/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0001719634\n",
      "Epoch [368/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001717543\n",
      "Epoch [368/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001715452\n",
      "Epoch [368/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001713363\n",
      "Epoch [368/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001711275\n",
      "Epoch [368/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001709188\n",
      "Epoch [368/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0001, LR: 0.0001707103\n",
      "Epoch [369/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001705018\n",
      "Epoch [369/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001702934\n",
      "Epoch [369/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001700852\n",
      "Epoch [369/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001698770\n",
      "Epoch [369/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001696690\n",
      "Epoch [369/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001694611\n",
      "Epoch [369/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001692532\n",
      "Epoch [369/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001690455\n",
      "Epoch [369/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001688379\n",
      "Epoch [369/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001686305\n",
      "Epoch [369/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001684231\n",
      "Epoch [370/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001682158\n",
      "Epoch [370/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001680087\n",
      "Epoch [370/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001678016\n",
      "Epoch [370/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0001675947\n",
      "Epoch [370/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0001673878\n",
      "Epoch [370/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001671811\n",
      "Epoch [370/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001669745\n",
      "Epoch [370/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001667680\n",
      "Epoch [370/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001665616\n",
      "Epoch [370/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001663553\n",
      "Epoch [370/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0001661492\n",
      "Epoch [371/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001659431\n",
      "Epoch [371/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001657372\n",
      "Epoch [371/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001655313\n",
      "Epoch [371/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001653256\n",
      "Epoch [371/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001651200\n",
      "Epoch [371/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001649145\n",
      "Epoch [371/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001647091\n",
      "Epoch [371/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001645038\n",
      "Epoch [371/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001642987\n",
      "Epoch [371/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001640936\n",
      "Epoch [371/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001638887\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 371: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 307.46 MB\n",
      "Epoch [372/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001636838\n",
      "Epoch [372/500], Batch [20/110], Train Loss: 0.0026, Val Loss: 0.0002, LR: 0.0001634791\n",
      "Epoch [372/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0001632745\n",
      "Epoch [372/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001630700\n",
      "Epoch [372/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0001628656\n",
      "Epoch [372/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001626613\n",
      "Epoch [372/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001624572\n",
      "Epoch [372/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001622531\n",
      "Epoch [372/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0001, LR: 0.0001620492\n",
      "Epoch [372/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001618453\n",
      "Epoch [372/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001616416\n",
      "Epoch [373/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001614380\n",
      "Epoch [373/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001612345\n",
      "Epoch [373/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001610311\n",
      "Epoch [373/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001608279\n",
      "Epoch [373/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001606247\n",
      "Epoch [373/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001604217\n",
      "Epoch [373/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001602187\n",
      "Epoch [373/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001600159\n",
      "Epoch [373/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001598132\n",
      "Epoch [373/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001596106\n",
      "Epoch [373/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001594081\n",
      "Epoch [374/500], Batch [10/110], Train Loss: 0.0013, Val Loss: 0.0002, LR: 0.0001592057\n",
      "Epoch [374/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001590035\n",
      "Epoch [374/500], Batch [30/110], Train Loss: 0.0049, Val Loss: 0.0003, LR: 0.0001588013\n",
      "Epoch [374/500], Batch [40/110], Train Loss: 0.0440, Val Loss: 0.0001, LR: 0.0001585993\n",
      "Epoch [374/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0001583974\n",
      "Epoch [374/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001581956\n",
      "Epoch [374/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0001579939\n",
      "Epoch [374/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001577923\n",
      "Epoch [374/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001575909\n",
      "Epoch [374/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001573895\n",
      "Epoch [374/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001571883\n",
      "Epoch [375/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001569871\n",
      "Epoch [375/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001567861\n",
      "Epoch [375/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0001, LR: 0.0001565852\n",
      "Epoch [375/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001563844\n",
      "Epoch [375/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001561838\n",
      "Epoch [375/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001559832\n",
      "Epoch [375/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001557828\n",
      "Epoch [375/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001555824\n",
      "Epoch [375/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001553822\n",
      "Epoch [375/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0001551821\n",
      "Epoch [375/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001549821\n",
      "Epoch [376/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001547823\n",
      "Epoch [376/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001545825\n",
      "Epoch [376/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001543829\n",
      "Epoch [376/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001541833\n",
      "Epoch [376/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001539839\n",
      "Epoch [376/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001537846\n",
      "Epoch [376/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001535854\n",
      "Epoch [376/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001533864\n",
      "Epoch [376/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001531874\n",
      "Epoch [376/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001529886\n",
      "Epoch [376/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001527898\n",
      "Epoch [377/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001525912\n",
      "Epoch [377/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001523927\n",
      "Epoch [377/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001521943\n",
      "Epoch [377/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001519961\n",
      "Epoch [377/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001517979\n",
      "Epoch [377/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001515999\n",
      "Epoch [377/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001514020\n",
      "Epoch [377/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001512042\n",
      "Epoch [377/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001510065\n",
      "Epoch [377/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001508089\n",
      "Epoch [377/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001506114\n",
      "Epoch [378/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001504141\n",
      "Epoch [378/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001502169\n",
      "Epoch [378/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001500198\n",
      "Epoch [378/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001498228\n",
      "Epoch [378/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001496259\n",
      "Epoch [378/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001494291\n",
      "Epoch [378/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001492325\n",
      "Epoch [378/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001490359\n",
      "Epoch [378/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001488395\n",
      "Epoch [378/500], Batch [100/110], Train Loss: 0.0032, Val Loss: 0.0004, LR: 0.0001486432\n",
      "Epoch [378/500], Batch [110/110], Train Loss: 0.0021, Val Loss: 0.0001, LR: 0.0001484470\n",
      "Epoch [379/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001482510\n",
      "Epoch [379/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001480550\n",
      "Epoch [379/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001478592\n",
      "Epoch [379/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001476635\n",
      "Epoch [379/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001474679\n",
      "Epoch [379/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001472724\n",
      "Epoch [379/500], Batch [70/110], Train Loss: 0.0068, Val Loss: 0.0002, LR: 0.0001470770\n",
      "Epoch [379/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001468818\n",
      "Epoch [379/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001466866\n",
      "Epoch [379/500], Batch [100/110], Train Loss: 0.0345, Val Loss: 0.0001, LR: 0.0001464916\n",
      "Epoch [379/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001462967\n",
      "Epoch [380/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0001461019\n",
      "Epoch [380/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001459073\n",
      "Epoch [380/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001457127\n",
      "Epoch [380/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001455183\n",
      "Epoch [380/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001453240\n",
      "Epoch [380/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001451298\n",
      "Epoch [380/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001449357\n",
      "Epoch [380/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001447417\n",
      "Epoch [380/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001445479\n",
      "Epoch [380/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001443541\n",
      "Epoch [380/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001441605\n",
      "Epoch [381/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001439670\n",
      "Epoch [381/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001437737\n",
      "Epoch [381/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001435804\n",
      "Epoch [381/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001433873\n",
      "Epoch [381/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001431942\n",
      "Epoch [381/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001430013\n",
      "Epoch [381/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001428086\n",
      "Epoch [381/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001426159\n",
      "Epoch [381/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001424233\n",
      "Epoch [381/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001422309\n",
      "Epoch [381/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001420386\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 381: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 307.46 MB\n",
      "Epoch [382/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001418464\n",
      "Epoch [382/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0001416543\n",
      "Epoch [382/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0001414624\n",
      "Epoch [382/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001412705\n",
      "Epoch [382/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001410788\n",
      "Epoch [382/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001408872\n",
      "Epoch [382/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001406957\n",
      "Epoch [382/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001405044\n",
      "Epoch [382/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001403131\n",
      "Epoch [382/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001401220\n",
      "Epoch [382/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001399310\n",
      "Epoch [383/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001397401\n",
      "Epoch [383/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001395494\n",
      "Epoch [383/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001393587\n",
      "Epoch [383/500], Batch [40/110], Train Loss: 0.2635, Val Loss: 0.0008, LR: 0.0001391682\n",
      "Epoch [383/500], Batch [50/110], Train Loss: 0.0029, Val Loss: 0.0003, LR: 0.0001389778\n",
      "Epoch [383/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001387875\n",
      "Epoch [383/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001385973\n",
      "Epoch [383/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001384073\n",
      "Epoch [383/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001382173\n",
      "Epoch [383/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0001380275\n",
      "Epoch [383/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001378378\n",
      "Epoch [384/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0001, LR: 0.0001376483\n",
      "Epoch [384/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001374588\n",
      "Epoch [384/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001372695\n",
      "Epoch [384/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001370803\n",
      "Epoch [384/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001368912\n",
      "Epoch [384/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001367022\n",
      "Epoch [384/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001365133\n",
      "Epoch [384/500], Batch [80/110], Train Loss: 0.0015, Val Loss: 0.0001, LR: 0.0001363246\n",
      "Epoch [384/500], Batch [90/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0001361360\n",
      "Epoch [384/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001359475\n",
      "Epoch [384/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001357591\n",
      "Epoch [385/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0001355709\n",
      "Epoch [385/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001353828\n",
      "Epoch [385/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001351947\n",
      "Epoch [385/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001350069\n",
      "Epoch [385/500], Batch [50/110], Train Loss: 0.0021, Val Loss: 0.0002, LR: 0.0001348191\n",
      "Epoch [385/500], Batch [60/110], Train Loss: 0.1266, Val Loss: 0.0002, LR: 0.0001346314\n",
      "Epoch [385/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001344439\n",
      "Epoch [385/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001342565\n",
      "Epoch [385/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0001340692\n",
      "Epoch [385/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001338821\n",
      "Epoch [385/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001336950\n",
      "Epoch [386/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001335081\n",
      "Epoch [386/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001333213\n",
      "Epoch [386/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001331346\n",
      "Epoch [386/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0001329481\n",
      "Epoch [386/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001327616\n",
      "Epoch [386/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001325753\n",
      "Epoch [386/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001323891\n",
      "Epoch [386/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001322031\n",
      "Epoch [386/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001320171\n",
      "Epoch [386/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001318313\n",
      "Epoch [386/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001316456\n",
      "Epoch [387/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001314600\n",
      "Epoch [387/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001312745\n",
      "Epoch [387/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001310892\n",
      "Epoch [387/500], Batch [40/110], Train Loss: 0.0013, Val Loss: 0.0002, LR: 0.0001309040\n",
      "Epoch [387/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001307189\n",
      "Epoch [387/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001305339\n",
      "Epoch [387/500], Batch [70/110], Train Loss: 0.0010, Val Loss: 0.0001, LR: 0.0001303490\n",
      "Epoch [387/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001301643\n",
      "Epoch [387/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001299797\n",
      "Epoch [387/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001297952\n",
      "Epoch [387/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001296109\n",
      "Epoch [388/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0001294266\n",
      "Epoch [388/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001292425\n",
      "Epoch [388/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001290585\n",
      "Epoch [388/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001288746\n",
      "Epoch [388/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001286909\n",
      "Epoch [388/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001285072\n",
      "Epoch [388/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001283237\n",
      "Epoch [388/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001281404\n",
      "Epoch [388/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001279571\n",
      "Epoch [388/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001277740\n",
      "Epoch [388/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001275910\n",
      "Epoch [389/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001274081\n",
      "Epoch [389/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001272253\n",
      "Epoch [389/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001270427\n",
      "Epoch [389/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001268601\n",
      "Epoch [389/500], Batch [50/110], Train Loss: 0.0010, Val Loss: 0.0001, LR: 0.0001266777\n",
      "Epoch [389/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001264955\n",
      "Epoch [389/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001263133\n",
      "Epoch [389/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001261313\n",
      "Epoch [389/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001259494\n",
      "Epoch [389/500], Batch [100/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0001257676\n",
      "Epoch [389/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001255860\n",
      "Epoch [390/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0001254044\n",
      "Epoch [390/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001252230\n",
      "Epoch [390/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001250417\n",
      "Epoch [390/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001248606\n",
      "Epoch [390/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001246795\n",
      "Epoch [390/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001244986\n",
      "Epoch [390/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001243178\n",
      "Epoch [390/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001241372\n",
      "Epoch [390/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001239566\n",
      "Epoch [390/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001237762\n",
      "Epoch [390/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001235959\n",
      "Epoch [391/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001234158\n",
      "Epoch [391/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001232357\n",
      "Epoch [391/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001230558\n",
      "Epoch [391/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001228760\n",
      "Epoch [391/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001226964\n",
      "Epoch [391/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001225168\n",
      "Epoch [391/500], Batch [70/110], Train Loss: 0.0034, Val Loss: 0.0004, LR: 0.0001223374\n",
      "Epoch [391/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001221581\n",
      "Epoch [391/500], Batch [90/110], Train Loss: 0.0017, Val Loss: 0.0002, LR: 0.0001219789\n",
      "Epoch [391/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001217999\n",
      "Epoch [391/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001216210\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 391: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 307.46 MB\n",
      "Epoch [392/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001214422\n",
      "Epoch [392/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001212635\n",
      "Epoch [392/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001210850\n",
      "Epoch [392/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001209066\n",
      "Epoch [392/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001207283\n",
      "Epoch [392/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001205501\n",
      "Epoch [392/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001203721\n",
      "Epoch [392/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001201942\n",
      "Epoch [392/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001200164\n",
      "Epoch [392/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001198387\n",
      "Epoch [392/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001196612\n",
      "Epoch [393/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001194837\n",
      "Epoch [393/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001193065\n",
      "Epoch [393/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001191293\n",
      "Epoch [393/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001189523\n",
      "Epoch [393/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001187754\n",
      "Epoch [393/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001185986\n",
      "Epoch [393/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001184219\n",
      "Epoch [393/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001182454\n",
      "Epoch [393/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0001180690\n",
      "Epoch [393/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001178927\n",
      "Epoch [393/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001177165\n",
      "Epoch [394/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001175405\n",
      "Epoch [394/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001173646\n",
      "Epoch [394/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001171889\n",
      "Epoch [394/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001170132\n",
      "Epoch [394/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0001168377\n",
      "Epoch [394/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0001166623\n",
      "Epoch [394/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001164870\n",
      "Epoch [394/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001163119\n",
      "Epoch [394/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001161369\n",
      "Epoch [394/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001159620\n",
      "Epoch [394/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001157872\n",
      "Epoch [395/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001156126\n",
      "Epoch [395/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001154381\n",
      "Epoch [395/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001152637\n",
      "Epoch [395/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001150895\n",
      "Epoch [395/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001149153\n",
      "Epoch [395/500], Batch [60/110], Train Loss: 0.1689, Val Loss: 0.0001, LR: 0.0001147413\n",
      "Epoch [395/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001145675\n",
      "Epoch [395/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001143937\n",
      "Epoch [395/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0001142201\n",
      "Epoch [395/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0000, LR: 0.0001140466\n",
      "Epoch [395/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001138733\n",
      "Epoch [396/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001137000\n",
      "Epoch [396/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001135269\n",
      "Epoch [396/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001133540\n",
      "Epoch [396/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001131811\n",
      "Epoch [396/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001130084\n",
      "Epoch [396/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001128358\n",
      "Epoch [396/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001126633\n",
      "Epoch [396/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0001124910\n",
      "Epoch [396/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0001123188\n",
      "Epoch [396/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001121467\n",
      "Epoch [396/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001119748\n",
      "Epoch [397/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001118029\n",
      "Epoch [397/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0001116312\n",
      "Epoch [397/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001114597\n",
      "Epoch [397/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001112882\n",
      "Epoch [397/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001111169\n",
      "Epoch [397/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001109457\n",
      "Epoch [397/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001107747\n",
      "Epoch [397/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001106038\n",
      "Epoch [397/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001104330\n",
      "Epoch [397/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001102623\n",
      "Epoch [397/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001100918\n",
      "Epoch [398/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001099213\n",
      "Epoch [398/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001097511\n",
      "Epoch [398/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001095809\n",
      "Epoch [398/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001094109\n",
      "Epoch [398/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001092410\n",
      "Epoch [398/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001090712\n",
      "Epoch [398/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0001089016\n",
      "Epoch [398/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001087321\n",
      "Epoch [398/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0001085627\n",
      "Epoch [398/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001083935\n",
      "Epoch [398/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001082243\n",
      "Epoch [399/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0001080554\n",
      "Epoch [399/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001078865\n",
      "Epoch [399/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001077178\n",
      "Epoch [399/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001075492\n",
      "Epoch [399/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001073807\n",
      "Epoch [399/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001072124\n",
      "Epoch [399/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001070441\n",
      "Epoch [399/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001068761\n",
      "Epoch [399/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001067081\n",
      "Epoch [399/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0001065403\n",
      "Epoch [399/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001063726\n",
      "Epoch [400/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001062050\n",
      "Epoch [400/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001060376\n",
      "Epoch [400/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001058703\n",
      "Epoch [400/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001057031\n",
      "Epoch [400/500], Batch [50/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0001055361\n",
      "Epoch [400/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001053692\n",
      "Epoch [400/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001052024\n",
      "Epoch [400/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0001050358\n",
      "Epoch [400/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001048692\n",
      "Epoch [400/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001047028\n",
      "Epoch [400/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001045366\n",
      "Epoch [401/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001043705\n",
      "Epoch [401/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001042045\n",
      "Epoch [401/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001040386\n",
      "Epoch [401/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001038729\n",
      "Epoch [401/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001037073\n",
      "Epoch [401/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0001035418\n",
      "Epoch [401/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001033764\n",
      "Epoch [401/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001032112\n",
      "Epoch [401/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001030462\n",
      "Epoch [401/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001028812\n",
      "Epoch [401/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001027164\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 401: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 307.46 MB\n",
      "Epoch [402/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0001025517\n",
      "Epoch [402/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001023871\n",
      "Epoch [402/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001022227\n",
      "Epoch [402/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001020584\n",
      "Epoch [402/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001018943\n",
      "Epoch [402/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001017302\n",
      "Epoch [402/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001015663\n",
      "Epoch [402/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001014026\n",
      "Epoch [402/500], Batch [90/110], Train Loss: 0.0379, Val Loss: 0.0001, LR: 0.0001012389\n",
      "Epoch [402/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001010754\n",
      "Epoch [402/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001009121\n",
      "Epoch [403/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0001007488\n",
      "Epoch [403/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0001005857\n",
      "Epoch [403/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001004227\n",
      "Epoch [403/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001002599\n",
      "Epoch [403/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001000972\n",
      "Epoch [403/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000999346\n",
      "Epoch [403/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000997722\n",
      "Epoch [403/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000996099\n",
      "Epoch [403/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000994477\n",
      "Epoch [403/500], Batch [100/110], Train Loss: 0.1075, Val Loss: 0.0001, LR: 0.0000992856\n",
      "Epoch [403/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000991237\n",
      "Epoch [404/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000989619\n",
      "Epoch [404/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000988003\n",
      "Epoch [404/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000986387\n",
      "Epoch [404/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000984774\n",
      "Epoch [404/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000983161\n",
      "Epoch [404/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000981550\n",
      "Epoch [404/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000979940\n",
      "Epoch [404/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000978331\n",
      "Epoch [404/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0000976724\n",
      "Epoch [404/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000975118\n",
      "Epoch [404/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000973514\n",
      "Epoch [405/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000971910\n",
      "Epoch [405/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000970308\n",
      "Epoch [405/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000968708\n",
      "Epoch [405/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000967109\n",
      "Epoch [405/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000965511\n",
      "Epoch [405/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000963914\n",
      "Epoch [405/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000962319\n",
      "Epoch [405/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000960725\n",
      "Epoch [405/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000959132\n",
      "Epoch [405/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000957541\n",
      "Epoch [405/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0000955951\n",
      "Epoch [406/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000954363\n",
      "Epoch [406/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000952775\n",
      "Epoch [406/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000951189\n",
      "Epoch [406/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000949605\n",
      "Epoch [406/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000948022\n",
      "Epoch [406/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000946440\n",
      "Epoch [406/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000944859\n",
      "Epoch [406/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000943280\n",
      "Epoch [406/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000941702\n",
      "Epoch [406/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000940125\n",
      "Epoch [406/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000938550\n",
      "Epoch [407/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0000936976\n",
      "Epoch [407/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000935404\n",
      "Epoch [407/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000933833\n",
      "Epoch [407/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000932263\n",
      "Epoch [407/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000930694\n",
      "Epoch [407/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000929127\n",
      "Epoch [407/500], Batch [70/110], Train Loss: 0.0017, Val Loss: 0.0001, LR: 0.0000927561\n",
      "Epoch [407/500], Batch [80/110], Train Loss: 0.0025, Val Loss: 0.0001, LR: 0.0000925997\n",
      "Epoch [407/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000924434\n",
      "Epoch [407/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000922872\n",
      "Epoch [407/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0001, LR: 0.0000921312\n",
      "Epoch [408/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000919753\n",
      "Epoch [408/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000918195\n",
      "Epoch [408/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000916639\n",
      "Epoch [408/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000915084\n",
      "Epoch [408/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000913530\n",
      "Epoch [408/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000911978\n",
      "Epoch [408/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000910427\n",
      "Epoch [408/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000908877\n",
      "Epoch [408/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000907329\n",
      "Epoch [408/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000905782\n",
      "Epoch [408/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000904236\n",
      "Epoch [409/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000902692\n",
      "Epoch [409/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000901149\n",
      "Epoch [409/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000899608\n",
      "Epoch [409/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000898067\n",
      "Epoch [409/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000896529\n",
      "Epoch [409/500], Batch [60/110], Train Loss: 0.0020, Val Loss: 0.0001, LR: 0.0000894991\n",
      "Epoch [409/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000893455\n",
      "Epoch [409/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000891920\n",
      "Epoch [409/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000890387\n",
      "Epoch [409/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000888855\n",
      "Epoch [409/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000887324\n",
      "Epoch [410/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000885795\n",
      "Epoch [410/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000884267\n",
      "Epoch [410/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000882740\n",
      "Epoch [410/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000881215\n",
      "Epoch [410/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000879691\n",
      "Epoch [410/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000878169\n",
      "Epoch [410/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000876648\n",
      "Epoch [410/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000875128\n",
      "Epoch [410/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000873610\n",
      "Epoch [410/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000872092\n",
      "Epoch [410/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000870577\n",
      "Epoch [411/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000869062\n",
      "Epoch [411/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000867549\n",
      "Epoch [411/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000866038\n",
      "Epoch [411/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000864528\n",
      "Epoch [411/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000863019\n",
      "Epoch [411/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000861511\n",
      "Epoch [411/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000860005\n",
      "Epoch [411/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000858500\n",
      "Epoch [411/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000856997\n",
      "Epoch [411/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000855495\n",
      "Epoch [411/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000853994\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 411: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 307.46 MB\n",
      "Epoch [412/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000852495\n",
      "Epoch [412/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000850997\n",
      "Epoch [412/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000849500\n",
      "Epoch [412/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000848005\n",
      "Epoch [412/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000846511\n",
      "Epoch [412/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000845019\n",
      "Epoch [412/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000843528\n",
      "Epoch [412/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000842038\n",
      "Epoch [412/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000840550\n",
      "Epoch [412/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000839063\n",
      "Epoch [412/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000837577\n",
      "Epoch [413/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000836093\n",
      "Epoch [413/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000834610\n",
      "Epoch [413/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000833129\n",
      "Epoch [413/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000831649\n",
      "Epoch [413/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000830170\n",
      "Epoch [413/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000828693\n",
      "Epoch [413/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000827217\n",
      "Epoch [413/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000825742\n",
      "Epoch [413/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000824269\n",
      "Epoch [413/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000822797\n",
      "Epoch [413/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000821327\n",
      "Epoch [414/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000819858\n",
      "Epoch [414/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000818390\n",
      "Epoch [414/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000816924\n",
      "Epoch [414/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000815459\n",
      "Epoch [414/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000813995\n",
      "Epoch [414/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000812533\n",
      "Epoch [414/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000811072\n",
      "Epoch [414/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000809613\n",
      "Epoch [414/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000808155\n",
      "Epoch [414/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000806698\n",
      "Epoch [414/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000805243\n",
      "Epoch [415/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000803789\n",
      "Epoch [415/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000802337\n",
      "Epoch [415/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000800886\n",
      "Epoch [415/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000799436\n",
      "Epoch [415/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000797988\n",
      "Epoch [415/500], Batch [60/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0000796541\n",
      "Epoch [415/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000795095\n",
      "Epoch [415/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000793651\n",
      "Epoch [415/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000792208\n",
      "Epoch [415/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000790767\n",
      "Epoch [415/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000789327\n",
      "Epoch [416/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000787888\n",
      "Epoch [416/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000786451\n",
      "Epoch [416/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000785015\n",
      "Epoch [416/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000783581\n",
      "Epoch [416/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000782148\n",
      "Epoch [416/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000780716\n",
      "Epoch [416/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000779286\n",
      "Epoch [416/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000777857\n",
      "Epoch [416/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000776430\n",
      "Epoch [416/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000775004\n",
      "Epoch [416/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000773579\n",
      "Epoch [417/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000772156\n",
      "Epoch [417/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000770734\n",
      "Epoch [417/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000769313\n",
      "Epoch [417/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000767894\n",
      "Epoch [417/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000766477\n",
      "Epoch [417/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000765060\n",
      "Epoch [417/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000763646\n",
      "Epoch [417/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000762232\n",
      "Epoch [417/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000760820\n",
      "Epoch [417/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000759409\n",
      "Epoch [417/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000758000\n",
      "Epoch [418/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000756592\n",
      "Epoch [418/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000755186\n",
      "Epoch [418/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000753781\n",
      "Epoch [418/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000752377\n",
      "Epoch [418/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000750975\n",
      "Epoch [418/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000749574\n",
      "Epoch [418/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000748174\n",
      "Epoch [418/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000746776\n",
      "Epoch [418/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000745380\n",
      "Epoch [418/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000743984\n",
      "Epoch [418/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000742590\n",
      "Epoch [419/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000741198\n",
      "Epoch [419/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000739807\n",
      "Epoch [419/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000738417\n",
      "Epoch [419/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000737029\n",
      "Epoch [419/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000735642\n",
      "Epoch [419/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000734257\n",
      "Epoch [419/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000732873\n",
      "Epoch [419/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000731490\n",
      "Epoch [419/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000730109\n",
      "Epoch [419/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000728729\n",
      "Epoch [419/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000727351\n",
      "Epoch [420/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000725974\n",
      "Epoch [420/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000724598\n",
      "Epoch [420/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000723224\n",
      "Epoch [420/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000721851\n",
      "Epoch [420/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000720480\n",
      "Epoch [420/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000719110\n",
      "Epoch [420/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000717742\n",
      "Epoch [420/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000716375\n",
      "Epoch [420/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000715009\n",
      "Epoch [420/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000713645\n",
      "Epoch [420/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000712282\n",
      "Epoch [421/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000710921\n",
      "Epoch [421/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000709561\n",
      "Epoch [421/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000708202\n",
      "Epoch [421/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000706845\n",
      "Epoch [421/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000705489\n",
      "Epoch [421/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000704135\n",
      "Epoch [421/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000702782\n",
      "Epoch [421/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000701430\n",
      "Epoch [421/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000700080\n",
      "Epoch [421/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000698732\n",
      "Epoch [421/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000697384\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 421: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.038 sec , Memory Usage: 307.46 MB\n",
      "Epoch [422/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000696038\n",
      "Epoch [422/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000694694\n",
      "Epoch [422/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000693351\n",
      "Epoch [422/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000692010\n",
      "Epoch [422/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000690669\n",
      "Epoch [422/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000689331\n",
      "Epoch [422/500], Batch [70/110], Train Loss: 0.0644, Val Loss: 0.0001, LR: 0.0000687993\n",
      "Epoch [422/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000686657\n",
      "Epoch [422/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000685323\n",
      "Epoch [422/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000683990\n",
      "Epoch [422/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000682658\n",
      "Epoch [423/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000681328\n",
      "Epoch [423/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000680000\n",
      "Epoch [423/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000678672\n",
      "Epoch [423/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000677346\n",
      "Epoch [423/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000676022\n",
      "Epoch [423/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000674699\n",
      "Epoch [423/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000673377\n",
      "Epoch [423/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000672057\n",
      "Epoch [423/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000670738\n",
      "Epoch [423/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000669421\n",
      "Epoch [423/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0001, LR: 0.0000668105\n",
      "Epoch [424/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000666791\n",
      "Epoch [424/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000665478\n",
      "Epoch [424/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000664166\n",
      "Epoch [424/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000662856\n",
      "Epoch [424/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000661547\n",
      "Epoch [424/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000660240\n",
      "Epoch [424/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000658934\n",
      "Epoch [424/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000657629\n",
      "Epoch [424/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000656326\n",
      "Epoch [424/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000655025\n",
      "Epoch [424/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000653725\n",
      "Epoch [425/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000652426\n",
      "Epoch [425/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000651129\n",
      "Epoch [425/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000649833\n",
      "Epoch [425/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000648538\n",
      "Epoch [425/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000647245\n",
      "Epoch [425/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000645954\n",
      "Epoch [425/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000644664\n",
      "Epoch [425/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000643375\n",
      "Epoch [425/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000642088\n",
      "Epoch [425/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000640802\n",
      "Epoch [425/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000639518\n",
      "Epoch [426/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000638235\n",
      "Epoch [426/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000636953\n",
      "Epoch [426/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000635673\n",
      "Epoch [426/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000634395\n",
      "Epoch [426/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000633118\n",
      "Epoch [426/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000631842\n",
      "Epoch [426/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000630568\n",
      "Epoch [426/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000629295\n",
      "Epoch [426/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000628023\n",
      "Epoch [426/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000626753\n",
      "Epoch [426/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000625485\n",
      "Epoch [427/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000624218\n",
      "Epoch [427/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000622952\n",
      "Epoch [427/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000621688\n",
      "Epoch [427/500], Batch [40/110], Train Loss: 0.0026, Val Loss: 0.0001, LR: 0.0000620425\n",
      "Epoch [427/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000619164\n",
      "Epoch [427/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000617904\n",
      "Epoch [427/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000616646\n",
      "Epoch [427/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000615389\n",
      "Epoch [427/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000614134\n",
      "Epoch [427/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000612879\n",
      "Epoch [427/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000611627\n",
      "Epoch [428/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000610376\n",
      "Epoch [428/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000609126\n",
      "Epoch [428/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000607878\n",
      "Epoch [428/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000606631\n",
      "Epoch [428/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000605386\n",
      "Epoch [428/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0000604142\n",
      "Epoch [428/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000602899\n",
      "Epoch [428/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000601658\n",
      "Epoch [428/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000600419\n",
      "Epoch [428/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000599181\n",
      "Epoch [428/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000597944\n",
      "Epoch [429/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000596709\n",
      "Epoch [429/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000595475\n",
      "Epoch [429/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000594243\n",
      "Epoch [429/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000593012\n",
      "Epoch [429/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000591783\n",
      "Epoch [429/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000590555\n",
      "Epoch [429/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000589328\n",
      "Epoch [429/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000588103\n",
      "Epoch [429/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000586880\n",
      "Epoch [429/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000585658\n",
      "Epoch [429/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000584437\n",
      "Epoch [430/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000583218\n",
      "Epoch [430/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000582000\n",
      "Epoch [430/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000580784\n",
      "Epoch [430/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000579569\n",
      "Epoch [430/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000578356\n",
      "Epoch [430/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000577144\n",
      "Epoch [430/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000575933\n",
      "Epoch [430/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000574724\n",
      "Epoch [430/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0000573517\n",
      "Epoch [430/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000572311\n",
      "Epoch [430/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000571106\n",
      "Epoch [431/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000569903\n",
      "Epoch [431/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000568701\n",
      "Epoch [431/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000567501\n",
      "Epoch [431/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000566302\n",
      "Epoch [431/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000565105\n",
      "Epoch [431/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000563909\n",
      "Epoch [431/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000562715\n",
      "Epoch [431/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000561522\n",
      "Epoch [431/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000560331\n",
      "Epoch [431/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000559141\n",
      "Epoch [431/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000557952\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 431: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 307.46 MB\n",
      "Epoch [432/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000556765\n",
      "Epoch [432/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000555580\n",
      "Epoch [432/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000554395\n",
      "Epoch [432/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000553213\n",
      "Epoch [432/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000552032\n",
      "Epoch [432/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000550852\n",
      "Epoch [432/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000549674\n",
      "Epoch [432/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000548497\n",
      "Epoch [432/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000547322\n",
      "Epoch [432/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000546148\n",
      "Epoch [432/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000544975\n",
      "Epoch [433/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000543805\n",
      "Epoch [433/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000542635\n",
      "Epoch [433/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000541467\n",
      "Epoch [433/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000540301\n",
      "Epoch [433/500], Batch [50/110], Train Loss: 0.1681, Val Loss: 0.0001, LR: 0.0000539136\n",
      "Epoch [433/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000537972\n",
      "Epoch [433/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000536810\n",
      "Epoch [433/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000535650\n",
      "Epoch [433/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000534490\n",
      "Epoch [433/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000533333\n",
      "Epoch [433/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000532177\n",
      "Epoch [434/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000531022\n",
      "Epoch [434/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000529869\n",
      "Epoch [434/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000528717\n",
      "Epoch [434/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000527567\n",
      "Epoch [434/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000526418\n",
      "Epoch [434/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0000525271\n",
      "Epoch [434/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000524125\n",
      "Epoch [434/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000522980\n",
      "Epoch [434/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000521838\n",
      "Epoch [434/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000520696\n",
      "Epoch [434/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000519556\n",
      "Epoch [435/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0000, LR: 0.0000518418\n",
      "Epoch [435/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000517281\n",
      "Epoch [435/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000516145\n",
      "Epoch [435/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000515011\n",
      "Epoch [435/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000513879\n",
      "Epoch [435/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000512748\n",
      "Epoch [435/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000511618\n",
      "Epoch [435/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000510490\n",
      "Epoch [435/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000509363\n",
      "Epoch [435/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000508238\n",
      "Epoch [435/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000507115\n",
      "Epoch [436/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000505992\n",
      "Epoch [436/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000504872\n",
      "Epoch [436/500], Batch [30/110], Train Loss: 0.0034, Val Loss: 0.0001, LR: 0.0000503753\n",
      "Epoch [436/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000502635\n",
      "Epoch [436/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000501519\n",
      "Epoch [436/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000500404\n",
      "Epoch [436/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000499291\n",
      "Epoch [436/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000498179\n",
      "Epoch [436/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000497068\n",
      "Epoch [436/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000495960\n",
      "Epoch [436/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000494852\n",
      "Epoch [437/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000493747\n",
      "Epoch [437/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000492642\n",
      "Epoch [437/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000491539\n",
      "Epoch [437/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000490438\n",
      "Epoch [437/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000489338\n",
      "Epoch [437/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000488240\n",
      "Epoch [437/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000487143\n",
      "Epoch [437/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000486047\n",
      "Epoch [437/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000484953\n",
      "Epoch [437/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000483861\n",
      "Epoch [437/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000482770\n",
      "Epoch [438/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000481680\n",
      "Epoch [438/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000480592\n",
      "Epoch [438/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000479506\n",
      "Epoch [438/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000478421\n",
      "Epoch [438/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000477338\n",
      "Epoch [438/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000476255\n",
      "Epoch [438/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000475175\n",
      "Epoch [438/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000474096\n",
      "Epoch [438/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000473018\n",
      "Epoch [438/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000471942\n",
      "Epoch [438/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000470868\n",
      "Epoch [439/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000469795\n",
      "Epoch [439/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000468723\n",
      "Epoch [439/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000467653\n",
      "Epoch [439/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000466585\n",
      "Epoch [439/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000465518\n",
      "Epoch [439/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000464452\n",
      "Epoch [439/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000463388\n",
      "Epoch [439/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000462325\n",
      "Epoch [439/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000461264\n",
      "Epoch [439/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000460205\n",
      "Epoch [439/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000459146\n",
      "Epoch [440/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000458090\n",
      "Epoch [440/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000457035\n",
      "Epoch [440/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000455981\n",
      "Epoch [440/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000454929\n",
      "Epoch [440/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000453878\n",
      "Epoch [440/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000452829\n",
      "Epoch [440/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000451782\n",
      "Epoch [440/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000450736\n",
      "Epoch [440/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000449691\n",
      "Epoch [440/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000448648\n",
      "Epoch [440/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000447606\n",
      "Epoch [441/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000446566\n",
      "Epoch [441/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000445528\n",
      "Epoch [441/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000444491\n",
      "Epoch [441/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000443455\n",
      "Epoch [441/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000442421\n",
      "Epoch [441/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000441388\n",
      "Epoch [441/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000440357\n",
      "Epoch [441/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000439328\n",
      "Epoch [441/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000438300\n",
      "Epoch [441/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000437273\n",
      "Epoch [441/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000436248\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 441: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.042 sec , Memory Usage: 307.47 MB\n",
      "Epoch [442/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000435224\n",
      "Epoch [442/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000434202\n",
      "Epoch [442/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000433182\n",
      "Epoch [442/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000432163\n",
      "Epoch [442/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000431145\n",
      "Epoch [442/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000430129\n",
      "Epoch [442/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000429115\n",
      "Epoch [442/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000428102\n",
      "Epoch [442/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000427090\n",
      "Epoch [442/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000426080\n",
      "Epoch [442/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000425072\n",
      "Epoch [443/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000424065\n",
      "Epoch [443/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000423059\n",
      "Epoch [443/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000422055\n",
      "Epoch [443/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000421053\n",
      "Epoch [443/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000420052\n",
      "Epoch [443/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000419053\n",
      "Epoch [443/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000418055\n",
      "Epoch [443/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000417058\n",
      "Epoch [443/500], Batch [90/110], Train Loss: 0.0370, Val Loss: 0.0000, LR: 0.0000416063\n",
      "Epoch [443/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000415070\n",
      "Epoch [443/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000414078\n",
      "Epoch [444/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000413088\n",
      "Epoch [444/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000412099\n",
      "Epoch [444/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000411112\n",
      "Epoch [444/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000410126\n",
      "Epoch [444/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0000, LR: 0.0000409141\n",
      "Epoch [444/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000408159\n",
      "Epoch [444/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000407177\n",
      "Epoch [444/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000406198\n",
      "Epoch [444/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000405219\n",
      "Epoch [444/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000404243\n",
      "Epoch [444/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000403267\n",
      "Epoch [445/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000402294\n",
      "Epoch [445/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000401322\n",
      "Epoch [445/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000400351\n",
      "Epoch [445/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000399382\n",
      "Epoch [445/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000398414\n",
      "Epoch [445/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000397448\n",
      "Epoch [445/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000396483\n",
      "Epoch [445/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000395520\n",
      "Epoch [445/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000394559\n",
      "Epoch [445/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000393599\n",
      "Epoch [445/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000392640\n",
      "Epoch [446/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000391683\n",
      "Epoch [446/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000390728\n",
      "Epoch [446/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000389774\n",
      "Epoch [446/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000388821\n",
      "Epoch [446/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000387870\n",
      "Epoch [446/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000386921\n",
      "Epoch [446/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000385973\n",
      "Epoch [446/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000385027\n",
      "Epoch [446/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000384082\n",
      "Epoch [446/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000383139\n",
      "Epoch [446/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000382197\n",
      "Epoch [447/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000381257\n",
      "Epoch [447/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000380318\n",
      "Epoch [447/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000379381\n",
      "Epoch [447/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000378445\n",
      "Epoch [447/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000377511\n",
      "Epoch [447/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000376578\n",
      "Epoch [447/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000375647\n",
      "Epoch [447/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000374717\n",
      "Epoch [447/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000373789\n",
      "Epoch [447/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000372863\n",
      "Epoch [447/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000371938\n",
      "Epoch [448/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000371014\n",
      "Epoch [448/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000370092\n",
      "Epoch [448/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000369172\n",
      "Epoch [448/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000368253\n",
      "Epoch [448/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000367336\n",
      "Epoch [448/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000366420\n",
      "Epoch [448/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000365505\n",
      "Epoch [448/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000364593\n",
      "Epoch [448/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000363681\n",
      "Epoch [448/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000362772\n",
      "Epoch [448/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000361863\n",
      "Epoch [449/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000360957\n",
      "Epoch [449/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000360052\n",
      "Epoch [449/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000359148\n",
      "Epoch [449/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000358246\n",
      "Epoch [449/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000357345\n",
      "Epoch [449/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000356446\n",
      "Epoch [449/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000355549\n",
      "Epoch [449/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000354653\n",
      "Epoch [449/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000353758\n",
      "Epoch [449/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000352865\n",
      "Epoch [449/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000351974\n",
      "Epoch [450/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000351084\n",
      "Epoch [450/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000350196\n",
      "Epoch [450/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000349309\n",
      "Epoch [450/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0001, LR: 0.0000348424\n",
      "Epoch [450/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000347540\n",
      "Epoch [450/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000346658\n",
      "Epoch [450/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000345777\n",
      "Epoch [450/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000344898\n",
      "Epoch [450/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000344021\n",
      "Epoch [450/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000343145\n",
      "Epoch [450/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000342270\n",
      "Epoch [451/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000341397\n",
      "Epoch [451/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000340526\n",
      "Epoch [451/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000339656\n",
      "Epoch [451/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0000, LR: 0.0000338788\n",
      "Epoch [451/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000337921\n",
      "Epoch [451/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000337056\n",
      "Epoch [451/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000336192\n",
      "Epoch [451/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000335330\n",
      "Epoch [451/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000334469\n",
      "Epoch [451/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000333610\n",
      "Epoch [451/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000332752\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 451: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.042 sec , Memory Usage: 307.47 MB\n",
      "Epoch [452/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000331896\n",
      "Epoch [452/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000331042\n",
      "Epoch [452/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000330189\n",
      "Epoch [452/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000329337\n",
      "Epoch [452/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000328487\n",
      "Epoch [452/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000327639\n",
      "Epoch [452/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000326792\n",
      "Epoch [452/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000325947\n",
      "Epoch [452/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000325103\n",
      "Epoch [452/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000324261\n",
      "Epoch [452/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000323421\n",
      "Epoch [453/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000322581\n",
      "Epoch [453/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000321744\n",
      "Epoch [453/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000320908\n",
      "Epoch [453/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000320073\n",
      "Epoch [453/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000319240\n",
      "Epoch [453/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000318409\n",
      "Epoch [453/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000317579\n",
      "Epoch [453/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0000, LR: 0.0000316751\n",
      "Epoch [453/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000315924\n",
      "Epoch [453/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000315099\n",
      "Epoch [453/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0000, LR: 0.0000314275\n",
      "Epoch [454/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000313453\n",
      "Epoch [454/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0000, LR: 0.0000312633\n",
      "Epoch [454/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000311814\n",
      "Epoch [454/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000310996\n",
      "Epoch [454/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000310180\n",
      "Epoch [454/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000309366\n",
      "Epoch [454/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000308553\n",
      "Epoch [454/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000307742\n",
      "Epoch [454/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000306932\n",
      "Epoch [454/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000306124\n",
      "Epoch [454/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000305317\n",
      "Epoch [455/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000304512\n",
      "Epoch [455/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000303708\n",
      "Epoch [455/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000302906\n",
      "Epoch [455/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000302106\n",
      "Epoch [455/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000301307\n",
      "Epoch [455/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000300510\n",
      "Epoch [455/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000299714\n",
      "Epoch [455/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000298920\n",
      "Epoch [455/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0000, LR: 0.0000298127\n",
      "Epoch [455/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0000, LR: 0.0000297336\n",
      "Epoch [455/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000296546\n",
      "Epoch [456/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000295758\n",
      "Epoch [456/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000294972\n",
      "Epoch [456/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000294187\n",
      "Epoch [456/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000293403\n",
      "Epoch [456/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0000292622\n",
      "Epoch [456/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000291841\n",
      "Epoch [456/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000291062\n",
      "Epoch [456/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000290285\n",
      "Epoch [456/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000289510\n",
      "Epoch [456/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000288736\n",
      "Epoch [456/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000287963\n",
      "Epoch [457/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000287192\n",
      "Epoch [457/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000286423\n",
      "Epoch [457/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000285655\n",
      "Epoch [457/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000284888\n",
      "Epoch [457/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000284124\n",
      "Epoch [457/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000283360\n",
      "Epoch [457/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000282599\n",
      "Epoch [457/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000281839\n",
      "Epoch [457/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0000, LR: 0.0000281080\n",
      "Epoch [457/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000280323\n",
      "Epoch [457/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0000, LR: 0.0000279568\n",
      "Epoch [458/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000278814\n",
      "Epoch [458/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000278062\n",
      "Epoch [458/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000277311\n",
      "Epoch [458/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000276562\n",
      "Epoch [458/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000275814\n",
      "Epoch [458/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000275068\n",
      "Epoch [458/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000274323\n",
      "Epoch [458/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000273580\n",
      "Epoch [458/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000272839\n",
      "Epoch [458/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000272099\n",
      "Epoch [458/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000271361\n",
      "Epoch [459/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000270624\n",
      "Epoch [459/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000269889\n",
      "Epoch [459/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000269155\n",
      "Epoch [459/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000268423\n",
      "Epoch [459/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000267693\n",
      "Epoch [459/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000266964\n",
      "Epoch [459/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000266237\n",
      "Epoch [459/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000265511\n",
      "Epoch [459/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000264786\n",
      "Epoch [459/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000264064\n",
      "Epoch [459/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000263343\n",
      "Epoch [460/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000262623\n",
      "Epoch [460/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000261905\n",
      "Epoch [460/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000261189\n",
      "Epoch [460/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000260474\n",
      "Epoch [460/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000259760\n",
      "Epoch [460/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000259049\n",
      "Epoch [460/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000258338\n",
      "Epoch [460/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000257630\n",
      "Epoch [460/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000256923\n",
      "Epoch [460/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000256217\n",
      "Epoch [460/500], Batch [110/110], Train Loss: 0.1349, Val Loss: 0.0001, LR: 0.0000255513\n",
      "Epoch [461/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000254811\n",
      "Epoch [461/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000254110\n",
      "Epoch [461/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000253411\n",
      "Epoch [461/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000252713\n",
      "Epoch [461/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000252017\n",
      "Epoch [461/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000251323\n",
      "Epoch [461/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000250630\n",
      "Epoch [461/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000249938\n",
      "Epoch [461/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000249248\n",
      "Epoch [461/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0000248560\n",
      "Epoch [461/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000247873\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 461: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.039 sec , Memory Usage: 307.44 MB\n",
      "Epoch [462/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000247188\n",
      "Epoch [462/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000246505\n",
      "Epoch [462/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000245823\n",
      "Epoch [462/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000245142\n",
      "Epoch [462/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000244463\n",
      "Epoch [462/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000243786\n",
      "Epoch [462/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000243110\n",
      "Epoch [462/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000242436\n",
      "Epoch [462/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000241763\n",
      "Epoch [462/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000241092\n",
      "Epoch [462/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000240423\n",
      "Epoch [463/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000239755\n",
      "Epoch [463/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000239089\n",
      "Epoch [463/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000238424\n",
      "Epoch [463/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000237761\n",
      "Epoch [463/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000237099\n",
      "Epoch [463/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000236439\n",
      "Epoch [463/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000235781\n",
      "Epoch [463/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000235124\n",
      "Epoch [463/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000234468\n",
      "Epoch [463/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000233815\n",
      "Epoch [463/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000233162\n",
      "Epoch [464/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000232512\n",
      "Epoch [464/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000231863\n",
      "Epoch [464/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000231215\n",
      "Epoch [464/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000230569\n",
      "Epoch [464/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000229925\n",
      "Epoch [464/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000229282\n",
      "Epoch [464/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000228641\n",
      "Epoch [464/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000228001\n",
      "Epoch [464/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000227363\n",
      "Epoch [464/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0000226727\n",
      "Epoch [464/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000226092\n",
      "Epoch [465/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000225459\n",
      "Epoch [465/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000224827\n",
      "Epoch [465/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000224197\n",
      "Epoch [465/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000223568\n",
      "Epoch [465/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000222941\n",
      "Epoch [465/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000222316\n",
      "Epoch [465/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000221692\n",
      "Epoch [465/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000221069\n",
      "Epoch [465/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000220449\n",
      "Epoch [465/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000219830\n",
      "Epoch [465/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000219212\n",
      "Epoch [466/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000218596\n",
      "Epoch [466/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000217982\n",
      "Epoch [466/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000217369\n",
      "Epoch [466/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000216757\n",
      "Epoch [466/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0000, LR: 0.0000216148\n",
      "Epoch [466/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000215540\n",
      "Epoch [466/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000214933\n",
      "Epoch [466/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000214328\n",
      "Epoch [466/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0000, LR: 0.0000213725\n",
      "Epoch [466/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000213123\n",
      "Epoch [466/500], Batch [110/110], Train Loss: 0.1398, Val Loss: 0.0001, LR: 0.0000212523\n",
      "Epoch [467/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000211924\n",
      "Epoch [467/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000211327\n",
      "Epoch [467/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000210732\n",
      "Epoch [467/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000210138\n",
      "Epoch [467/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000209545\n",
      "Epoch [467/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000208955\n",
      "Epoch [467/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000208365\n",
      "Epoch [467/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000207778\n",
      "Epoch [467/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000207192\n",
      "Epoch [467/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000206607\n",
      "Epoch [467/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000206025\n",
      "Epoch [468/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000205443\n",
      "Epoch [468/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000204864\n",
      "Epoch [468/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000204285\n",
      "Epoch [468/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000203709\n",
      "Epoch [468/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000203134\n",
      "Epoch [468/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000202561\n",
      "Epoch [468/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000201989\n",
      "Epoch [468/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000201419\n",
      "Epoch [468/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000200850\n",
      "Epoch [468/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000200283\n",
      "Epoch [468/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000199717\n",
      "Epoch [469/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000199154\n",
      "Epoch [469/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000198591\n",
      "Epoch [469/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000198031\n",
      "Epoch [469/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0000, LR: 0.0000197471\n",
      "Epoch [469/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0000, LR: 0.0000196914\n",
      "Epoch [469/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000196358\n",
      "Epoch [469/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000195804\n",
      "Epoch [469/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000195251\n",
      "Epoch [469/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000194700\n",
      "Epoch [469/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000194150\n",
      "Epoch [469/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000193602\n",
      "Epoch [470/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000193055\n",
      "Epoch [470/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000192511\n",
      "Epoch [470/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000191967\n",
      "Epoch [470/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000191426\n",
      "Epoch [470/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000190886\n",
      "Epoch [470/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000190347\n",
      "Epoch [470/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000189810\n",
      "Epoch [470/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000189275\n",
      "Epoch [470/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000188741\n",
      "Epoch [470/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000188209\n",
      "Epoch [470/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000187678\n",
      "Epoch [471/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000187149\n",
      "Epoch [471/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000186622\n",
      "Epoch [471/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000186096\n",
      "Epoch [471/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000185572\n",
      "Epoch [471/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000185049\n",
      "Epoch [471/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000184528\n",
      "Epoch [471/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000184008\n",
      "Epoch [471/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000183490\n",
      "Epoch [471/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000182974\n",
      "Epoch [471/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000182459\n",
      "Epoch [471/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000181946\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 471: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.036 sec , Memory Usage: 307.44 MB\n",
      "Epoch [472/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000181435\n",
      "Epoch [472/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0000, LR: 0.0000180925\n",
      "Epoch [472/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000180416\n",
      "Epoch [472/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000179910\n",
      "Epoch [472/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000179404\n",
      "Epoch [472/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000178901\n",
      "Epoch [472/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000178399\n",
      "Epoch [472/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000177898\n",
      "Epoch [472/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000177399\n",
      "Epoch [472/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000176902\n",
      "Epoch [472/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000176407\n",
      "Epoch [473/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000175912\n",
      "Epoch [473/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000175420\n",
      "Epoch [473/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000174929\n",
      "Epoch [473/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000174440\n",
      "Epoch [473/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000173952\n",
      "Epoch [473/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000173466\n",
      "Epoch [473/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000172981\n",
      "Epoch [473/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000172499\n",
      "Epoch [473/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000172017\n",
      "Epoch [473/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000171537\n",
      "Epoch [473/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000171059\n",
      "Epoch [474/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000170583\n",
      "Epoch [474/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000170108\n",
      "Epoch [474/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000169634\n",
      "Epoch [474/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000169163\n",
      "Epoch [474/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000168692\n",
      "Epoch [474/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000168224\n",
      "Epoch [474/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000167757\n",
      "Epoch [474/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000167291\n",
      "Epoch [474/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000166827\n",
      "Epoch [474/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000166365\n",
      "Epoch [474/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000165905\n",
      "Epoch [475/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000165446\n",
      "Epoch [475/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000164988\n",
      "Epoch [475/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000164532\n",
      "Epoch [475/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000164078\n",
      "Epoch [475/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000163625\n",
      "Epoch [475/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000163174\n",
      "Epoch [475/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0000, LR: 0.0000162725\n",
      "Epoch [475/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000162277\n",
      "Epoch [475/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000161831\n",
      "Epoch [475/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000161386\n",
      "Epoch [475/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000160943\n",
      "Epoch [476/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000160501\n",
      "Epoch [476/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000160061\n",
      "Epoch [476/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000159623\n",
      "Epoch [476/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000159186\n",
      "Epoch [476/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000158751\n",
      "Epoch [476/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000158318\n",
      "Epoch [476/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000157886\n",
      "Epoch [476/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000157455\n",
      "Epoch [476/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000157027\n",
      "Epoch [476/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000156599\n",
      "Epoch [476/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000156174\n",
      "Epoch [477/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000155750\n",
      "Epoch [477/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000155328\n",
      "Epoch [477/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000154907\n",
      "Epoch [477/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000154488\n",
      "Epoch [477/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000154070\n",
      "Epoch [477/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000153654\n",
      "Epoch [477/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000153240\n",
      "Epoch [477/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000152827\n",
      "Epoch [477/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000152416\n",
      "Epoch [477/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000152006\n",
      "Epoch [477/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000151598\n",
      "Epoch [478/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0000, LR: 0.0000151192\n",
      "Epoch [478/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000150787\n",
      "Epoch [478/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000150384\n",
      "Epoch [478/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000149982\n",
      "Epoch [478/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000149582\n",
      "Epoch [478/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000149184\n",
      "Epoch [478/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000148787\n",
      "Epoch [478/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000148392\n",
      "Epoch [478/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000147998\n",
      "Epoch [478/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000147606\n",
      "Epoch [478/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000147216\n",
      "Epoch [479/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000146827\n",
      "Epoch [479/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000146440\n",
      "Epoch [479/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000146054\n",
      "Epoch [479/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000145670\n",
      "Epoch [479/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000145288\n",
      "Epoch [479/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000144907\n",
      "Epoch [479/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0000, LR: 0.0000144528\n",
      "Epoch [479/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000144150\n",
      "Epoch [479/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000143774\n",
      "Epoch [479/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000143400\n",
      "Epoch [479/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000143027\n",
      "Epoch [480/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000142656\n",
      "Epoch [480/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000142286\n",
      "Epoch [480/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000141918\n",
      "Epoch [480/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000141552\n",
      "Epoch [480/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000141187\n",
      "Epoch [480/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000140824\n",
      "Epoch [480/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000140463\n",
      "Epoch [480/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000140103\n",
      "Epoch [480/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000139744\n",
      "Epoch [480/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000139387\n",
      "Epoch [480/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000139032\n",
      "Epoch [481/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000138679\n",
      "Epoch [481/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000138327\n",
      "Epoch [481/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000137976\n",
      "Epoch [481/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000137628\n",
      "Epoch [481/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000137280\n",
      "Epoch [481/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000136935\n",
      "Epoch [481/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000136591\n",
      "Epoch [481/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000136249\n",
      "Epoch [481/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000135908\n",
      "Epoch [481/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000135569\n",
      "Epoch [481/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000135231\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 481: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.036 sec , Memory Usage: 307.45 MB\n",
      "Epoch [482/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000134895\n",
      "Epoch [482/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000134561\n",
      "Epoch [482/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000134228\n",
      "Epoch [482/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000133897\n",
      "Epoch [482/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0000, LR: 0.0000133567\n",
      "Epoch [482/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000133240\n",
      "Epoch [482/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000132913\n",
      "Epoch [482/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000132589\n",
      "Epoch [482/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000132265\n",
      "Epoch [482/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000131944\n",
      "Epoch [482/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000131624\n",
      "Epoch [483/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000131306\n",
      "Epoch [483/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000130989\n",
      "Epoch [483/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000130674\n",
      "Epoch [483/500], Batch [40/110], Train Loss: 0.1382, Val Loss: 0.0001, LR: 0.0000130360\n",
      "Epoch [483/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000130049\n",
      "Epoch [483/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000129738\n",
      "Epoch [483/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000129430\n",
      "Epoch [483/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000129123\n",
      "Epoch [483/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000128817\n",
      "Epoch [483/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000128513\n",
      "Epoch [483/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000128211\n",
      "Epoch [484/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000127910\n",
      "Epoch [484/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000127611\n",
      "Epoch [484/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000127314\n",
      "Epoch [484/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000127018\n",
      "Epoch [484/500], Batch [50/110], Train Loss: 0.0010, Val Loss: 0.0000, LR: 0.0000126724\n",
      "Epoch [484/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000126431\n",
      "Epoch [484/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0000, LR: 0.0000126140\n",
      "Epoch [484/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000125851\n",
      "Epoch [484/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000125563\n",
      "Epoch [484/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000125277\n",
      "Epoch [484/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000124992\n",
      "Epoch [485/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000124710\n",
      "Epoch [485/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000124428\n",
      "Epoch [485/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000124148\n",
      "Epoch [485/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000123870\n",
      "Epoch [485/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000123594\n",
      "Epoch [485/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000123319\n",
      "Epoch [485/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000123045\n",
      "Epoch [485/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000122774\n",
      "Epoch [485/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000122504\n",
      "Epoch [485/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000122235\n",
      "Epoch [485/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000121968\n",
      "Epoch [486/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000121703\n",
      "Epoch [486/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000121439\n",
      "Epoch [486/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000121177\n",
      "Epoch [486/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000120917\n",
      "Epoch [486/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000120658\n",
      "Epoch [486/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000120401\n",
      "Epoch [486/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0000, LR: 0.0000120145\n",
      "Epoch [486/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000119891\n",
      "Epoch [486/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000119639\n",
      "Epoch [486/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000119388\n",
      "Epoch [486/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000119139\n",
      "Epoch [487/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000118891\n",
      "Epoch [487/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000118645\n",
      "Epoch [487/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000118401\n",
      "Epoch [487/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000118158\n",
      "Epoch [487/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000117917\n",
      "Epoch [487/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000117677\n",
      "Epoch [487/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000117439\n",
      "Epoch [487/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000117203\n",
      "Epoch [487/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000116968\n",
      "Epoch [487/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000116735\n",
      "Epoch [487/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000116504\n",
      "Epoch [488/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000116274\n",
      "Epoch [488/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000116045\n",
      "Epoch [488/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000115819\n",
      "Epoch [488/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000115594\n",
      "Epoch [488/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000115370\n",
      "Epoch [488/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000115148\n",
      "Epoch [488/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000114928\n",
      "Epoch [488/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000114710\n",
      "Epoch [488/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000114493\n",
      "Epoch [488/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000114277\n",
      "Epoch [488/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000114063\n",
      "Epoch [489/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000113851\n",
      "Epoch [489/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000113641\n",
      "Epoch [489/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000113432\n",
      "Epoch [489/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000113224\n",
      "Epoch [489/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000113019\n",
      "Epoch [489/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000112815\n",
      "Epoch [489/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000112612\n",
      "Epoch [489/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000112411\n",
      "Epoch [489/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000112212\n",
      "Epoch [489/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000112014\n",
      "Epoch [489/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000111818\n",
      "Epoch [490/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000111624\n",
      "Epoch [490/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000111431\n",
      "Epoch [490/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000111240\n",
      "Epoch [490/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000111050\n",
      "Epoch [490/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000110862\n",
      "Epoch [490/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000110676\n",
      "Epoch [490/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000110491\n",
      "Epoch [490/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000110308\n",
      "Epoch [490/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000110126\n",
      "Epoch [490/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000109946\n",
      "Epoch [490/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000109768\n",
      "Epoch [491/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000109591\n",
      "Epoch [491/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000109416\n",
      "Epoch [491/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000109242\n",
      "Epoch [491/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000109070\n",
      "Epoch [491/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000108900\n",
      "Epoch [491/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000108731\n",
      "Epoch [491/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000108564\n",
      "Epoch [491/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000108399\n",
      "Epoch [491/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000108235\n",
      "Epoch [491/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000108073\n",
      "Epoch [491/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000107912\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 491: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.036 sec , Memory Usage: 307.46 MB\n",
      "Epoch [492/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000107753\n",
      "Epoch [492/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000107596\n",
      "Epoch [492/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000107440\n",
      "Epoch [492/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000107286\n",
      "Epoch [492/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000107133\n",
      "Epoch [492/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000106983\n",
      "Epoch [492/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000106833\n",
      "Epoch [492/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000106686\n",
      "Epoch [492/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000106539\n",
      "Epoch [492/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000106395\n",
      "Epoch [492/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000106252\n",
      "Epoch [493/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000106111\n",
      "Epoch [493/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105971\n",
      "Epoch [493/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105833\n",
      "Epoch [493/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105697\n",
      "Epoch [493/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105562\n",
      "Epoch [493/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105429\n",
      "Epoch [493/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105297\n",
      "Epoch [493/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105167\n",
      "Epoch [493/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105039\n",
      "Epoch [493/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000104912\n",
      "Epoch [493/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0000, LR: 0.0000104787\n",
      "Epoch [494/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000104663\n",
      "Epoch [494/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000104542\n",
      "Epoch [494/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000104421\n",
      "Epoch [494/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000104303\n",
      "Epoch [494/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000104186\n",
      "Epoch [494/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000104070\n",
      "Epoch [494/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103956\n",
      "Epoch [494/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103844\n",
      "Epoch [494/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103733\n",
      "Epoch [494/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103624\n",
      "Epoch [494/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103517\n",
      "Epoch [495/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000103411\n",
      "Epoch [495/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000103307\n",
      "Epoch [495/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103205\n",
      "Epoch [495/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103104\n",
      "Epoch [495/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103004\n",
      "Epoch [495/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102907\n",
      "Epoch [495/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102811\n",
      "Epoch [495/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102716\n",
      "Epoch [495/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102623\n",
      "Epoch [495/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102532\n",
      "Epoch [495/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102443\n",
      "Epoch [496/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000102355\n",
      "Epoch [496/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000102268\n",
      "Epoch [496/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000102183\n",
      "Epoch [496/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000102100\n",
      "Epoch [496/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000102019\n",
      "Epoch [496/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000101939\n",
      "Epoch [496/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000101860\n",
      "Epoch [496/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000101784\n",
      "Epoch [496/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0000, LR: 0.0000101709\n",
      "Epoch [496/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000101635\n",
      "Epoch [496/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000101563\n",
      "Epoch [497/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000101493\n",
      "Epoch [497/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000101424\n",
      "Epoch [497/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000101357\n",
      "Epoch [497/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000101292\n",
      "Epoch [497/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000101228\n",
      "Epoch [497/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0000101166\n",
      "Epoch [497/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000101105\n",
      "Epoch [497/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000101047\n",
      "Epoch [497/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000100989\n",
      "Epoch [497/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100933\n",
      "Epoch [497/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000100879\n",
      "Epoch [498/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100827\n",
      "Epoch [498/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100776\n",
      "Epoch [498/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100727\n",
      "Epoch [498/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100679\n",
      "Epoch [498/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100633\n",
      "Epoch [498/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000100589\n",
      "Epoch [498/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100546\n",
      "Epoch [498/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100505\n",
      "Epoch [498/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100465\n",
      "Epoch [498/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100427\n",
      "Epoch [498/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100391\n",
      "Epoch [499/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100356\n",
      "Epoch [499/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0000, LR: 0.0000100323\n",
      "Epoch [499/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0000, LR: 0.0000100292\n",
      "Epoch [499/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100262\n",
      "Epoch [499/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100233\n",
      "Epoch [499/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100207\n",
      "Epoch [499/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100182\n",
      "Epoch [499/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100158\n",
      "Epoch [499/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100136\n",
      "Epoch [499/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0000, LR: 0.0000100116\n",
      "Epoch [499/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0000, LR: 0.0000100098\n",
      "Epoch [500/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100081\n",
      "Epoch [500/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100065\n",
      "Epoch [500/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100052\n",
      "Epoch [500/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100040\n",
      "Epoch [500/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100029\n",
      "Epoch [500/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100020\n",
      "Epoch [500/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100013\n",
      "Epoch [500/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100007\n",
      "Epoch [500/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100003\n",
      "Epoch [500/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100001\n",
      "Epoch [500/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0000, LR: 0.0000100000\n",
      "Confusion Matrix:\n",
      "[[635   0]\n",
      " [  0 865]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       635\n",
      "           1    1.00000   1.00000   1.00000       865\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 309.64 MB\n",
      "=========== SEED: 1234 , FOLD: 3/4, D: cpu ===========\n",
      " Label\n",
      "1    5722\n",
      "0    4278\n",
      "Name: count, dtype: int64\n",
      "=========== TP: 2,879 ===========\n",
      "Epoch [1/500], Batch [10/110], Train Loss: 0.6780, Val Loss: 0.6833, LR: 0.0009999999\n",
      "Epoch [1/500], Batch [20/110], Train Loss: 0.7133, Val Loss: 0.6688, LR: 0.0009999997\n",
      "Epoch [1/500], Batch [30/110], Train Loss: 0.6611, Val Loss: 0.6563, LR: 0.0009999993\n",
      "Epoch [1/500], Batch [40/110], Train Loss: 0.6481, Val Loss: 0.6341, LR: 0.0009999987\n",
      "Epoch [1/500], Batch [50/110], Train Loss: 0.5125, Val Loss: 0.5954, LR: 0.0009999980\n",
      "Epoch [1/500], Batch [60/110], Train Loss: 0.4156, Val Loss: 0.5494, LR: 0.0009999971\n",
      "Epoch [1/500], Batch [70/110], Train Loss: 0.4539, Val Loss: 0.5114, LR: 0.0009999960\n",
      "Epoch [1/500], Batch [80/110], Train Loss: 0.3904, Val Loss: 0.4382, LR: 0.0009999948\n",
      "Epoch [1/500], Batch [90/110], Train Loss: 0.2139, Val Loss: 0.3601, LR: 0.0009999935\n",
      "Epoch [1/500], Batch [100/110], Train Loss: 0.1913, Val Loss: 0.2636, LR: 0.0009999919\n",
      "Epoch [1/500], Batch [110/110], Train Loss: 0.2008, Val Loss: 0.1941, LR: 0.0009999902\n",
      "Confusion Matrix:\n",
      "[[615  37]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.94325   0.97080       652\n",
      "           1    0.95819   1.00000   0.97865       848\n",
      "\n",
      "    accuracy                        0.97533      1500\n",
      "   macro avg    0.97910   0.97163   0.97472      1500\n",
      "weighted avg    0.97636   0.97533   0.97524      1500\n",
      "\n",
      "Total Errors: 37\n",
      "Index: 7, Predicted: 1, Actual: 0\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 170, Predicted: 1, Actual: 0\n",
      "Index: 207, Predicted: 1, Actual: 0\n",
      "Index: 258, Predicted: 1, Actual: 0\n",
      "Epoch 1: OK- Accuracy: 0.97533, Precision: 0.95819, Recall: 1.00000, F1: 0.97865, ROC AUC: 0.97163, AUPR (PR-AUC): 0.95819, Sensitivity: 1.00000, Specificity: 0.94325, Far: 0.05674846625766871, False Positive Rate (FPR): 0.05675, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 196.46 MB\n",
      "Epoch [2/500], Batch [10/110], Train Loss: 0.1440, Val Loss: 0.1478, LR: 0.0009999884\n",
      "Epoch [2/500], Batch [20/110], Train Loss: 0.0949, Val Loss: 0.1171, LR: 0.0009999864\n",
      "Epoch [2/500], Batch [30/110], Train Loss: 0.1690, Val Loss: 0.1042, LR: 0.0009999842\n",
      "Epoch [2/500], Batch [40/110], Train Loss: 0.0581, Val Loss: 0.0815, LR: 0.0009999818\n",
      "Epoch [2/500], Batch [50/110], Train Loss: 0.1621, Val Loss: 0.0906, LR: 0.0009999793\n",
      "Epoch [2/500], Batch [60/110], Train Loss: 0.0329, Val Loss: 0.0715, LR: 0.0009999767\n",
      "Epoch [2/500], Batch [70/110], Train Loss: 0.0693, Val Loss: 0.0711, LR: 0.0009999738\n",
      "Epoch [2/500], Batch [80/110], Train Loss: 0.0604, Val Loss: 0.0655, LR: 0.0009999708\n",
      "Epoch [2/500], Batch [90/110], Train Loss: 0.0185, Val Loss: 0.0664, LR: 0.0009999677\n",
      "Epoch [2/500], Batch [100/110], Train Loss: 0.2155, Val Loss: 0.0624, LR: 0.0009999644\n",
      "Epoch [2/500], Batch [110/110], Train Loss: 0.1265, Val Loss: 0.0710, LR: 0.0009999609\n",
      "Epoch [3/500], Batch [10/110], Train Loss: 0.0392, Val Loss: 0.0587, LR: 0.0009999573\n",
      "Epoch [3/500], Batch [20/110], Train Loss: 0.0149, Val Loss: 0.0644, LR: 0.0009999535\n",
      "Epoch [3/500], Batch [30/110], Train Loss: 0.0248, Val Loss: 0.0563, LR: 0.0009999495\n",
      "Epoch [3/500], Batch [40/110], Train Loss: 0.0083, Val Loss: 0.0812, LR: 0.0009999454\n",
      "Epoch [3/500], Batch [50/110], Train Loss: 0.0135, Val Loss: 0.0564, LR: 0.0009999411\n",
      "Epoch [3/500], Batch [60/110], Train Loss: 0.0132, Val Loss: 0.0594, LR: 0.0009999367\n",
      "Epoch [3/500], Batch [70/110], Train Loss: 0.0188, Val Loss: 0.0538, LR: 0.0009999321\n",
      "Epoch [3/500], Batch [80/110], Train Loss: 0.0102, Val Loss: 0.0537, LR: 0.0009999273\n",
      "Epoch [3/500], Batch [90/110], Train Loss: 0.0074, Val Loss: 0.0881, LR: 0.0009999224\n",
      "Epoch [3/500], Batch [100/110], Train Loss: 0.0298, Val Loss: 0.0518, LR: 0.0009999173\n",
      "Epoch [3/500], Batch [110/110], Train Loss: 0.0088, Val Loss: 0.0554, LR: 0.0009999121\n",
      "Epoch [4/500], Batch [10/110], Train Loss: 0.0304, Val Loss: 0.0501, LR: 0.0009999067\n",
      "Epoch [4/500], Batch [20/110], Train Loss: 0.0268, Val Loss: 0.0570, LR: 0.0009999011\n",
      "Epoch [4/500], Batch [30/110], Train Loss: 0.0142, Val Loss: 0.0475, LR: 0.0009998953\n",
      "Epoch [4/500], Batch [40/110], Train Loss: 0.0320, Val Loss: 0.0538, LR: 0.0009998895\n",
      "Epoch [4/500], Batch [50/110], Train Loss: 0.0218, Val Loss: 0.0469, LR: 0.0009998834\n",
      "Epoch [4/500], Batch [60/110], Train Loss: 0.1757, Val Loss: 0.0630, LR: 0.0009998772\n",
      "Epoch [4/500], Batch [70/110], Train Loss: 0.0062, Val Loss: 0.0492, LR: 0.0009998708\n",
      "Epoch [4/500], Batch [80/110], Train Loss: 0.1245, Val Loss: 0.0482, LR: 0.0009998643\n",
      "Epoch [4/500], Batch [90/110], Train Loss: 0.0127, Val Loss: 0.0477, LR: 0.0009998576\n",
      "Epoch [4/500], Batch [100/110], Train Loss: 0.0096, Val Loss: 0.0524, LR: 0.0009998507\n",
      "Epoch [4/500], Batch [110/110], Train Loss: 0.0084, Val Loss: 0.0480, LR: 0.0009998437\n",
      "Epoch [5/500], Batch [10/110], Train Loss: 0.0933, Val Loss: 0.0466, LR: 0.0009998365\n",
      "Epoch [5/500], Batch [20/110], Train Loss: 0.0115, Val Loss: 0.0530, LR: 0.0009998291\n",
      "Epoch [5/500], Batch [30/110], Train Loss: 0.1455, Val Loss: 0.0537, LR: 0.0009998216\n",
      "Epoch [5/500], Batch [40/110], Train Loss: 0.0111, Val Loss: 0.0500, LR: 0.0009998140\n",
      "Epoch [5/500], Batch [50/110], Train Loss: 0.0061, Val Loss: 0.0565, LR: 0.0009998061\n",
      "Epoch [5/500], Batch [60/110], Train Loss: 0.0161, Val Loss: 0.0455, LR: 0.0009997981\n",
      "Epoch [5/500], Batch [70/110], Train Loss: 0.0095, Val Loss: 0.0452, LR: 0.0009997900\n",
      "Epoch [5/500], Batch [80/110], Train Loss: 0.0110, Val Loss: 0.0584, LR: 0.0009997817\n",
      "Epoch [5/500], Batch [90/110], Train Loss: 0.0239, Val Loss: 0.0462, LR: 0.0009997732\n",
      "Epoch [5/500], Batch [100/110], Train Loss: 0.0139, Val Loss: 0.0472, LR: 0.0009997645\n",
      "Epoch [5/500], Batch [110/110], Train Loss: 0.1254, Val Loss: 0.0442, LR: 0.0009997557\n",
      "Epoch [6/500], Batch [10/110], Train Loss: 0.1388, Val Loss: 0.0492, LR: 0.0009997468\n",
      "Epoch [6/500], Batch [20/110], Train Loss: 0.0812, Val Loss: 0.0441, LR: 0.0009997377\n",
      "Epoch [6/500], Batch [30/110], Train Loss: 0.0107, Val Loss: 0.0507, LR: 0.0009997284\n",
      "Epoch [6/500], Batch [40/110], Train Loss: 0.0078, Val Loss: 0.0475, LR: 0.0009997189\n",
      "Epoch [6/500], Batch [50/110], Train Loss: 0.0550, Val Loss: 0.0418, LR: 0.0009997093\n",
      "Epoch [6/500], Batch [60/110], Train Loss: 0.0082, Val Loss: 0.0420, LR: 0.0009996996\n",
      "Epoch [6/500], Batch [70/110], Train Loss: 0.0053, Val Loss: 0.0513, LR: 0.0009996896\n",
      "Epoch [6/500], Batch [80/110], Train Loss: 0.1381, Val Loss: 0.0459, LR: 0.0009996795\n",
      "Epoch [6/500], Batch [90/110], Train Loss: 0.2479, Val Loss: 0.0440, LR: 0.0009996693\n",
      "Epoch [6/500], Batch [100/110], Train Loss: 0.1436, Val Loss: 0.0412, LR: 0.0009996589\n",
      "Epoch [6/500], Batch [110/110], Train Loss: 0.1199, Val Loss: 0.0434, LR: 0.0009996483\n",
      "Epoch [7/500], Batch [10/110], Train Loss: 0.2609, Val Loss: 0.0432, LR: 0.0009996376\n",
      "Epoch [7/500], Batch [20/110], Train Loss: 0.2605, Val Loss: 0.0414, LR: 0.0009996267\n",
      "Epoch [7/500], Batch [30/110], Train Loss: 0.0087, Val Loss: 0.0483, LR: 0.0009996156\n",
      "Epoch [7/500], Batch [40/110], Train Loss: 0.0095, Val Loss: 0.0431, LR: 0.0009996044\n",
      "Epoch [7/500], Batch [50/110], Train Loss: 0.0057, Val Loss: 0.0440, LR: 0.0009995930\n",
      "Epoch [7/500], Batch [60/110], Train Loss: 0.0078, Val Loss: 0.0472, LR: 0.0009995814\n",
      "Epoch [7/500], Batch [70/110], Train Loss: 0.0093, Val Loss: 0.0402, LR: 0.0009995697\n",
      "Epoch [7/500], Batch [80/110], Train Loss: 0.0071, Val Loss: 0.0427, LR: 0.0009995579\n",
      "Epoch [7/500], Batch [90/110], Train Loss: 0.1125, Val Loss: 0.0406, LR: 0.0009995458\n",
      "Epoch [7/500], Batch [100/110], Train Loss: 0.0072, Val Loss: 0.0403, LR: 0.0009995337\n",
      "Epoch [7/500], Batch [110/110], Train Loss: 0.0050, Val Loss: 0.0439, LR: 0.0009995213\n",
      "Epoch [8/500], Batch [10/110], Train Loss: 0.0065, Val Loss: 0.0443, LR: 0.0009995088\n",
      "Epoch [8/500], Batch [20/110], Train Loss: 0.0042, Val Loss: 0.0465, LR: 0.0009994961\n",
      "Epoch [8/500], Batch [30/110], Train Loss: 0.0089, Val Loss: 0.0402, LR: 0.0009994833\n",
      "Epoch [8/500], Batch [40/110], Train Loss: 0.0054, Val Loss: 0.0456, LR: 0.0009994703\n",
      "Epoch [8/500], Batch [50/110], Train Loss: 0.0076, Val Loss: 0.0399, LR: 0.0009994571\n",
      "Epoch [8/500], Batch [60/110], Train Loss: 0.0206, Val Loss: 0.0378, LR: 0.0009994438\n",
      "Epoch [8/500], Batch [70/110], Train Loss: 0.0023, Val Loss: 0.0500, LR: 0.0009994303\n",
      "Epoch [8/500], Batch [80/110], Train Loss: 0.0942, Val Loss: 0.0417, LR: 0.0009994167\n",
      "Epoch [8/500], Batch [90/110], Train Loss: 0.0125, Val Loss: 0.0407, LR: 0.0009994029\n",
      "Epoch [8/500], Batch [100/110], Train Loss: 0.0287, Val Loss: 0.0442, LR: 0.0009993889\n",
      "Epoch [8/500], Batch [110/110], Train Loss: 0.1355, Val Loss: 0.0387, LR: 0.0009993748\n",
      "Epoch [9/500], Batch [10/110], Train Loss: 0.0040, Val Loss: 0.0445, LR: 0.0009993605\n",
      "Epoch [9/500], Batch [20/110], Train Loss: 0.0109, Val Loss: 0.0380, LR: 0.0009993461\n",
      "Epoch [9/500], Batch [30/110], Train Loss: 0.0028, Val Loss: 0.0426, LR: 0.0009993314\n",
      "Epoch [9/500], Batch [40/110], Train Loss: 0.0064, Val Loss: 0.0394, LR: 0.0009993167\n",
      "Epoch [9/500], Batch [50/110], Train Loss: 0.0082, Val Loss: 0.0386, LR: 0.0009993017\n",
      "Epoch [9/500], Batch [60/110], Train Loss: 0.0046, Val Loss: 0.0416, LR: 0.0009992867\n",
      "Epoch [9/500], Batch [70/110], Train Loss: 0.0137, Val Loss: 0.0367, LR: 0.0009992714\n",
      "Epoch [9/500], Batch [80/110], Train Loss: 0.0023, Val Loss: 0.0562, LR: 0.0009992560\n",
      "Epoch [9/500], Batch [90/110], Train Loss: 0.0047, Val Loss: 0.0516, LR: 0.0009992404\n",
      "Epoch [9/500], Batch [100/110], Train Loss: 0.0378, Val Loss: 0.0365, LR: 0.0009992247\n",
      "Epoch [9/500], Batch [110/110], Train Loss: 0.0072, Val Loss: 0.0430, LR: 0.0009992088\n",
      "Epoch [10/500], Batch [10/110], Train Loss: 0.1642, Val Loss: 0.0524, LR: 0.0009991927\n",
      "Epoch [10/500], Batch [20/110], Train Loss: 0.0080, Val Loss: 0.0412, LR: 0.0009991765\n",
      "Epoch [10/500], Batch [30/110], Train Loss: 0.1202, Val Loss: 0.0402, LR: 0.0009991601\n",
      "Epoch [10/500], Batch [40/110], Train Loss: 0.0122, Val Loss: 0.0363, LR: 0.0009991436\n",
      "Epoch [10/500], Batch [50/110], Train Loss: 0.1226, Val Loss: 0.0354, LR: 0.0009991269\n",
      "Epoch [10/500], Batch [60/110], Train Loss: 0.2567, Val Loss: 0.0405, LR: 0.0009991100\n",
      "Epoch [10/500], Batch [70/110], Train Loss: 0.0108, Val Loss: 0.0389, LR: 0.0009990930\n",
      "Epoch [10/500], Batch [80/110], Train Loss: 0.0073, Val Loss: 0.0411, LR: 0.0009990758\n",
      "Epoch [10/500], Batch [90/110], Train Loss: 0.0074, Val Loss: 0.0354, LR: 0.0009990584\n",
      "Epoch [10/500], Batch [100/110], Train Loss: 0.0036, Val Loss: 0.0419, LR: 0.0009990409\n",
      "Epoch [10/500], Batch [110/110], Train Loss: 0.0153, Val Loss: 0.0357, LR: 0.0009990232\n",
      "Epoch [11/500], Batch [10/110], Train Loss: 0.3105, Val Loss: 0.0407, LR: 0.0009990054\n",
      "Epoch [11/500], Batch [20/110], Train Loss: 0.1477, Val Loss: 0.0412, LR: 0.0009989874\n",
      "Epoch [11/500], Batch [30/110], Train Loss: 0.0101, Val Loss: 0.0358, LR: 0.0009989692\n",
      "Epoch [11/500], Batch [40/110], Train Loss: 0.1441, Val Loss: 0.0448, LR: 0.0009989509\n",
      "Epoch [11/500], Batch [50/110], Train Loss: 0.2733, Val Loss: 0.0374, LR: 0.0009989324\n",
      "Epoch [11/500], Batch [60/110], Train Loss: 0.0094, Val Loss: 0.0417, LR: 0.0009989138\n",
      "Epoch [11/500], Batch [70/110], Train Loss: 0.0039, Val Loss: 0.0489, LR: 0.0009988950\n",
      "Epoch [11/500], Batch [80/110], Train Loss: 0.0079, Val Loss: 0.0345, LR: 0.0009988760\n",
      "Epoch [11/500], Batch [90/110], Train Loss: 0.0093, Val Loss: 0.0402, LR: 0.0009988569\n",
      "Epoch [11/500], Batch [100/110], Train Loss: 0.0100, Val Loss: 0.0359, LR: 0.0009988376\n",
      "Epoch [11/500], Batch [110/110], Train Loss: 0.0955, Val Loss: 0.0354, LR: 0.0009988182\n",
      "Confusion Matrix:\n",
      "[[626  26]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.96012   0.97966       652\n",
      "           1    0.97025   1.00000   0.98490       848\n",
      "\n",
      "    accuracy                        0.98267      1500\n",
      "   macro avg    0.98513   0.98006   0.98228      1500\n",
      "weighted avg    0.98318   0.98267   0.98262      1500\n",
      "\n",
      "Total Errors: 26\n",
      "Index: 7, Predicted: 1, Actual: 0\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 170, Predicted: 1, Actual: 0\n",
      "Index: 258, Predicted: 1, Actual: 0\n",
      "Index: 299, Predicted: 1, Actual: 0\n",
      "Epoch 11: OK- Accuracy: 0.98267, Precision: 0.97025, Recall: 1.00000, F1: 0.98490, ROC AUC: 0.98006, AUPR (PR-AUC): 0.97025, Sensitivity: 1.00000, Specificity: 0.96012, Far: 0.03987730061349693, False Positive Rate (FPR): 0.03988, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 196.46 MB\n",
      "Epoch [12/500], Batch [10/110], Train Loss: 0.1129, Val Loss: 0.0505, LR: 0.0009987986\n",
      "Epoch [12/500], Batch [20/110], Train Loss: 0.0164, Val Loss: 0.0363, LR: 0.0009987788\n",
      "Epoch [12/500], Batch [30/110], Train Loss: 0.0057, Val Loss: 0.0368, LR: 0.0009987589\n",
      "Epoch [12/500], Batch [40/110], Train Loss: 0.0100, Val Loss: 0.0335, LR: 0.0009987388\n",
      "Epoch [12/500], Batch [50/110], Train Loss: 0.0118, Val Loss: 0.0365, LR: 0.0009987185\n",
      "Epoch [12/500], Batch [60/110], Train Loss: 0.0025, Val Loss: 0.0484, LR: 0.0009986981\n",
      "Epoch [12/500], Batch [70/110], Train Loss: 0.0097, Val Loss: 0.0359, LR: 0.0009986776\n",
      "Epoch [12/500], Batch [80/110], Train Loss: 0.0371, Val Loss: 0.0356, LR: 0.0009986568\n",
      "Epoch [12/500], Batch [90/110], Train Loss: 0.0026, Val Loss: 0.0622, LR: 0.0009986359\n",
      "Epoch [12/500], Batch [100/110], Train Loss: 0.0037, Val Loss: 0.0387, LR: 0.0009986149\n",
      "Epoch [12/500], Batch [110/110], Train Loss: 0.0034, Val Loss: 0.0331, LR: 0.0009985937\n",
      "Epoch [13/500], Batch [10/110], Train Loss: 0.1151, Val Loss: 0.0344, LR: 0.0009985723\n",
      "Epoch [13/500], Batch [20/110], Train Loss: 0.0027, Val Loss: 0.0551, LR: 0.0009985507\n",
      "Epoch [13/500], Batch [30/110], Train Loss: 0.0076, Val Loss: 0.0512, LR: 0.0009985290\n",
      "Epoch [13/500], Batch [40/110], Train Loss: 0.0327, Val Loss: 0.0345, LR: 0.0009985072\n",
      "Epoch [13/500], Batch [50/110], Train Loss: 0.0040, Val Loss: 0.0415, LR: 0.0009984852\n",
      "Epoch [13/500], Batch [60/110], Train Loss: 0.0637, Val Loss: 0.0387, LR: 0.0009984630\n",
      "Epoch [13/500], Batch [70/110], Train Loss: 0.0072, Val Loss: 0.0345, LR: 0.0009984406\n",
      "Epoch [13/500], Batch [80/110], Train Loss: 0.0053, Val Loss: 0.0322, LR: 0.0009984181\n",
      "Epoch [13/500], Batch [90/110], Train Loss: 0.0065, Val Loss: 0.0371, LR: 0.0009983955\n",
      "Epoch [13/500], Batch [100/110], Train Loss: 0.0034, Val Loss: 0.0448, LR: 0.0009983726\n",
      "Epoch [13/500], Batch [110/110], Train Loss: 0.1067, Val Loss: 0.0356, LR: 0.0009983496\n",
      "Epoch [14/500], Batch [10/110], Train Loss: 0.0806, Val Loss: 0.0376, LR: 0.0009983265\n",
      "Epoch [14/500], Batch [20/110], Train Loss: 0.0040, Val Loss: 0.0343, LR: 0.0009983032\n",
      "Epoch [14/500], Batch [30/110], Train Loss: 0.1574, Val Loss: 0.0403, LR: 0.0009982797\n",
      "Epoch [14/500], Batch [40/110], Train Loss: 0.1003, Val Loss: 0.0358, LR: 0.0009982561\n",
      "Epoch [14/500], Batch [50/110], Train Loss: 0.0468, Val Loss: 0.0322, LR: 0.0009982323\n",
      "Epoch [14/500], Batch [60/110], Train Loss: 0.0045, Val Loss: 0.0371, LR: 0.0009982083\n",
      "Epoch [14/500], Batch [70/110], Train Loss: 0.0021, Val Loss: 0.0483, LR: 0.0009981842\n",
      "Epoch [14/500], Batch [80/110], Train Loss: 0.0079, Val Loss: 0.0310, LR: 0.0009981599\n",
      "Epoch [14/500], Batch [90/110], Train Loss: 0.0079, Val Loss: 0.0345, LR: 0.0009981355\n",
      "Epoch [14/500], Batch [100/110], Train Loss: 0.0078, Val Loss: 0.0356, LR: 0.0009981109\n",
      "Epoch [14/500], Batch [110/110], Train Loss: 0.1024, Val Loss: 0.0344, LR: 0.0009980861\n",
      "Epoch [15/500], Batch [10/110], Train Loss: 0.0107, Val Loss: 0.0312, LR: 0.0009980612\n",
      "Epoch [15/500], Batch [20/110], Train Loss: 0.0089, Val Loss: 0.0359, LR: 0.0009980361\n",
      "Epoch [15/500], Batch [30/110], Train Loss: 0.0050, Val Loss: 0.0310, LR: 0.0009980109\n",
      "Epoch [15/500], Batch [40/110], Train Loss: 0.0055, Val Loss: 0.0406, LR: 0.0009979855\n",
      "Epoch [15/500], Batch [50/110], Train Loss: 0.0057, Val Loss: 0.0407, LR: 0.0009979599\n",
      "Epoch [15/500], Batch [60/110], Train Loss: 0.0074, Val Loss: 0.0325, LR: 0.0009979342\n",
      "Epoch [15/500], Batch [70/110], Train Loss: 0.0029, Val Loss: 0.0453, LR: 0.0009979083\n",
      "Epoch [15/500], Batch [80/110], Train Loss: 0.0832, Val Loss: 0.0304, LR: 0.0009978823\n",
      "Epoch [15/500], Batch [90/110], Train Loss: 0.0056, Val Loss: 0.0373, LR: 0.0009978561\n",
      "Epoch [15/500], Batch [100/110], Train Loss: 0.2505, Val Loss: 0.0403, LR: 0.0009978297\n",
      "Epoch [15/500], Batch [110/110], Train Loss: 0.0020, Val Loss: 0.0403, LR: 0.0009978032\n",
      "Epoch [16/500], Batch [10/110], Train Loss: 0.0016, Val Loss: 0.0313, LR: 0.0009977765\n",
      "Epoch [16/500], Batch [20/110], Train Loss: 0.0054, Val Loss: 0.0357, LR: 0.0009977496\n",
      "Epoch [16/500], Batch [30/110], Train Loss: 0.1108, Val Loss: 0.0361, LR: 0.0009977226\n",
      "Epoch [16/500], Batch [40/110], Train Loss: 0.2128, Val Loss: 0.0348, LR: 0.0009976955\n",
      "Epoch [16/500], Batch [50/110], Train Loss: 0.1076, Val Loss: 0.0351, LR: 0.0009976681\n",
      "Epoch [16/500], Batch [60/110], Train Loss: 0.0109, Val Loss: 0.0361, LR: 0.0009976406\n",
      "Epoch [16/500], Batch [70/110], Train Loss: 0.1752, Val Loss: 0.0329, LR: 0.0009976130\n",
      "Epoch [16/500], Batch [80/110], Train Loss: 0.0084, Val Loss: 0.0314, LR: 0.0009975852\n",
      "Epoch [16/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0297, LR: 0.0009975572\n",
      "Epoch [16/500], Batch [100/110], Train Loss: 0.0968, Val Loss: 0.0321, LR: 0.0009975290\n",
      "Epoch [16/500], Batch [110/110], Train Loss: 0.0070, Val Loss: 0.0293, LR: 0.0009975008\n",
      "Epoch [17/500], Batch [10/110], Train Loss: 0.0029, Val Loss: 0.0453, LR: 0.0009974723\n",
      "Epoch [17/500], Batch [20/110], Train Loss: 0.0021, Val Loss: 0.0425, LR: 0.0009974437\n",
      "Epoch [17/500], Batch [30/110], Train Loss: 0.0102, Val Loss: 0.0303, LR: 0.0009974149\n",
      "Epoch [17/500], Batch [40/110], Train Loss: 0.1139, Val Loss: 0.0365, LR: 0.0009973860\n",
      "Epoch [17/500], Batch [50/110], Train Loss: 0.0366, Val Loss: 0.0287, LR: 0.0009973569\n",
      "Epoch [17/500], Batch [60/110], Train Loss: 0.0043, Val Loss: 0.0345, LR: 0.0009973276\n",
      "Epoch [17/500], Batch [70/110], Train Loss: 0.0055, Val Loss: 0.0300, LR: 0.0009972982\n",
      "Epoch [17/500], Batch [80/110], Train Loss: 0.1157, Val Loss: 0.0366, LR: 0.0009972686\n",
      "Epoch [17/500], Batch [90/110], Train Loss: 0.1168, Val Loss: 0.0362, LR: 0.0009972389\n",
      "Epoch [17/500], Batch [100/110], Train Loss: 0.0052, Val Loss: 0.0325, LR: 0.0009972090\n",
      "Epoch [17/500], Batch [110/110], Train Loss: 0.0221, Val Loss: 0.0306, LR: 0.0009971789\n",
      "Epoch [18/500], Batch [10/110], Train Loss: 0.0052, Val Loss: 0.0318, LR: 0.0009971487\n",
      "Epoch [18/500], Batch [20/110], Train Loss: 0.0623, Val Loss: 0.0291, LR: 0.0009971183\n",
      "Epoch [18/500], Batch [30/110], Train Loss: 0.2440, Val Loss: 0.0342, LR: 0.0009970877\n",
      "Epoch [18/500], Batch [40/110], Train Loss: 0.0186, Val Loss: 0.0359, LR: 0.0009970570\n",
      "Epoch [18/500], Batch [50/110], Train Loss: 0.0103, Val Loss: 0.0298, LR: 0.0009970262\n",
      "Epoch [18/500], Batch [60/110], Train Loss: 0.0047, Val Loss: 0.0272, LR: 0.0009969951\n",
      "Epoch [18/500], Batch [70/110], Train Loss: 0.0135, Val Loss: 0.0389, LR: 0.0009969640\n",
      "Epoch [18/500], Batch [80/110], Train Loss: 0.0014, Val Loss: 0.0437, LR: 0.0009969326\n",
      "Epoch [18/500], Batch [90/110], Train Loss: 0.0029, Val Loss: 0.0357, LR: 0.0009969011\n",
      "Epoch [18/500], Batch [100/110], Train Loss: 0.0098, Val Loss: 0.0293, LR: 0.0009968694\n",
      "Epoch [18/500], Batch [110/110], Train Loss: 0.0037, Val Loss: 0.0298, LR: 0.0009968376\n",
      "Epoch [19/500], Batch [10/110], Train Loss: 0.0061, Val Loss: 0.0294, LR: 0.0009968056\n",
      "Epoch [19/500], Batch [20/110], Train Loss: 0.2869, Val Loss: 0.0351, LR: 0.0009967735\n",
      "Epoch [19/500], Batch [30/110], Train Loss: 0.0164, Val Loss: 0.0275, LR: 0.0009967411\n",
      "Epoch [19/500], Batch [40/110], Train Loss: 0.0020, Val Loss: 0.0386, LR: 0.0009967087\n",
      "Epoch [19/500], Batch [50/110], Train Loss: 0.0056, Val Loss: 0.0337, LR: 0.0009966760\n",
      "Epoch [19/500], Batch [60/110], Train Loss: 0.0345, Val Loss: 0.0352, LR: 0.0009966433\n",
      "Epoch [19/500], Batch [70/110], Train Loss: 0.0057, Val Loss: 0.0304, LR: 0.0009966103\n",
      "Epoch [19/500], Batch [80/110], Train Loss: 0.1066, Val Loss: 0.0332, LR: 0.0009965772\n",
      "Epoch [19/500], Batch [90/110], Train Loss: 0.0194, Val Loss: 0.0279, LR: 0.0009965439\n",
      "Epoch [19/500], Batch [100/110], Train Loss: 0.0062, Val Loss: 0.0343, LR: 0.0009965105\n",
      "Epoch [19/500], Batch [110/110], Train Loss: 0.0015, Val Loss: 0.0480, LR: 0.0009964769\n",
      "Epoch [20/500], Batch [10/110], Train Loss: 0.0072, Val Loss: 0.0270, LR: 0.0009964431\n",
      "Epoch [20/500], Batch [20/110], Train Loss: 0.0018, Val Loss: 0.0364, LR: 0.0009964092\n",
      "Epoch [20/500], Batch [30/110], Train Loss: 0.0024, Val Loss: 0.0432, LR: 0.0009963751\n",
      "Epoch [20/500], Batch [40/110], Train Loss: 0.1134, Val Loss: 0.0356, LR: 0.0009963409\n",
      "Epoch [20/500], Batch [50/110], Train Loss: 0.0054, Val Loss: 0.0307, LR: 0.0009963065\n",
      "Epoch [20/500], Batch [60/110], Train Loss: 0.1559, Val Loss: 0.0271, LR: 0.0009962720\n",
      "Epoch [20/500], Batch [70/110], Train Loss: 0.0091, Val Loss: 0.0393, LR: 0.0009962372\n",
      "Epoch [20/500], Batch [80/110], Train Loss: 0.0696, Val Loss: 0.0266, LR: 0.0009962024\n",
      "Epoch [20/500], Batch [90/110], Train Loss: 0.1179, Val Loss: 0.0364, LR: 0.0009961673\n",
      "Epoch [20/500], Batch [100/110], Train Loss: 0.0989, Val Loss: 0.0275, LR: 0.0009961321\n",
      "Epoch [20/500], Batch [110/110], Train Loss: 0.0959, Val Loss: 0.0320, LR: 0.0009960968\n",
      "Epoch [21/500], Batch [10/110], Train Loss: 0.1205, Val Loss: 0.0339, LR: 0.0009960613\n",
      "Epoch [21/500], Batch [20/110], Train Loss: 0.1797, Val Loss: 0.0290, LR: 0.0009960256\n",
      "Epoch [21/500], Batch [30/110], Train Loss: 0.0023, Val Loss: 0.0278, LR: 0.0009959897\n",
      "Epoch [21/500], Batch [40/110], Train Loss: 0.1126, Val Loss: 0.0282, LR: 0.0009959537\n",
      "Epoch [21/500], Batch [50/110], Train Loss: 0.0064, Val Loss: 0.0281, LR: 0.0009959176\n",
      "Epoch [21/500], Batch [60/110], Train Loss: 0.0032, Val Loss: 0.0281, LR: 0.0009958813\n",
      "Epoch [21/500], Batch [70/110], Train Loss: 0.0111, Val Loss: 0.0296, LR: 0.0009958448\n",
      "Epoch [21/500], Batch [80/110], Train Loss: 0.0866, Val Loss: 0.0296, LR: 0.0009958082\n",
      "Epoch [21/500], Batch [90/110], Train Loss: 0.1004, Val Loss: 0.0269, LR: 0.0009957714\n",
      "Epoch [21/500], Batch [100/110], Train Loss: 0.0012, Val Loss: 0.0406, LR: 0.0009957344\n",
      "Epoch [21/500], Batch [110/110], Train Loss: 0.0042, Val Loss: 0.0291, LR: 0.0009956973\n",
      "Confusion Matrix:\n",
      "[[627  25]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.96166   0.98045       652\n",
      "           1    0.97136   1.00000   0.98547       848\n",
      "\n",
      "    accuracy                        0.98333      1500\n",
      "   macro avg    0.98568   0.98083   0.98296      1500\n",
      "weighted avg    0.98381   0.98333   0.98329      1500\n",
      "\n",
      "Total Errors: 25\n",
      "Index: 7, Predicted: 1, Actual: 0\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 258, Predicted: 1, Actual: 0\n",
      "Index: 299, Predicted: 1, Actual: 0\n",
      "Index: 307, Predicted: 1, Actual: 0\n",
      "Epoch 21: OK- Accuracy: 0.98333, Precision: 0.97136, Recall: 1.00000, F1: 0.98547, ROC AUC: 0.98083, AUPR (PR-AUC): 0.97136, Sensitivity: 1.00000, Specificity: 0.96166, Far: 0.03834355828220859, False Positive Rate (FPR): 0.03834, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 196.47 MB\n",
      "Epoch [22/500], Batch [10/110], Train Loss: 0.0154, Val Loss: 0.0253, LR: 0.0009956600\n",
      "Epoch [22/500], Batch [20/110], Train Loss: 0.0060, Val Loss: 0.0287, LR: 0.0009956226\n",
      "Epoch [22/500], Batch [30/110], Train Loss: 0.0017, Val Loss: 0.0312, LR: 0.0009955850\n",
      "Epoch [22/500], Batch [40/110], Train Loss: 0.0977, Val Loss: 0.0301, LR: 0.0009955472\n",
      "Epoch [22/500], Batch [50/110], Train Loss: 0.0026, Val Loss: 0.0289, LR: 0.0009955093\n",
      "Epoch [22/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0286, LR: 0.0009954712\n",
      "Epoch [22/500], Batch [70/110], Train Loss: 0.0101, Val Loss: 0.0301, LR: 0.0009954330\n",
      "Epoch [22/500], Batch [80/110], Train Loss: 0.0012, Val Loss: 0.0317, LR: 0.0009953946\n",
      "Epoch [22/500], Batch [90/110], Train Loss: 0.0048, Val Loss: 0.0363, LR: 0.0009953560\n",
      "Epoch [22/500], Batch [100/110], Train Loss: 0.3097, Val Loss: 0.0257, LR: 0.0009953173\n",
      "Epoch [22/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0361, LR: 0.0009952784\n",
      "Epoch [23/500], Batch [10/110], Train Loss: 0.3415, Val Loss: 0.0461, LR: 0.0009952394\n",
      "Epoch [23/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0279, LR: 0.0009952002\n",
      "Epoch [23/500], Batch [30/110], Train Loss: 0.0018, Val Loss: 0.0257, LR: 0.0009951608\n",
      "Epoch [23/500], Batch [40/110], Train Loss: 0.0979, Val Loss: 0.0257, LR: 0.0009951213\n",
      "Epoch [23/500], Batch [50/110], Train Loss: 0.0029, Val Loss: 0.0288, LR: 0.0009950816\n",
      "Epoch [23/500], Batch [60/110], Train Loss: 0.0698, Val Loss: 0.0252, LR: 0.0009950418\n",
      "Epoch [23/500], Batch [70/110], Train Loss: 0.0365, Val Loss: 0.0258, LR: 0.0009950018\n",
      "Epoch [23/500], Batch [80/110], Train Loss: 0.0062, Val Loss: 0.0420, LR: 0.0009949616\n",
      "Epoch [23/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0343, LR: 0.0009949213\n",
      "Epoch [23/500], Batch [100/110], Train Loss: 0.0164, Val Loss: 0.0240, LR: 0.0009948808\n",
      "Epoch [23/500], Batch [110/110], Train Loss: 0.0027, Val Loss: 0.0370, LR: 0.0009948402\n",
      "Epoch [24/500], Batch [10/110], Train Loss: 0.0781, Val Loss: 0.0241, LR: 0.0009947994\n",
      "Epoch [24/500], Batch [20/110], Train Loss: 0.3114, Val Loss: 0.0255, LR: 0.0009947584\n",
      "Epoch [24/500], Batch [30/110], Train Loss: 0.0023, Val Loss: 0.0287, LR: 0.0009947173\n",
      "Epoch [24/500], Batch [40/110], Train Loss: 0.0196, Val Loss: 0.0278, LR: 0.0009946760\n",
      "Epoch [24/500], Batch [50/110], Train Loss: 0.0087, Val Loss: 0.0386, LR: 0.0009946346\n",
      "Epoch [24/500], Batch [60/110], Train Loss: 0.0029, Val Loss: 0.0302, LR: 0.0009945930\n",
      "Epoch [24/500], Batch [70/110], Train Loss: 0.0034, Val Loss: 0.0258, LR: 0.0009945512\n",
      "Epoch [24/500], Batch [80/110], Train Loss: 0.0040, Val Loss: 0.0383, LR: 0.0009945093\n",
      "Epoch [24/500], Batch [90/110], Train Loss: 0.0319, Val Loss: 0.0410, LR: 0.0009944672\n",
      "Epoch [24/500], Batch [100/110], Train Loss: 0.1024, Val Loss: 0.0319, LR: 0.0009944250\n",
      "Epoch [24/500], Batch [110/110], Train Loss: 0.0156, Val Loss: 0.0263, LR: 0.0009943826\n",
      "Epoch [25/500], Batch [10/110], Train Loss: 0.0069, Val Loss: 0.0293, LR: 0.0009943401\n",
      "Epoch [25/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0397, LR: 0.0009942973\n",
      "Epoch [25/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0297, LR: 0.0009942545\n",
      "Epoch [25/500], Batch [40/110], Train Loss: 0.0701, Val Loss: 0.0259, LR: 0.0009942114\n",
      "Epoch [25/500], Batch [50/110], Train Loss: 0.0050, Val Loss: 0.0245, LR: 0.0009941682\n",
      "Epoch [25/500], Batch [60/110], Train Loss: 0.0040, Val Loss: 0.0244, LR: 0.0009941249\n",
      "Epoch [25/500], Batch [70/110], Train Loss: 0.0048, Val Loss: 0.0368, LR: 0.0009940814\n",
      "Epoch [25/500], Batch [80/110], Train Loss: 0.0137, Val Loss: 0.0244, LR: 0.0009940377\n",
      "Epoch [25/500], Batch [90/110], Train Loss: 0.1164, Val Loss: 0.0234, LR: 0.0009939939\n",
      "Epoch [25/500], Batch [100/110], Train Loss: 0.0018, Val Loss: 0.0294, LR: 0.0009939499\n",
      "Epoch [25/500], Batch [110/110], Train Loss: 0.0083, Val Loss: 0.0243, LR: 0.0009939057\n",
      "Epoch [26/500], Batch [10/110], Train Loss: 0.0016, Val Loss: 0.0299, LR: 0.0009938614\n",
      "Epoch [26/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0409, LR: 0.0009938169\n",
      "Epoch [26/500], Batch [30/110], Train Loss: 0.0131, Val Loss: 0.0263, LR: 0.0009937723\n",
      "Epoch [26/500], Batch [40/110], Train Loss: 0.0020, Val Loss: 0.0260, LR: 0.0009937275\n",
      "Epoch [26/500], Batch [50/110], Train Loss: 0.0223, Val Loss: 0.0250, LR: 0.0009936826\n",
      "Epoch [26/500], Batch [60/110], Train Loss: 0.0108, Val Loss: 0.0239, LR: 0.0009936375\n",
      "Epoch [26/500], Batch [70/110], Train Loss: 0.0838, Val Loss: 0.0226, LR: 0.0009935922\n",
      "Epoch [26/500], Batch [80/110], Train Loss: 0.0063, Val Loss: 0.0302, LR: 0.0009935468\n",
      "Epoch [26/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0312, LR: 0.0009935012\n",
      "Epoch [26/500], Batch [100/110], Train Loss: 0.0055, Val Loss: 0.0261, LR: 0.0009934554\n",
      "Epoch [26/500], Batch [110/110], Train Loss: 0.0100, Val Loss: 0.0242, LR: 0.0009934095\n",
      "Epoch [27/500], Batch [10/110], Train Loss: 0.0020, Val Loss: 0.0215, LR: 0.0009933635\n",
      "Epoch [27/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0334, LR: 0.0009933173\n",
      "Epoch [27/500], Batch [30/110], Train Loss: 0.0012, Val Loss: 0.0321, LR: 0.0009932709\n",
      "Epoch [27/500], Batch [40/110], Train Loss: 0.0029, Val Loss: 0.0224, LR: 0.0009932243\n",
      "Epoch [27/500], Batch [50/110], Train Loss: 0.0142, Val Loss: 0.0267, LR: 0.0009931776\n",
      "Epoch [27/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0247, LR: 0.0009931308\n",
      "Epoch [27/500], Batch [70/110], Train Loss: 0.0024, Val Loss: 0.0269, LR: 0.0009930837\n",
      "Epoch [27/500], Batch [80/110], Train Loss: 0.0030, Val Loss: 0.0240, LR: 0.0009930366\n",
      "Epoch [27/500], Batch [90/110], Train Loss: 0.0153, Val Loss: 0.0215, LR: 0.0009929892\n",
      "Epoch [27/500], Batch [100/110], Train Loss: 0.0024, Val Loss: 0.0431, LR: 0.0009929417\n",
      "Epoch [27/500], Batch [110/110], Train Loss: 0.0033, Val Loss: 0.0327, LR: 0.0009928941\n",
      "Epoch [28/500], Batch [10/110], Train Loss: 0.0359, Val Loss: 0.0228, LR: 0.0009928463\n",
      "Epoch [28/500], Batch [20/110], Train Loss: 0.1536, Val Loss: 0.0323, LR: 0.0009927983\n",
      "Epoch [28/500], Batch [30/110], Train Loss: 0.0202, Val Loss: 0.0225, LR: 0.0009927501\n",
      "Epoch [28/500], Batch [40/110], Train Loss: 0.1181, Val Loss: 0.0228, LR: 0.0009927019\n",
      "Epoch [28/500], Batch [50/110], Train Loss: 0.0129, Val Loss: 0.0279, LR: 0.0009926534\n",
      "Epoch [28/500], Batch [60/110], Train Loss: 0.0094, Val Loss: 0.0211, LR: 0.0009926048\n",
      "Epoch [28/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0350, LR: 0.0009925560\n",
      "Epoch [28/500], Batch [80/110], Train Loss: 0.0051, Val Loss: 0.0225, LR: 0.0009925071\n",
      "Epoch [28/500], Batch [90/110], Train Loss: 0.0048, Val Loss: 0.0254, LR: 0.0009924580\n",
      "Epoch [28/500], Batch [100/110], Train Loss: 0.0745, Val Loss: 0.0240, LR: 0.0009924088\n",
      "Epoch [28/500], Batch [110/110], Train Loss: 0.2940, Val Loss: 0.0300, LR: 0.0009923593\n",
      "Epoch [29/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0266, LR: 0.0009923098\n",
      "Epoch [29/500], Batch [20/110], Train Loss: 0.2050, Val Loss: 0.0317, LR: 0.0009922601\n",
      "Epoch [29/500], Batch [30/110], Train Loss: 0.0068, Val Loss: 0.0211, LR: 0.0009922102\n",
      "Epoch [29/500], Batch [40/110], Train Loss: 0.0010, Val Loss: 0.0242, LR: 0.0009921601\n",
      "Epoch [29/500], Batch [50/110], Train Loss: 0.0094, Val Loss: 0.0288, LR: 0.0009921099\n",
      "Epoch [29/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0379, LR: 0.0009920596\n",
      "Epoch [29/500], Batch [70/110], Train Loss: 0.0031, Val Loss: 0.0329, LR: 0.0009920090\n",
      "Epoch [29/500], Batch [80/110], Train Loss: 0.0513, Val Loss: 0.0218, LR: 0.0009919584\n",
      "Epoch [29/500], Batch [90/110], Train Loss: 0.0038, Val Loss: 0.0304, LR: 0.0009919075\n",
      "Epoch [29/500], Batch [100/110], Train Loss: 0.1468, Val Loss: 0.0360, LR: 0.0009918565\n",
      "Epoch [29/500], Batch [110/110], Train Loss: 0.0109, Val Loss: 0.0242, LR: 0.0009918054\n",
      "Epoch [30/500], Batch [10/110], Train Loss: 0.3057, Val Loss: 0.0448, LR: 0.0009917541\n",
      "Epoch [30/500], Batch [20/110], Train Loss: 0.1283, Val Loss: 0.0374, LR: 0.0009917026\n",
      "Epoch [30/500], Batch [30/110], Train Loss: 0.0215, Val Loss: 0.0224, LR: 0.0009916510\n",
      "Epoch [30/500], Batch [40/110], Train Loss: 0.0035, Val Loss: 0.0260, LR: 0.0009915992\n",
      "Epoch [30/500], Batch [50/110], Train Loss: 0.0040, Val Loss: 0.0242, LR: 0.0009915472\n",
      "Epoch [30/500], Batch [60/110], Train Loss: 0.0502, Val Loss: 0.0210, LR: 0.0009914951\n",
      "Epoch [30/500], Batch [70/110], Train Loss: 0.0155, Val Loss: 0.0219, LR: 0.0009914428\n",
      "Epoch [30/500], Batch [80/110], Train Loss: 0.0061, Val Loss: 0.0311, LR: 0.0009913904\n",
      "Epoch [30/500], Batch [90/110], Train Loss: 0.0033, Val Loss: 0.0244, LR: 0.0009913378\n",
      "Epoch [30/500], Batch [100/110], Train Loss: 0.1107, Val Loss: 0.0295, LR: 0.0009912851\n",
      "Epoch [30/500], Batch [110/110], Train Loss: 0.0082, Val Loss: 0.0278, LR: 0.0009912322\n",
      "Epoch [31/500], Batch [10/110], Train Loss: 0.0040, Val Loss: 0.0214, LR: 0.0009911791\n",
      "Epoch [31/500], Batch [20/110], Train Loss: 0.0043, Val Loss: 0.0311, LR: 0.0009911259\n",
      "Epoch [31/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0318, LR: 0.0009910725\n",
      "Epoch [31/500], Batch [40/110], Train Loss: 0.0401, Val Loss: 0.0202, LR: 0.0009910190\n",
      "Epoch [31/500], Batch [50/110], Train Loss: 0.0030, Val Loss: 0.0214, LR: 0.0009909653\n",
      "Epoch [31/500], Batch [60/110], Train Loss: 0.0481, Val Loss: 0.0213, LR: 0.0009909114\n",
      "Epoch [31/500], Batch [70/110], Train Loss: 0.0021, Val Loss: 0.0213, LR: 0.0009908574\n",
      "Epoch [31/500], Batch [80/110], Train Loss: 0.0538, Val Loss: 0.0215, LR: 0.0009908033\n",
      "Epoch [31/500], Batch [90/110], Train Loss: 0.0889, Val Loss: 0.0261, LR: 0.0009907489\n",
      "Epoch [31/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0312, LR: 0.0009906945\n",
      "Epoch [31/500], Batch [110/110], Train Loss: 0.0088, Val Loss: 0.0209, LR: 0.0009906398\n",
      "Confusion Matrix:\n",
      "[[635  17]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.97393   0.98679       652\n",
      "           1    0.98035   1.00000   0.99008       848\n",
      "\n",
      "    accuracy                        0.98867      1500\n",
      "   macro avg    0.99017   0.98696   0.98843      1500\n",
      "weighted avg    0.98889   0.98867   0.98865      1500\n",
      "\n",
      "Total Errors: 17\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 258, Predicted: 1, Actual: 0\n",
      "Index: 307, Predicted: 1, Actual: 0\n",
      "Index: 625, Predicted: 1, Actual: 0\n",
      "Index: 776, Predicted: 1, Actual: 0\n",
      "Epoch 31: OK- Accuracy: 0.98867, Precision: 0.98035, Recall: 1.00000, F1: 0.99008, ROC AUC: 0.98696, AUPR (PR-AUC): 0.98035, Sensitivity: 1.00000, Specificity: 0.97393, Far: 0.02607361963190184, False Positive Rate (FPR): 0.02607, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 196.49 MB\n",
      "Epoch [32/500], Batch [10/110], Train Loss: 0.0045, Val Loss: 0.0274, LR: 0.0009905850\n",
      "Epoch [32/500], Batch [20/110], Train Loss: 0.0016, Val Loss: 0.0425, LR: 0.0009905300\n",
      "Epoch [32/500], Batch [30/110], Train Loss: 0.0087, Val Loss: 0.0256, LR: 0.0009904749\n",
      "Epoch [32/500], Batch [40/110], Train Loss: 0.0108, Val Loss: 0.0222, LR: 0.0009904196\n",
      "Epoch [32/500], Batch [50/110], Train Loss: 0.0031, Val Loss: 0.0224, LR: 0.0009903642\n",
      "Epoch [32/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0213, LR: 0.0009903086\n",
      "Epoch [32/500], Batch [70/110], Train Loss: 0.0518, Val Loss: 0.0196, LR: 0.0009902529\n",
      "Epoch [32/500], Batch [80/110], Train Loss: 0.0062, Val Loss: 0.0248, LR: 0.0009901969\n",
      "Epoch [32/500], Batch [90/110], Train Loss: 0.0091, Val Loss: 0.0229, LR: 0.0009901409\n",
      "Epoch [32/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0244, LR: 0.0009900846\n",
      "Epoch [32/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0285, LR: 0.0009900283\n",
      "Epoch [33/500], Batch [10/110], Train Loss: 0.0026, Val Loss: 0.0197, LR: 0.0009899717\n",
      "Epoch [33/500], Batch [20/110], Train Loss: 0.0021, Val Loss: 0.0267, LR: 0.0009899150\n",
      "Epoch [33/500], Batch [30/110], Train Loss: 0.0814, Val Loss: 0.0199, LR: 0.0009898581\n",
      "Epoch [33/500], Batch [40/110], Train Loss: 0.3260, Val Loss: 0.0211, LR: 0.0009898011\n",
      "Epoch [33/500], Batch [50/110], Train Loss: 0.0019, Val Loss: 0.0374, LR: 0.0009897439\n",
      "Epoch [33/500], Batch [60/110], Train Loss: 0.0183, Val Loss: 0.0262, LR: 0.0009896866\n",
      "Epoch [33/500], Batch [70/110], Train Loss: 0.0016, Val Loss: 0.0212, LR: 0.0009896291\n",
      "Epoch [33/500], Batch [80/110], Train Loss: 0.0058, Val Loss: 0.0263, LR: 0.0009895715\n",
      "Epoch [33/500], Batch [90/110], Train Loss: 0.0016, Val Loss: 0.0197, LR: 0.0009895136\n",
      "Epoch [33/500], Batch [100/110], Train Loss: 0.0631, Val Loss: 0.0236, LR: 0.0009894557\n",
      "Epoch [33/500], Batch [110/110], Train Loss: 0.0521, Val Loss: 0.0199, LR: 0.0009893975\n",
      "Epoch [34/500], Batch [10/110], Train Loss: 0.0151, Val Loss: 0.0189, LR: 0.0009893393\n",
      "Epoch [34/500], Batch [20/110], Train Loss: 0.0079, Val Loss: 0.0218, LR: 0.0009892808\n",
      "Epoch [34/500], Batch [30/110], Train Loss: 0.0776, Val Loss: 0.0232, LR: 0.0009892222\n",
      "Epoch [34/500], Batch [40/110], Train Loss: 0.0380, Val Loss: 0.0260, LR: 0.0009891635\n",
      "Epoch [34/500], Batch [50/110], Train Loss: 0.0023, Val Loss: 0.0270, LR: 0.0009891045\n",
      "Epoch [34/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0332, LR: 0.0009890455\n",
      "Epoch [34/500], Batch [70/110], Train Loss: 0.0021, Val Loss: 0.0214, LR: 0.0009889862\n",
      "Epoch [34/500], Batch [80/110], Train Loss: 0.0013, Val Loss: 0.0220, LR: 0.0009889268\n",
      "Epoch [34/500], Batch [90/110], Train Loss: 0.0023, Val Loss: 0.0196, LR: 0.0009888673\n",
      "Epoch [34/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0355, LR: 0.0009888076\n",
      "Epoch [34/500], Batch [110/110], Train Loss: 0.0065, Val Loss: 0.0211, LR: 0.0009887477\n",
      "Epoch [35/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0199, LR: 0.0009886877\n",
      "Epoch [35/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0354, LR: 0.0009886275\n",
      "Epoch [35/500], Batch [30/110], Train Loss: 0.0014, Val Loss: 0.0240, LR: 0.0009885672\n",
      "Epoch [35/500], Batch [40/110], Train Loss: 0.0064, Val Loss: 0.0197, LR: 0.0009885067\n",
      "Epoch [35/500], Batch [50/110], Train Loss: 0.0067, Val Loss: 0.0249, LR: 0.0009884460\n",
      "Epoch [35/500], Batch [60/110], Train Loss: 0.0033, Val Loss: 0.0260, LR: 0.0009883852\n",
      "Epoch [35/500], Batch [70/110], Train Loss: 0.0113, Val Loss: 0.0181, LR: 0.0009883243\n",
      "Epoch [35/500], Batch [80/110], Train Loss: 0.0096, Val Loss: 0.0199, LR: 0.0009882631\n",
      "Epoch [35/500], Batch [90/110], Train Loss: 0.0041, Val Loss: 0.0206, LR: 0.0009882018\n",
      "Epoch [35/500], Batch [100/110], Train Loss: 0.0028, Val Loss: 0.0320, LR: 0.0009881404\n",
      "Epoch [35/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0254, LR: 0.0009880788\n",
      "Epoch [36/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0223, LR: 0.0009880170\n",
      "Epoch [36/500], Batch [20/110], Train Loss: 0.0184, Val Loss: 0.0182, LR: 0.0009879551\n",
      "Epoch [36/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0287, LR: 0.0009878931\n",
      "Epoch [36/500], Batch [40/110], Train Loss: 0.0501, Val Loss: 0.0185, LR: 0.0009878308\n",
      "Epoch [36/500], Batch [50/110], Train Loss: 0.0085, Val Loss: 0.0193, LR: 0.0009877684\n",
      "Epoch [36/500], Batch [60/110], Train Loss: 0.1179, Val Loss: 0.0266, LR: 0.0009877059\n",
      "Epoch [36/500], Batch [70/110], Train Loss: 0.1051, Val Loss: 0.0286, LR: 0.0009876432\n",
      "Epoch [36/500], Batch [80/110], Train Loss: 0.0191, Val Loss: 0.0178, LR: 0.0009875803\n",
      "Epoch [36/500], Batch [90/110], Train Loss: 0.0629, Val Loss: 0.0176, LR: 0.0009875173\n",
      "Epoch [36/500], Batch [100/110], Train Loss: 0.1066, Val Loss: 0.0318, LR: 0.0009874541\n",
      "Epoch [36/500], Batch [110/110], Train Loss: 0.0339, Val Loss: 0.0175, LR: 0.0009873908\n",
      "Epoch [37/500], Batch [10/110], Train Loss: 0.0057, Val Loss: 0.0238, LR: 0.0009873273\n",
      "Epoch [37/500], Batch [20/110], Train Loss: 0.0010, Val Loss: 0.0173, LR: 0.0009872637\n",
      "Epoch [37/500], Batch [30/110], Train Loss: 0.0023, Val Loss: 0.0179, LR: 0.0009871999\n",
      "Epoch [37/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0247, LR: 0.0009871359\n",
      "Epoch [37/500], Batch [50/110], Train Loss: 0.0039, Val Loss: 0.0180, LR: 0.0009870718\n",
      "Epoch [37/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0247, LR: 0.0009870075\n",
      "Epoch [37/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0223, LR: 0.0009869431\n",
      "Epoch [37/500], Batch [80/110], Train Loss: 0.0029, Val Loss: 0.0254, LR: 0.0009868785\n",
      "Epoch [37/500], Batch [90/110], Train Loss: 0.0830, Val Loss: 0.0176, LR: 0.0009868137\n",
      "Epoch [37/500], Batch [100/110], Train Loss: 0.0023, Val Loss: 0.0208, LR: 0.0009867488\n",
      "Epoch [37/500], Batch [110/110], Train Loss: 0.0671, Val Loss: 0.0175, LR: 0.0009866838\n",
      "Epoch [38/500], Batch [10/110], Train Loss: 0.2824, Val Loss: 0.0298, LR: 0.0009866185\n",
      "Epoch [38/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0225, LR: 0.0009865532\n",
      "Epoch [38/500], Batch [30/110], Train Loss: 0.0094, Val Loss: 0.0299, LR: 0.0009864876\n",
      "Epoch [38/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0275, LR: 0.0009864219\n",
      "Epoch [38/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0389, LR: 0.0009863561\n",
      "Epoch [38/500], Batch [60/110], Train Loss: 0.0211, Val Loss: 0.0171, LR: 0.0009862901\n",
      "Epoch [38/500], Batch [70/110], Train Loss: 0.0020, Val Loss: 0.0196, LR: 0.0009862239\n",
      "Epoch [38/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0219, LR: 0.0009861576\n",
      "Epoch [38/500], Batch [90/110], Train Loss: 0.0053, Val Loss: 0.0260, LR: 0.0009860911\n",
      "Epoch [38/500], Batch [100/110], Train Loss: 0.0123, Val Loss: 0.0180, LR: 0.0009860245\n",
      "Epoch [38/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0168, LR: 0.0009859577\n",
      "Epoch [39/500], Batch [10/110], Train Loss: 0.1440, Val Loss: 0.0400, LR: 0.0009858908\n",
      "Epoch [39/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0203, LR: 0.0009858237\n",
      "Epoch [39/500], Batch [30/110], Train Loss: 0.0205, Val Loss: 0.0179, LR: 0.0009857564\n",
      "Epoch [39/500], Batch [40/110], Train Loss: 0.0021, Val Loss: 0.0198, LR: 0.0009856890\n",
      "Epoch [39/500], Batch [50/110], Train Loss: 0.2614, Val Loss: 0.0206, LR: 0.0009856214\n",
      "Epoch [39/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0202, LR: 0.0009855537\n",
      "Epoch [39/500], Batch [70/110], Train Loss: 0.0031, Val Loss: 0.0165, LR: 0.0009854858\n",
      "Epoch [39/500], Batch [80/110], Train Loss: 0.0043, Val Loss: 0.0220, LR: 0.0009854177\n",
      "Epoch [39/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0381, LR: 0.0009853495\n",
      "Epoch [39/500], Batch [100/110], Train Loss: 0.0133, Val Loss: 0.0203, LR: 0.0009852812\n",
      "Epoch [39/500], Batch [110/110], Train Loss: 0.0422, Val Loss: 0.0167, LR: 0.0009852127\n",
      "Epoch [40/500], Batch [10/110], Train Loss: 0.0594, Val Loss: 0.0198, LR: 0.0009851440\n",
      "Epoch [40/500], Batch [20/110], Train Loss: 0.0037, Val Loss: 0.0163, LR: 0.0009850752\n",
      "Epoch [40/500], Batch [30/110], Train Loss: 0.0860, Val Loss: 0.0240, LR: 0.0009850062\n",
      "Epoch [40/500], Batch [40/110], Train Loss: 0.0792, Val Loss: 0.0214, LR: 0.0009849370\n",
      "Epoch [40/500], Batch [50/110], Train Loss: 0.0501, Val Loss: 0.0179, LR: 0.0009848677\n",
      "Epoch [40/500], Batch [60/110], Train Loss: 0.0087, Val Loss: 0.0165, LR: 0.0009847983\n",
      "Epoch [40/500], Batch [70/110], Train Loss: 0.1088, Val Loss: 0.0312, LR: 0.0009847287\n",
      "Epoch [40/500], Batch [80/110], Train Loss: 0.0011, Val Loss: 0.0271, LR: 0.0009846589\n",
      "Epoch [40/500], Batch [90/110], Train Loss: 0.0279, Val Loss: 0.0164, LR: 0.0009845890\n",
      "Epoch [40/500], Batch [100/110], Train Loss: 0.0110, Val Loss: 0.0162, LR: 0.0009845189\n",
      "Epoch [40/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0212, LR: 0.0009844487\n",
      "Epoch [41/500], Batch [10/110], Train Loss: 0.0581, Val Loss: 0.0160, LR: 0.0009843783\n",
      "Epoch [41/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0270, LR: 0.0009843077\n",
      "Epoch [41/500], Batch [30/110], Train Loss: 0.0022, Val Loss: 0.0208, LR: 0.0009842370\n",
      "Epoch [41/500], Batch [40/110], Train Loss: 0.0017, Val Loss: 0.0186, LR: 0.0009841662\n",
      "Epoch [41/500], Batch [50/110], Train Loss: 0.0038, Val Loss: 0.0169, LR: 0.0009840951\n",
      "Epoch [41/500], Batch [60/110], Train Loss: 0.0025, Val Loss: 0.0239, LR: 0.0009840240\n",
      "Epoch [41/500], Batch [70/110], Train Loss: 0.0447, Val Loss: 0.0164, LR: 0.0009839526\n",
      "Epoch [41/500], Batch [80/110], Train Loss: 0.0041, Val Loss: 0.0183, LR: 0.0009838811\n",
      "Epoch [41/500], Batch [90/110], Train Loss: 0.0805, Val Loss: 0.0254, LR: 0.0009838095\n",
      "Epoch [41/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0154, LR: 0.0009837377\n",
      "Epoch [41/500], Batch [110/110], Train Loss: 0.0030, Val Loss: 0.0180, LR: 0.0009836657\n",
      "Confusion Matrix:\n",
      "[[627  25]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.96166   0.98045       652\n",
      "           1    0.97136   1.00000   0.98547       848\n",
      "\n",
      "    accuracy                        0.98333      1500\n",
      "   macro avg    0.98568   0.98083   0.98296      1500\n",
      "weighted avg    0.98381   0.98333   0.98329      1500\n",
      "\n",
      "Total Errors: 25\n",
      "Index: 7, Predicted: 1, Actual: 0\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 258, Predicted: 1, Actual: 0\n",
      "Index: 299, Predicted: 1, Actual: 0\n",
      "Index: 307, Predicted: 1, Actual: 0\n",
      "Epoch 41: OK- Accuracy: 0.98333, Precision: 0.97136, Recall: 1.00000, F1: 0.98547, ROC AUC: 0.98083, AUPR (PR-AUC): 0.97136, Sensitivity: 1.00000, Specificity: 0.96166, Far: 0.03834355828220859, False Positive Rate (FPR): 0.03834, False Negative Rate (FNR): 0.00000, Runtime: 0.039 sec , Memory Usage: 196.51 MB\n",
      "Epoch [42/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0264, LR: 0.0009835936\n",
      "Epoch [42/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0170, LR: 0.0009835214\n",
      "Epoch [42/500], Batch [30/110], Train Loss: 0.0172, Val Loss: 0.0158, LR: 0.0009834489\n",
      "Epoch [42/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0340, LR: 0.0009833763\n",
      "Epoch [42/500], Batch [50/110], Train Loss: 0.0207, Val Loss: 0.0158, LR: 0.0009833036\n",
      "Epoch [42/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0166, LR: 0.0009832307\n",
      "Epoch [42/500], Batch [70/110], Train Loss: 0.0256, Val Loss: 0.0271, LR: 0.0009831577\n",
      "Epoch [42/500], Batch [80/110], Train Loss: 0.2656, Val Loss: 0.0215, LR: 0.0009830845\n",
      "Epoch [42/500], Batch [90/110], Train Loss: 0.0763, Val Loss: 0.0244, LR: 0.0009830111\n",
      "Epoch [42/500], Batch [100/110], Train Loss: 0.0052, Val Loss: 0.0162, LR: 0.0009829376\n",
      "Epoch [42/500], Batch [110/110], Train Loss: 0.0048, Val Loss: 0.0153, LR: 0.0009828639\n",
      "Epoch [43/500], Batch [10/110], Train Loss: 0.0098, Val Loss: 0.0153, LR: 0.0009827901\n",
      "Epoch [43/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0182, LR: 0.0009827161\n",
      "Epoch [43/500], Batch [30/110], Train Loss: 0.0063, Val Loss: 0.0187, LR: 0.0009826420\n",
      "Epoch [43/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0267, LR: 0.0009825677\n",
      "Epoch [43/500], Batch [50/110], Train Loss: 0.0027, Val Loss: 0.0166, LR: 0.0009824932\n",
      "Epoch [43/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0171, LR: 0.0009824186\n",
      "Epoch [43/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0203, LR: 0.0009823438\n",
      "Epoch [43/500], Batch [80/110], Train Loss: 0.0174, Val Loss: 0.0195, LR: 0.0009822689\n",
      "Epoch [43/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0259, LR: 0.0009821938\n",
      "Epoch [43/500], Batch [100/110], Train Loss: 0.0016, Val Loss: 0.0148, LR: 0.0009821186\n",
      "Epoch [43/500], Batch [110/110], Train Loss: 0.0035, Val Loss: 0.0177, LR: 0.0009820432\n",
      "Epoch [44/500], Batch [10/110], Train Loss: 0.0838, Val Loss: 0.0223, LR: 0.0009819677\n",
      "Epoch [44/500], Batch [20/110], Train Loss: 0.0259, Val Loss: 0.0148, LR: 0.0009818920\n",
      "Epoch [44/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0210, LR: 0.0009818161\n",
      "Epoch [44/500], Batch [40/110], Train Loss: 0.0084, Val Loss: 0.0188, LR: 0.0009817401\n",
      "Epoch [44/500], Batch [50/110], Train Loss: 0.0017, Val Loss: 0.0156, LR: 0.0009816640\n",
      "Epoch [44/500], Batch [60/110], Train Loss: 0.0506, Val Loss: 0.0201, LR: 0.0009815876\n",
      "Epoch [44/500], Batch [70/110], Train Loss: 0.0033, Val Loss: 0.0189, LR: 0.0009815112\n",
      "Epoch [44/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0145, LR: 0.0009814345\n",
      "Epoch [44/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0177, LR: 0.0009813577\n",
      "Epoch [44/500], Batch [100/110], Train Loss: 0.0110, Val Loss: 0.0190, LR: 0.0009812808\n",
      "Epoch [44/500], Batch [110/110], Train Loss: 0.0027, Val Loss: 0.0144, LR: 0.0009812037\n",
      "Epoch [45/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0145, LR: 0.0009811264\n",
      "Epoch [45/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0167, LR: 0.0009810490\n",
      "Epoch [45/500], Batch [30/110], Train Loss: 0.0093, Val Loss: 0.0168, LR: 0.0009809715\n",
      "Epoch [45/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0161, LR: 0.0009808938\n",
      "Epoch [45/500], Batch [50/110], Train Loss: 0.0182, Val Loss: 0.0142, LR: 0.0009808159\n",
      "Epoch [45/500], Batch [60/110], Train Loss: 0.0015, Val Loss: 0.0234, LR: 0.0009807378\n",
      "Epoch [45/500], Batch [70/110], Train Loss: 0.0374, Val Loss: 0.0172, LR: 0.0009806597\n",
      "Epoch [45/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0170, LR: 0.0009805813\n",
      "Epoch [45/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0240, LR: 0.0009805028\n",
      "Epoch [45/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0181, LR: 0.0009804242\n",
      "Epoch [45/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0155, LR: 0.0009803454\n",
      "Epoch [46/500], Batch [10/110], Train Loss: 0.0016, Val Loss: 0.0181, LR: 0.0009802664\n",
      "Epoch [46/500], Batch [20/110], Train Loss: 0.0028, Val Loss: 0.0175, LR: 0.0009801873\n",
      "Epoch [46/500], Batch [30/110], Train Loss: 0.0444, Val Loss: 0.0141, LR: 0.0009801080\n",
      "Epoch [46/500], Batch [40/110], Train Loss: 0.0028, Val Loss: 0.0164, LR: 0.0009800286\n",
      "Epoch [46/500], Batch [50/110], Train Loss: 0.0075, Val Loss: 0.0175, LR: 0.0009799490\n",
      "Epoch [46/500], Batch [60/110], Train Loss: 0.0036, Val Loss: 0.0159, LR: 0.0009798693\n",
      "Epoch [46/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0169, LR: 0.0009797894\n",
      "Epoch [46/500], Batch [80/110], Train Loss: 0.0177, Val Loss: 0.0141, LR: 0.0009797094\n",
      "Epoch [46/500], Batch [90/110], Train Loss: 0.0014, Val Loss: 0.0140, LR: 0.0009796292\n",
      "Epoch [46/500], Batch [100/110], Train Loss: 0.1363, Val Loss: 0.0339, LR: 0.0009795488\n",
      "Epoch [46/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0177, LR: 0.0009794683\n",
      "Epoch [47/500], Batch [10/110], Train Loss: 0.0222, Val Loss: 0.0145, LR: 0.0009793876\n",
      "Epoch [47/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0180, LR: 0.0009793068\n",
      "Epoch [47/500], Batch [30/110], Train Loss: 0.0141, Val Loss: 0.0151, LR: 0.0009792258\n",
      "Epoch [47/500], Batch [40/110], Train Loss: 0.0119, Val Loss: 0.0150, LR: 0.0009791447\n",
      "Epoch [47/500], Batch [50/110], Train Loss: 0.0010, Val Loss: 0.0174, LR: 0.0009790634\n",
      "Epoch [47/500], Batch [60/110], Train Loss: 0.0252, Val Loss: 0.0133, LR: 0.0009789820\n",
      "Epoch [47/500], Batch [70/110], Train Loss: 0.0038, Val Loss: 0.0135, LR: 0.0009789004\n",
      "Epoch [47/500], Batch [80/110], Train Loss: 0.0071, Val Loss: 0.0178, LR: 0.0009788186\n",
      "Epoch [47/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0233, LR: 0.0009787367\n",
      "Epoch [47/500], Batch [100/110], Train Loss: 0.0147, Val Loss: 0.0138, LR: 0.0009786547\n",
      "Epoch [47/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0149, LR: 0.0009785725\n",
      "Epoch [48/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0298, LR: 0.0009784901\n",
      "Epoch [48/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0152, LR: 0.0009784076\n",
      "Epoch [48/500], Batch [30/110], Train Loss: 0.0131, Val Loss: 0.0138, LR: 0.0009783249\n",
      "Epoch [48/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0174, LR: 0.0009782421\n",
      "Epoch [48/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0188, LR: 0.0009781591\n",
      "Epoch [48/500], Batch [60/110], Train Loss: 0.0791, Val Loss: 0.0168, LR: 0.0009780760\n",
      "Epoch [48/500], Batch [70/110], Train Loss: 0.0058, Val Loss: 0.0141, LR: 0.0009779927\n",
      "Epoch [48/500], Batch [80/110], Train Loss: 0.0020, Val Loss: 0.0311, LR: 0.0009779092\n",
      "Epoch [48/500], Batch [90/110], Train Loss: 0.0056, Val Loss: 0.0333, LR: 0.0009778256\n",
      "Epoch [48/500], Batch [100/110], Train Loss: 0.0270, Val Loss: 0.0142, LR: 0.0009777419\n",
      "Epoch [48/500], Batch [110/110], Train Loss: 0.0025, Val Loss: 0.0154, LR: 0.0009776579\n",
      "Epoch [49/500], Batch [10/110], Train Loss: 0.0763, Val Loss: 0.0198, LR: 0.0009775739\n",
      "Epoch [49/500], Batch [20/110], Train Loss: 0.0013, Val Loss: 0.0143, LR: 0.0009774897\n",
      "Epoch [49/500], Batch [30/110], Train Loss: 0.0014, Val Loss: 0.0147, LR: 0.0009774053\n",
      "Epoch [49/500], Batch [40/110], Train Loss: 0.0055, Val Loss: 0.0212, LR: 0.0009773208\n",
      "Epoch [49/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0183, LR: 0.0009772361\n",
      "Epoch [49/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0135, LR: 0.0009771513\n",
      "Epoch [49/500], Batch [70/110], Train Loss: 0.0022, Val Loss: 0.0132, LR: 0.0009770663\n",
      "Epoch [49/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0174, LR: 0.0009769811\n",
      "Epoch [49/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0141, LR: 0.0009768958\n",
      "Epoch [49/500], Batch [100/110], Train Loss: 0.0201, Val Loss: 0.0128, LR: 0.0009768104\n",
      "Epoch [49/500], Batch [110/110], Train Loss: 0.0091, Val Loss: 0.0164, LR: 0.0009767248\n",
      "Epoch [50/500], Batch [10/110], Train Loss: 0.0264, Val Loss: 0.0138, LR: 0.0009766390\n",
      "Epoch [50/500], Batch [20/110], Train Loss: 0.0039, Val Loss: 0.0131, LR: 0.0009765531\n",
      "Epoch [50/500], Batch [30/110], Train Loss: 0.0075, Val Loss: 0.0169, LR: 0.0009764670\n",
      "Epoch [50/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0222, LR: 0.0009763808\n",
      "Epoch [50/500], Batch [50/110], Train Loss: 0.0031, Val Loss: 0.0127, LR: 0.0009762944\n",
      "Epoch [50/500], Batch [60/110], Train Loss: 0.0674, Val Loss: 0.0165, LR: 0.0009762079\n",
      "Epoch [50/500], Batch [70/110], Train Loss: 0.0106, Val Loss: 0.0153, LR: 0.0009761212\n",
      "Epoch [50/500], Batch [80/110], Train Loss: 0.0073, Val Loss: 0.0140, LR: 0.0009760344\n",
      "Epoch [50/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0130, LR: 0.0009759474\n",
      "Epoch [50/500], Batch [100/110], Train Loss: 0.0012, Val Loss: 0.0127, LR: 0.0009758603\n",
      "Epoch [50/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0157, LR: 0.0009757730\n",
      "Epoch [51/500], Batch [10/110], Train Loss: 0.0779, Val Loss: 0.0125, LR: 0.0009756855\n",
      "Epoch [51/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0180, LR: 0.0009755979\n",
      "Epoch [51/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0168, LR: 0.0009755102\n",
      "Epoch [51/500], Batch [40/110], Train Loss: 0.0015, Val Loss: 0.0170, LR: 0.0009754223\n",
      "Epoch [51/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0160, LR: 0.0009753342\n",
      "Epoch [51/500], Batch [60/110], Train Loss: 0.0185, Val Loss: 0.0133, LR: 0.0009752460\n",
      "Epoch [51/500], Batch [70/110], Train Loss: 0.0023, Val Loss: 0.0184, LR: 0.0009751576\n",
      "Epoch [51/500], Batch [80/110], Train Loss: 0.0091, Val Loss: 0.0149, LR: 0.0009750691\n",
      "Epoch [51/500], Batch [90/110], Train Loss: 0.0192, Val Loss: 0.0133, LR: 0.0009749804\n",
      "Epoch [51/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0225, LR: 0.0009748916\n",
      "Epoch [51/500], Batch [110/110], Train Loss: 0.0035, Val Loss: 0.0256, LR: 0.0009748026\n",
      "Confusion Matrix:\n",
      "[[627  25]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.96166   0.98045       652\n",
      "           1    0.97136   1.00000   0.98547       848\n",
      "\n",
      "    accuracy                        0.98333      1500\n",
      "   macro avg    0.98568   0.98083   0.98296      1500\n",
      "weighted avg    0.98381   0.98333   0.98329      1500\n",
      "\n",
      "Total Errors: 25\n",
      "Index: 7, Predicted: 1, Actual: 0\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 258, Predicted: 1, Actual: 0\n",
      "Index: 299, Predicted: 1, Actual: 0\n",
      "Index: 307, Predicted: 1, Actual: 0\n",
      "Epoch 51: OK- Accuracy: 0.98333, Precision: 0.97136, Recall: 1.00000, F1: 0.98547, ROC AUC: 0.98083, AUPR (PR-AUC): 0.97136, Sensitivity: 1.00000, Specificity: 0.96166, Far: 0.03834355828220859, False Positive Rate (FPR): 0.03834, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 196.52 MB\n",
      "Epoch [52/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0128, LR: 0.0009747135\n",
      "Epoch [52/500], Batch [20/110], Train Loss: 0.0050, Val Loss: 0.0119, LR: 0.0009746242\n",
      "Epoch [52/500], Batch [30/110], Train Loss: 0.0019, Val Loss: 0.0159, LR: 0.0009745347\n",
      "Epoch [52/500], Batch [40/110], Train Loss: 0.0337, Val Loss: 0.0145, LR: 0.0009744451\n",
      "Epoch [52/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0124, LR: 0.0009743554\n",
      "Epoch [52/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0157, LR: 0.0009742655\n",
      "Epoch [52/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0209, LR: 0.0009741754\n",
      "Epoch [52/500], Batch [80/110], Train Loss: 0.0536, Val Loss: 0.0118, LR: 0.0009740852\n",
      "Epoch [52/500], Batch [90/110], Train Loss: 0.0903, Val Loss: 0.0203, LR: 0.0009739948\n",
      "Epoch [52/500], Batch [100/110], Train Loss: 0.0593, Val Loss: 0.0143, LR: 0.0009739043\n",
      "Epoch [52/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0157, LR: 0.0009738137\n",
      "Epoch [53/500], Batch [10/110], Train Loss: 0.0035, Val Loss: 0.0191, LR: 0.0009737228\n",
      "Epoch [53/500], Batch [20/110], Train Loss: 0.0021, Val Loss: 0.0120, LR: 0.0009736319\n",
      "Epoch [53/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0121, LR: 0.0009735407\n",
      "Epoch [53/500], Batch [40/110], Train Loss: 0.0375, Val Loss: 0.0179, LR: 0.0009734495\n",
      "Epoch [53/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0184, LR: 0.0009733580\n",
      "Epoch [53/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0114, LR: 0.0009732664\n",
      "Epoch [53/500], Batch [70/110], Train Loss: 0.0022, Val Loss: 0.0141, LR: 0.0009731747\n",
      "Epoch [53/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0128, LR: 0.0009730828\n",
      "Epoch [53/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0168, LR: 0.0009729908\n",
      "Epoch [53/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0123, LR: 0.0009728986\n",
      "Epoch [53/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0114, LR: 0.0009728062\n",
      "Epoch [54/500], Batch [10/110], Train Loss: 0.0546, Val Loss: 0.0136, LR: 0.0009727137\n",
      "Epoch [54/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0119, LR: 0.0009726211\n",
      "Epoch [54/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0115, LR: 0.0009725283\n",
      "Epoch [54/500], Batch [40/110], Train Loss: 0.0050, Val Loss: 0.0252, LR: 0.0009724353\n",
      "Epoch [54/500], Batch [50/110], Train Loss: 0.0758, Val Loss: 0.0173, LR: 0.0009723422\n",
      "Epoch [54/500], Batch [60/110], Train Loss: 0.0192, Val Loss: 0.0142, LR: 0.0009722489\n",
      "Epoch [54/500], Batch [70/110], Train Loss: 0.0299, Val Loss: 0.0113, LR: 0.0009721555\n",
      "Epoch [54/500], Batch [80/110], Train Loss: 0.0042, Val Loss: 0.0124, LR: 0.0009720619\n",
      "Epoch [54/500], Batch [90/110], Train Loss: 0.0140, Val Loss: 0.0134, LR: 0.0009719682\n",
      "Epoch [54/500], Batch [100/110], Train Loss: 0.0366, Val Loss: 0.0148, LR: 0.0009718743\n",
      "Epoch [54/500], Batch [110/110], Train Loss: 0.0348, Val Loss: 0.0114, LR: 0.0009717803\n",
      "Epoch [55/500], Batch [10/110], Train Loss: 0.0036, Val Loss: 0.0114, LR: 0.0009716861\n",
      "Epoch [55/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0150, LR: 0.0009715918\n",
      "Epoch [55/500], Batch [30/110], Train Loss: 0.0206, Val Loss: 0.0109, LR: 0.0009714973\n",
      "Epoch [55/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0119, LR: 0.0009714027\n",
      "Epoch [55/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0111, LR: 0.0009713079\n",
      "Epoch [55/500], Batch [60/110], Train Loss: 0.0255, Val Loss: 0.0112, LR: 0.0009712130\n",
      "Epoch [55/500], Batch [70/110], Train Loss: 0.1310, Val Loss: 0.0303, LR: 0.0009711179\n",
      "Epoch [55/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0251, LR: 0.0009710226\n",
      "Epoch [55/500], Batch [90/110], Train Loss: 0.0029, Val Loss: 0.0207, LR: 0.0009709272\n",
      "Epoch [55/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0114, LR: 0.0009708317\n",
      "Epoch [55/500], Batch [110/110], Train Loss: 0.0919, Val Loss: 0.0129, LR: 0.0009707360\n",
      "Epoch [56/500], Batch [10/110], Train Loss: 0.0187, Val Loss: 0.0110, LR: 0.0009706401\n",
      "Epoch [56/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0110, LR: 0.0009705441\n",
      "Epoch [56/500], Batch [30/110], Train Loss: 0.0022, Val Loss: 0.0111, LR: 0.0009704480\n",
      "Epoch [56/500], Batch [40/110], Train Loss: 0.0136, Val Loss: 0.0131, LR: 0.0009703517\n",
      "Epoch [56/500], Batch [50/110], Train Loss: 0.0601, Val Loss: 0.0104, LR: 0.0009702552\n",
      "Epoch [56/500], Batch [60/110], Train Loss: 0.0804, Val Loss: 0.0169, LR: 0.0009701586\n",
      "Epoch [56/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0192, LR: 0.0009700618\n",
      "Epoch [56/500], Batch [80/110], Train Loss: 0.0224, Val Loss: 0.0119, LR: 0.0009699649\n",
      "Epoch [56/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0169, LR: 0.0009698678\n",
      "Epoch [56/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0150, LR: 0.0009697706\n",
      "Epoch [56/500], Batch [110/110], Train Loss: 0.0024, Val Loss: 0.0157, LR: 0.0009696733\n",
      "Epoch [57/500], Batch [10/110], Train Loss: 0.1281, Val Loss: 0.0292, LR: 0.0009695757\n",
      "Epoch [57/500], Batch [20/110], Train Loss: 0.0545, Val Loss: 0.0125, LR: 0.0009694781\n",
      "Epoch [57/500], Batch [30/110], Train Loss: 0.0195, Val Loss: 0.0126, LR: 0.0009693802\n",
      "Epoch [57/500], Batch [40/110], Train Loss: 0.0017, Val Loss: 0.0105, LR: 0.0009692823\n",
      "Epoch [57/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0155, LR: 0.0009691841\n",
      "Epoch [57/500], Batch [60/110], Train Loss: 0.0158, Val Loss: 0.0116, LR: 0.0009690859\n",
      "Epoch [57/500], Batch [70/110], Train Loss: 0.0510, Val Loss: 0.0126, LR: 0.0009689874\n",
      "Epoch [57/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0110, LR: 0.0009688888\n",
      "Epoch [57/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0117, LR: 0.0009687901\n",
      "Epoch [57/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0105, LR: 0.0009686912\n",
      "Epoch [57/500], Batch [110/110], Train Loss: 0.1260, Val Loss: 0.0124, LR: 0.0009685922\n",
      "Epoch [58/500], Batch [10/110], Train Loss: 0.0089, Val Loss: 0.0117, LR: 0.0009684930\n",
      "Epoch [58/500], Batch [20/110], Train Loss: 0.0224, Val Loss: 0.0098, LR: 0.0009683937\n",
      "Epoch [58/500], Batch [30/110], Train Loss: 0.0437, Val Loss: 0.0100, LR: 0.0009682942\n",
      "Epoch [58/500], Batch [40/110], Train Loss: 0.0015, Val Loss: 0.0155, LR: 0.0009681945\n",
      "Epoch [58/500], Batch [50/110], Train Loss: 0.0031, Val Loss: 0.0097, LR: 0.0009680947\n",
      "Epoch [58/500], Batch [60/110], Train Loss: 0.0273, Val Loss: 0.0098, LR: 0.0009679948\n",
      "Epoch [58/500], Batch [70/110], Train Loss: 0.0059, Val Loss: 0.0140, LR: 0.0009678947\n",
      "Epoch [58/500], Batch [80/110], Train Loss: 0.0053, Val Loss: 0.0101, LR: 0.0009677945\n",
      "Epoch [58/500], Batch [90/110], Train Loss: 0.0038, Val Loss: 0.0214, LR: 0.0009676941\n",
      "Epoch [58/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0102, LR: 0.0009675935\n",
      "Epoch [58/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0115, LR: 0.0009674928\n",
      "Epoch [59/500], Batch [10/110], Train Loss: 0.3561, Val Loss: 0.0135, LR: 0.0009673920\n",
      "Epoch [59/500], Batch [20/110], Train Loss: 0.0082, Val Loss: 0.0101, LR: 0.0009672910\n",
      "Epoch [59/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0101, LR: 0.0009671898\n",
      "Epoch [59/500], Batch [40/110], Train Loss: 0.0035, Val Loss: 0.0154, LR: 0.0009670885\n",
      "Epoch [59/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0105, LR: 0.0009669871\n",
      "Epoch [59/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0107, LR: 0.0009668855\n",
      "Epoch [59/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0254, LR: 0.0009667837\n",
      "Epoch [59/500], Batch [80/110], Train Loss: 0.0460, Val Loss: 0.0142, LR: 0.0009666818\n",
      "Epoch [59/500], Batch [90/110], Train Loss: 0.0429, Val Loss: 0.0112, LR: 0.0009665798\n",
      "Epoch [59/500], Batch [100/110], Train Loss: 0.0350, Val Loss: 0.0115, LR: 0.0009664776\n",
      "Epoch [59/500], Batch [110/110], Train Loss: 0.0064, Val Loss: 0.0179, LR: 0.0009663752\n",
      "Epoch [60/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0106, LR: 0.0009662727\n",
      "Epoch [60/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0121, LR: 0.0009661700\n",
      "Epoch [60/500], Batch [30/110], Train Loss: 0.0047, Val Loss: 0.0149, LR: 0.0009660672\n",
      "Epoch [60/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0113, LR: 0.0009659643\n",
      "Epoch [60/500], Batch [50/110], Train Loss: 0.0254, Val Loss: 0.0104, LR: 0.0009658612\n",
      "Epoch [60/500], Batch [60/110], Train Loss: 0.0011, Val Loss: 0.0100, LR: 0.0009657579\n",
      "Epoch [60/500], Batch [70/110], Train Loss: 0.0063, Val Loss: 0.0115, LR: 0.0009656545\n",
      "Epoch [60/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0171, LR: 0.0009655509\n",
      "Epoch [60/500], Batch [90/110], Train Loss: 0.0551, Val Loss: 0.0093, LR: 0.0009654472\n",
      "Epoch [60/500], Batch [100/110], Train Loss: 0.0248, Val Loss: 0.0097, LR: 0.0009653434\n",
      "Epoch [60/500], Batch [110/110], Train Loss: 0.0026, Val Loss: 0.0128, LR: 0.0009652394\n",
      "Epoch [61/500], Batch [10/110], Train Loss: 0.0048, Val Loss: 0.0127, LR: 0.0009651352\n",
      "Epoch [61/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0114, LR: 0.0009650309\n",
      "Epoch [61/500], Batch [30/110], Train Loss: 0.0038, Val Loss: 0.0145, LR: 0.0009649264\n",
      "Epoch [61/500], Batch [40/110], Train Loss: 0.0366, Val Loss: 0.0094, LR: 0.0009648218\n",
      "Epoch [61/500], Batch [50/110], Train Loss: 0.0041, Val Loss: 0.0148, LR: 0.0009647171\n",
      "Epoch [61/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0127, LR: 0.0009646122\n",
      "Epoch [61/500], Batch [70/110], Train Loss: 0.0039, Val Loss: 0.0103, LR: 0.0009645071\n",
      "Epoch [61/500], Batch [80/110], Train Loss: 0.2603, Val Loss: 0.0095, LR: 0.0009644019\n",
      "Epoch [61/500], Batch [90/110], Train Loss: 0.0342, Val Loss: 0.0096, LR: 0.0009642965\n",
      "Epoch [61/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0191, LR: 0.0009641910\n",
      "Epoch [61/500], Batch [110/110], Train Loss: 0.0072, Val Loss: 0.0093, LR: 0.0009640854\n",
      "Confusion Matrix:\n",
      "[[648   4]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99387   0.99692       652\n",
      "           1    0.99531   1.00000   0.99765       848\n",
      "\n",
      "    accuracy                        0.99733      1500\n",
      "   macro avg    0.99765   0.99693   0.99729      1500\n",
      "weighted avg    0.99735   0.99733   0.99733      1500\n",
      "\n",
      "Total Errors: 4\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 307, Predicted: 1, Actual: 0\n",
      "Index: 1218, Predicted: 1, Actual: 0\n",
      "Index: 1397, Predicted: 1, Actual: 0\n",
      "Epoch 61: OK- Accuracy: 0.99733, Precision: 0.99531, Recall: 1.00000, F1: 0.99765, ROC AUC: 0.99693, AUPR (PR-AUC): 0.99531, Sensitivity: 1.00000, Specificity: 0.99387, Far: 0.006134969325153374, False Positive Rate (FPR): 0.00613, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 196.54 MB\n",
      "Epoch [62/500], Batch [10/110], Train Loss: 0.0548, Val Loss: 0.0165, LR: 0.0009639795\n",
      "Epoch [62/500], Batch [20/110], Train Loss: 0.0164, Val Loss: 0.0092, LR: 0.0009638736\n",
      "Epoch [62/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0091, LR: 0.0009637675\n",
      "Epoch [62/500], Batch [40/110], Train Loss: 0.3602, Val Loss: 0.0121, LR: 0.0009636612\n",
      "Epoch [62/500], Batch [50/110], Train Loss: 0.4299, Val Loss: 0.0156, LR: 0.0009635548\n",
      "Epoch [62/500], Batch [60/110], Train Loss: 0.0113, Val Loss: 0.0104, LR: 0.0009634482\n",
      "Epoch [62/500], Batch [70/110], Train Loss: 0.0056, Val Loss: 0.0142, LR: 0.0009633415\n",
      "Epoch [62/500], Batch [80/110], Train Loss: 0.0245, Val Loss: 0.0092, LR: 0.0009632347\n",
      "Epoch [62/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0104, LR: 0.0009631277\n",
      "Epoch [62/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0108, LR: 0.0009630205\n",
      "Epoch [62/500], Batch [110/110], Train Loss: 0.3549, Val Loss: 0.0132, LR: 0.0009629132\n",
      "Epoch [63/500], Batch [10/110], Train Loss: 0.0157, Val Loss: 0.0090, LR: 0.0009628058\n",
      "Epoch [63/500], Batch [20/110], Train Loss: 0.0009, Val Loss: 0.0111, LR: 0.0009626982\n",
      "Epoch [63/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0113, LR: 0.0009625904\n",
      "Epoch [63/500], Batch [40/110], Train Loss: 0.0092, Val Loss: 0.0088, LR: 0.0009624825\n",
      "Epoch [63/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0181, LR: 0.0009623745\n",
      "Epoch [63/500], Batch [60/110], Train Loss: 0.0133, Val Loss: 0.0090, LR: 0.0009622662\n",
      "Epoch [63/500], Batch [70/110], Train Loss: 0.0465, Val Loss: 0.0154, LR: 0.0009621579\n",
      "Epoch [63/500], Batch [80/110], Train Loss: 0.0022, Val Loss: 0.0084, LR: 0.0009620494\n",
      "Epoch [63/500], Batch [90/110], Train Loss: 0.0157, Val Loss: 0.0084, LR: 0.0009619408\n",
      "Epoch [63/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0202, LR: 0.0009618320\n",
      "Epoch [63/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0095, LR: 0.0009617230\n",
      "Epoch [64/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0105, LR: 0.0009616139\n",
      "Epoch [64/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0215, LR: 0.0009615047\n",
      "Epoch [64/500], Batch [30/110], Train Loss: 0.0332, Val Loss: 0.0089, LR: 0.0009613953\n",
      "Epoch [64/500], Batch [40/110], Train Loss: 0.0250, Val Loss: 0.0088, LR: 0.0009612857\n",
      "Epoch [64/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0178, LR: 0.0009611760\n",
      "Epoch [64/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0088, LR: 0.0009610662\n",
      "Epoch [64/500], Batch [70/110], Train Loss: 0.0123, Val Loss: 0.0090, LR: 0.0009609562\n",
      "Epoch [64/500], Batch [80/110], Train Loss: 0.0022, Val Loss: 0.0171, LR: 0.0009608461\n",
      "Epoch [64/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0089, LR: 0.0009607358\n",
      "Epoch [64/500], Batch [100/110], Train Loss: 0.0751, Val Loss: 0.0128, LR: 0.0009606253\n",
      "Epoch [64/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0129, LR: 0.0009605148\n",
      "Epoch [65/500], Batch [10/110], Train Loss: 0.0018, Val Loss: 0.0093, LR: 0.0009604040\n",
      "Epoch [65/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0129, LR: 0.0009602932\n",
      "Epoch [65/500], Batch [30/110], Train Loss: 0.0018, Val Loss: 0.0117, LR: 0.0009601821\n",
      "Epoch [65/500], Batch [40/110], Train Loss: 0.0077, Val Loss: 0.0115, LR: 0.0009600709\n",
      "Epoch [65/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0099, LR: 0.0009599596\n",
      "Epoch [65/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0116, LR: 0.0009598481\n",
      "Epoch [65/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0131, LR: 0.0009597365\n",
      "Epoch [65/500], Batch [80/110], Train Loss: 0.0234, Val Loss: 0.0090, LR: 0.0009596247\n",
      "Epoch [65/500], Batch [90/110], Train Loss: 0.0068, Val Loss: 0.0098, LR: 0.0009595128\n",
      "Epoch [65/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0086, LR: 0.0009594008\n",
      "Epoch [65/500], Batch [110/110], Train Loss: 0.0024, Val Loss: 0.0090, LR: 0.0009592885\n",
      "Epoch [66/500], Batch [10/110], Train Loss: 0.0727, Val Loss: 0.0121, LR: 0.0009591762\n",
      "Epoch [66/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0084, LR: 0.0009590637\n",
      "Epoch [66/500], Batch [30/110], Train Loss: 0.0784, Val Loss: 0.0165, LR: 0.0009589510\n",
      "Epoch [66/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0083, LR: 0.0009588382\n",
      "Epoch [66/500], Batch [50/110], Train Loss: 0.0140, Val Loss: 0.0083, LR: 0.0009587252\n",
      "Epoch [66/500], Batch [60/110], Train Loss: 0.0796, Val Loss: 0.0100, LR: 0.0009586121\n",
      "Epoch [66/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0082, LR: 0.0009584989\n",
      "Epoch [66/500], Batch [80/110], Train Loss: 0.0991, Val Loss: 0.0122, LR: 0.0009583855\n",
      "Epoch [66/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0090, LR: 0.0009582719\n",
      "Epoch [66/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0106, LR: 0.0009581582\n",
      "Epoch [66/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0112, LR: 0.0009580444\n",
      "Epoch [67/500], Batch [10/110], Train Loss: 0.0013, Val Loss: 0.0151, LR: 0.0009579304\n",
      "Epoch [67/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0127, LR: 0.0009578162\n",
      "Epoch [67/500], Batch [30/110], Train Loss: 0.0019, Val Loss: 0.0078, LR: 0.0009577020\n",
      "Epoch [67/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0085, LR: 0.0009575875\n",
      "Epoch [67/500], Batch [50/110], Train Loss: 0.0080, Val Loss: 0.0082, LR: 0.0009574729\n",
      "Epoch [67/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0074, LR: 0.0009573582\n",
      "Epoch [67/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0091, LR: 0.0009572433\n",
      "Epoch [67/500], Batch [80/110], Train Loss: 0.1367, Val Loss: 0.0183, LR: 0.0009571283\n",
      "Epoch [67/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0084, LR: 0.0009570131\n",
      "Epoch [67/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0095, LR: 0.0009568978\n",
      "Epoch [67/500], Batch [110/110], Train Loss: 0.0050, Val Loss: 0.0098, LR: 0.0009567823\n",
      "Epoch [68/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0084, LR: 0.0009566667\n",
      "Epoch [68/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0225, LR: 0.0009565510\n",
      "Epoch [68/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0085, LR: 0.0009564350\n",
      "Epoch [68/500], Batch [40/110], Train Loss: 0.0062, Val Loss: 0.0086, LR: 0.0009563190\n",
      "Epoch [68/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0136, LR: 0.0009562028\n",
      "Epoch [68/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0127, LR: 0.0009560864\n",
      "Epoch [68/500], Batch [70/110], Train Loss: 0.0136, Val Loss: 0.0098, LR: 0.0009559699\n",
      "Epoch [68/500], Batch [80/110], Train Loss: 0.0031, Val Loss: 0.0083, LR: 0.0009558533\n",
      "Epoch [68/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0083, LR: 0.0009557365\n",
      "Epoch [68/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0114, LR: 0.0009556195\n",
      "Epoch [68/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0084, LR: 0.0009555025\n",
      "Epoch [69/500], Batch [10/110], Train Loss: 0.0058, Val Loss: 0.0078, LR: 0.0009553852\n",
      "Epoch [69/500], Batch [20/110], Train Loss: 0.0092, Val Loss: 0.0092, LR: 0.0009552678\n",
      "Epoch [69/500], Batch [30/110], Train Loss: 0.0049, Val Loss: 0.0104, LR: 0.0009551503\n",
      "Epoch [69/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0163, LR: 0.0009550326\n",
      "Epoch [69/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0009549148\n",
      "Epoch [69/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0074, LR: 0.0009547968\n",
      "Epoch [69/500], Batch [70/110], Train Loss: 0.0047, Val Loss: 0.0078, LR: 0.0009546787\n",
      "Epoch [69/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0099, LR: 0.0009545605\n",
      "Epoch [69/500], Batch [90/110], Train Loss: 0.0156, Val Loss: 0.0077, LR: 0.0009544420\n",
      "Epoch [69/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0099, LR: 0.0009543235\n",
      "Epoch [69/500], Batch [110/110], Train Loss: 0.0219, Val Loss: 0.0077, LR: 0.0009542048\n",
      "Epoch [70/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0150, LR: 0.0009540859\n",
      "Epoch [70/500], Batch [20/110], Train Loss: 0.0952, Val Loss: 0.0107, LR: 0.0009539669\n",
      "Epoch [70/500], Batch [30/110], Train Loss: 0.0126, Val Loss: 0.0111, LR: 0.0009538478\n",
      "Epoch [70/500], Batch [40/110], Train Loss: 0.0309, Val Loss: 0.0105, LR: 0.0009537285\n",
      "Epoch [70/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0175, LR: 0.0009536091\n",
      "Epoch [70/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0140, LR: 0.0009534895\n",
      "Epoch [70/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0079, LR: 0.0009533698\n",
      "Epoch [70/500], Batch [80/110], Train Loss: 0.0262, Val Loss: 0.0109, LR: 0.0009532499\n",
      "Epoch [70/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0277, LR: 0.0009531299\n",
      "Epoch [70/500], Batch [100/110], Train Loss: 0.0016, Val Loss: 0.0075, LR: 0.0009530097\n",
      "Epoch [70/500], Batch [110/110], Train Loss: 0.0256, Val Loss: 0.0082, LR: 0.0009528894\n",
      "Epoch [71/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0086, LR: 0.0009527689\n",
      "Epoch [71/500], Batch [20/110], Train Loss: 0.0341, Val Loss: 0.0078, LR: 0.0009526483\n",
      "Epoch [71/500], Batch [30/110], Train Loss: 0.0029, Val Loss: 0.0075, LR: 0.0009525276\n",
      "Epoch [71/500], Batch [40/110], Train Loss: 0.0013, Val Loss: 0.0107, LR: 0.0009524067\n",
      "Epoch [71/500], Batch [50/110], Train Loss: 0.0050, Val Loss: 0.0098, LR: 0.0009522856\n",
      "Epoch [71/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0098, LR: 0.0009521644\n",
      "Epoch [71/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0097, LR: 0.0009520431\n",
      "Epoch [71/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0103, LR: 0.0009519216\n",
      "Epoch [71/500], Batch [90/110], Train Loss: 0.0122, Val Loss: 0.0076, LR: 0.0009518000\n",
      "Epoch [71/500], Batch [100/110], Train Loss: 0.0023, Val Loss: 0.0079, LR: 0.0009516782\n",
      "Epoch [71/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0136, LR: 0.0009515563\n",
      "Confusion Matrix:\n",
      "[[632  20]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.96933   0.98442       652\n",
      "           1    0.97696   1.00000   0.98834       848\n",
      "\n",
      "    accuracy                        0.98667      1500\n",
      "   macro avg    0.98848   0.98466   0.98638      1500\n",
      "weighted avg    0.98697   0.98667   0.98664      1500\n",
      "\n",
      "Total Errors: 20\n",
      "Index: 7, Predicted: 1, Actual: 0\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 258, Predicted: 1, Actual: 0\n",
      "Index: 307, Predicted: 1, Actual: 0\n",
      "Index: 625, Predicted: 1, Actual: 0\n",
      "Epoch 71: OK- Accuracy: 0.98667, Precision: 0.97696, Recall: 1.00000, F1: 0.98834, ROC AUC: 0.98466, AUPR (PR-AUC): 0.97696, Sensitivity: 1.00000, Specificity: 0.96933, Far: 0.03067484662576687, False Positive Rate (FPR): 0.03067, False Negative Rate (FNR): 0.00000, Runtime: 0.039 sec , Memory Usage: 196.44 MB\n",
      "Epoch [72/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0108, LR: 0.0009514342\n",
      "Epoch [72/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0009513120\n",
      "Epoch [72/500], Batch [30/110], Train Loss: 0.0127, Val Loss: 0.0075, LR: 0.0009511897\n",
      "Epoch [72/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0117, LR: 0.0009510672\n",
      "Epoch [72/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0075, LR: 0.0009509445\n",
      "Epoch [72/500], Batch [60/110], Train Loss: 0.0214, Val Loss: 0.0075, LR: 0.0009508217\n",
      "Epoch [72/500], Batch [70/110], Train Loss: 0.0034, Val Loss: 0.0134, LR: 0.0009506988\n",
      "Epoch [72/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0077, LR: 0.0009505757\n",
      "Epoch [72/500], Batch [90/110], Train Loss: 0.0235, Val Loss: 0.0073, LR: 0.0009504525\n",
      "Epoch [72/500], Batch [100/110], Train Loss: 0.0015, Val Loss: 0.0090, LR: 0.0009503291\n",
      "Epoch [72/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0009502056\n",
      "Epoch [73/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0162, LR: 0.0009500819\n",
      "Epoch [73/500], Batch [20/110], Train Loss: 0.0498, Val Loss: 0.0084, LR: 0.0009499581\n",
      "Epoch [73/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0009498342\n",
      "Epoch [73/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0077, LR: 0.0009497101\n",
      "Epoch [73/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0009495858\n",
      "Epoch [73/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0071, LR: 0.0009494614\n",
      "Epoch [73/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0009493369\n",
      "Epoch [73/500], Batch [80/110], Train Loss: 0.0023, Val Loss: 0.0146, LR: 0.0009492122\n",
      "Epoch [73/500], Batch [90/110], Train Loss: 0.0019, Val Loss: 0.0104, LR: 0.0009490874\n",
      "Epoch [73/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0077, LR: 0.0009489624\n",
      "Epoch [73/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0177, LR: 0.0009488373\n",
      "Epoch [74/500], Batch [10/110], Train Loss: 0.0049, Val Loss: 0.0073, LR: 0.0009487121\n",
      "Epoch [74/500], Batch [20/110], Train Loss: 0.0488, Val Loss: 0.0083, LR: 0.0009485866\n",
      "Epoch [74/500], Batch [30/110], Train Loss: 0.0188, Val Loss: 0.0070, LR: 0.0009484611\n",
      "Epoch [74/500], Batch [40/110], Train Loss: 0.0009, Val Loss: 0.0081, LR: 0.0009483354\n",
      "Epoch [74/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0083, LR: 0.0009482096\n",
      "Epoch [74/500], Batch [60/110], Train Loss: 0.0110, Val Loss: 0.0078, LR: 0.0009480836\n",
      "Epoch [74/500], Batch [70/110], Train Loss: 0.0288, Val Loss: 0.0068, LR: 0.0009479575\n",
      "Epoch [74/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0147, LR: 0.0009478312\n",
      "Epoch [74/500], Batch [90/110], Train Loss: 0.0021, Val Loss: 0.0110, LR: 0.0009477048\n",
      "Epoch [74/500], Batch [100/110], Train Loss: 0.0471, Val Loss: 0.0070, LR: 0.0009475782\n",
      "Epoch [74/500], Batch [110/110], Train Loss: 0.0214, Val Loss: 0.0087, LR: 0.0009474515\n",
      "Epoch [75/500], Batch [10/110], Train Loss: 0.0016, Val Loss: 0.0148, LR: 0.0009473247\n",
      "Epoch [75/500], Batch [20/110], Train Loss: 0.0127, Val Loss: 0.0068, LR: 0.0009471977\n",
      "Epoch [75/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0009470705\n",
      "Epoch [75/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0082, LR: 0.0009469432\n",
      "Epoch [75/500], Batch [50/110], Train Loss: 0.0437, Val Loss: 0.0077, LR: 0.0009468158\n",
      "Epoch [75/500], Batch [60/110], Train Loss: 0.0035, Val Loss: 0.0109, LR: 0.0009466882\n",
      "Epoch [75/500], Batch [70/110], Train Loss: 0.0336, Val Loss: 0.0078, LR: 0.0009465605\n",
      "Epoch [75/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0072, LR: 0.0009464327\n",
      "Epoch [75/500], Batch [90/110], Train Loss: 0.0242, Val Loss: 0.0066, LR: 0.0009463047\n",
      "Epoch [75/500], Batch [100/110], Train Loss: 0.0051, Val Loss: 0.0078, LR: 0.0009461765\n",
      "Epoch [75/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0112, LR: 0.0009460482\n",
      "Epoch [76/500], Batch [10/110], Train Loss: 0.0066, Val Loss: 0.0063, LR: 0.0009459198\n",
      "Epoch [76/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0071, LR: 0.0009457912\n",
      "Epoch [76/500], Batch [30/110], Train Loss: 0.0631, Val Loss: 0.0092, LR: 0.0009456625\n",
      "Epoch [76/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0063, LR: 0.0009455336\n",
      "Epoch [76/500], Batch [50/110], Train Loss: 0.0061, Val Loss: 0.0066, LR: 0.0009454046\n",
      "Epoch [76/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0092, LR: 0.0009452755\n",
      "Epoch [76/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0061, LR: 0.0009451462\n",
      "Epoch [76/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0099, LR: 0.0009450167\n",
      "Epoch [76/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0060, LR: 0.0009448871\n",
      "Epoch [76/500], Batch [100/110], Train Loss: 0.0376, Val Loss: 0.0092, LR: 0.0009447574\n",
      "Epoch [76/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0255, LR: 0.0009446275\n",
      "Epoch [77/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0120, LR: 0.0009444975\n",
      "Epoch [77/500], Batch [20/110], Train Loss: 0.0011, Val Loss: 0.0087, LR: 0.0009443674\n",
      "Epoch [77/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0009442371\n",
      "Epoch [77/500], Batch [40/110], Train Loss: 0.0369, Val Loss: 0.0083, LR: 0.0009441066\n",
      "Epoch [77/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0065, LR: 0.0009439760\n",
      "Epoch [77/500], Batch [60/110], Train Loss: 0.0112, Val Loss: 0.0067, LR: 0.0009438453\n",
      "Epoch [77/500], Batch [70/110], Train Loss: 0.0237, Val Loss: 0.0073, LR: 0.0009437144\n",
      "Epoch [77/500], Batch [80/110], Train Loss: 0.0014, Val Loss: 0.0070, LR: 0.0009435834\n",
      "Epoch [77/500], Batch [90/110], Train Loss: 0.0278, Val Loss: 0.0075, LR: 0.0009434522\n",
      "Epoch [77/500], Batch [100/110], Train Loss: 0.0052, Val Loss: 0.0078, LR: 0.0009433209\n",
      "Epoch [77/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0160, LR: 0.0009431895\n",
      "Epoch [78/500], Batch [10/110], Train Loss: 0.0381, Val Loss: 0.0124, LR: 0.0009430579\n",
      "Epoch [78/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0066, LR: 0.0009429262\n",
      "Epoch [78/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0079, LR: 0.0009427943\n",
      "Epoch [78/500], Batch [40/110], Train Loss: 0.0083, Val Loss: 0.0066, LR: 0.0009426623\n",
      "Epoch [78/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0067, LR: 0.0009425301\n",
      "Epoch [78/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0067, LR: 0.0009423978\n",
      "Epoch [78/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0076, LR: 0.0009422654\n",
      "Epoch [78/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0066, LR: 0.0009421328\n",
      "Epoch [78/500], Batch [90/110], Train Loss: 0.0260, Val Loss: 0.0063, LR: 0.0009420000\n",
      "Epoch [78/500], Batch [100/110], Train Loss: 0.0071, Val Loss: 0.0066, LR: 0.0009418672\n",
      "Epoch [78/500], Batch [110/110], Train Loss: 0.0027, Val Loss: 0.0073, LR: 0.0009417342\n",
      "Epoch [79/500], Batch [10/110], Train Loss: 0.0157, Val Loss: 0.0060, LR: 0.0009416010\n",
      "Epoch [79/500], Batch [20/110], Train Loss: 0.0039, Val Loss: 0.0061, LR: 0.0009414677\n",
      "Epoch [79/500], Batch [30/110], Train Loss: 0.0024, Val Loss: 0.0066, LR: 0.0009413343\n",
      "Epoch [79/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0071, LR: 0.0009412007\n",
      "Epoch [79/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0009410669\n",
      "Epoch [79/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0057, LR: 0.0009409331\n",
      "Epoch [79/500], Batch [70/110], Train Loss: 0.0107, Val Loss: 0.0058, LR: 0.0009407990\n",
      "Epoch [79/500], Batch [80/110], Train Loss: 0.0401, Val Loss: 0.0097, LR: 0.0009406649\n",
      "Epoch [79/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0009405306\n",
      "Epoch [79/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0060, LR: 0.0009403962\n",
      "Epoch [79/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0064, LR: 0.0009402616\n",
      "Epoch [80/500], Batch [10/110], Train Loss: 0.0009, Val Loss: 0.0072, LR: 0.0009401268\n",
      "Epoch [80/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0058, LR: 0.0009399920\n",
      "Epoch [80/500], Batch [30/110], Train Loss: 0.0016, Val Loss: 0.0058, LR: 0.0009398570\n",
      "Epoch [80/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0009397218\n",
      "Epoch [80/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0196, LR: 0.0009395865\n",
      "Epoch [80/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0071, LR: 0.0009394511\n",
      "Epoch [80/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0067, LR: 0.0009393155\n",
      "Epoch [80/500], Batch [80/110], Train Loss: 0.0195, Val Loss: 0.0059, LR: 0.0009391798\n",
      "Epoch [80/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0088, LR: 0.0009390439\n",
      "Epoch [80/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0059, LR: 0.0009389079\n",
      "Epoch [80/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0063, LR: 0.0009387718\n",
      "Epoch [81/500], Batch [10/110], Train Loss: 0.0052, Val Loss: 0.0080, LR: 0.0009386355\n",
      "Epoch [81/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0065, LR: 0.0009384991\n",
      "Epoch [81/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0069, LR: 0.0009383625\n",
      "Epoch [81/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0076, LR: 0.0009382258\n",
      "Epoch [81/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0067, LR: 0.0009380890\n",
      "Epoch [81/500], Batch [60/110], Train Loss: 0.0174, Val Loss: 0.0059, LR: 0.0009379520\n",
      "Epoch [81/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0072, LR: 0.0009378149\n",
      "Epoch [81/500], Batch [80/110], Train Loss: 0.0134, Val Loss: 0.0060, LR: 0.0009376776\n",
      "Epoch [81/500], Batch [90/110], Train Loss: 0.0148, Val Loss: 0.0059, LR: 0.0009375402\n",
      "Epoch [81/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0060, LR: 0.0009374026\n",
      "Epoch [81/500], Batch [110/110], Train Loss: 0.0176, Val Loss: 0.0075, LR: 0.0009372649\n",
      "Confusion Matrix:\n",
      "[[642  10]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.98466   0.99227       652\n",
      "           1    0.98834   1.00000   0.99414       848\n",
      "\n",
      "    accuracy                        0.99333      1500\n",
      "   macro avg    0.99417   0.99233   0.99321      1500\n",
      "weighted avg    0.99341   0.99333   0.99333      1500\n",
      "\n",
      "Total Errors: 10\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 258, Predicted: 1, Actual: 0\n",
      "Index: 625, Predicted: 1, Actual: 0\n",
      "Index: 776, Predicted: 1, Actual: 0\n",
      "Index: 877, Predicted: 1, Actual: 0\n",
      "Epoch 81: OK- Accuracy: 0.99333, Precision: 0.98834, Recall: 1.00000, F1: 0.99414, ROC AUC: 0.99233, AUPR (PR-AUC): 0.98834, Sensitivity: 1.00000, Specificity: 0.98466, Far: 0.015337423312883436, False Positive Rate (FPR): 0.01534, False Negative Rate (FNR): 0.00000, Runtime: 0.027 sec , Memory Usage: 196.44 MB\n",
      "Epoch [82/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0009371271\n",
      "Epoch [82/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0060, LR: 0.0009369891\n",
      "Epoch [82/500], Batch [30/110], Train Loss: 0.0018, Val Loss: 0.0063, LR: 0.0009368510\n",
      "Epoch [82/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0053, LR: 0.0009367127\n",
      "Epoch [82/500], Batch [50/110], Train Loss: 0.0041, Val Loss: 0.0054, LR: 0.0009365743\n",
      "Epoch [82/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0069, LR: 0.0009364358\n",
      "Epoch [82/500], Batch [70/110], Train Loss: 0.0052, Val Loss: 0.0054, LR: 0.0009362971\n",
      "Epoch [82/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0121, LR: 0.0009361583\n",
      "Epoch [82/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0082, LR: 0.0009360193\n",
      "Epoch [82/500], Batch [100/110], Train Loss: 0.0226, Val Loss: 0.0068, LR: 0.0009358802\n",
      "Epoch [82/500], Batch [110/110], Train Loss: 0.0014, Val Loss: 0.0090, LR: 0.0009357410\n",
      "Epoch [83/500], Batch [10/110], Train Loss: 0.0133, Val Loss: 0.0054, LR: 0.0009356016\n",
      "Epoch [83/500], Batch [20/110], Train Loss: 0.0058, Val Loss: 0.0058, LR: 0.0009354620\n",
      "Epoch [83/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0104, LR: 0.0009353224\n",
      "Epoch [83/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0117, LR: 0.0009351826\n",
      "Epoch [83/500], Batch [50/110], Train Loss: 0.3420, Val Loss: 0.0066, LR: 0.0009350426\n",
      "Epoch [83/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0054, LR: 0.0009349025\n",
      "Epoch [83/500], Batch [70/110], Train Loss: 0.0041, Val Loss: 0.0058, LR: 0.0009347623\n",
      "Epoch [83/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0050, LR: 0.0009346219\n",
      "Epoch [83/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0124, LR: 0.0009344814\n",
      "Epoch [83/500], Batch [100/110], Train Loss: 0.0012, Val Loss: 0.0232, LR: 0.0009343408\n",
      "Epoch [83/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0192, LR: 0.0009342000\n",
      "Epoch [84/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0052, LR: 0.0009340591\n",
      "Epoch [84/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0073, LR: 0.0009339180\n",
      "Epoch [84/500], Batch [30/110], Train Loss: 0.0051, Val Loss: 0.0061, LR: 0.0009337768\n",
      "Epoch [84/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0055, LR: 0.0009336354\n",
      "Epoch [84/500], Batch [50/110], Train Loss: 0.0172, Val Loss: 0.0054, LR: 0.0009334940\n",
      "Epoch [84/500], Batch [60/110], Train Loss: 0.0369, Val Loss: 0.0111, LR: 0.0009333523\n",
      "Epoch [84/500], Batch [70/110], Train Loss: 0.0051, Val Loss: 0.0053, LR: 0.0009332106\n",
      "Epoch [84/500], Batch [80/110], Train Loss: 0.0178, Val Loss: 0.0052, LR: 0.0009330687\n",
      "Epoch [84/500], Batch [90/110], Train Loss: 0.0568, Val Loss: 0.0060, LR: 0.0009329266\n",
      "Epoch [84/500], Batch [100/110], Train Loss: 0.0558, Val Loss: 0.0169, LR: 0.0009327844\n",
      "Epoch [84/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0053, LR: 0.0009326421\n",
      "Epoch [85/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0009324996\n",
      "Epoch [85/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0107, LR: 0.0009323570\n",
      "Epoch [85/500], Batch [30/110], Train Loss: 0.0518, Val Loss: 0.0113, LR: 0.0009322143\n",
      "Epoch [85/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0062, LR: 0.0009320714\n",
      "Epoch [85/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0106, LR: 0.0009319284\n",
      "Epoch [85/500], Batch [60/110], Train Loss: 0.0325, Val Loss: 0.0104, LR: 0.0009317852\n",
      "Epoch [85/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0051, LR: 0.0009316419\n",
      "Epoch [85/500], Batch [80/110], Train Loss: 0.0066, Val Loss: 0.0051, LR: 0.0009314985\n",
      "Epoch [85/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0190, LR: 0.0009313549\n",
      "Epoch [85/500], Batch [100/110], Train Loss: 0.0024, Val Loss: 0.0079, LR: 0.0009312112\n",
      "Epoch [85/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0051, LR: 0.0009310673\n",
      "Epoch [86/500], Batch [10/110], Train Loss: 0.0058, Val Loss: 0.0050, LR: 0.0009309233\n",
      "Epoch [86/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0009307792\n",
      "Epoch [86/500], Batch [30/110], Train Loss: 0.0085, Val Loss: 0.0056, LR: 0.0009306349\n",
      "Epoch [86/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0056, LR: 0.0009304905\n",
      "Epoch [86/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0068, LR: 0.0009303459\n",
      "Epoch [86/500], Batch [60/110], Train Loss: 0.0023, Val Loss: 0.0051, LR: 0.0009302012\n",
      "Epoch [86/500], Batch [70/110], Train Loss: 0.0150, Val Loss: 0.0059, LR: 0.0009300564\n",
      "Epoch [86/500], Batch [80/110], Train Loss: 0.0025, Val Loss: 0.0060, LR: 0.0009299114\n",
      "Epoch [86/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0060, LR: 0.0009297663\n",
      "Epoch [86/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0059, LR: 0.0009296211\n",
      "Epoch [86/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0051, LR: 0.0009294757\n",
      "Epoch [87/500], Batch [10/110], Train Loss: 0.0242, Val Loss: 0.0058, LR: 0.0009293302\n",
      "Epoch [87/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0129, LR: 0.0009291845\n",
      "Epoch [87/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0050, LR: 0.0009290387\n",
      "Epoch [87/500], Batch [40/110], Train Loss: 0.0198, Val Loss: 0.0054, LR: 0.0009288928\n",
      "Epoch [87/500], Batch [50/110], Train Loss: 0.0158, Val Loss: 0.0063, LR: 0.0009287467\n",
      "Epoch [87/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0088, LR: 0.0009286005\n",
      "Epoch [87/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0057, LR: 0.0009284541\n",
      "Epoch [87/500], Batch [80/110], Train Loss: 0.0316, Val Loss: 0.0054, LR: 0.0009283076\n",
      "Epoch [87/500], Batch [90/110], Train Loss: 0.0303, Val Loss: 0.0091, LR: 0.0009281610\n",
      "Epoch [87/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0085, LR: 0.0009280142\n",
      "Epoch [87/500], Batch [110/110], Train Loss: 0.0048, Val Loss: 0.0051, LR: 0.0009278673\n",
      "Epoch [88/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0056, LR: 0.0009277203\n",
      "Epoch [88/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0132, LR: 0.0009275731\n",
      "Epoch [88/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0054, LR: 0.0009274258\n",
      "Epoch [88/500], Batch [40/110], Train Loss: 0.0049, Val Loss: 0.0059, LR: 0.0009272783\n",
      "Epoch [88/500], Batch [50/110], Train Loss: 0.0129, Val Loss: 0.0067, LR: 0.0009271307\n",
      "Epoch [88/500], Batch [60/110], Train Loss: 0.0084, Val Loss: 0.0054, LR: 0.0009269830\n",
      "Epoch [88/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0062, LR: 0.0009268351\n",
      "Epoch [88/500], Batch [80/110], Train Loss: 0.0020, Val Loss: 0.0050, LR: 0.0009266871\n",
      "Epoch [88/500], Batch [90/110], Train Loss: 0.0215, Val Loss: 0.0075, LR: 0.0009265390\n",
      "Epoch [88/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0009263907\n",
      "Epoch [88/500], Batch [110/110], Train Loss: 0.0140, Val Loss: 0.0058, LR: 0.0009262423\n",
      "Epoch [89/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0101, LR: 0.0009260937\n",
      "Epoch [89/500], Batch [20/110], Train Loss: 0.0015, Val Loss: 0.0132, LR: 0.0009259450\n",
      "Epoch [89/500], Batch [30/110], Train Loss: 0.0112, Val Loss: 0.0057, LR: 0.0009257962\n",
      "Epoch [89/500], Batch [40/110], Train Loss: 0.0266, Val Loss: 0.0063, LR: 0.0009256472\n",
      "Epoch [89/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0116, LR: 0.0009254981\n",
      "Epoch [89/500], Batch [60/110], Train Loss: 0.0070, Val Loss: 0.0054, LR: 0.0009253489\n",
      "Epoch [89/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0199, LR: 0.0009251995\n",
      "Epoch [89/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0050, LR: 0.0009250500\n",
      "Epoch [89/500], Batch [90/110], Train Loss: 0.2766, Val Loss: 0.0066, LR: 0.0009249003\n",
      "Epoch [89/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0053, LR: 0.0009247505\n",
      "Epoch [89/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0051, LR: 0.0009246006\n",
      "Epoch [90/500], Batch [10/110], Train Loss: 0.0060, Val Loss: 0.0054, LR: 0.0009244505\n",
      "Epoch [90/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0069, LR: 0.0009243003\n",
      "Epoch [90/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0051, LR: 0.0009241500\n",
      "Epoch [90/500], Batch [40/110], Train Loss: 0.0048, Val Loss: 0.0055, LR: 0.0009239995\n",
      "Epoch [90/500], Batch [50/110], Train Loss: 0.0059, Val Loss: 0.0050, LR: 0.0009238489\n",
      "Epoch [90/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0052, LR: 0.0009236981\n",
      "Epoch [90/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0076, LR: 0.0009235472\n",
      "Epoch [90/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0049, LR: 0.0009233962\n",
      "Epoch [90/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0122, LR: 0.0009232451\n",
      "Epoch [90/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0065, LR: 0.0009230938\n",
      "Epoch [90/500], Batch [110/110], Train Loss: 0.0190, Val Loss: 0.0048, LR: 0.0009229423\n",
      "Epoch [91/500], Batch [10/110], Train Loss: 0.0067, Val Loss: 0.0077, LR: 0.0009227908\n",
      "Epoch [91/500], Batch [20/110], Train Loss: 0.0019, Val Loss: 0.0097, LR: 0.0009226390\n",
      "Epoch [91/500], Batch [30/110], Train Loss: 0.0076, Val Loss: 0.0049, LR: 0.0009224872\n",
      "Epoch [91/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0054, LR: 0.0009223352\n",
      "Epoch [91/500], Batch [50/110], Train Loss: 0.0281, Val Loss: 0.0059, LR: 0.0009221831\n",
      "Epoch [91/500], Batch [60/110], Train Loss: 0.0234, Val Loss: 0.0094, LR: 0.0009220309\n",
      "Epoch [91/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0050, LR: 0.0009218785\n",
      "Epoch [91/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0055, LR: 0.0009217260\n",
      "Epoch [91/500], Batch [90/110], Train Loss: 0.0256, Val Loss: 0.0048, LR: 0.0009215733\n",
      "Epoch [91/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0047, LR: 0.0009214205\n",
      "Epoch [91/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0047, LR: 0.0009212676\n",
      "Confusion Matrix:\n",
      "[[649   3]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99540   0.99769       652\n",
      "           1    0.99647   1.00000   0.99823       848\n",
      "\n",
      "    accuracy                        0.99800      1500\n",
      "   macro avg    0.99824   0.99770   0.99796      1500\n",
      "weighted avg    0.99801   0.99800   0.99800      1500\n",
      "\n",
      "Total Errors: 3\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 1218, Predicted: 1, Actual: 0\n",
      "Index: 1397, Predicted: 1, Actual: 0\n",
      "Epoch 91: OK- Accuracy: 0.99800, Precision: 0.99647, Recall: 1.00000, F1: 0.99823, ROC AUC: 0.99770, AUPR (PR-AUC): 0.99647, Sensitivity: 1.00000, Specificity: 0.99540, Far: 0.004601226993865031, False Positive Rate (FPR): 0.00460, False Negative Rate (FNR): 0.00000, Runtime: 0.022 sec , Memory Usage: 196.44 MB\n",
      "Epoch [92/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0044, LR: 0.0009211145\n",
      "Epoch [92/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0061, LR: 0.0009209613\n",
      "Epoch [92/500], Batch [30/110], Train Loss: 0.0055, Val Loss: 0.0044, LR: 0.0009208080\n",
      "Epoch [92/500], Batch [40/110], Train Loss: 0.3556, Val Loss: 0.0051, LR: 0.0009206545\n",
      "Epoch [92/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0009205009\n",
      "Epoch [92/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0068, LR: 0.0009203471\n",
      "Epoch [92/500], Batch [70/110], Train Loss: 0.1583, Val Loss: 0.0105, LR: 0.0009201933\n",
      "Epoch [92/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0044, LR: 0.0009200392\n",
      "Epoch [92/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0044, LR: 0.0009198851\n",
      "Epoch [92/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0009197308\n",
      "Epoch [92/500], Batch [110/110], Train Loss: 0.0099, Val Loss: 0.0046, LR: 0.0009195764\n",
      "Epoch [93/500], Batch [10/110], Train Loss: 0.0046, Val Loss: 0.0044, LR: 0.0009194218\n",
      "Epoch [93/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0078, LR: 0.0009192671\n",
      "Epoch [93/500], Batch [30/110], Train Loss: 0.0032, Val Loss: 0.0044, LR: 0.0009191123\n",
      "Epoch [93/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0110, LR: 0.0009189573\n",
      "Epoch [93/500], Batch [50/110], Train Loss: 0.0020, Val Loss: 0.0045, LR: 0.0009188022\n",
      "Epoch [93/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0041, LR: 0.0009186470\n",
      "Epoch [93/500], Batch [70/110], Train Loss: 0.0028, Val Loss: 0.0065, LR: 0.0009184916\n",
      "Epoch [93/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0058, LR: 0.0009183361\n",
      "Epoch [93/500], Batch [90/110], Train Loss: 0.0102, Val Loss: 0.0050, LR: 0.0009181805\n",
      "Epoch [93/500], Batch [100/110], Train Loss: 0.0214, Val Loss: 0.0046, LR: 0.0009180247\n",
      "Epoch [93/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0071, LR: 0.0009178688\n",
      "Epoch [94/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0046, LR: 0.0009177128\n",
      "Epoch [94/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0070, LR: 0.0009175566\n",
      "Epoch [94/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0044, LR: 0.0009174003\n",
      "Epoch [94/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0042, LR: 0.0009172439\n",
      "Epoch [94/500], Batch [50/110], Train Loss: 0.0204, Val Loss: 0.0041, LR: 0.0009170873\n",
      "Epoch [94/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0041, LR: 0.0009169306\n",
      "Epoch [94/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0041, LR: 0.0009167737\n",
      "Epoch [94/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0041, LR: 0.0009166167\n",
      "Epoch [94/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0079, LR: 0.0009164596\n",
      "Epoch [94/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0043, LR: 0.0009163024\n",
      "Epoch [94/500], Batch [110/110], Train Loss: 0.0174, Val Loss: 0.0044, LR: 0.0009161450\n",
      "Epoch [95/500], Batch [10/110], Train Loss: 0.0017, Val Loss: 0.0067, LR: 0.0009159875\n",
      "Epoch [95/500], Batch [20/110], Train Loss: 0.0490, Val Loss: 0.0055, LR: 0.0009158298\n",
      "Epoch [95/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0086, LR: 0.0009156720\n",
      "Epoch [95/500], Batch [40/110], Train Loss: 0.0059, Val Loss: 0.0041, LR: 0.0009155141\n",
      "Epoch [95/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0009153560\n",
      "Epoch [95/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0069, LR: 0.0009151978\n",
      "Epoch [95/500], Batch [70/110], Train Loss: 0.0421, Val Loss: 0.0042, LR: 0.0009150395\n",
      "Epoch [95/500], Batch [80/110], Train Loss: 0.1051, Val Loss: 0.0165, LR: 0.0009148811\n",
      "Epoch [95/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0081, LR: 0.0009147225\n",
      "Epoch [95/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0080, LR: 0.0009145637\n",
      "Epoch [95/500], Batch [110/110], Train Loss: 0.0019, Val Loss: 0.0041, LR: 0.0009144049\n",
      "Epoch [96/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0009142459\n",
      "Epoch [96/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0040, LR: 0.0009140868\n",
      "Epoch [96/500], Batch [30/110], Train Loss: 0.0021, Val Loss: 0.0052, LR: 0.0009139275\n",
      "Epoch [96/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0046, LR: 0.0009137681\n",
      "Epoch [96/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0050, LR: 0.0009136086\n",
      "Epoch [96/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0075, LR: 0.0009134489\n",
      "Epoch [96/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0112, LR: 0.0009132891\n",
      "Epoch [96/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0077, LR: 0.0009131292\n",
      "Epoch [96/500], Batch [90/110], Train Loss: 0.0108, Val Loss: 0.0044, LR: 0.0009129692\n",
      "Epoch [96/500], Batch [100/110], Train Loss: 0.0025, Val Loss: 0.0040, LR: 0.0009128090\n",
      "Epoch [96/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0009126486\n",
      "Epoch [97/500], Batch [10/110], Train Loss: 0.2332, Val Loss: 0.0041, LR: 0.0009124882\n",
      "Epoch [97/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0042, LR: 0.0009123276\n",
      "Epoch [97/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0080, LR: 0.0009121669\n",
      "Epoch [97/500], Batch [40/110], Train Loss: 0.0552, Val Loss: 0.0093, LR: 0.0009120060\n",
      "Epoch [97/500], Batch [50/110], Train Loss: 0.0214, Val Loss: 0.0064, LR: 0.0009118450\n",
      "Epoch [97/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0071, LR: 0.0009116839\n",
      "Epoch [97/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0173, LR: 0.0009115226\n",
      "Epoch [97/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0073, LR: 0.0009113613\n",
      "Epoch [97/500], Batch [90/110], Train Loss: 0.0156, Val Loss: 0.0044, LR: 0.0009111997\n",
      "Epoch [97/500], Batch [100/110], Train Loss: 0.0177, Val Loss: 0.0044, LR: 0.0009110381\n",
      "Epoch [97/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0057, LR: 0.0009108763\n",
      "Epoch [98/500], Batch [10/110], Train Loss: 0.0089, Val Loss: 0.0047, LR: 0.0009107144\n",
      "Epoch [98/500], Batch [20/110], Train Loss: 0.0574, Val Loss: 0.0065, LR: 0.0009105523\n",
      "Epoch [98/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0041, LR: 0.0009103901\n",
      "Epoch [98/500], Batch [40/110], Train Loss: 0.0014, Val Loss: 0.0045, LR: 0.0009102278\n",
      "Epoch [98/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0041, LR: 0.0009100654\n",
      "Epoch [98/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0009099028\n",
      "Epoch [98/500], Batch [70/110], Train Loss: 0.0020, Val Loss: 0.0040, LR: 0.0009097401\n",
      "Epoch [98/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0050, LR: 0.0009095773\n",
      "Epoch [98/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0040, LR: 0.0009094143\n",
      "Epoch [98/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0045, LR: 0.0009092512\n",
      "Epoch [98/500], Batch [110/110], Train Loss: 0.0503, Val Loss: 0.0069, LR: 0.0009090879\n",
      "Epoch [99/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0079, LR: 0.0009089246\n",
      "Epoch [99/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0051, LR: 0.0009087611\n",
      "Epoch [99/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0040, LR: 0.0009085974\n",
      "Epoch [99/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0063, LR: 0.0009084337\n",
      "Epoch [99/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0041, LR: 0.0009082698\n",
      "Epoch [99/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0038, LR: 0.0009081057\n",
      "Epoch [99/500], Batch [70/110], Train Loss: 0.0181, Val Loss: 0.0042, LR: 0.0009079416\n",
      "Epoch [99/500], Batch [80/110], Train Loss: 0.0168, Val Loss: 0.0040, LR: 0.0009077773\n",
      "Epoch [99/500], Batch [90/110], Train Loss: 0.0038, Val Loss: 0.0052, LR: 0.0009076129\n",
      "Epoch [99/500], Batch [100/110], Train Loss: 0.0013, Val Loss: 0.0079, LR: 0.0009074483\n",
      "Epoch [99/500], Batch [110/110], Train Loss: 0.0283, Val Loss: 0.0039, LR: 0.0009072836\n",
      "Epoch [100/500], Batch [10/110], Train Loss: 0.0606, Val Loss: 0.0042, LR: 0.0009071188\n",
      "Epoch [100/500], Batch [20/110], Train Loss: 0.0042, Val Loss: 0.0041, LR: 0.0009069538\n",
      "Epoch [100/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0058, LR: 0.0009067888\n",
      "Epoch [100/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0039, LR: 0.0009066236\n",
      "Epoch [100/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0041, LR: 0.0009064582\n",
      "Epoch [100/500], Batch [60/110], Train Loss: 0.0211, Val Loss: 0.0036, LR: 0.0009062927\n",
      "Epoch [100/500], Batch [70/110], Train Loss: 0.0072, Val Loss: 0.0036, LR: 0.0009061271\n",
      "Epoch [100/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0071, LR: 0.0009059614\n",
      "Epoch [100/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0085, LR: 0.0009057955\n",
      "Epoch [100/500], Batch [100/110], Train Loss: 0.0532, Val Loss: 0.0109, LR: 0.0009056295\n",
      "Epoch [100/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0036, LR: 0.0009054634\n",
      "Epoch [101/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0043, LR: 0.0009052972\n",
      "Epoch [101/500], Batch [20/110], Train Loss: 0.0051, Val Loss: 0.0040, LR: 0.0009051308\n",
      "Epoch [101/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0036, LR: 0.0009049642\n",
      "Epoch [101/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0037, LR: 0.0009047976\n",
      "Epoch [101/500], Batch [50/110], Train Loss: 0.0069, Val Loss: 0.0035, LR: 0.0009046308\n",
      "Epoch [101/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0009044639\n",
      "Epoch [101/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0009042969\n",
      "Epoch [101/500], Batch [80/110], Train Loss: 0.0053, Val Loss: 0.0034, LR: 0.0009041297\n",
      "Epoch [101/500], Batch [90/110], Train Loss: 0.0025, Val Loss: 0.0045, LR: 0.0009039624\n",
      "Epoch [101/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0036, LR: 0.0009037950\n",
      "Epoch [101/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0039, LR: 0.0009036274\n",
      "Confusion Matrix:\n",
      "[[648   4]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99387   0.99692       652\n",
      "           1    0.99531   1.00000   0.99765       848\n",
      "\n",
      "    accuracy                        0.99733      1500\n",
      "   macro avg    0.99765   0.99693   0.99729      1500\n",
      "weighted avg    0.99735   0.99733   0.99733      1500\n",
      "\n",
      "Total Errors: 4\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 877, Predicted: 1, Actual: 0\n",
      "Index: 1218, Predicted: 1, Actual: 0\n",
      "Index: 1397, Predicted: 1, Actual: 0\n",
      "Epoch 101: OK- Accuracy: 0.99733, Precision: 0.99531, Recall: 1.00000, F1: 0.99765, ROC AUC: 0.99693, AUPR (PR-AUC): 0.99531, Sensitivity: 1.00000, Specificity: 0.99387, Far: 0.006134969325153374, False Positive Rate (FPR): 0.00613, False Negative Rate (FNR): 0.00000, Runtime: 0.051 sec , Memory Usage: 196.44 MB\n",
      "Epoch [102/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0039, LR: 0.0009034597\n",
      "Epoch [102/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0034, LR: 0.0009032919\n",
      "Epoch [102/500], Batch [30/110], Train Loss: 0.0012, Val Loss: 0.0034, LR: 0.0009031239\n",
      "Epoch [102/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0036, LR: 0.0009029559\n",
      "Epoch [102/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0036, LR: 0.0009027876\n",
      "Epoch [102/500], Batch [60/110], Train Loss: 0.0075, Val Loss: 0.0035, LR: 0.0009026193\n",
      "Epoch [102/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0062, LR: 0.0009024508\n",
      "Epoch [102/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0065, LR: 0.0009022822\n",
      "Epoch [102/500], Batch [90/110], Train Loss: 0.0015, Val Loss: 0.0045, LR: 0.0009021135\n",
      "Epoch [102/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0039, LR: 0.0009019446\n",
      "Epoch [102/500], Batch [110/110], Train Loss: 0.0009, Val Loss: 0.0051, LR: 0.0009017757\n",
      "Epoch [103/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0034, LR: 0.0009016065\n",
      "Epoch [103/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0037, LR: 0.0009014373\n",
      "Epoch [103/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0035, LR: 0.0009012679\n",
      "Epoch [103/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0062, LR: 0.0009010984\n",
      "Epoch [103/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0035, LR: 0.0009009288\n",
      "Epoch [103/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0036, LR: 0.0009007590\n",
      "Epoch [103/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0044, LR: 0.0009005891\n",
      "Epoch [103/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0072, LR: 0.0009004191\n",
      "Epoch [103/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0066, LR: 0.0009002489\n",
      "Epoch [103/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0040, LR: 0.0009000787\n",
      "Epoch [103/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0045, LR: 0.0008999082\n",
      "Epoch [104/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0074, LR: 0.0008997377\n",
      "Epoch [104/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0076, LR: 0.0008995670\n",
      "Epoch [104/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0069, LR: 0.0008993962\n",
      "Epoch [104/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0092, LR: 0.0008992253\n",
      "Epoch [104/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0072, LR: 0.0008990543\n",
      "Epoch [104/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0129, LR: 0.0008988831\n",
      "Epoch [104/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0092, LR: 0.0008987118\n",
      "Epoch [104/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0042, LR: 0.0008985403\n",
      "Epoch [104/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0041, LR: 0.0008983688\n",
      "Epoch [104/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0050, LR: 0.0008981971\n",
      "Epoch [104/500], Batch [110/110], Train Loss: 0.0031, Val Loss: 0.0039, LR: 0.0008980252\n",
      "Epoch [105/500], Batch [10/110], Train Loss: 0.0062, Val Loss: 0.0040, LR: 0.0008978533\n",
      "Epoch [105/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0042, LR: 0.0008976812\n",
      "Epoch [105/500], Batch [30/110], Train Loss: 0.0258, Val Loss: 0.0036, LR: 0.0008975090\n",
      "Epoch [105/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0051, LR: 0.0008973367\n",
      "Epoch [105/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0057, LR: 0.0008971642\n",
      "Epoch [105/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0035, LR: 0.0008969916\n",
      "Epoch [105/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0039, LR: 0.0008968189\n",
      "Epoch [105/500], Batch [80/110], Train Loss: 0.0027, Val Loss: 0.0039, LR: 0.0008966460\n",
      "Epoch [105/500], Batch [90/110], Train Loss: 0.0073, Val Loss: 0.0037, LR: 0.0008964731\n",
      "Epoch [105/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0035, LR: 0.0008963000\n",
      "Epoch [105/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0053, LR: 0.0008961267\n",
      "Epoch [106/500], Batch [10/110], Train Loss: 0.0111, Val Loss: 0.0037, LR: 0.0008959534\n",
      "Epoch [106/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0057, LR: 0.0008957799\n",
      "Epoch [106/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0044, LR: 0.0008956063\n",
      "Epoch [106/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0037, LR: 0.0008954325\n",
      "Epoch [106/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0040, LR: 0.0008952587\n",
      "Epoch [106/500], Batch [60/110], Train Loss: 0.0361, Val Loss: 0.0043, LR: 0.0008950847\n",
      "Epoch [106/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0035, LR: 0.0008949105\n",
      "Epoch [106/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0033, LR: 0.0008947363\n",
      "Epoch [106/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0062, LR: 0.0008945619\n",
      "Epoch [106/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0033, LR: 0.0008943874\n",
      "Epoch [106/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0033, LR: 0.0008942128\n",
      "Epoch [107/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0047, LR: 0.0008940380\n",
      "Epoch [107/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0034, LR: 0.0008938631\n",
      "Epoch [107/500], Batch [30/110], Train Loss: 0.0234, Val Loss: 0.0036, LR: 0.0008936881\n",
      "Epoch [107/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0033, LR: 0.0008935130\n",
      "Epoch [107/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0077, LR: 0.0008933377\n",
      "Epoch [107/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0048, LR: 0.0008931623\n",
      "Epoch [107/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0030, LR: 0.0008929868\n",
      "Epoch [107/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0037, LR: 0.0008928111\n",
      "Epoch [107/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0039, LR: 0.0008926354\n",
      "Epoch [107/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0034, LR: 0.0008924595\n",
      "Epoch [107/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0041, LR: 0.0008922835\n",
      "Epoch [108/500], Batch [10/110], Train Loss: 0.0200, Val Loss: 0.0043, LR: 0.0008921073\n",
      "Epoch [108/500], Batch [20/110], Train Loss: 0.0086, Val Loss: 0.0063, LR: 0.0008919310\n",
      "Epoch [108/500], Batch [30/110], Train Loss: 0.0024, Val Loss: 0.0036, LR: 0.0008917546\n",
      "Epoch [108/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0032, LR: 0.0008915781\n",
      "Epoch [108/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0051, LR: 0.0008914014\n",
      "Epoch [108/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0039, LR: 0.0008912246\n",
      "Epoch [108/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0030, LR: 0.0008910477\n",
      "Epoch [108/500], Batch [80/110], Train Loss: 0.0048, Val Loss: 0.0032, LR: 0.0008908707\n",
      "Epoch [108/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0047, LR: 0.0008906935\n",
      "Epoch [108/500], Batch [100/110], Train Loss: 0.0057, Val Loss: 0.0032, LR: 0.0008905163\n",
      "Epoch [108/500], Batch [110/110], Train Loss: 0.0236, Val Loss: 0.0032, LR: 0.0008903388\n",
      "Epoch [109/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0008901613\n",
      "Epoch [109/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0008899836\n",
      "Epoch [109/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0080, LR: 0.0008898058\n",
      "Epoch [109/500], Batch [40/110], Train Loss: 0.2348, Val Loss: 0.0027, LR: 0.0008896279\n",
      "Epoch [109/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0038, LR: 0.0008894499\n",
      "Epoch [109/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0052, LR: 0.0008892717\n",
      "Epoch [109/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0008890934\n",
      "Epoch [109/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0043, LR: 0.0008889150\n",
      "Epoch [109/500], Batch [90/110], Train Loss: 0.0025, Val Loss: 0.0050, LR: 0.0008887365\n",
      "Epoch [109/500], Batch [100/110], Train Loss: 0.0015, Val Loss: 0.0036, LR: 0.0008885578\n",
      "Epoch [109/500], Batch [110/110], Train Loss: 0.0079, Val Loss: 0.0033, LR: 0.0008883790\n",
      "Epoch [110/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0046, LR: 0.0008882001\n",
      "Epoch [110/500], Batch [20/110], Train Loss: 0.0082, Val Loss: 0.0029, LR: 0.0008880211\n",
      "Epoch [110/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0029, LR: 0.0008878419\n",
      "Epoch [110/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0008876626\n",
      "Epoch [110/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0080, LR: 0.0008874832\n",
      "Epoch [110/500], Batch [60/110], Train Loss: 0.0082, Val Loss: 0.0059, LR: 0.0008873036\n",
      "Epoch [110/500], Batch [70/110], Train Loss: 0.0021, Val Loss: 0.0036, LR: 0.0008871240\n",
      "Epoch [110/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0036, LR: 0.0008869442\n",
      "Epoch [110/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0093, LR: 0.0008867643\n",
      "Epoch [110/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0031, LR: 0.0008865842\n",
      "Epoch [110/500], Batch [110/110], Train Loss: 0.0134, Val Loss: 0.0029, LR: 0.0008864041\n",
      "Epoch [111/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0008862238\n",
      "Epoch [111/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0008860434\n",
      "Epoch [111/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0034, LR: 0.0008858628\n",
      "Epoch [111/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0029, LR: 0.0008856822\n",
      "Epoch [111/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0032, LR: 0.0008855014\n",
      "Epoch [111/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0037, LR: 0.0008853205\n",
      "Epoch [111/500], Batch [70/110], Train Loss: 0.0196, Val Loss: 0.0034, LR: 0.0008851394\n",
      "Epoch [111/500], Batch [80/110], Train Loss: 0.0013, Val Loss: 0.0030, LR: 0.0008849583\n",
      "Epoch [111/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0044, LR: 0.0008847770\n",
      "Epoch [111/500], Batch [100/110], Train Loss: 0.0404, Val Loss: 0.0044, LR: 0.0008845956\n",
      "Epoch [111/500], Batch [110/110], Train Loss: 0.0145, Val Loss: 0.0028, LR: 0.0008844140\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 111: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 196.44 MB\n",
      "Epoch [112/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0030, LR: 0.0008842324\n",
      "Epoch [112/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0041, LR: 0.0008840506\n",
      "Epoch [112/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0056, LR: 0.0008838687\n",
      "Epoch [112/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0028, LR: 0.0008836867\n",
      "Epoch [112/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0031, LR: 0.0008835045\n",
      "Epoch [112/500], Batch [60/110], Train Loss: 0.0043, Val Loss: 0.0035, LR: 0.0008833223\n",
      "Epoch [112/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0052, LR: 0.0008831399\n",
      "Epoch [112/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0040, LR: 0.0008829573\n",
      "Epoch [112/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0031, LR: 0.0008827747\n",
      "Epoch [112/500], Batch [100/110], Train Loss: 0.0016, Val Loss: 0.0036, LR: 0.0008825919\n",
      "Epoch [112/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0030, LR: 0.0008824090\n",
      "Epoch [113/500], Batch [10/110], Train Loss: 0.0014, Val Loss: 0.0050, LR: 0.0008822260\n",
      "Epoch [113/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0064, LR: 0.0008820429\n",
      "Epoch [113/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0008818596\n",
      "Epoch [113/500], Batch [40/110], Train Loss: 0.0168, Val Loss: 0.0028, LR: 0.0008816763\n",
      "Epoch [113/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0028, LR: 0.0008814928\n",
      "Epoch [113/500], Batch [60/110], Train Loss: 0.0122, Val Loss: 0.0063, LR: 0.0008813091\n",
      "Epoch [113/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0029, LR: 0.0008811254\n",
      "Epoch [113/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0028, LR: 0.0008809415\n",
      "Epoch [113/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0030, LR: 0.0008807575\n",
      "Epoch [113/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0032, LR: 0.0008805734\n",
      "Epoch [113/500], Batch [110/110], Train Loss: 0.0022, Val Loss: 0.0033, LR: 0.0008803891\n",
      "Epoch [114/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0027, LR: 0.0008802048\n",
      "Epoch [114/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0031, LR: 0.0008800203\n",
      "Epoch [114/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0030, LR: 0.0008798357\n",
      "Epoch [114/500], Batch [40/110], Train Loss: 0.0119, Val Loss: 0.0027, LR: 0.0008796510\n",
      "Epoch [114/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0030, LR: 0.0008794661\n",
      "Epoch [114/500], Batch [60/110], Train Loss: 0.0021, Val Loss: 0.0049, LR: 0.0008792811\n",
      "Epoch [114/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0034, LR: 0.0008790960\n",
      "Epoch [114/500], Batch [80/110], Train Loss: 0.0054, Val Loss: 0.0085, LR: 0.0008789108\n",
      "Epoch [114/500], Batch [90/110], Train Loss: 0.0034, Val Loss: 0.0057, LR: 0.0008787255\n",
      "Epoch [114/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0031, LR: 0.0008785400\n",
      "Epoch [114/500], Batch [110/110], Train Loss: 0.0285, Val Loss: 0.0037, LR: 0.0008783544\n",
      "Epoch [115/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0028, LR: 0.0008781687\n",
      "Epoch [115/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0027, LR: 0.0008779829\n",
      "Epoch [115/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0031, LR: 0.0008777969\n",
      "Epoch [115/500], Batch [40/110], Train Loss: 0.0014, Val Loss: 0.0030, LR: 0.0008776109\n",
      "Epoch [115/500], Batch [50/110], Train Loss: 0.0172, Val Loss: 0.0026, LR: 0.0008774247\n",
      "Epoch [115/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0027, LR: 0.0008772384\n",
      "Epoch [115/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0027, LR: 0.0008770519\n",
      "Epoch [115/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0027, LR: 0.0008768654\n",
      "Epoch [115/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0030, LR: 0.0008766787\n",
      "Epoch [115/500], Batch [100/110], Train Loss: 0.0015, Val Loss: 0.0027, LR: 0.0008764919\n",
      "Epoch [115/500], Batch [110/110], Train Loss: 0.0026, Val Loss: 0.0026, LR: 0.0008763050\n",
      "Epoch [116/500], Batch [10/110], Train Loss: 0.0080, Val Loss: 0.0028, LR: 0.0008761179\n",
      "Epoch [116/500], Batch [20/110], Train Loss: 0.0011, Val Loss: 0.0043, LR: 0.0008759308\n",
      "Epoch [116/500], Batch [30/110], Train Loss: 0.0049, Val Loss: 0.0053, LR: 0.0008757435\n",
      "Epoch [116/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0034, LR: 0.0008755561\n",
      "Epoch [116/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0048, LR: 0.0008753686\n",
      "Epoch [116/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0026, LR: 0.0008751809\n",
      "Epoch [116/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0030, LR: 0.0008749931\n",
      "Epoch [116/500], Batch [80/110], Train Loss: 0.0008, Val Loss: 0.0036, LR: 0.0008748053\n",
      "Epoch [116/500], Batch [90/110], Train Loss: 0.0058, Val Loss: 0.0026, LR: 0.0008746172\n",
      "Epoch [116/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0053, LR: 0.0008744291\n",
      "Epoch [116/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0030, LR: 0.0008742409\n",
      "Epoch [117/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0030, LR: 0.0008740525\n",
      "Epoch [117/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0028, LR: 0.0008738640\n",
      "Epoch [117/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0028, LR: 0.0008736754\n",
      "Epoch [117/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0039, LR: 0.0008734867\n",
      "Epoch [117/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0067, LR: 0.0008732978\n",
      "Epoch [117/500], Batch [60/110], Train Loss: 0.0092, Val Loss: 0.0066, LR: 0.0008731088\n",
      "Epoch [117/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0039, LR: 0.0008729197\n",
      "Epoch [117/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0037, LR: 0.0008727305\n",
      "Epoch [117/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0033, LR: 0.0008725412\n",
      "Epoch [117/500], Batch [100/110], Train Loss: 0.0393, Val Loss: 0.0036, LR: 0.0008723517\n",
      "Epoch [117/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0028, LR: 0.0008721622\n",
      "Epoch [118/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0037, LR: 0.0008719725\n",
      "Epoch [118/500], Batch [20/110], Train Loss: 0.0208, Val Loss: 0.0042, LR: 0.0008717827\n",
      "Epoch [118/500], Batch [30/110], Train Loss: 0.0013, Val Loss: 0.0027, LR: 0.0008715927\n",
      "Epoch [118/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0044, LR: 0.0008714027\n",
      "Epoch [118/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0072, LR: 0.0008712125\n",
      "Epoch [118/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0008710222\n",
      "Epoch [118/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0032, LR: 0.0008708318\n",
      "Epoch [118/500], Batch [80/110], Train Loss: 0.0019, Val Loss: 0.0046, LR: 0.0008706413\n",
      "Epoch [118/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0047, LR: 0.0008704506\n",
      "Epoch [118/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0033, LR: 0.0008702599\n",
      "Epoch [118/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0138, LR: 0.0008700690\n",
      "Epoch [119/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0028, LR: 0.0008698780\n",
      "Epoch [119/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0088, LR: 0.0008696869\n",
      "Epoch [119/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0037, LR: 0.0008694956\n",
      "Epoch [119/500], Batch [40/110], Train Loss: 0.0023, Val Loss: 0.0031, LR: 0.0008693043\n",
      "Epoch [119/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0030, LR: 0.0008691128\n",
      "Epoch [119/500], Batch [60/110], Train Loss: 0.0024, Val Loss: 0.0047, LR: 0.0008689212\n",
      "Epoch [119/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0046, LR: 0.0008687295\n",
      "Epoch [119/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0028, LR: 0.0008685376\n",
      "Epoch [119/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0026, LR: 0.0008683457\n",
      "Epoch [119/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0028, LR: 0.0008681536\n",
      "Epoch [119/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0039, LR: 0.0008679614\n",
      "Epoch [120/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0082, LR: 0.0008677691\n",
      "Epoch [120/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0034, LR: 0.0008675767\n",
      "Epoch [120/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0028, LR: 0.0008673841\n",
      "Epoch [120/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0038, LR: 0.0008671914\n",
      "Epoch [120/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0052, LR: 0.0008669987\n",
      "Epoch [120/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0030, LR: 0.0008668058\n",
      "Epoch [120/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0027, LR: 0.0008666127\n",
      "Epoch [120/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0035, LR: 0.0008664196\n",
      "Epoch [120/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0043, LR: 0.0008662263\n",
      "Epoch [120/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0033, LR: 0.0008660330\n",
      "Epoch [120/500], Batch [110/110], Train Loss: 0.0009, Val Loss: 0.0027, LR: 0.0008658395\n",
      "Epoch [121/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0025, LR: 0.0008656459\n",
      "Epoch [121/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0008654521\n",
      "Epoch [121/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0030, LR: 0.0008652583\n",
      "Epoch [121/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0035, LR: 0.0008650643\n",
      "Epoch [121/500], Batch [50/110], Train Loss: 0.0058, Val Loss: 0.0027, LR: 0.0008648702\n",
      "Epoch [121/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0032, LR: 0.0008646760\n",
      "Epoch [121/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0023, LR: 0.0008644817\n",
      "Epoch [121/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0024, LR: 0.0008642873\n",
      "Epoch [121/500], Batch [90/110], Train Loss: 0.0457, Val Loss: 0.0039, LR: 0.0008640927\n",
      "Epoch [121/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0052, LR: 0.0008638981\n",
      "Epoch [121/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0024, LR: 0.0008637033\n",
      "Confusion Matrix:\n",
      "[[650   2]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99693   0.99846       652\n",
      "           1    0.99765   1.00000   0.99882       848\n",
      "\n",
      "    accuracy                        0.99867      1500\n",
      "   macro avg    0.99882   0.99847   0.99864      1500\n",
      "weighted avg    0.99867   0.99867   0.99867      1500\n",
      "\n",
      "Total Errors: 2\n",
      "Index: 1218, Predicted: 1, Actual: 0\n",
      "Index: 1397, Predicted: 1, Actual: 0\n",
      "Epoch 121: OK- Accuracy: 0.99867, Precision: 0.99765, Recall: 1.00000, F1: 0.99882, ROC AUC: 0.99847, AUPR (PR-AUC): 0.99765, Sensitivity: 1.00000, Specificity: 0.99693, Far: 0.003067484662576687, False Positive Rate (FPR): 0.00307, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 196.44 MB\n",
      "Epoch [122/500], Batch [10/110], Train Loss: 0.0065, Val Loss: 0.0029, LR: 0.0008635084\n",
      "Epoch [122/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0073, LR: 0.0008633134\n",
      "Epoch [122/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0038, LR: 0.0008631182\n",
      "Epoch [122/500], Batch [40/110], Train Loss: 0.1626, Val Loss: 0.0023, LR: 0.0008629230\n",
      "Epoch [122/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0008627276\n",
      "Epoch [122/500], Batch [60/110], Train Loss: 0.0036, Val Loss: 0.0022, LR: 0.0008625321\n",
      "Epoch [122/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0022, LR: 0.0008623365\n",
      "Epoch [122/500], Batch [80/110], Train Loss: 0.0120, Val Loss: 0.0023, LR: 0.0008621408\n",
      "Epoch [122/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0026, LR: 0.0008619450\n",
      "Epoch [122/500], Batch [100/110], Train Loss: 0.0017, Val Loss: 0.0025, LR: 0.0008617490\n",
      "Epoch [122/500], Batch [110/110], Train Loss: 0.0025, Val Loss: 0.0048, LR: 0.0008615530\n",
      "Epoch [123/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0042, LR: 0.0008613568\n",
      "Epoch [123/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0061, LR: 0.0008611605\n",
      "Epoch [123/500], Batch [30/110], Train Loss: 0.0043, Val Loss: 0.0026, LR: 0.0008609641\n",
      "Epoch [123/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0025, LR: 0.0008607675\n",
      "Epoch [123/500], Batch [50/110], Train Loss: 0.0106, Val Loss: 0.0060, LR: 0.0008605709\n",
      "Epoch [123/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0027, LR: 0.0008603741\n",
      "Epoch [123/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0036, LR: 0.0008601772\n",
      "Epoch [123/500], Batch [80/110], Train Loss: 0.0011, Val Loss: 0.0053, LR: 0.0008599802\n",
      "Epoch [123/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0029, LR: 0.0008597831\n",
      "Epoch [123/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0008595859\n",
      "Epoch [123/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0024, LR: 0.0008593886\n",
      "Epoch [124/500], Batch [10/110], Train Loss: 0.0043, Val Loss: 0.0038, LR: 0.0008591911\n",
      "Epoch [124/500], Batch [20/110], Train Loss: 0.0093, Val Loss: 0.0022, LR: 0.0008589935\n",
      "Epoch [124/500], Batch [30/110], Train Loss: 0.0030, Val Loss: 0.0021, LR: 0.0008587958\n",
      "Epoch [124/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0008585980\n",
      "Epoch [124/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0025, LR: 0.0008584001\n",
      "Epoch [124/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0038, LR: 0.0008582021\n",
      "Epoch [124/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0008580039\n",
      "Epoch [124/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0022, LR: 0.0008578057\n",
      "Epoch [124/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0091, LR: 0.0008576073\n",
      "Epoch [124/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0026, LR: 0.0008574088\n",
      "Epoch [124/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0023, LR: 0.0008572102\n",
      "Epoch [125/500], Batch [10/110], Train Loss: 0.0014, Val Loss: 0.0025, LR: 0.0008570114\n",
      "Epoch [125/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0027, LR: 0.0008568126\n",
      "Epoch [125/500], Batch [30/110], Train Loss: 0.0019, Val Loss: 0.0025, LR: 0.0008566136\n",
      "Epoch [125/500], Batch [40/110], Train Loss: 0.0238, Val Loss: 0.0040, LR: 0.0008564146\n",
      "Epoch [125/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0025, LR: 0.0008562154\n",
      "Epoch [125/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0025, LR: 0.0008560161\n",
      "Epoch [125/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0033, LR: 0.0008558167\n",
      "Epoch [125/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0031, LR: 0.0008556171\n",
      "Epoch [125/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0026, LR: 0.0008554175\n",
      "Epoch [125/500], Batch [100/110], Train Loss: 0.0098, Val Loss: 0.0024, LR: 0.0008552177\n",
      "Epoch [125/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0022, LR: 0.0008550179\n",
      "Epoch [126/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0021, LR: 0.0008548179\n",
      "Epoch [126/500], Batch [20/110], Train Loss: 0.0016, Val Loss: 0.0028, LR: 0.0008546178\n",
      "Epoch [126/500], Batch [30/110], Train Loss: 0.0048, Val Loss: 0.0023, LR: 0.0008544176\n",
      "Epoch [126/500], Batch [40/110], Train Loss: 0.0108, Val Loss: 0.0030, LR: 0.0008542172\n",
      "Epoch [126/500], Batch [50/110], Train Loss: 0.0087, Val Loss: 0.0021, LR: 0.0008540168\n",
      "Epoch [126/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0025, LR: 0.0008538162\n",
      "Epoch [126/500], Batch [70/110], Train Loss: 0.0091, Val Loss: 0.0027, LR: 0.0008536156\n",
      "Epoch [126/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0040, LR: 0.0008534148\n",
      "Epoch [126/500], Batch [90/110], Train Loss: 0.0073, Val Loss: 0.0022, LR: 0.0008532139\n",
      "Epoch [126/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0008530129\n",
      "Epoch [126/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0041, LR: 0.0008528117\n",
      "Epoch [127/500], Batch [10/110], Train Loss: 0.0028, Val Loss: 0.0021, LR: 0.0008526105\n",
      "Epoch [127/500], Batch [20/110], Train Loss: 0.0199, Val Loss: 0.0116, LR: 0.0008524091\n",
      "Epoch [127/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0096, LR: 0.0008522077\n",
      "Epoch [127/500], Batch [40/110], Train Loss: 0.2634, Val Loss: 0.0065, LR: 0.0008520061\n",
      "Epoch [127/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0113, LR: 0.0008518044\n",
      "Epoch [127/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0025, LR: 0.0008516026\n",
      "Epoch [127/500], Batch [70/110], Train Loss: 0.0237, Val Loss: 0.0040, LR: 0.0008514007\n",
      "Epoch [127/500], Batch [80/110], Train Loss: 0.0030, Val Loss: 0.0030, LR: 0.0008511987\n",
      "Epoch [127/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0044, LR: 0.0008509965\n",
      "Epoch [127/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0008507943\n",
      "Epoch [127/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0021, LR: 0.0008505919\n",
      "Epoch [128/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0008503894\n",
      "Epoch [128/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0021, LR: 0.0008501868\n",
      "Epoch [128/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0035, LR: 0.0008499841\n",
      "Epoch [128/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0008497813\n",
      "Epoch [128/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0008495783\n",
      "Epoch [128/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0008493753\n",
      "Epoch [128/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0008491721\n",
      "Epoch [128/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0042, LR: 0.0008489689\n",
      "Epoch [128/500], Batch [90/110], Train Loss: 0.0021, Val Loss: 0.0023, LR: 0.0008487655\n",
      "Epoch [128/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0021, LR: 0.0008485620\n",
      "Epoch [128/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0020, LR: 0.0008483584\n",
      "Epoch [129/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0008481547\n",
      "Epoch [129/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0022, LR: 0.0008479508\n",
      "Epoch [129/500], Batch [30/110], Train Loss: 0.0035, Val Loss: 0.0038, LR: 0.0008477469\n",
      "Epoch [129/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0023, LR: 0.0008475428\n",
      "Epoch [129/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0068, LR: 0.0008473387\n",
      "Epoch [129/500], Batch [60/110], Train Loss: 0.0230, Val Loss: 0.0027, LR: 0.0008471344\n",
      "Epoch [129/500], Batch [70/110], Train Loss: 0.0036, Val Loss: 0.0076, LR: 0.0008469300\n",
      "Epoch [129/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0032, LR: 0.0008467255\n",
      "Epoch [129/500], Batch [90/110], Train Loss: 0.0873, Val Loss: 0.0024, LR: 0.0008465209\n",
      "Epoch [129/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0024, LR: 0.0008463162\n",
      "Epoch [129/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0021, LR: 0.0008461113\n",
      "Epoch [130/500], Batch [10/110], Train Loss: 0.0240, Val Loss: 0.0027, LR: 0.0008459064\n",
      "Epoch [130/500], Batch [20/110], Train Loss: 0.0060, Val Loss: 0.0021, LR: 0.0008457013\n",
      "Epoch [130/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0021, LR: 0.0008454962\n",
      "Epoch [130/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0046, LR: 0.0008452909\n",
      "Epoch [130/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0008450855\n",
      "Epoch [130/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0008448800\n",
      "Epoch [130/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0024, LR: 0.0008446744\n",
      "Epoch [130/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0023, LR: 0.0008444687\n",
      "Epoch [130/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0026, LR: 0.0008442628\n",
      "Epoch [130/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0045, LR: 0.0008440569\n",
      "Epoch [130/500], Batch [110/110], Train Loss: 0.0188, Val Loss: 0.0020, LR: 0.0008438508\n",
      "Epoch [131/500], Batch [10/110], Train Loss: 0.0078, Val Loss: 0.0026, LR: 0.0008436447\n",
      "Epoch [131/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0020, LR: 0.0008434384\n",
      "Epoch [131/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0035, LR: 0.0008432320\n",
      "Epoch [131/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0008430255\n",
      "Epoch [131/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0030, LR: 0.0008428189\n",
      "Epoch [131/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0008426122\n",
      "Epoch [131/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0023, LR: 0.0008424053\n",
      "Epoch [131/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0021, LR: 0.0008421984\n",
      "Epoch [131/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0040, LR: 0.0008419913\n",
      "Epoch [131/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0008417842\n",
      "Epoch [131/500], Batch [110/110], Train Loss: 0.2629, Val Loss: 0.0023, LR: 0.0008415769\n",
      "Confusion Matrix:\n",
      "[[650   2]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99693   0.99846       652\n",
      "           1    0.99765   1.00000   0.99882       848\n",
      "\n",
      "    accuracy                        0.99867      1500\n",
      "   macro avg    0.99882   0.99847   0.99864      1500\n",
      "weighted avg    0.99867   0.99867   0.99867      1500\n",
      "\n",
      "Total Errors: 2\n",
      "Index: 1218, Predicted: 1, Actual: 0\n",
      "Index: 1397, Predicted: 1, Actual: 0\n",
      "Epoch 131: OK- Accuracy: 0.99867, Precision: 0.99765, Recall: 1.00000, F1: 0.99882, ROC AUC: 0.99847, AUPR (PR-AUC): 0.99765, Sensitivity: 1.00000, Specificity: 0.99693, Far: 0.003067484662576687, False Positive Rate (FPR): 0.00307, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 196.45 MB\n",
      "Epoch [132/500], Batch [10/110], Train Loss: 0.0020, Val Loss: 0.0035, LR: 0.0008413695\n",
      "Epoch [132/500], Batch [20/110], Train Loss: 0.0013, Val Loss: 0.0022, LR: 0.0008411621\n",
      "Epoch [132/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0022, LR: 0.0008409545\n",
      "Epoch [132/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0025, LR: 0.0008407468\n",
      "Epoch [132/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0025, LR: 0.0008405389\n",
      "Epoch [132/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0054, LR: 0.0008403310\n",
      "Epoch [132/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0028, LR: 0.0008401230\n",
      "Epoch [132/500], Batch [80/110], Train Loss: 0.0011, Val Loss: 0.0035, LR: 0.0008399148\n",
      "Epoch [132/500], Batch [90/110], Train Loss: 0.0016, Val Loss: 0.0022, LR: 0.0008397066\n",
      "Epoch [132/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0019, LR: 0.0008394982\n",
      "Epoch [132/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0043, LR: 0.0008392897\n",
      "Epoch [133/500], Batch [10/110], Train Loss: 0.0310, Val Loss: 0.0031, LR: 0.0008390812\n",
      "Epoch [133/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0022, LR: 0.0008388725\n",
      "Epoch [133/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0020, LR: 0.0008386637\n",
      "Epoch [133/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0008384548\n",
      "Epoch [133/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0019, LR: 0.0008382457\n",
      "Epoch [133/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0008380366\n",
      "Epoch [133/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0063, LR: 0.0008378274\n",
      "Epoch [133/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0018, LR: 0.0008376180\n",
      "Epoch [133/500], Batch [90/110], Train Loss: 0.0180, Val Loss: 0.0019, LR: 0.0008374086\n",
      "Epoch [133/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0019, LR: 0.0008371990\n",
      "Epoch [133/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0008369894\n",
      "Epoch [134/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0020, LR: 0.0008367796\n",
      "Epoch [134/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0035, LR: 0.0008365697\n",
      "Epoch [134/500], Batch [30/110], Train Loss: 0.0037, Val Loss: 0.0025, LR: 0.0008363597\n",
      "Epoch [134/500], Batch [40/110], Train Loss: 0.0021, Val Loss: 0.0021, LR: 0.0008361496\n",
      "Epoch [134/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0008359394\n",
      "Epoch [134/500], Batch [60/110], Train Loss: 0.0070, Val Loss: 0.0031, LR: 0.0008357291\n",
      "Epoch [134/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0041, LR: 0.0008355187\n",
      "Epoch [134/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0024, LR: 0.0008353081\n",
      "Epoch [134/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0008350975\n",
      "Epoch [134/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0008348867\n",
      "Epoch [134/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0023, LR: 0.0008346759\n",
      "Epoch [135/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0026, LR: 0.0008344649\n",
      "Epoch [135/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0045, LR: 0.0008342538\n",
      "Epoch [135/500], Batch [30/110], Train Loss: 0.0048, Val Loss: 0.0027, LR: 0.0008340427\n",
      "Epoch [135/500], Batch [40/110], Train Loss: 0.0010, Val Loss: 0.0021, LR: 0.0008338314\n",
      "Epoch [135/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0008336200\n",
      "Epoch [135/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0008334085\n",
      "Epoch [135/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0055, LR: 0.0008331969\n",
      "Epoch [135/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0008329852\n",
      "Epoch [135/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0026, LR: 0.0008327733\n",
      "Epoch [135/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0019, LR: 0.0008325614\n",
      "Epoch [135/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0019, LR: 0.0008323494\n",
      "Epoch [136/500], Batch [10/110], Train Loss: 0.0264, Val Loss: 0.0025, LR: 0.0008321372\n",
      "Epoch [136/500], Batch [20/110], Train Loss: 0.0144, Val Loss: 0.0023, LR: 0.0008319250\n",
      "Epoch [136/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0008317126\n",
      "Epoch [136/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0024, LR: 0.0008315002\n",
      "Epoch [136/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0023, LR: 0.0008312876\n",
      "Epoch [136/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0027, LR: 0.0008310749\n",
      "Epoch [136/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0021, LR: 0.0008308621\n",
      "Epoch [136/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0037, LR: 0.0008306493\n",
      "Epoch [136/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0019, LR: 0.0008304363\n",
      "Epoch [136/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0008302232\n",
      "Epoch [136/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0016, LR: 0.0008300099\n",
      "Epoch [137/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0046, LR: 0.0008297966\n",
      "Epoch [137/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0018, LR: 0.0008295832\n",
      "Epoch [137/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0041, LR: 0.0008293697\n",
      "Epoch [137/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0042, LR: 0.0008291561\n",
      "Epoch [137/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0030, LR: 0.0008289423\n",
      "Epoch [137/500], Batch [60/110], Train Loss: 0.0014, Val Loss: 0.0035, LR: 0.0008287285\n",
      "Epoch [137/500], Batch [70/110], Train Loss: 0.0030, Val Loss: 0.0022, LR: 0.0008285145\n",
      "Epoch [137/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0016, LR: 0.0008283005\n",
      "Epoch [137/500], Batch [90/110], Train Loss: 0.0164, Val Loss: 0.0019, LR: 0.0008280863\n",
      "Epoch [137/500], Batch [100/110], Train Loss: 0.0208, Val Loss: 0.0042, LR: 0.0008278721\n",
      "Epoch [137/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0038, LR: 0.0008276577\n",
      "Epoch [138/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0008274432\n",
      "Epoch [138/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0018, LR: 0.0008272286\n",
      "Epoch [138/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0008270140\n",
      "Epoch [138/500], Batch [40/110], Train Loss: 0.0075, Val Loss: 0.0023, LR: 0.0008267992\n",
      "Epoch [138/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0019, LR: 0.0008265843\n",
      "Epoch [138/500], Batch [60/110], Train Loss: 0.0173, Val Loss: 0.0018, LR: 0.0008263693\n",
      "Epoch [138/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0025, LR: 0.0008261542\n",
      "Epoch [138/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0021, LR: 0.0008259390\n",
      "Epoch [138/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0026, LR: 0.0008257236\n",
      "Epoch [138/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0008255082\n",
      "Epoch [138/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0036, LR: 0.0008252927\n",
      "Epoch [139/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0008250771\n",
      "Epoch [139/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0008248613\n",
      "Epoch [139/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0033, LR: 0.0008246455\n",
      "Epoch [139/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0023, LR: 0.0008244296\n",
      "Epoch [139/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0036, LR: 0.0008242135\n",
      "Epoch [139/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0008239974\n",
      "Epoch [139/500], Batch [70/110], Train Loss: 0.0028, Val Loss: 0.0020, LR: 0.0008237811\n",
      "Epoch [139/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0015, LR: 0.0008235648\n",
      "Epoch [139/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0008233483\n",
      "Epoch [139/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0027, LR: 0.0008231317\n",
      "Epoch [139/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0008229151\n",
      "Epoch [140/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0029, LR: 0.0008226983\n",
      "Epoch [140/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0021, LR: 0.0008224814\n",
      "Epoch [140/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0015, LR: 0.0008222644\n",
      "Epoch [140/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0018, LR: 0.0008220474\n",
      "Epoch [140/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0039, LR: 0.0008218302\n",
      "Epoch [140/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0018, LR: 0.0008216129\n",
      "Epoch [140/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0008213955\n",
      "Epoch [140/500], Batch [80/110], Train Loss: 0.0017, Val Loss: 0.0016, LR: 0.0008211780\n",
      "Epoch [140/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0015, LR: 0.0008209604\n",
      "Epoch [140/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0017, LR: 0.0008207427\n",
      "Epoch [140/500], Batch [110/110], Train Loss: 0.0240, Val Loss: 0.0023, LR: 0.0008205249\n",
      "Epoch [141/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0008203070\n",
      "Epoch [141/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0015, LR: 0.0008200890\n",
      "Epoch [141/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0038, LR: 0.0008198708\n",
      "Epoch [141/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0015, LR: 0.0008196526\n",
      "Epoch [141/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0008194343\n",
      "Epoch [141/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0008192159\n",
      "Epoch [141/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0008189974\n",
      "Epoch [141/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0028, LR: 0.0008187787\n",
      "Epoch [141/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0008185600\n",
      "Epoch [141/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0040, LR: 0.0008183412\n",
      "Epoch [141/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0074, LR: 0.0008181222\n",
      "Confusion Matrix:\n",
      "[[642  10]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.98466   0.99227       652\n",
      "           1    0.98834   1.00000   0.99414       848\n",
      "\n",
      "    accuracy                        0.99333      1500\n",
      "   macro avg    0.99417   0.99233   0.99321      1500\n",
      "weighted avg    0.99341   0.99333   0.99333      1500\n",
      "\n",
      "Total Errors: 10\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 258, Predicted: 1, Actual: 0\n",
      "Index: 625, Predicted: 1, Actual: 0\n",
      "Index: 776, Predicted: 1, Actual: 0\n",
      "Index: 877, Predicted: 1, Actual: 0\n",
      "Epoch 141: OK- Accuracy: 0.99333, Precision: 0.98834, Recall: 1.00000, F1: 0.99414, ROC AUC: 0.99233, AUPR (PR-AUC): 0.98834, Sensitivity: 1.00000, Specificity: 0.98466, Far: 0.015337423312883436, False Positive Rate (FPR): 0.01534, False Negative Rate (FNR): 0.00000, Runtime: 0.046 sec , Memory Usage: 196.45 MB\n",
      "Epoch [142/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0008179032\n",
      "Epoch [142/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0025, LR: 0.0008176841\n",
      "Epoch [142/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0016, LR: 0.0008174648\n",
      "Epoch [142/500], Batch [40/110], Train Loss: 0.0015, Val Loss: 0.0018, LR: 0.0008172455\n",
      "Epoch [142/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0032, LR: 0.0008170260\n",
      "Epoch [142/500], Batch [60/110], Train Loss: 0.0102, Val Loss: 0.0015, LR: 0.0008168065\n",
      "Epoch [142/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0008165868\n",
      "Epoch [142/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0008163671\n",
      "Epoch [142/500], Batch [90/110], Train Loss: 0.0216, Val Loss: 0.0018, LR: 0.0008161472\n",
      "Epoch [142/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0021, LR: 0.0008159273\n",
      "Epoch [142/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0008157072\n",
      "Epoch [143/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0026, LR: 0.0008154871\n",
      "Epoch [143/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0029, LR: 0.0008152668\n",
      "Epoch [143/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0017, LR: 0.0008150465\n",
      "Epoch [143/500], Batch [40/110], Train Loss: 0.0078, Val Loss: 0.0014, LR: 0.0008148260\n",
      "Epoch [143/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0023, LR: 0.0008146054\n",
      "Epoch [143/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0017, LR: 0.0008143848\n",
      "Epoch [143/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0008141640\n",
      "Epoch [143/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0008139431\n",
      "Epoch [143/500], Batch [90/110], Train Loss: 0.0078, Val Loss: 0.0016, LR: 0.0008137222\n",
      "Epoch [143/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0008135011\n",
      "Epoch [143/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0008132800\n",
      "Epoch [144/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0018, LR: 0.0008130587\n",
      "Epoch [144/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0044, LR: 0.0008128373\n",
      "Epoch [144/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0014, LR: 0.0008126159\n",
      "Epoch [144/500], Batch [40/110], Train Loss: 0.0027, Val Loss: 0.0015, LR: 0.0008123943\n",
      "Epoch [144/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0089, LR: 0.0008121726\n",
      "Epoch [144/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0008119508\n",
      "Epoch [144/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0053, LR: 0.0008117290\n",
      "Epoch [144/500], Batch [80/110], Train Loss: 0.0887, Val Loss: 0.0051, LR: 0.0008115070\n",
      "Epoch [144/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0052, LR: 0.0008112849\n",
      "Epoch [144/500], Batch [100/110], Train Loss: 0.0043, Val Loss: 0.0015, LR: 0.0008110628\n",
      "Epoch [144/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0035, LR: 0.0008108405\n",
      "Epoch [145/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0014, LR: 0.0008106181\n",
      "Epoch [145/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0008103957\n",
      "Epoch [145/500], Batch [30/110], Train Loss: 0.0043, Val Loss: 0.0013, LR: 0.0008101731\n",
      "Epoch [145/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0008099504\n",
      "Epoch [145/500], Batch [50/110], Train Loss: 0.2989, Val Loss: 0.0014, LR: 0.0008097277\n",
      "Epoch [145/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0027, LR: 0.0008095048\n",
      "Epoch [145/500], Batch [70/110], Train Loss: 0.0039, Val Loss: 0.0019, LR: 0.0008092818\n",
      "Epoch [145/500], Batch [80/110], Train Loss: 0.2354, Val Loss: 0.0020, LR: 0.0008090588\n",
      "Epoch [145/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0040, LR: 0.0008088356\n",
      "Epoch [145/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0140, LR: 0.0008086124\n",
      "Epoch [145/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0008083890\n",
      "Epoch [146/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0026, LR: 0.0008081655\n",
      "Epoch [146/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0008079420\n",
      "Epoch [146/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0008077183\n",
      "Epoch [146/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0008074946\n",
      "Epoch [146/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0008072707\n",
      "Epoch [146/500], Batch [60/110], Train Loss: 0.0032, Val Loss: 0.0032, LR: 0.0008070467\n",
      "Epoch [146/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0008068227\n",
      "Epoch [146/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0008065985\n",
      "Epoch [146/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0008063743\n",
      "Epoch [146/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0008061499\n",
      "Epoch [146/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0012, LR: 0.0008059255\n",
      "Epoch [147/500], Batch [10/110], Train Loss: 0.0094, Val Loss: 0.0039, LR: 0.0008057010\n",
      "Epoch [147/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0008054763\n",
      "Epoch [147/500], Batch [30/110], Train Loss: 0.0026, Val Loss: 0.0016, LR: 0.0008052516\n",
      "Epoch [147/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0016, LR: 0.0008050267\n",
      "Epoch [147/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0021, LR: 0.0008048018\n",
      "Epoch [147/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0039, LR: 0.0008045768\n",
      "Epoch [147/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0022, LR: 0.0008043516\n",
      "Epoch [147/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0014, LR: 0.0008041264\n",
      "Epoch [147/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0019, LR: 0.0008039011\n",
      "Epoch [147/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0014, LR: 0.0008036756\n",
      "Epoch [147/500], Batch [110/110], Train Loss: 0.0035, Val Loss: 0.0013, LR: 0.0008034501\n",
      "Epoch [148/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0021, LR: 0.0008032245\n",
      "Epoch [148/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0008029988\n",
      "Epoch [148/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0008027730\n",
      "Epoch [148/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0020, LR: 0.0008025471\n",
      "Epoch [148/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0018, LR: 0.0008023211\n",
      "Epoch [148/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0013, LR: 0.0008020949\n",
      "Epoch [148/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0013, LR: 0.0008018687\n",
      "Epoch [148/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0053, LR: 0.0008016424\n",
      "Epoch [148/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0008014160\n",
      "Epoch [148/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0008011896\n",
      "Epoch [148/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0008009630\n",
      "Epoch [149/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0018, LR: 0.0008007363\n",
      "Epoch [149/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0008005095\n",
      "Epoch [149/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0008002826\n",
      "Epoch [149/500], Batch [40/110], Train Loss: 0.0018, Val Loss: 0.0013, LR: 0.0008000556\n",
      "Epoch [149/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0007998286\n",
      "Epoch [149/500], Batch [60/110], Train Loss: 0.0910, Val Loss: 0.0038, LR: 0.0007996014\n",
      "Epoch [149/500], Batch [70/110], Train Loss: 0.0019, Val Loss: 0.0016, LR: 0.0007993741\n",
      "Epoch [149/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0027, LR: 0.0007991468\n",
      "Epoch [149/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0102, LR: 0.0007989193\n",
      "Epoch [149/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0065, LR: 0.0007986918\n",
      "Epoch [149/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007984641\n",
      "Epoch [150/500], Batch [10/110], Train Loss: 0.0211, Val Loss: 0.0017, LR: 0.0007982364\n",
      "Epoch [150/500], Batch [20/110], Train Loss: 0.0015, Val Loss: 0.0014, LR: 0.0007980085\n",
      "Epoch [150/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0017, LR: 0.0007977806\n",
      "Epoch [150/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0015, LR: 0.0007975526\n",
      "Epoch [150/500], Batch [50/110], Train Loss: 0.2633, Val Loss: 0.0012, LR: 0.0007973245\n",
      "Epoch [150/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007970962\n",
      "Epoch [150/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007968679\n",
      "Epoch [150/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0007966395\n",
      "Epoch [150/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0007964110\n",
      "Epoch [150/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0017, LR: 0.0007961824\n",
      "Epoch [150/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0019, LR: 0.0007959537\n",
      "Epoch [151/500], Batch [10/110], Train Loss: 0.0067, Val Loss: 0.0035, LR: 0.0007957249\n",
      "Epoch [151/500], Batch [20/110], Train Loss: 0.0014, Val Loss: 0.0044, LR: 0.0007954960\n",
      "Epoch [151/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0149, LR: 0.0007952670\n",
      "Epoch [151/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0259, LR: 0.0007950380\n",
      "Epoch [151/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0106, LR: 0.0007948088\n",
      "Epoch [151/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0040, LR: 0.0007945795\n",
      "Epoch [151/500], Batch [70/110], Train Loss: 0.0092, Val Loss: 0.0021, LR: 0.0007943502\n",
      "Epoch [151/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007941207\n",
      "Epoch [151/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0028, LR: 0.0007938912\n",
      "Epoch [151/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0039, LR: 0.0007936615\n",
      "Epoch [151/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0040, LR: 0.0007934318\n",
      "Confusion Matrix:\n",
      "[[647   5]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99233   0.99615       652\n",
      "           1    0.99414   1.00000   0.99706       848\n",
      "\n",
      "    accuracy                        0.99667      1500\n",
      "   macro avg    0.99707   0.99617   0.99661      1500\n",
      "weighted avg    0.99669   0.99667   0.99667      1500\n",
      "\n",
      "Total Errors: 5\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 877, Predicted: 1, Actual: 0\n",
      "Index: 1218, Predicted: 1, Actual: 0\n",
      "Index: 1237, Predicted: 1, Actual: 0\n",
      "Index: 1397, Predicted: 1, Actual: 0\n",
      "Epoch 151: OK- Accuracy: 0.99667, Precision: 0.99414, Recall: 1.00000, F1: 0.99706, ROC AUC: 0.99617, AUPR (PR-AUC): 0.99414, Sensitivity: 1.00000, Specificity: 0.99233, Far: 0.007668711656441718, False Positive Rate (FPR): 0.00767, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 196.45 MB\n",
      "Epoch [152/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0036, LR: 0.0007932020\n",
      "Epoch [152/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007929720\n",
      "Epoch [152/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0007927420\n",
      "Epoch [152/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0007925119\n",
      "Epoch [152/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0007922817\n",
      "Epoch [152/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0007920514\n",
      "Epoch [152/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0015, LR: 0.0007918210\n",
      "Epoch [152/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0007915905\n",
      "Epoch [152/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007913599\n",
      "Epoch [152/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0007911293\n",
      "Epoch [152/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0016, LR: 0.0007908985\n",
      "Epoch [153/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0015, LR: 0.0007906676\n",
      "Epoch [153/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0016, LR: 0.0007904367\n",
      "Epoch [153/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0020, LR: 0.0007902056\n",
      "Epoch [153/500], Batch [40/110], Train Loss: 0.0354, Val Loss: 0.0026, LR: 0.0007899745\n",
      "Epoch [153/500], Batch [50/110], Train Loss: 0.0019, Val Loss: 0.0014, LR: 0.0007897433\n",
      "Epoch [153/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0007895119\n",
      "Epoch [153/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0026, LR: 0.0007892805\n",
      "Epoch [153/500], Batch [80/110], Train Loss: 0.0008, Val Loss: 0.0024, LR: 0.0007890490\n",
      "Epoch [153/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0007888174\n",
      "Epoch [153/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0016, LR: 0.0007885857\n",
      "Epoch [153/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0021, LR: 0.0007883539\n",
      "Epoch [154/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0014, LR: 0.0007881220\n",
      "Epoch [154/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007878900\n",
      "Epoch [154/500], Batch [30/110], Train Loss: 0.0036, Val Loss: 0.0013, LR: 0.0007876580\n",
      "Epoch [154/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0016, LR: 0.0007874258\n",
      "Epoch [154/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0026, LR: 0.0007871936\n",
      "Epoch [154/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0030, LR: 0.0007869612\n",
      "Epoch [154/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0007867288\n",
      "Epoch [154/500], Batch [80/110], Train Loss: 0.0012, Val Loss: 0.0013, LR: 0.0007864963\n",
      "Epoch [154/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0031, LR: 0.0007862636\n",
      "Epoch [154/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0099, LR: 0.0007860309\n",
      "Epoch [154/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0041, LR: 0.0007857981\n",
      "Epoch [155/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0007855652\n",
      "Epoch [155/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0026, LR: 0.0007853322\n",
      "Epoch [155/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0041, LR: 0.0007850992\n",
      "Epoch [155/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0012, LR: 0.0007848660\n",
      "Epoch [155/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007846327\n",
      "Epoch [155/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0007843994\n",
      "Epoch [155/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0013, LR: 0.0007841660\n",
      "Epoch [155/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007839324\n",
      "Epoch [155/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007836988\n",
      "Epoch [155/500], Batch [100/110], Train Loss: 0.0015, Val Loss: 0.0014, LR: 0.0007834651\n",
      "Epoch [155/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0021, LR: 0.0007832313\n",
      "Epoch [156/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0007829974\n",
      "Epoch [156/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0024, LR: 0.0007827634\n",
      "Epoch [156/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0015, LR: 0.0007825293\n",
      "Epoch [156/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0049, LR: 0.0007822951\n",
      "Epoch [156/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0027, LR: 0.0007820609\n",
      "Epoch [156/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0021, LR: 0.0007818265\n",
      "Epoch [156/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0007815921\n",
      "Epoch [156/500], Batch [80/110], Train Loss: 0.0186, Val Loss: 0.0018, LR: 0.0007813576\n",
      "Epoch [156/500], Batch [90/110], Train Loss: 0.0026, Val Loss: 0.0015, LR: 0.0007811229\n",
      "Epoch [156/500], Batch [100/110], Train Loss: 0.0064, Val Loss: 0.0023, LR: 0.0007808882\n",
      "Epoch [156/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0027, LR: 0.0007806534\n",
      "Epoch [157/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007804185\n",
      "Epoch [157/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007801836\n",
      "Epoch [157/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0007799485\n",
      "Epoch [157/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0031, LR: 0.0007797133\n",
      "Epoch [157/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0036, LR: 0.0007794781\n",
      "Epoch [157/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0031, LR: 0.0007792427\n",
      "Epoch [157/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0007790073\n",
      "Epoch [157/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0018, LR: 0.0007787718\n",
      "Epoch [157/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0014, LR: 0.0007785362\n",
      "Epoch [157/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007783005\n",
      "Epoch [157/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0007780647\n",
      "Epoch [158/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0036, LR: 0.0007778288\n",
      "Epoch [158/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0007775929\n",
      "Epoch [158/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0024, LR: 0.0007773568\n",
      "Epoch [158/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0013, LR: 0.0007771207\n",
      "Epoch [158/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007768844\n",
      "Epoch [158/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0025, LR: 0.0007766481\n",
      "Epoch [158/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0037, LR: 0.0007764117\n",
      "Epoch [158/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0016, LR: 0.0007761752\n",
      "Epoch [158/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0007759386\n",
      "Epoch [158/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0021, LR: 0.0007757020\n",
      "Epoch [158/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0012, LR: 0.0007754652\n",
      "Epoch [159/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0012, LR: 0.0007752284\n",
      "Epoch [159/500], Batch [20/110], Train Loss: 0.0055, Val Loss: 0.0012, LR: 0.0007749914\n",
      "Epoch [159/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0012, LR: 0.0007747544\n",
      "Epoch [159/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0012, LR: 0.0007745173\n",
      "Epoch [159/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0016, LR: 0.0007742801\n",
      "Epoch [159/500], Batch [60/110], Train Loss: 0.0016, Val Loss: 0.0016, LR: 0.0007740428\n",
      "Epoch [159/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0019, LR: 0.0007738054\n",
      "Epoch [159/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0031, LR: 0.0007735679\n",
      "Epoch [159/500], Batch [90/110], Train Loss: 0.0374, Val Loss: 0.0035, LR: 0.0007733304\n",
      "Epoch [159/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0013, LR: 0.0007730927\n",
      "Epoch [159/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0018, LR: 0.0007728550\n",
      "Epoch [160/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0022, LR: 0.0007726172\n",
      "Epoch [160/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0007723793\n",
      "Epoch [160/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0007721413\n",
      "Epoch [160/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0017, LR: 0.0007719032\n",
      "Epoch [160/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0007716651\n",
      "Epoch [160/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0022, LR: 0.0007714268\n",
      "Epoch [160/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0014, LR: 0.0007711885\n",
      "Epoch [160/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007709501\n",
      "Epoch [160/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0007707115\n",
      "Epoch [160/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0032, LR: 0.0007704729\n",
      "Epoch [160/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0007702343\n",
      "Epoch [161/500], Batch [10/110], Train Loss: 0.0089, Val Loss: 0.0057, LR: 0.0007699955\n",
      "Epoch [161/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0012, LR: 0.0007697566\n",
      "Epoch [161/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0007695177\n",
      "Epoch [161/500], Batch [40/110], Train Loss: 0.0021, Val Loss: 0.0016, LR: 0.0007692787\n",
      "Epoch [161/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007690395\n",
      "Epoch [161/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0035, LR: 0.0007688003\n",
      "Epoch [161/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0007685611\n",
      "Epoch [161/500], Batch [80/110], Train Loss: 0.0136, Val Loss: 0.0019, LR: 0.0007683217\n",
      "Epoch [161/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0007680822\n",
      "Epoch [161/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0012, LR: 0.0007678427\n",
      "Epoch [161/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0012, LR: 0.0007676030\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 161: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.040 sec , Memory Usage: 196.45 MB\n",
      "Epoch [162/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0013, LR: 0.0007673633\n",
      "Epoch [162/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0007671235\n",
      "Epoch [162/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0025, LR: 0.0007668836\n",
      "Epoch [162/500], Batch [40/110], Train Loss: 0.0057, Val Loss: 0.0012, LR: 0.0007666437\n",
      "Epoch [162/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0007664036\n",
      "Epoch [162/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0031, LR: 0.0007661634\n",
      "Epoch [162/500], Batch [70/110], Train Loss: 0.0015, Val Loss: 0.0012, LR: 0.0007659232\n",
      "Epoch [162/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0011, LR: 0.0007656829\n",
      "Epoch [162/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0015, LR: 0.0007654425\n",
      "Epoch [162/500], Batch [100/110], Train Loss: 0.0349, Val Loss: 0.0056, LR: 0.0007652020\n",
      "Epoch [162/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0035, LR: 0.0007649614\n",
      "Epoch [163/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0012, LR: 0.0007647208\n",
      "Epoch [163/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0013, LR: 0.0007644800\n",
      "Epoch [163/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0007642392\n",
      "Epoch [163/500], Batch [40/110], Train Loss: 0.0009, Val Loss: 0.0024, LR: 0.0007639983\n",
      "Epoch [163/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0007637573\n",
      "Epoch [163/500], Batch [60/110], Train Loss: 0.0026, Val Loss: 0.0010, LR: 0.0007635162\n",
      "Epoch [163/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0007632751\n",
      "Epoch [163/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0036, LR: 0.0007630338\n",
      "Epoch [163/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0018, LR: 0.0007627925\n",
      "Epoch [163/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0011, LR: 0.0007625511\n",
      "Epoch [163/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0013, LR: 0.0007623096\n",
      "Epoch [164/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0012, LR: 0.0007620680\n",
      "Epoch [164/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0017, LR: 0.0007618263\n",
      "Epoch [164/500], Batch [30/110], Train Loss: 0.0091, Val Loss: 0.0030, LR: 0.0007615846\n",
      "Epoch [164/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0011, LR: 0.0007613427\n",
      "Epoch [164/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0007611008\n",
      "Epoch [164/500], Batch [60/110], Train Loss: 0.0014, Val Loss: 0.0014, LR: 0.0007608588\n",
      "Epoch [164/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007606167\n",
      "Epoch [164/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0010, LR: 0.0007603746\n",
      "Epoch [164/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0107, LR: 0.0007601323\n",
      "Epoch [164/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0018, LR: 0.0007598900\n",
      "Epoch [164/500], Batch [110/110], Train Loss: 0.0054, Val Loss: 0.0012, LR: 0.0007596476\n",
      "Epoch [165/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0007594051\n",
      "Epoch [165/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007591625\n",
      "Epoch [165/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0007589198\n",
      "Epoch [165/500], Batch [40/110], Train Loss: 0.0057, Val Loss: 0.0022, LR: 0.0007586771\n",
      "Epoch [165/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0078, LR: 0.0007584342\n",
      "Epoch [165/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0039, LR: 0.0007581913\n",
      "Epoch [165/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0016, LR: 0.0007579483\n",
      "Epoch [165/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0012, LR: 0.0007577052\n",
      "Epoch [165/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0035, LR: 0.0007574621\n",
      "Epoch [165/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0031, LR: 0.0007572188\n",
      "Epoch [165/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0038, LR: 0.0007569755\n",
      "Epoch [166/500], Batch [10/110], Train Loss: 0.0363, Val Loss: 0.0027, LR: 0.0007567321\n",
      "Epoch [166/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0038, LR: 0.0007564886\n",
      "Epoch [166/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0007562450\n",
      "Epoch [166/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0016, LR: 0.0007560014\n",
      "Epoch [166/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0015, LR: 0.0007557576\n",
      "Epoch [166/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0017, LR: 0.0007555138\n",
      "Epoch [166/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0030, LR: 0.0007552699\n",
      "Epoch [166/500], Batch [80/110], Train Loss: 0.0019, Val Loss: 0.0025, LR: 0.0007550259\n",
      "Epoch [166/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0010, LR: 0.0007547819\n",
      "Epoch [166/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0007545377\n",
      "Epoch [166/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007542935\n",
      "Epoch [167/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0036, LR: 0.0007540492\n",
      "Epoch [167/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0007538048\n",
      "Epoch [167/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0010, LR: 0.0007535603\n",
      "Epoch [167/500], Batch [40/110], Train Loss: 0.0032, Val Loss: 0.0011, LR: 0.0007533158\n",
      "Epoch [167/500], Batch [50/110], Train Loss: 0.0051, Val Loss: 0.0009, LR: 0.0007530711\n",
      "Epoch [167/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0015, LR: 0.0007528264\n",
      "Epoch [167/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0030, LR: 0.0007525816\n",
      "Epoch [167/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0020, LR: 0.0007523367\n",
      "Epoch [167/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007520918\n",
      "Epoch [167/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007518467\n",
      "Epoch [167/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007516016\n",
      "Epoch [168/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0007513564\n",
      "Epoch [168/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0017, LR: 0.0007511111\n",
      "Epoch [168/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0013, LR: 0.0007508658\n",
      "Epoch [168/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0011, LR: 0.0007506204\n",
      "Epoch [168/500], Batch [50/110], Train Loss: 0.0011, Val Loss: 0.0020, LR: 0.0007503748\n",
      "Epoch [168/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0010, LR: 0.0007501292\n",
      "Epoch [168/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0015, LR: 0.0007498836\n",
      "Epoch [168/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0020, LR: 0.0007496378\n",
      "Epoch [168/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0007493920\n",
      "Epoch [168/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0025, LR: 0.0007491460\n",
      "Epoch [168/500], Batch [110/110], Train Loss: 0.1740, Val Loss: 0.0009, LR: 0.0007489000\n",
      "Epoch [169/500], Batch [10/110], Train Loss: 0.0046, Val Loss: 0.0010, LR: 0.0007486540\n",
      "Epoch [169/500], Batch [20/110], Train Loss: 0.0092, Val Loss: 0.0027, LR: 0.0007484078\n",
      "Epoch [169/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0026, LR: 0.0007481616\n",
      "Epoch [169/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0012, LR: 0.0007479152\n",
      "Epoch [169/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007476688\n",
      "Epoch [169/500], Batch [60/110], Train Loss: 0.0020, Val Loss: 0.0013, LR: 0.0007474224\n",
      "Epoch [169/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007471758\n",
      "Epoch [169/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0007469292\n",
      "Epoch [169/500], Batch [90/110], Train Loss: 0.0070, Val Loss: 0.0009, LR: 0.0007466825\n",
      "Epoch [169/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0007464357\n",
      "Epoch [169/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0016, LR: 0.0007461888\n",
      "Epoch [170/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007459419\n",
      "Epoch [170/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0009, LR: 0.0007456948\n",
      "Epoch [170/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0007454477\n",
      "Epoch [170/500], Batch [40/110], Train Loss: 0.0049, Val Loss: 0.0021, LR: 0.0007452005\n",
      "Epoch [170/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007449533\n",
      "Epoch [170/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0009, LR: 0.0007447059\n",
      "Epoch [170/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007444585\n",
      "Epoch [170/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0007442110\n",
      "Epoch [170/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007439635\n",
      "Epoch [170/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0007437158\n",
      "Epoch [170/500], Batch [110/110], Train Loss: 0.0797, Val Loss: 0.0026, LR: 0.0007434681\n",
      "Epoch [171/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0007432203\n",
      "Epoch [171/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0018, LR: 0.0007429724\n",
      "Epoch [171/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007427244\n",
      "Epoch [171/500], Batch [40/110], Train Loss: 0.0199, Val Loss: 0.0015, LR: 0.0007424764\n",
      "Epoch [171/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007422282\n",
      "Epoch [171/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0007419801\n",
      "Epoch [171/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0025, LR: 0.0007417318\n",
      "Epoch [171/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007414834\n",
      "Epoch [171/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0009, LR: 0.0007412350\n",
      "Epoch [171/500], Batch [100/110], Train Loss: 0.0095, Val Loss: 0.0012, LR: 0.0007409865\n",
      "Epoch [171/500], Batch [110/110], Train Loss: 0.2612, Val Loss: 0.0009, LR: 0.0007407379\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 171: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.037 sec , Memory Usage: 196.45 MB\n",
      "Epoch [172/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0010, LR: 0.0007404893\n",
      "Epoch [172/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007402405\n",
      "Epoch [172/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0008, LR: 0.0007399917\n",
      "Epoch [172/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0007397428\n",
      "Epoch [172/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007394938\n",
      "Epoch [172/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0007392448\n",
      "Epoch [172/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0009, LR: 0.0007389957\n",
      "Epoch [172/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0010, LR: 0.0007387465\n",
      "Epoch [172/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007384972\n",
      "Epoch [172/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0020, LR: 0.0007382479\n",
      "Epoch [172/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0025, LR: 0.0007379984\n",
      "Epoch [173/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0016, LR: 0.0007377489\n",
      "Epoch [173/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007374994\n",
      "Epoch [173/500], Batch [30/110], Train Loss: 0.0016, Val Loss: 0.0008, LR: 0.0007372497\n",
      "Epoch [173/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0007370000\n",
      "Epoch [173/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0055, LR: 0.0007367502\n",
      "Epoch [173/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0042, LR: 0.0007365003\n",
      "Epoch [173/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0007362504\n",
      "Epoch [173/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0007360003\n",
      "Epoch [173/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0015, LR: 0.0007357502\n",
      "Epoch [173/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0007355000\n",
      "Epoch [173/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007352498\n",
      "Epoch [174/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0007349995\n",
      "Epoch [174/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0010, LR: 0.0007347490\n",
      "Epoch [174/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0007344986\n",
      "Epoch [174/500], Batch [40/110], Train Loss: 0.0034, Val Loss: 0.0009, LR: 0.0007342480\n",
      "Epoch [174/500], Batch [50/110], Train Loss: 0.0016, Val Loss: 0.0029, LR: 0.0007339974\n",
      "Epoch [174/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0026, LR: 0.0007337467\n",
      "Epoch [174/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0013, LR: 0.0007334959\n",
      "Epoch [174/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0008, LR: 0.0007332450\n",
      "Epoch [174/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0007329941\n",
      "Epoch [174/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0007327431\n",
      "Epoch [174/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007324920\n",
      "Epoch [175/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0007322409\n",
      "Epoch [175/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007319897\n",
      "Epoch [175/500], Batch [30/110], Train Loss: 0.0022, Val Loss: 0.0008, LR: 0.0007317384\n",
      "Epoch [175/500], Batch [40/110], Train Loss: 0.0102, Val Loss: 0.0016, LR: 0.0007314870\n",
      "Epoch [175/500], Batch [50/110], Train Loss: 0.0032, Val Loss: 0.0017, LR: 0.0007312355\n",
      "Epoch [175/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007309840\n",
      "Epoch [175/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0007307324\n",
      "Epoch [175/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0010, LR: 0.0007304807\n",
      "Epoch [175/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007302290\n",
      "Epoch [175/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0007299772\n",
      "Epoch [175/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0027, LR: 0.0007297253\n",
      "Epoch [176/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007294733\n",
      "Epoch [176/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0007292213\n",
      "Epoch [176/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0007289692\n",
      "Epoch [176/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0007287170\n",
      "Epoch [176/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007284648\n",
      "Epoch [176/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0007282124\n",
      "Epoch [176/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0085, LR: 0.0007279600\n",
      "Epoch [176/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0007277075\n",
      "Epoch [176/500], Batch [90/110], Train Loss: 0.0017, Val Loss: 0.0011, LR: 0.0007274550\n",
      "Epoch [176/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0009, LR: 0.0007272024\n",
      "Epoch [176/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0030, LR: 0.0007269497\n",
      "Epoch [177/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0048, LR: 0.0007266969\n",
      "Epoch [177/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007264441\n",
      "Epoch [177/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0007261912\n",
      "Epoch [177/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0007259382\n",
      "Epoch [177/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0017, LR: 0.0007256851\n",
      "Epoch [177/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0011, LR: 0.0007254320\n",
      "Epoch [177/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007251788\n",
      "Epoch [177/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0009, LR: 0.0007249256\n",
      "Epoch [177/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0089, LR: 0.0007246722\n",
      "Epoch [177/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0007244188\n",
      "Epoch [177/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0023, LR: 0.0007241653\n",
      "Epoch [178/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007239118\n",
      "Epoch [178/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0007236581\n",
      "Epoch [178/500], Batch [30/110], Train Loss: 0.0012, Val Loss: 0.0010, LR: 0.0007234044\n",
      "Epoch [178/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007231507\n",
      "Epoch [178/500], Batch [50/110], Train Loss: 0.0019, Val Loss: 0.0016, LR: 0.0007228968\n",
      "Epoch [178/500], Batch [60/110], Train Loss: 0.1248, Val Loss: 0.0037, LR: 0.0007226429\n",
      "Epoch [178/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0018, LR: 0.0007223889\n",
      "Epoch [178/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007221349\n",
      "Epoch [178/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0007218808\n",
      "Epoch [178/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007216266\n",
      "Epoch [178/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0007213723\n",
      "Epoch [179/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0007211180\n",
      "Epoch [179/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0018, LR: 0.0007208636\n",
      "Epoch [179/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007206091\n",
      "Epoch [179/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0008, LR: 0.0007203545\n",
      "Epoch [179/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0007200999\n",
      "Epoch [179/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0011, LR: 0.0007198452\n",
      "Epoch [179/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0012, LR: 0.0007195905\n",
      "Epoch [179/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007193356\n",
      "Epoch [179/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007190807\n",
      "Epoch [179/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007188258\n",
      "Epoch [179/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007185707\n",
      "Epoch [180/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007183156\n",
      "Epoch [180/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0007180605\n",
      "Epoch [180/500], Batch [30/110], Train Loss: 0.0017, Val Loss: 0.0022, LR: 0.0007178052\n",
      "Epoch [180/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0007175499\n",
      "Epoch [180/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0034, LR: 0.0007172945\n",
      "Epoch [180/500], Batch [60/110], Train Loss: 0.0036, Val Loss: 0.0007, LR: 0.0007170391\n",
      "Epoch [180/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0010, LR: 0.0007167835\n",
      "Epoch [180/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007165279\n",
      "Epoch [180/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0021, LR: 0.0007162723\n",
      "Epoch [180/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007160165\n",
      "Epoch [180/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0015, LR: 0.0007157607\n",
      "Epoch [181/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0019, LR: 0.0007155049\n",
      "Epoch [181/500], Batch [20/110], Train Loss: 0.0011, Val Loss: 0.0015, LR: 0.0007152489\n",
      "Epoch [181/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007149929\n",
      "Epoch [181/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0010, LR: 0.0007147369\n",
      "Epoch [181/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0007144807\n",
      "Epoch [181/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007142245\n",
      "Epoch [181/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0007139682\n",
      "Epoch [181/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0010, LR: 0.0007137119\n",
      "Epoch [181/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0007134555\n",
      "Epoch [181/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0007131990\n",
      "Epoch [181/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007129424\n",
      "Confusion Matrix:\n",
      "[[649   3]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99540   0.99769       652\n",
      "           1    0.99647   1.00000   0.99823       848\n",
      "\n",
      "    accuracy                        0.99800      1500\n",
      "   macro avg    0.99824   0.99770   0.99796      1500\n",
      "weighted avg    0.99801   0.99800   0.99800      1500\n",
      "\n",
      "Total Errors: 3\n",
      "Index: 77, Predicted: 1, Actual: 0\n",
      "Index: 1218, Predicted: 1, Actual: 0\n",
      "Index: 1397, Predicted: 1, Actual: 0\n",
      "Epoch 181: OK- Accuracy: 0.99800, Precision: 0.99647, Recall: 1.00000, F1: 0.99823, ROC AUC: 0.99770, AUPR (PR-AUC): 0.99647, Sensitivity: 1.00000, Specificity: 0.99540, Far: 0.004601226993865031, False Positive Rate (FPR): 0.00460, False Negative Rate (FNR): 0.00000, Runtime: 0.039 sec , Memory Usage: 196.45 MB\n",
      "Epoch [182/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0007126858\n",
      "Epoch [182/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007124291\n",
      "Epoch [182/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0013, LR: 0.0007121724\n",
      "Epoch [182/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0013, LR: 0.0007119156\n",
      "Epoch [182/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0007, LR: 0.0007116587\n",
      "Epoch [182/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007114017\n",
      "Epoch [182/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0061, LR: 0.0007111447\n",
      "Epoch [182/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0007108876\n",
      "Epoch [182/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007106304\n",
      "Epoch [182/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007103732\n",
      "Epoch [182/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007101159\n",
      "Epoch [183/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0007098586\n",
      "Epoch [183/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0007096011\n",
      "Epoch [183/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007093436\n",
      "Epoch [183/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0009, LR: 0.0007090861\n",
      "Epoch [183/500], Batch [50/110], Train Loss: 0.2471, Val Loss: 0.0007, LR: 0.0007088284\n",
      "Epoch [183/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007085708\n",
      "Epoch [183/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0032, LR: 0.0007083130\n",
      "Epoch [183/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0056, LR: 0.0007080552\n",
      "Epoch [183/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0031, LR: 0.0007077973\n",
      "Epoch [183/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007075393\n",
      "Epoch [183/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0007072813\n",
      "Epoch [184/500], Batch [10/110], Train Loss: 0.0022, Val Loss: 0.0008, LR: 0.0007070232\n",
      "Epoch [184/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0007067650\n",
      "Epoch [184/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0007065068\n",
      "Epoch [184/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0011, LR: 0.0007062485\n",
      "Epoch [184/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007059902\n",
      "Epoch [184/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0007057318\n",
      "Epoch [184/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007054733\n",
      "Epoch [184/500], Batch [80/110], Train Loss: 0.0155, Val Loss: 0.0015, LR: 0.0007052147\n",
      "Epoch [184/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007049561\n",
      "Epoch [184/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0013, LR: 0.0007046974\n",
      "Epoch [184/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007044387\n",
      "Epoch [185/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0007041799\n",
      "Epoch [185/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007039210\n",
      "Epoch [185/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007036621\n",
      "Epoch [185/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0007034031\n",
      "Epoch [185/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0011, LR: 0.0007031440\n",
      "Epoch [185/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0009, LR: 0.0007028848\n",
      "Epoch [185/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0011, LR: 0.0007026256\n",
      "Epoch [185/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007023664\n",
      "Epoch [185/500], Batch [90/110], Train Loss: 0.0073, Val Loss: 0.0012, LR: 0.0007021071\n",
      "Epoch [185/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0010, LR: 0.0007018477\n",
      "Epoch [185/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007015882\n",
      "Epoch [186/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0007013287\n",
      "Epoch [186/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007010691\n",
      "Epoch [186/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0007008095\n",
      "Epoch [186/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0007005497\n",
      "Epoch [186/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0007002900\n",
      "Epoch [186/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0036, LR: 0.0007000301\n",
      "Epoch [186/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0008, LR: 0.0006997702\n",
      "Epoch [186/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006995102\n",
      "Epoch [186/500], Batch [90/110], Train Loss: 0.2301, Val Loss: 0.0014, LR: 0.0006992502\n",
      "Epoch [186/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0006989901\n",
      "Epoch [186/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0006987300\n",
      "Epoch [187/500], Batch [10/110], Train Loss: 0.0023, Val Loss: 0.0017, LR: 0.0006984697\n",
      "Epoch [187/500], Batch [20/110], Train Loss: 0.0013, Val Loss: 0.0011, LR: 0.0006982095\n",
      "Epoch [187/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0006979491\n",
      "Epoch [187/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0006976887\n",
      "Epoch [187/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0006974282\n",
      "Epoch [187/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0006971677\n",
      "Epoch [187/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006969071\n",
      "Epoch [187/500], Batch [80/110], Train Loss: 0.0067, Val Loss: 0.0009, LR: 0.0006966464\n",
      "Epoch [187/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0034, LR: 0.0006963857\n",
      "Epoch [187/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0023, LR: 0.0006961249\n",
      "Epoch [187/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006958641\n",
      "Epoch [188/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006956032\n",
      "Epoch [188/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0006953422\n",
      "Epoch [188/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0011, LR: 0.0006950812\n",
      "Epoch [188/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006948201\n",
      "Epoch [188/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006945589\n",
      "Epoch [188/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0008, LR: 0.0006942977\n",
      "Epoch [188/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006940364\n",
      "Epoch [188/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006937751\n",
      "Epoch [188/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0006935136\n",
      "Epoch [188/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0016, LR: 0.0006932522\n",
      "Epoch [188/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0006929907\n",
      "Epoch [189/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006927291\n",
      "Epoch [189/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0007, LR: 0.0006924674\n",
      "Epoch [189/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0007, LR: 0.0006922057\n",
      "Epoch [189/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0012, LR: 0.0006919439\n",
      "Epoch [189/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0006916821\n",
      "Epoch [189/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006914202\n",
      "Epoch [189/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0009, LR: 0.0006911582\n",
      "Epoch [189/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0006908962\n",
      "Epoch [189/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006906341\n",
      "Epoch [189/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0007, LR: 0.0006903720\n",
      "Epoch [189/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0010, LR: 0.0006901098\n",
      "Epoch [190/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006898475\n",
      "Epoch [190/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0006895852\n",
      "Epoch [190/500], Batch [30/110], Train Loss: 0.0713, Val Loss: 0.0008, LR: 0.0006893228\n",
      "Epoch [190/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0006890604\n",
      "Epoch [190/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0006887979\n",
      "Epoch [190/500], Batch [60/110], Train Loss: 0.0023, Val Loss: 0.0027, LR: 0.0006885353\n",
      "Epoch [190/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0007, LR: 0.0006882727\n",
      "Epoch [190/500], Batch [80/110], Train Loss: 0.0013, Val Loss: 0.0014, LR: 0.0006880100\n",
      "Epoch [190/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006877473\n",
      "Epoch [190/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006874845\n",
      "Epoch [190/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0009, LR: 0.0006872217\n",
      "Epoch [191/500], Batch [10/110], Train Loss: 0.0091, Val Loss: 0.0011, LR: 0.0006869587\n",
      "Epoch [191/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0013, LR: 0.0006866958\n",
      "Epoch [191/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0010, LR: 0.0006864327\n",
      "Epoch [191/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0006861696\n",
      "Epoch [191/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0015, LR: 0.0006859065\n",
      "Epoch [191/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0006856433\n",
      "Epoch [191/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006853800\n",
      "Epoch [191/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006851167\n",
      "Epoch [191/500], Batch [90/110], Train Loss: 0.0059, Val Loss: 0.0028, LR: 0.0006848533\n",
      "Epoch [191/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0006845898\n",
      "Epoch [191/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0007, LR: 0.0006843263\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 191: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.042 sec , Memory Usage: 192.43 MB\n",
      "Epoch [192/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006840627\n",
      "Epoch [192/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0006837991\n",
      "Epoch [192/500], Batch [30/110], Train Loss: 0.0094, Val Loss: 0.0013, LR: 0.0006835354\n",
      "Epoch [192/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0006832717\n",
      "Epoch [192/500], Batch [50/110], Train Loss: 0.0032, Val Loss: 0.0020, LR: 0.0006830079\n",
      "Epoch [192/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006827440\n",
      "Epoch [192/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006824801\n",
      "Epoch [192/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0006822161\n",
      "Epoch [192/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006819521\n",
      "Epoch [192/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006816880\n",
      "Epoch [192/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0006814239\n",
      "Epoch [193/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0006811597\n",
      "Epoch [193/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0006808954\n",
      "Epoch [193/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0019, LR: 0.0006806311\n",
      "Epoch [193/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006803667\n",
      "Epoch [193/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006801023\n",
      "Epoch [193/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006798378\n",
      "Epoch [193/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006795732\n",
      "Epoch [193/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0006793086\n",
      "Epoch [193/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006790440\n",
      "Epoch [193/500], Batch [100/110], Train Loss: 0.0056, Val Loss: 0.0010, LR: 0.0006787793\n",
      "Epoch [193/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006785145\n",
      "Epoch [194/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0010, LR: 0.0006782497\n",
      "Epoch [194/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0006779848\n",
      "Epoch [194/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0006777198\n",
      "Epoch [194/500], Batch [40/110], Train Loss: 0.1797, Val Loss: 0.0025, LR: 0.0006774548\n",
      "Epoch [194/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0006771898\n",
      "Epoch [194/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0006769247\n",
      "Epoch [194/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006766595\n",
      "Epoch [194/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0006763943\n",
      "Epoch [194/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006761290\n",
      "Epoch [194/500], Batch [100/110], Train Loss: 0.0015, Val Loss: 0.0019, LR: 0.0006758636\n",
      "Epoch [194/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0006755982\n",
      "Epoch [195/500], Batch [10/110], Train Loss: 0.0019, Val Loss: 0.0011, LR: 0.0006753328\n",
      "Epoch [195/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006750673\n",
      "Epoch [195/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0006748017\n",
      "Epoch [195/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006745361\n",
      "Epoch [195/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0006742704\n",
      "Epoch [195/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0006740047\n",
      "Epoch [195/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0006737389\n",
      "Epoch [195/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0006734731\n",
      "Epoch [195/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0006732072\n",
      "Epoch [195/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006729413\n",
      "Epoch [195/500], Batch [110/110], Train Loss: 0.0018, Val Loss: 0.0018, LR: 0.0006726753\n",
      "Epoch [196/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0006724092\n",
      "Epoch [196/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0011, LR: 0.0006721431\n",
      "Epoch [196/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006718769\n",
      "Epoch [196/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006716107\n",
      "Epoch [196/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0014, LR: 0.0006713444\n",
      "Epoch [196/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0007, LR: 0.0006710781\n",
      "Epoch [196/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006708117\n",
      "Epoch [196/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0006705453\n",
      "Epoch [196/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0010, LR: 0.0006702788\n",
      "Epoch [196/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006700123\n",
      "Epoch [196/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0006697457\n",
      "Epoch [197/500], Batch [10/110], Train Loss: 0.0021, Val Loss: 0.0023, LR: 0.0006694790\n",
      "Epoch [197/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0023, LR: 0.0006692123\n",
      "Epoch [197/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0006689456\n",
      "Epoch [197/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0020, LR: 0.0006686787\n",
      "Epoch [197/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006684119\n",
      "Epoch [197/500], Batch [60/110], Train Loss: 0.0052, Val Loss: 0.0010, LR: 0.0006681450\n",
      "Epoch [197/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0010, LR: 0.0006678780\n",
      "Epoch [197/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0006676110\n",
      "Epoch [197/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006673439\n",
      "Epoch [197/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006670768\n",
      "Epoch [197/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006668096\n",
      "Epoch [198/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006665423\n",
      "Epoch [198/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006662750\n",
      "Epoch [198/500], Batch [30/110], Train Loss: 0.0060, Val Loss: 0.0010, LR: 0.0006660077\n",
      "Epoch [198/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0010, LR: 0.0006657403\n",
      "Epoch [198/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006654729\n",
      "Epoch [198/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0006652054\n",
      "Epoch [198/500], Batch [70/110], Train Loss: 0.0037, Val Loss: 0.0013, LR: 0.0006649378\n",
      "Epoch [198/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006646702\n",
      "Epoch [198/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0006644026\n",
      "Epoch [198/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006641348\n",
      "Epoch [198/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0010, LR: 0.0006638671\n",
      "Epoch [199/500], Batch [10/110], Train Loss: 0.0902, Val Loss: 0.0010, LR: 0.0006635993\n",
      "Epoch [199/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006633314\n",
      "Epoch [199/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006630635\n",
      "Epoch [199/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0006627955\n",
      "Epoch [199/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0006625275\n",
      "Epoch [199/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006622594\n",
      "Epoch [199/500], Batch [70/110], Train Loss: 0.0030, Val Loss: 0.0028, LR: 0.0006619913\n",
      "Epoch [199/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0017, LR: 0.0006617231\n",
      "Epoch [199/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006614549\n",
      "Epoch [199/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0007, LR: 0.0006611867\n",
      "Epoch [199/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006609183\n",
      "Epoch [200/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0006606500\n",
      "Epoch [200/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0006603815\n",
      "Epoch [200/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0018, LR: 0.0006601130\n",
      "Epoch [200/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0006598445\n",
      "Epoch [200/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006595759\n",
      "Epoch [200/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0006593073\n",
      "Epoch [200/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006590386\n",
      "Epoch [200/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0009, LR: 0.0006587699\n",
      "Epoch [200/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006585011\n",
      "Epoch [200/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006582323\n",
      "Epoch [200/500], Batch [110/110], Train Loss: 0.0019, Val Loss: 0.0021, LR: 0.0006579634\n",
      "Epoch [201/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0006576945\n",
      "Epoch [201/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0006574255\n",
      "Epoch [201/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006571565\n",
      "Epoch [201/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0006568874\n",
      "Epoch [201/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006566183\n",
      "Epoch [201/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0006563491\n",
      "Epoch [201/500], Batch [70/110], Train Loss: 0.0021, Val Loss: 0.0005, LR: 0.0006560799\n",
      "Epoch [201/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006558106\n",
      "Epoch [201/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006555413\n",
      "Epoch [201/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006552719\n",
      "Epoch [201/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0006550025\n",
      "Confusion Matrix:\n",
      "[[649   3]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99540   0.99769       652\n",
      "           1    0.99647   1.00000   0.99823       848\n",
      "\n",
      "    accuracy                        0.99800      1500\n",
      "   macro avg    0.99824   0.99770   0.99796      1500\n",
      "weighted avg    0.99801   0.99800   0.99800      1500\n",
      "\n",
      "Total Errors: 3\n",
      "Index: 970, Predicted: 1, Actual: 0\n",
      "Index: 1218, Predicted: 1, Actual: 0\n",
      "Index: 1397, Predicted: 1, Actual: 0\n",
      "Epoch 201: OK- Accuracy: 0.99800, Precision: 0.99647, Recall: 1.00000, F1: 0.99823, ROC AUC: 0.99770, AUPR (PR-AUC): 0.99647, Sensitivity: 1.00000, Specificity: 0.99540, Far: 0.004601226993865031, False Positive Rate (FPR): 0.00460, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 192.43 MB\n",
      "Epoch [202/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0006547330\n",
      "Epoch [202/500], Batch [20/110], Train Loss: 0.0021, Val Loss: 0.0007, LR: 0.0006544635\n",
      "Epoch [202/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0006541939\n",
      "Epoch [202/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006539243\n",
      "Epoch [202/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0006536546\n",
      "Epoch [202/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006533849\n",
      "Epoch [202/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006531151\n",
      "Epoch [202/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006528453\n",
      "Epoch [202/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0006525754\n",
      "Epoch [202/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006523055\n",
      "Epoch [202/500], Batch [110/110], Train Loss: 0.0017, Val Loss: 0.0014, LR: 0.0006520356\n",
      "Epoch [203/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0011, LR: 0.0006517656\n",
      "Epoch [203/500], Batch [20/110], Train Loss: 0.0019, Val Loss: 0.0007, LR: 0.0006514955\n",
      "Epoch [203/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0006512254\n",
      "Epoch [203/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006509553\n",
      "Epoch [203/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006506851\n",
      "Epoch [203/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0006504148\n",
      "Epoch [203/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0006501445\n",
      "Epoch [203/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0005, LR: 0.0006498742\n",
      "Epoch [203/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006496038\n",
      "Epoch [203/500], Batch [100/110], Train Loss: 0.0028, Val Loss: 0.0005, LR: 0.0006493334\n",
      "Epoch [203/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0006490629\n",
      "Epoch [204/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0011, LR: 0.0006487924\n",
      "Epoch [204/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006485218\n",
      "Epoch [204/500], Batch [30/110], Train Loss: 0.0023, Val Loss: 0.0009, LR: 0.0006482512\n",
      "Epoch [204/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0006479805\n",
      "Epoch [204/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0006477098\n",
      "Epoch [204/500], Batch [60/110], Train Loss: 0.0030, Val Loss: 0.0008, LR: 0.0006474390\n",
      "Epoch [204/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0006471682\n",
      "Epoch [204/500], Batch [80/110], Train Loss: 0.1058, Val Loss: 0.0005, LR: 0.0006468974\n",
      "Epoch [204/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0006466265\n",
      "Epoch [204/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0018, LR: 0.0006463555\n",
      "Epoch [204/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0006460845\n",
      "Epoch [205/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0046, LR: 0.0006458135\n",
      "Epoch [205/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0007, LR: 0.0006455424\n",
      "Epoch [205/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0006452713\n",
      "Epoch [205/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0006450001\n",
      "Epoch [205/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0009, LR: 0.0006447289\n",
      "Epoch [205/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006444576\n",
      "Epoch [205/500], Batch [70/110], Train Loss: 0.0433, Val Loss: 0.0028, LR: 0.0006441863\n",
      "Epoch [205/500], Batch [80/110], Train Loss: 0.0653, Val Loss: 0.0026, LR: 0.0006439149\n",
      "Epoch [205/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0006436435\n",
      "Epoch [205/500], Batch [100/110], Train Loss: 0.0013, Val Loss: 0.0006, LR: 0.0006433721\n",
      "Epoch [205/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0006431006\n",
      "Epoch [206/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0014, LR: 0.0006428291\n",
      "Epoch [206/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0031, LR: 0.0006425575\n",
      "Epoch [206/500], Batch [30/110], Train Loss: 0.0185, Val Loss: 0.0026, LR: 0.0006422858\n",
      "Epoch [206/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006420142\n",
      "Epoch [206/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0006417425\n",
      "Epoch [206/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0009, LR: 0.0006414707\n",
      "Epoch [206/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006411989\n",
      "Epoch [206/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0007, LR: 0.0006409270\n",
      "Epoch [206/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006406551\n",
      "Epoch [206/500], Batch [100/110], Train Loss: 0.0016, Val Loss: 0.0018, LR: 0.0006403832\n",
      "Epoch [206/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006401112\n",
      "Epoch [207/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006398392\n",
      "Epoch [207/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0047, LR: 0.0006395671\n",
      "Epoch [207/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0022, LR: 0.0006392950\n",
      "Epoch [207/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0006390228\n",
      "Epoch [207/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006387506\n",
      "Epoch [207/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006384784\n",
      "Epoch [207/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0006382061\n",
      "Epoch [207/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0006379337\n",
      "Epoch [207/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006376614\n",
      "Epoch [207/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0009, LR: 0.0006373889\n",
      "Epoch [207/500], Batch [110/110], Train Loss: 0.0267, Val Loss: 0.0020, LR: 0.0006371165\n",
      "Epoch [208/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0006368440\n",
      "Epoch [208/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0031, LR: 0.0006365714\n",
      "Epoch [208/500], Batch [30/110], Train Loss: 0.0013, Val Loss: 0.0009, LR: 0.0006362988\n",
      "Epoch [208/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006360262\n",
      "Epoch [208/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0006357535\n",
      "Epoch [208/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0014, LR: 0.0006354808\n",
      "Epoch [208/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0009, LR: 0.0006352080\n",
      "Epoch [208/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006349352\n",
      "Epoch [208/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006346624\n",
      "Epoch [208/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0006343895\n",
      "Epoch [208/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006341165\n",
      "Epoch [209/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006338436\n",
      "Epoch [209/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0006335706\n",
      "Epoch [209/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006332975\n",
      "Epoch [209/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0006330244\n",
      "Epoch [209/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006327512\n",
      "Epoch [209/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0006324781\n",
      "Epoch [209/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0006322048\n",
      "Epoch [209/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006319316\n",
      "Epoch [209/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006316583\n",
      "Epoch [209/500], Batch [100/110], Train Loss: 0.0032, Val Loss: 0.0007, LR: 0.0006313849\n",
      "Epoch [209/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0006, LR: 0.0006311115\n",
      "Epoch [210/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0006308381\n",
      "Epoch [210/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006305646\n",
      "Epoch [210/500], Batch [30/110], Train Loss: 0.1548, Val Loss: 0.0019, LR: 0.0006302911\n",
      "Epoch [210/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0006300175\n",
      "Epoch [210/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0006, LR: 0.0006297439\n",
      "Epoch [210/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006294703\n",
      "Epoch [210/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006291966\n",
      "Epoch [210/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006289229\n",
      "Epoch [210/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0006286491\n",
      "Epoch [210/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0008, LR: 0.0006283753\n",
      "Epoch [210/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006281015\n",
      "Epoch [211/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006278276\n",
      "Epoch [211/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0013, LR: 0.0006275537\n",
      "Epoch [211/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006272797\n",
      "Epoch [211/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006270057\n",
      "Epoch [211/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006267317\n",
      "Epoch [211/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006264576\n",
      "Epoch [211/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006261835\n",
      "Epoch [211/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006259093\n",
      "Epoch [211/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0006, LR: 0.0006256351\n",
      "Epoch [211/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006253609\n",
      "Epoch [211/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006250866\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 211: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.036 sec , Memory Usage: 192.43 MB\n",
      "Epoch [212/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0006248123\n",
      "Epoch [212/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006245379\n",
      "Epoch [212/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006242636\n",
      "Epoch [212/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006239891\n",
      "Epoch [212/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006237146\n",
      "Epoch [212/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006234401\n",
      "Epoch [212/500], Batch [70/110], Train Loss: 0.0016, Val Loss: 0.0005, LR: 0.0006231656\n",
      "Epoch [212/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006228910\n",
      "Epoch [212/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0013, LR: 0.0006226164\n",
      "Epoch [212/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0006223417\n",
      "Epoch [212/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006220670\n",
      "Epoch [213/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0009, LR: 0.0006217923\n",
      "Epoch [213/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006215175\n",
      "Epoch [213/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0006212427\n",
      "Epoch [213/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006209678\n",
      "Epoch [213/500], Batch [50/110], Train Loss: 0.0011, Val Loss: 0.0007, LR: 0.0006206929\n",
      "Epoch [213/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006204180\n",
      "Epoch [213/500], Batch [70/110], Train Loss: 0.0029, Val Loss: 0.0007, LR: 0.0006201430\n",
      "Epoch [213/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0006198680\n",
      "Epoch [213/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0005, LR: 0.0006195930\n",
      "Epoch [213/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0006193179\n",
      "Epoch [213/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0006190428\n",
      "Epoch [214/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0026, LR: 0.0006187676\n",
      "Epoch [214/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006184924\n",
      "Epoch [214/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0006182172\n",
      "Epoch [214/500], Batch [40/110], Train Loss: 0.0043, Val Loss: 0.0006, LR: 0.0006179419\n",
      "Epoch [214/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0006176666\n",
      "Epoch [214/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006173913\n",
      "Epoch [214/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006171159\n",
      "Epoch [214/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0008, LR: 0.0006168405\n",
      "Epoch [214/500], Batch [90/110], Train Loss: 0.0021, Val Loss: 0.0006, LR: 0.0006165650\n",
      "Epoch [214/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006162895\n",
      "Epoch [214/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006160140\n",
      "Epoch [215/500], Batch [10/110], Train Loss: 0.0072, Val Loss: 0.0013, LR: 0.0006157385\n",
      "Epoch [215/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0006154629\n",
      "Epoch [215/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0006151872\n",
      "Epoch [215/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0006149116\n",
      "Epoch [215/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006146359\n",
      "Epoch [215/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0014, LR: 0.0006143601\n",
      "Epoch [215/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006140844\n",
      "Epoch [215/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0009, LR: 0.0006138085\n",
      "Epoch [215/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006135327\n",
      "Epoch [215/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0006132568\n",
      "Epoch [215/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0006129809\n",
      "Epoch [216/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006127050\n",
      "Epoch [216/500], Batch [20/110], Train Loss: 0.0156, Val Loss: 0.0032, LR: 0.0006124290\n",
      "Epoch [216/500], Batch [30/110], Train Loss: 0.0019, Val Loss: 0.0022, LR: 0.0006121529\n",
      "Epoch [216/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0006, LR: 0.0006118769\n",
      "Epoch [216/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006116008\n",
      "Epoch [216/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0006113247\n",
      "Epoch [216/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006110485\n",
      "Epoch [216/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006107723\n",
      "Epoch [216/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0006104961\n",
      "Epoch [216/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0006102198\n",
      "Epoch [216/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0006099435\n",
      "Epoch [217/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006096672\n",
      "Epoch [217/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006093908\n",
      "Epoch [217/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0006091144\n",
      "Epoch [217/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0005, LR: 0.0006088380\n",
      "Epoch [217/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006085615\n",
      "Epoch [217/500], Batch [60/110], Train Loss: 0.0629, Val Loss: 0.0006, LR: 0.0006082850\n",
      "Epoch [217/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0006080085\n",
      "Epoch [217/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006077319\n",
      "Epoch [217/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006074553\n",
      "Epoch [217/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0006071787\n",
      "Epoch [217/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0015, LR: 0.0006069020\n",
      "Epoch [218/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006066253\n",
      "Epoch [218/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0006063486\n",
      "Epoch [218/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0005, LR: 0.0006060718\n",
      "Epoch [218/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006057950\n",
      "Epoch [218/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0011, LR: 0.0006055181\n",
      "Epoch [218/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006052413\n",
      "Epoch [218/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0006049644\n",
      "Epoch [218/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0005, LR: 0.0006046874\n",
      "Epoch [218/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0006044105\n",
      "Epoch [218/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006041335\n",
      "Epoch [218/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0006038564\n",
      "Epoch [219/500], Batch [10/110], Train Loss: 0.0014, Val Loss: 0.0005, LR: 0.0006035794\n",
      "Epoch [219/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0006, LR: 0.0006033023\n",
      "Epoch [219/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006030252\n",
      "Epoch [219/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006027480\n",
      "Epoch [219/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006024708\n",
      "Epoch [219/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0007, LR: 0.0006021936\n",
      "Epoch [219/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0006019163\n",
      "Epoch [219/500], Batch [80/110], Train Loss: 0.0013, Val Loss: 0.0005, LR: 0.0006016390\n",
      "Epoch [219/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0006013617\n",
      "Epoch [219/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0009, LR: 0.0006010844\n",
      "Epoch [219/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0013, LR: 0.0006008070\n",
      "Epoch [220/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0006005296\n",
      "Epoch [220/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0006002521\n",
      "Epoch [220/500], Batch [30/110], Train Loss: 0.0037, Val Loss: 0.0009, LR: 0.0005999747\n",
      "Epoch [220/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005996972\n",
      "Epoch [220/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005994196\n",
      "Epoch [220/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005991420\n",
      "Epoch [220/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005988644\n",
      "Epoch [220/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005985868\n",
      "Epoch [220/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0005983092\n",
      "Epoch [220/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005980315\n",
      "Epoch [220/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0005, LR: 0.0005977538\n",
      "Epoch [221/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0005974760\n",
      "Epoch [221/500], Batch [20/110], Train Loss: 0.0046, Val Loss: 0.0010, LR: 0.0005971982\n",
      "Epoch [221/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0005969204\n",
      "Epoch [221/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0005966426\n",
      "Epoch [221/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0005963647\n",
      "Epoch [221/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0005960868\n",
      "Epoch [221/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0006, LR: 0.0005958089\n",
      "Epoch [221/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005955309\n",
      "Epoch [221/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0039, LR: 0.0005952529\n",
      "Epoch [221/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0005949749\n",
      "Epoch [221/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005946969\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 221: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.038 sec , Memory Usage: 192.43 MB\n",
      "Epoch [222/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0005944188\n",
      "Epoch [222/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0005941407\n",
      "Epoch [222/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005938625\n",
      "Epoch [222/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0005935844\n",
      "Epoch [222/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0005933062\n",
      "Epoch [222/500], Batch [60/110], Train Loss: 0.0021, Val Loss: 0.0006, LR: 0.0005930280\n",
      "Epoch [222/500], Batch [70/110], Train Loss: 0.0015, Val Loss: 0.0005, LR: 0.0005927497\n",
      "Epoch [222/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0005924714\n",
      "Epoch [222/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0011, LR: 0.0005921931\n",
      "Epoch [222/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0005919148\n",
      "Epoch [222/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0005916364\n",
      "Epoch [223/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0018, LR: 0.0005913580\n",
      "Epoch [223/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0016, LR: 0.0005910796\n",
      "Epoch [223/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005908012\n",
      "Epoch [223/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005905227\n",
      "Epoch [223/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005902442\n",
      "Epoch [223/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005899656\n",
      "Epoch [223/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005896871\n",
      "Epoch [223/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005894085\n",
      "Epoch [223/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005891299\n",
      "Epoch [223/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005888512\n",
      "Epoch [223/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005885726\n",
      "Epoch [224/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005882939\n",
      "Epoch [224/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005880151\n",
      "Epoch [224/500], Batch [30/110], Train Loss: 0.0033, Val Loss: 0.0012, LR: 0.0005877364\n",
      "Epoch [224/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0005874576\n",
      "Epoch [224/500], Batch [50/110], Train Loss: 0.0010, Val Loss: 0.0021, LR: 0.0005871788\n",
      "Epoch [224/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0005869000\n",
      "Epoch [224/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005866211\n",
      "Epoch [224/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005863422\n",
      "Epoch [224/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0005860633\n",
      "Epoch [224/500], Batch [100/110], Train Loss: 0.0015, Val Loss: 0.0004, LR: 0.0005857844\n",
      "Epoch [224/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005855054\n",
      "Epoch [225/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0005852264\n",
      "Epoch [225/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005849474\n",
      "Epoch [225/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0005846683\n",
      "Epoch [225/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005843893\n",
      "Epoch [225/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0021, LR: 0.0005841102\n",
      "Epoch [225/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0005838311\n",
      "Epoch [225/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005835519\n",
      "Epoch [225/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005832727\n",
      "Epoch [225/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0009, LR: 0.0005829935\n",
      "Epoch [225/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0005827143\n",
      "Epoch [225/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0007, LR: 0.0005824351\n",
      "Epoch [226/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005821558\n",
      "Epoch [226/500], Batch [20/110], Train Loss: 0.0036, Val Loss: 0.0009, LR: 0.0005818765\n",
      "Epoch [226/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005815972\n",
      "Epoch [226/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005813178\n",
      "Epoch [226/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005810384\n",
      "Epoch [226/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0008, LR: 0.0005807590\n",
      "Epoch [226/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005804796\n",
      "Epoch [226/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005802002\n",
      "Epoch [226/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0005799207\n",
      "Epoch [226/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0005796412\n",
      "Epoch [226/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0005793617\n",
      "Epoch [227/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0005790821\n",
      "Epoch [227/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005788025\n",
      "Epoch [227/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005785230\n",
      "Epoch [227/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005782433\n",
      "Epoch [227/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0034, LR: 0.0005779637\n",
      "Epoch [227/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0005776840\n",
      "Epoch [227/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005774043\n",
      "Epoch [227/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005771246\n",
      "Epoch [227/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0005768449\n",
      "Epoch [227/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005765651\n",
      "Epoch [227/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0012, LR: 0.0005762853\n",
      "Epoch [228/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0005760055\n",
      "Epoch [228/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0005757257\n",
      "Epoch [228/500], Batch [30/110], Train Loss: 0.0108, Val Loss: 0.0015, LR: 0.0005754458\n",
      "Epoch [228/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0005751660\n",
      "Epoch [228/500], Batch [50/110], Train Loss: 0.1002, Val Loss: 0.0013, LR: 0.0005748861\n",
      "Epoch [228/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005746061\n",
      "Epoch [228/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005743262\n",
      "Epoch [228/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005740462\n",
      "Epoch [228/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0005737662\n",
      "Epoch [228/500], Batch [100/110], Train Loss: 0.0092, Val Loss: 0.0018, LR: 0.0005734862\n",
      "Epoch [228/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005732062\n",
      "Epoch [229/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0005729261\n",
      "Epoch [229/500], Batch [20/110], Train Loss: 0.0070, Val Loss: 0.0015, LR: 0.0005726461\n",
      "Epoch [229/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0005723660\n",
      "Epoch [229/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005720858\n",
      "Epoch [229/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0005718057\n",
      "Epoch [229/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005715255\n",
      "Epoch [229/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0005712453\n",
      "Epoch [229/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005709651\n",
      "Epoch [229/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0005706849\n",
      "Epoch [229/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0009, LR: 0.0005704046\n",
      "Epoch [229/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005701244\n",
      "Epoch [230/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005698441\n",
      "Epoch [230/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0016, LR: 0.0005695637\n",
      "Epoch [230/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0015, LR: 0.0005692834\n",
      "Epoch [230/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005690030\n",
      "Epoch [230/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005687227\n",
      "Epoch [230/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0015, LR: 0.0005684423\n",
      "Epoch [230/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0005681618\n",
      "Epoch [230/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005678814\n",
      "Epoch [230/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0016, LR: 0.0005676009\n",
      "Epoch [230/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0005673205\n",
      "Epoch [230/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005670400\n",
      "Epoch [231/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005667594\n",
      "Epoch [231/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005664789\n",
      "Epoch [231/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005661983\n",
      "Epoch [231/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005659177\n",
      "Epoch [231/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005656371\n",
      "Epoch [231/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005653565\n",
      "Epoch [231/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005650759\n",
      "Epoch [231/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0005647952\n",
      "Epoch [231/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0005645145\n",
      "Epoch [231/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0005642338\n",
      "Epoch [231/500], Batch [110/110], Train Loss: 0.0042, Val Loss: 0.0008, LR: 0.0005639531\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 231: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.049 sec , Memory Usage: 192.43 MB\n",
      "Epoch [232/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0005636724\n",
      "Epoch [232/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0034, LR: 0.0005633916\n",
      "Epoch [232/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0011, LR: 0.0005631108\n",
      "Epoch [232/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0005628300\n",
      "Epoch [232/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005625492\n",
      "Epoch [232/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0006, LR: 0.0005622684\n",
      "Epoch [232/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0006, LR: 0.0005619875\n",
      "Epoch [232/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0005617066\n",
      "Epoch [232/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0005614257\n",
      "Epoch [232/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005611448\n",
      "Epoch [232/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0005608639\n",
      "Epoch [233/500], Batch [10/110], Train Loss: 0.0196, Val Loss: 0.0021, LR: 0.0005605830\n",
      "Epoch [233/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005603020\n",
      "Epoch [233/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005600210\n",
      "Epoch [233/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005597400\n",
      "Epoch [233/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005594590\n",
      "Epoch [233/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0026, LR: 0.0005591780\n",
      "Epoch [233/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0042, LR: 0.0005588969\n",
      "Epoch [233/500], Batch [80/110], Train Loss: 0.0011, Val Loss: 0.0005, LR: 0.0005586158\n",
      "Epoch [233/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0006, LR: 0.0005583348\n",
      "Epoch [233/500], Batch [100/110], Train Loss: 0.0019, Val Loss: 0.0018, LR: 0.0005580536\n",
      "Epoch [233/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0022, LR: 0.0005577725\n",
      "Epoch [234/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005574914\n",
      "Epoch [234/500], Batch [20/110], Train Loss: 0.0035, Val Loss: 0.0007, LR: 0.0005572102\n",
      "Epoch [234/500], Batch [30/110], Train Loss: 0.0019, Val Loss: 0.0007, LR: 0.0005569290\n",
      "Epoch [234/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005566479\n",
      "Epoch [234/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005563666\n",
      "Epoch [234/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0005560854\n",
      "Epoch [234/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0005558042\n",
      "Epoch [234/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0005, LR: 0.0005555229\n",
      "Epoch [234/500], Batch [90/110], Train Loss: 0.0016, Val Loss: 0.0005, LR: 0.0005552416\n",
      "Epoch [234/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005549604\n",
      "Epoch [234/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0005546790\n",
      "Epoch [235/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005543977\n",
      "Epoch [235/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0005541164\n",
      "Epoch [235/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0005538350\n",
      "Epoch [235/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0006, LR: 0.0005535537\n",
      "Epoch [235/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0011, LR: 0.0005532723\n",
      "Epoch [235/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005529909\n",
      "Epoch [235/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005527094\n",
      "Epoch [235/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005524280\n",
      "Epoch [235/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005521466\n",
      "Epoch [235/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005518651\n",
      "Epoch [235/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0005515836\n",
      "Epoch [236/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005513021\n",
      "Epoch [236/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0005, LR: 0.0005510206\n",
      "Epoch [236/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0010, LR: 0.0005507391\n",
      "Epoch [236/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005504575\n",
      "Epoch [236/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005501760\n",
      "Epoch [236/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0005498944\n",
      "Epoch [236/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0009, LR: 0.0005496128\n",
      "Epoch [236/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005493312\n",
      "Epoch [236/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0005490496\n",
      "Epoch [236/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005487680\n",
      "Epoch [236/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0005484863\n",
      "Epoch [237/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005482047\n",
      "Epoch [237/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0011, LR: 0.0005479230\n",
      "Epoch [237/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005476413\n",
      "Epoch [237/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005473596\n",
      "Epoch [237/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005470779\n",
      "Epoch [237/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0004, LR: 0.0005467962\n",
      "Epoch [237/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005465144\n",
      "Epoch [237/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0007, LR: 0.0005462327\n",
      "Epoch [237/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0005459509\n",
      "Epoch [237/500], Batch [100/110], Train Loss: 0.0220, Val Loss: 0.0013, LR: 0.0005456691\n",
      "Epoch [237/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005453874\n",
      "Epoch [238/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0005451055\n",
      "Epoch [238/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0005448237\n",
      "Epoch [238/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0005445419\n",
      "Epoch [238/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0005442600\n",
      "Epoch [238/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005439782\n",
      "Epoch [238/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005436963\n",
      "Epoch [238/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005434144\n",
      "Epoch [238/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005431325\n",
      "Epoch [238/500], Batch [90/110], Train Loss: 0.0022, Val Loss: 0.0013, LR: 0.0005428506\n",
      "Epoch [238/500], Batch [100/110], Train Loss: 0.0025, Val Loss: 0.0008, LR: 0.0005425687\n",
      "Epoch [238/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0006, LR: 0.0005422868\n",
      "Epoch [239/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005420048\n",
      "Epoch [239/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0013, LR: 0.0005417229\n",
      "Epoch [239/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0005414409\n",
      "Epoch [239/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005411589\n",
      "Epoch [239/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005408769\n",
      "Epoch [239/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005405949\n",
      "Epoch [239/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005403129\n",
      "Epoch [239/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0005400309\n",
      "Epoch [239/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0005397488\n",
      "Epoch [239/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005394668\n",
      "Epoch [239/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005391847\n",
      "Epoch [240/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0005389026\n",
      "Epoch [240/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005386206\n",
      "Epoch [240/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005383385\n",
      "Epoch [240/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005380564\n",
      "Epoch [240/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0005377742\n",
      "Epoch [240/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0005374921\n",
      "Epoch [240/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0005372100\n",
      "Epoch [240/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0005369278\n",
      "Epoch [240/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005366457\n",
      "Epoch [240/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005363635\n",
      "Epoch [240/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0005360813\n",
      "Epoch [241/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0005357991\n",
      "Epoch [241/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005355169\n",
      "Epoch [241/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005352347\n",
      "Epoch [241/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0011, LR: 0.0005349525\n",
      "Epoch [241/500], Batch [50/110], Train Loss: 0.0430, Val Loss: 0.0012, LR: 0.0005346703\n",
      "Epoch [241/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0005343880\n",
      "Epoch [241/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0005341058\n",
      "Epoch [241/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0005338235\n",
      "Epoch [241/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0021, LR: 0.0005335412\n",
      "Epoch [241/500], Batch [100/110], Train Loss: 0.0300, Val Loss: 0.0016, LR: 0.0005332590\n",
      "Epoch [241/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005329767\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 241: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.035 sec , Memory Usage: 192.43 MB\n",
      "Epoch [242/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005326944\n",
      "Epoch [242/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0005324121\n",
      "Epoch [242/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0005321298\n",
      "Epoch [242/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0009, LR: 0.0005318474\n",
      "Epoch [242/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005315651\n",
      "Epoch [242/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005312828\n",
      "Epoch [242/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005310004\n",
      "Epoch [242/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005307181\n",
      "Epoch [242/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005304357\n",
      "Epoch [242/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005301533\n",
      "Epoch [242/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005298709\n",
      "Epoch [243/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005295885\n",
      "Epoch [243/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0005293061\n",
      "Epoch [243/500], Batch [30/110], Train Loss: 0.0111, Val Loss: 0.0020, LR: 0.0005290237\n",
      "Epoch [243/500], Batch [40/110], Train Loss: 0.0009, Val Loss: 0.0003, LR: 0.0005287413\n",
      "Epoch [243/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0016, LR: 0.0005284589\n",
      "Epoch [243/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005281765\n",
      "Epoch [243/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0005278940\n",
      "Epoch [243/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0036, LR: 0.0005276116\n",
      "Epoch [243/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0005273291\n",
      "Epoch [243/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005270467\n",
      "Epoch [243/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005267642\n",
      "Epoch [244/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005264817\n",
      "Epoch [244/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005261993\n",
      "Epoch [244/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005259168\n",
      "Epoch [244/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0007, LR: 0.0005256343\n",
      "Epoch [244/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005253518\n",
      "Epoch [244/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0005250693\n",
      "Epoch [244/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005247868\n",
      "Epoch [244/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005245042\n",
      "Epoch [244/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0005242217\n",
      "Epoch [244/500], Batch [100/110], Train Loss: 0.0016, Val Loss: 0.0006, LR: 0.0005239392\n",
      "Epoch [244/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005236566\n",
      "Epoch [245/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0005233741\n",
      "Epoch [245/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005230915\n",
      "Epoch [245/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005228090\n",
      "Epoch [245/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005225264\n",
      "Epoch [245/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0005222439\n",
      "Epoch [245/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0005219613\n",
      "Epoch [245/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005216787\n",
      "Epoch [245/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005213961\n",
      "Epoch [245/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005211135\n",
      "Epoch [245/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005208309\n",
      "Epoch [245/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0005205483\n",
      "Epoch [246/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0005202657\n",
      "Epoch [246/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005199831\n",
      "Epoch [246/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0005197005\n",
      "Epoch [246/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005194179\n",
      "Epoch [246/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0006, LR: 0.0005191352\n",
      "Epoch [246/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005188526\n",
      "Epoch [246/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005185700\n",
      "Epoch [246/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0005182873\n",
      "Epoch [246/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0005180047\n",
      "Epoch [246/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0005177220\n",
      "Epoch [246/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0007, LR: 0.0005174394\n",
      "Epoch [247/500], Batch [10/110], Train Loss: 0.0035, Val Loss: 0.0006, LR: 0.0005171567\n",
      "Epoch [247/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005168741\n",
      "Epoch [247/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005165914\n",
      "Epoch [247/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005163087\n",
      "Epoch [247/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005160261\n",
      "Epoch [247/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005157434\n",
      "Epoch [247/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0018, LR: 0.0005154607\n",
      "Epoch [247/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005151780\n",
      "Epoch [247/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0007, LR: 0.0005148954\n",
      "Epoch [247/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0005146127\n",
      "Epoch [247/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0005143300\n",
      "Epoch [248/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0005140473\n",
      "Epoch [248/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0005137646\n",
      "Epoch [248/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005134819\n",
      "Epoch [248/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0005131992\n",
      "Epoch [248/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005129165\n",
      "Epoch [248/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005126338\n",
      "Epoch [248/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0005123511\n",
      "Epoch [248/500], Batch [80/110], Train Loss: 0.0010, Val Loss: 0.0005, LR: 0.0005120683\n",
      "Epoch [248/500], Batch [90/110], Train Loss: 0.0013, Val Loss: 0.0019, LR: 0.0005117856\n",
      "Epoch [248/500], Batch [100/110], Train Loss: 0.0013, Val Loss: 0.0023, LR: 0.0005115029\n",
      "Epoch [248/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0005112202\n",
      "Epoch [249/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005109375\n",
      "Epoch [249/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0005106547\n",
      "Epoch [249/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005103720\n",
      "Epoch [249/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0005100893\n",
      "Epoch [249/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0007, LR: 0.0005098066\n",
      "Epoch [249/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005095238\n",
      "Epoch [249/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0010, LR: 0.0005092411\n",
      "Epoch [249/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005089584\n",
      "Epoch [249/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0011, LR: 0.0005086756\n",
      "Epoch [249/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0005083929\n",
      "Epoch [249/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0005081102\n",
      "Epoch [250/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005078274\n",
      "Epoch [250/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005075447\n",
      "Epoch [250/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0005072619\n",
      "Epoch [250/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005069792\n",
      "Epoch [250/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0005066965\n",
      "Epoch [250/500], Batch [60/110], Train Loss: 0.0157, Val Loss: 0.0022, LR: 0.0005064137\n",
      "Epoch [250/500], Batch [70/110], Train Loss: 0.0019, Val Loss: 0.0018, LR: 0.0005061310\n",
      "Epoch [250/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005058482\n",
      "Epoch [250/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0009, LR: 0.0005055655\n",
      "Epoch [250/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005052827\n",
      "Epoch [250/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0005050000\n",
      "Epoch [251/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0005047173\n",
      "Epoch [251/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0005044345\n",
      "Epoch [251/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0005041518\n",
      "Epoch [251/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0005038690\n",
      "Epoch [251/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005035863\n",
      "Epoch [251/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0008, LR: 0.0005033035\n",
      "Epoch [251/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005030208\n",
      "Epoch [251/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005027381\n",
      "Epoch [251/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0005, LR: 0.0005024553\n",
      "Epoch [251/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005021726\n",
      "Epoch [251/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0005018898\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 251: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.040 sec , Memory Usage: 192.43 MB\n",
      "Epoch [252/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0005016071\n",
      "Epoch [252/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0005013244\n",
      "Epoch [252/500], Batch [30/110], Train Loss: 0.0012, Val Loss: 0.0018, LR: 0.0005010416\n",
      "Epoch [252/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0005007589\n",
      "Epoch [252/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005004762\n",
      "Epoch [252/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005001934\n",
      "Epoch [252/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0003, LR: 0.0004999107\n",
      "Epoch [252/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004996280\n",
      "Epoch [252/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0004, LR: 0.0004993453\n",
      "Epoch [252/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004990625\n",
      "Epoch [252/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004987798\n",
      "Epoch [253/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004984971\n",
      "Epoch [253/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004982144\n",
      "Epoch [253/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004979317\n",
      "Epoch [253/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004976489\n",
      "Epoch [253/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004973662\n",
      "Epoch [253/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004970835\n",
      "Epoch [253/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004968008\n",
      "Epoch [253/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004965181\n",
      "Epoch [253/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004962354\n",
      "Epoch [253/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004959527\n",
      "Epoch [253/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004956700\n",
      "Epoch [254/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0004953873\n",
      "Epoch [254/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004951046\n",
      "Epoch [254/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0005, LR: 0.0004948220\n",
      "Epoch [254/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0004945393\n",
      "Epoch [254/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004942566\n",
      "Epoch [254/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004939739\n",
      "Epoch [254/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0004936913\n",
      "Epoch [254/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0016, LR: 0.0004934086\n",
      "Epoch [254/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004931259\n",
      "Epoch [254/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004928433\n",
      "Epoch [254/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0004, LR: 0.0004925606\n",
      "Epoch [255/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004922780\n",
      "Epoch [255/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004919953\n",
      "Epoch [255/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0004917127\n",
      "Epoch [255/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0004914300\n",
      "Epoch [255/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004911474\n",
      "Epoch [255/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004908648\n",
      "Epoch [255/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004905821\n",
      "Epoch [255/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004902995\n",
      "Epoch [255/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004900169\n",
      "Epoch [255/500], Batch [100/110], Train Loss: 0.0215, Val Loss: 0.0007, LR: 0.0004897343\n",
      "Epoch [255/500], Batch [110/110], Train Loss: 0.0023, Val Loss: 0.0007, LR: 0.0004894517\n",
      "Epoch [256/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004891691\n",
      "Epoch [256/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0006, LR: 0.0004888865\n",
      "Epoch [256/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004886039\n",
      "Epoch [256/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0004883213\n",
      "Epoch [256/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004880387\n",
      "Epoch [256/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0004877561\n",
      "Epoch [256/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0010, LR: 0.0004874736\n",
      "Epoch [256/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0004871910\n",
      "Epoch [256/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004869085\n",
      "Epoch [256/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0004866259\n",
      "Epoch [256/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0007, LR: 0.0004863434\n",
      "Epoch [257/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004860608\n",
      "Epoch [257/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004857783\n",
      "Epoch [257/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004854958\n",
      "Epoch [257/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004852132\n",
      "Epoch [257/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004849307\n",
      "Epoch [257/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004846482\n",
      "Epoch [257/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0004843657\n",
      "Epoch [257/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004840832\n",
      "Epoch [257/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004838007\n",
      "Epoch [257/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004835183\n",
      "Epoch [257/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0017, LR: 0.0004832358\n",
      "Epoch [258/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0004829533\n",
      "Epoch [258/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0004826709\n",
      "Epoch [258/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004823884\n",
      "Epoch [258/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004821060\n",
      "Epoch [258/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004818235\n",
      "Epoch [258/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0004815411\n",
      "Epoch [258/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0004812587\n",
      "Epoch [258/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0006, LR: 0.0004809763\n",
      "Epoch [258/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0004806939\n",
      "Epoch [258/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004804115\n",
      "Epoch [258/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0004801291\n",
      "Epoch [259/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004798467\n",
      "Epoch [259/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004795643\n",
      "Epoch [259/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004792819\n",
      "Epoch [259/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004789996\n",
      "Epoch [259/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0004787172\n",
      "Epoch [259/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0009, LR: 0.0004784349\n",
      "Epoch [259/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004781526\n",
      "Epoch [259/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0004778702\n",
      "Epoch [259/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0007, LR: 0.0004775879\n",
      "Epoch [259/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004773056\n",
      "Epoch [259/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004770233\n",
      "Epoch [260/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0004767410\n",
      "Epoch [260/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004764588\n",
      "Epoch [260/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004761765\n",
      "Epoch [260/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0004758942\n",
      "Epoch [260/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0004756120\n",
      "Epoch [260/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0004753297\n",
      "Epoch [260/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004750475\n",
      "Epoch [260/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004747653\n",
      "Epoch [260/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004744831\n",
      "Epoch [260/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0004742009\n",
      "Epoch [260/500], Batch [110/110], Train Loss: 0.0288, Val Loss: 0.0011, LR: 0.0004739187\n",
      "Epoch [261/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004736365\n",
      "Epoch [261/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004733543\n",
      "Epoch [261/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004730722\n",
      "Epoch [261/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004727900\n",
      "Epoch [261/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004725079\n",
      "Epoch [261/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004722258\n",
      "Epoch [261/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004719436\n",
      "Epoch [261/500], Batch [80/110], Train Loss: 0.0379, Val Loss: 0.0008, LR: 0.0004716615\n",
      "Epoch [261/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0010, LR: 0.0004713794\n",
      "Epoch [261/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0004710974\n",
      "Epoch [261/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004708153\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 261: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.030 sec , Memory Usage: 192.43 MB\n",
      "Epoch [262/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0004705332\n",
      "Epoch [262/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0004702512\n",
      "Epoch [262/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004699691\n",
      "Epoch [262/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0004696871\n",
      "Epoch [262/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004694051\n",
      "Epoch [262/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0004691231\n",
      "Epoch [262/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004688411\n",
      "Epoch [262/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004685591\n",
      "Epoch [262/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004682771\n",
      "Epoch [262/500], Batch [100/110], Train Loss: 0.0013, Val Loss: 0.0007, LR: 0.0004679952\n",
      "Epoch [262/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0004677132\n",
      "Epoch [263/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0004674313\n",
      "Epoch [263/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0007, LR: 0.0004671494\n",
      "Epoch [263/500], Batch [30/110], Train Loss: 0.0017, Val Loss: 0.0006, LR: 0.0004668675\n",
      "Epoch [263/500], Batch [40/110], Train Loss: 0.0023, Val Loss: 0.0004, LR: 0.0004665856\n",
      "Epoch [263/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004663037\n",
      "Epoch [263/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004660218\n",
      "Epoch [263/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004657400\n",
      "Epoch [263/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004654581\n",
      "Epoch [263/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004651763\n",
      "Epoch [263/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004648945\n",
      "Epoch [263/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004646126\n",
      "Epoch [264/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004643309\n",
      "Epoch [264/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004640491\n",
      "Epoch [264/500], Batch [30/110], Train Loss: 0.0012, Val Loss: 0.0004, LR: 0.0004637673\n",
      "Epoch [264/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004634856\n",
      "Epoch [264/500], Batch [50/110], Train Loss: 0.0157, Val Loss: 0.0013, LR: 0.0004632038\n",
      "Epoch [264/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0004629221\n",
      "Epoch [264/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004626404\n",
      "Epoch [264/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0009, LR: 0.0004623587\n",
      "Epoch [264/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004620770\n",
      "Epoch [264/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0008, LR: 0.0004617953\n",
      "Epoch [264/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0005, LR: 0.0004615137\n",
      "Epoch [265/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004612320\n",
      "Epoch [265/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004609504\n",
      "Epoch [265/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0006, LR: 0.0004606688\n",
      "Epoch [265/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004603872\n",
      "Epoch [265/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0004601056\n",
      "Epoch [265/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004598240\n",
      "Epoch [265/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004595425\n",
      "Epoch [265/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0004592609\n",
      "Epoch [265/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0004589794\n",
      "Epoch [265/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0004586979\n",
      "Epoch [265/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004584164\n",
      "Epoch [266/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0038, LR: 0.0004581349\n",
      "Epoch [266/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004578534\n",
      "Epoch [266/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0004575720\n",
      "Epoch [266/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0011, LR: 0.0004572906\n",
      "Epoch [266/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004570091\n",
      "Epoch [266/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0004567277\n",
      "Epoch [266/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004564463\n",
      "Epoch [266/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0004561650\n",
      "Epoch [266/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0004558836\n",
      "Epoch [266/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004556023\n",
      "Epoch [266/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0004553210\n",
      "Epoch [267/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0004550396\n",
      "Epoch [267/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004547584\n",
      "Epoch [267/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004544771\n",
      "Epoch [267/500], Batch [40/110], Train Loss: 0.0556, Val Loss: 0.0003, LR: 0.0004541958\n",
      "Epoch [267/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004539146\n",
      "Epoch [267/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004536334\n",
      "Epoch [267/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004533521\n",
      "Epoch [267/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004530710\n",
      "Epoch [267/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004527898\n",
      "Epoch [267/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004525086\n",
      "Epoch [267/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0015, LR: 0.0004522275\n",
      "Epoch [268/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0004519464\n",
      "Epoch [268/500], Batch [20/110], Train Loss: 0.0008, Val Loss: 0.0011, LR: 0.0004516652\n",
      "Epoch [268/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004513842\n",
      "Epoch [268/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004511031\n",
      "Epoch [268/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004508220\n",
      "Epoch [268/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0006, LR: 0.0004505410\n",
      "Epoch [268/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004502600\n",
      "Epoch [268/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004499790\n",
      "Epoch [268/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004496980\n",
      "Epoch [268/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004494170\n",
      "Epoch [268/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0004491361\n",
      "Epoch [269/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0004488552\n",
      "Epoch [269/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0004485743\n",
      "Epoch [269/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004482934\n",
      "Epoch [269/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0004480125\n",
      "Epoch [269/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0004477316\n",
      "Epoch [269/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004474508\n",
      "Epoch [269/500], Batch [70/110], Train Loss: 0.0600, Val Loss: 0.0005, LR: 0.0004471700\n",
      "Epoch [269/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0005, LR: 0.0004468892\n",
      "Epoch [269/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0004466084\n",
      "Epoch [269/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004463276\n",
      "Epoch [269/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004460469\n",
      "Epoch [270/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004457662\n",
      "Epoch [270/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004454855\n",
      "Epoch [270/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004452048\n",
      "Epoch [270/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004449241\n",
      "Epoch [270/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004446435\n",
      "Epoch [270/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004443629\n",
      "Epoch [270/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0004440823\n",
      "Epoch [270/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0004438017\n",
      "Epoch [270/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0004435211\n",
      "Epoch [270/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0004432406\n",
      "Epoch [270/500], Batch [110/110], Train Loss: 0.0054, Val Loss: 0.0008, LR: 0.0004429600\n",
      "Epoch [271/500], Batch [10/110], Train Loss: 0.0022, Val Loss: 0.0005, LR: 0.0004426795\n",
      "Epoch [271/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004423991\n",
      "Epoch [271/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004421186\n",
      "Epoch [271/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004418382\n",
      "Epoch [271/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0009, LR: 0.0004415577\n",
      "Epoch [271/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004412773\n",
      "Epoch [271/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004409970\n",
      "Epoch [271/500], Batch [80/110], Train Loss: 0.0044, Val Loss: 0.0011, LR: 0.0004407166\n",
      "Epoch [271/500], Batch [90/110], Train Loss: 0.0051, Val Loss: 0.0006, LR: 0.0004404363\n",
      "Epoch [271/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0004401559\n",
      "Epoch [271/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0014, LR: 0.0004398756\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 271: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.035 sec , Memory Usage: 192.45 MB\n",
      "Epoch [272/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0004395954\n",
      "Epoch [272/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004393151\n",
      "Epoch [272/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004390349\n",
      "Epoch [272/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004387547\n",
      "Epoch [272/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004384745\n",
      "Epoch [272/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004381943\n",
      "Epoch [272/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004379142\n",
      "Epoch [272/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0004376340\n",
      "Epoch [272/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004373539\n",
      "Epoch [272/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004370739\n",
      "Epoch [272/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0005, LR: 0.0004367938\n",
      "Epoch [273/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004365138\n",
      "Epoch [273/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004362338\n",
      "Epoch [273/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0004, LR: 0.0004359538\n",
      "Epoch [273/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0004356738\n",
      "Epoch [273/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004353939\n",
      "Epoch [273/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004351139\n",
      "Epoch [273/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004348340\n",
      "Epoch [273/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0004345542\n",
      "Epoch [273/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004342743\n",
      "Epoch [273/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004339945\n",
      "Epoch [273/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004337147\n",
      "Epoch [274/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0004334349\n",
      "Epoch [274/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004331551\n",
      "Epoch [274/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0004328754\n",
      "Epoch [274/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0004325957\n",
      "Epoch [274/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004323160\n",
      "Epoch [274/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0039, LR: 0.0004320363\n",
      "Epoch [274/500], Batch [70/110], Train Loss: 0.0423, Val Loss: 0.0005, LR: 0.0004317567\n",
      "Epoch [274/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0004314770\n",
      "Epoch [274/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0004311975\n",
      "Epoch [274/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0042, LR: 0.0004309179\n",
      "Epoch [274/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004306383\n",
      "Epoch [275/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004303588\n",
      "Epoch [275/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0004300793\n",
      "Epoch [275/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004297998\n",
      "Epoch [275/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004295204\n",
      "Epoch [275/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0004292410\n",
      "Epoch [275/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0004289616\n",
      "Epoch [275/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004286822\n",
      "Epoch [275/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0004284028\n",
      "Epoch [275/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0005, LR: 0.0004281235\n",
      "Epoch [275/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004278442\n",
      "Epoch [275/500], Batch [110/110], Train Loss: 0.0014, Val Loss: 0.0006, LR: 0.0004275649\n",
      "Epoch [276/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004272857\n",
      "Epoch [276/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0004270065\n",
      "Epoch [276/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004267273\n",
      "Epoch [276/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0004264481\n",
      "Epoch [276/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004261689\n",
      "Epoch [276/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0004258898\n",
      "Epoch [276/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004256107\n",
      "Epoch [276/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004253317\n",
      "Epoch [276/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0004250526\n",
      "Epoch [276/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004247736\n",
      "Epoch [276/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0004244946\n",
      "Epoch [277/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004242156\n",
      "Epoch [277/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004239367\n",
      "Epoch [277/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004236578\n",
      "Epoch [277/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0004233789\n",
      "Epoch [277/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004231000\n",
      "Epoch [277/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004228212\n",
      "Epoch [277/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004225424\n",
      "Epoch [277/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004222636\n",
      "Epoch [277/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004219849\n",
      "Epoch [277/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004217061\n",
      "Epoch [277/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0004214274\n",
      "Epoch [278/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0021, LR: 0.0004211488\n",
      "Epoch [278/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0004208701\n",
      "Epoch [278/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004205915\n",
      "Epoch [278/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0004203129\n",
      "Epoch [278/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004200344\n",
      "Epoch [278/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004197558\n",
      "Epoch [278/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004194773\n",
      "Epoch [278/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0004191988\n",
      "Epoch [278/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0004189204\n",
      "Epoch [278/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0007, LR: 0.0004186420\n",
      "Epoch [278/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0004183636\n",
      "Epoch [279/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004180852\n",
      "Epoch [279/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004178069\n",
      "Epoch [279/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0004175286\n",
      "Epoch [279/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004172503\n",
      "Epoch [279/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004169720\n",
      "Epoch [279/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004166938\n",
      "Epoch [279/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004164156\n",
      "Epoch [279/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004161375\n",
      "Epoch [279/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004158593\n",
      "Epoch [279/500], Batch [100/110], Train Loss: 0.0013, Val Loss: 0.0015, LR: 0.0004155812\n",
      "Epoch [279/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0004153031\n",
      "Epoch [280/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0004150251\n",
      "Epoch [280/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004147471\n",
      "Epoch [280/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004144691\n",
      "Epoch [280/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0004141911\n",
      "Epoch [280/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004139132\n",
      "Epoch [280/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0012, LR: 0.0004136353\n",
      "Epoch [280/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0004133574\n",
      "Epoch [280/500], Batch [80/110], Train Loss: 0.0018, Val Loss: 0.0014, LR: 0.0004130796\n",
      "Epoch [280/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004128018\n",
      "Epoch [280/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0003, LR: 0.0004125240\n",
      "Epoch [280/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004122462\n",
      "Epoch [281/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004119685\n",
      "Epoch [281/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004116908\n",
      "Epoch [281/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0004114132\n",
      "Epoch [281/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004111356\n",
      "Epoch [281/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004108580\n",
      "Epoch [281/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0004105804\n",
      "Epoch [281/500], Batch [70/110], Train Loss: 0.0466, Val Loss: 0.0004, LR: 0.0004103028\n",
      "Epoch [281/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004100253\n",
      "Epoch [281/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004097479\n",
      "Epoch [281/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004094704\n",
      "Epoch [281/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0004091930\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 281: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.037 sec , Memory Usage: 192.45 MB\n",
      "Epoch [282/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004089156\n",
      "Epoch [282/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0004086383\n",
      "Epoch [282/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004083610\n",
      "Epoch [282/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004080837\n",
      "Epoch [282/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004078064\n",
      "Epoch [282/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004075292\n",
      "Epoch [282/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004072520\n",
      "Epoch [282/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0004069748\n",
      "Epoch [282/500], Batch [90/110], Train Loss: 0.0022, Val Loss: 0.0015, LR: 0.0004066977\n",
      "Epoch [282/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004064206\n",
      "Epoch [282/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004061436\n",
      "Epoch [283/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004058665\n",
      "Epoch [283/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0004055895\n",
      "Epoch [283/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004053126\n",
      "Epoch [283/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004050356\n",
      "Epoch [283/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004047587\n",
      "Epoch [283/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0004044819\n",
      "Epoch [283/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0004042050\n",
      "Epoch [283/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004039282\n",
      "Epoch [283/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0009, LR: 0.0004036514\n",
      "Epoch [283/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004033747\n",
      "Epoch [283/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0004030980\n",
      "Epoch [284/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004028213\n",
      "Epoch [284/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004025447\n",
      "Epoch [284/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004022681\n",
      "Epoch [284/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004019915\n",
      "Epoch [284/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0005, LR: 0.0004017150\n",
      "Epoch [284/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004014385\n",
      "Epoch [284/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004011620\n",
      "Epoch [284/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0004008856\n",
      "Epoch [284/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004006092\n",
      "Epoch [284/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0011, LR: 0.0004003328\n",
      "Epoch [284/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004000565\n",
      "Epoch [285/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003997802\n",
      "Epoch [285/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003995039\n",
      "Epoch [285/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003992277\n",
      "Epoch [285/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003989515\n",
      "Epoch [285/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0009, LR: 0.0003986753\n",
      "Epoch [285/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0003983992\n",
      "Epoch [285/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0018, LR: 0.0003981231\n",
      "Epoch [285/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0003978471\n",
      "Epoch [285/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0003975710\n",
      "Epoch [285/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003972950\n",
      "Epoch [285/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003970191\n",
      "Epoch [286/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003967432\n",
      "Epoch [286/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0003964673\n",
      "Epoch [286/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003961915\n",
      "Epoch [286/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003959156\n",
      "Epoch [286/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003956399\n",
      "Epoch [286/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003953641\n",
      "Epoch [286/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003950884\n",
      "Epoch [286/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003948128\n",
      "Epoch [286/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003945371\n",
      "Epoch [286/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0003942615\n",
      "Epoch [286/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003939860\n",
      "Epoch [287/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003937105\n",
      "Epoch [287/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0006, LR: 0.0003934350\n",
      "Epoch [287/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0003931595\n",
      "Epoch [287/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003928841\n",
      "Epoch [287/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0003926087\n",
      "Epoch [287/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003923334\n",
      "Epoch [287/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0003920581\n",
      "Epoch [287/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0003917828\n",
      "Epoch [287/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003915076\n",
      "Epoch [287/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003912324\n",
      "Epoch [287/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0003909572\n",
      "Epoch [288/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0003906821\n",
      "Epoch [288/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0003904070\n",
      "Epoch [288/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003901320\n",
      "Epoch [288/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003898570\n",
      "Epoch [288/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0003895820\n",
      "Epoch [288/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0003893071\n",
      "Epoch [288/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0007, LR: 0.0003890322\n",
      "Epoch [288/500], Batch [80/110], Train Loss: 0.0009, Val Loss: 0.0005, LR: 0.0003887573\n",
      "Epoch [288/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0003884825\n",
      "Epoch [288/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0007, LR: 0.0003882077\n",
      "Epoch [288/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0003879330\n",
      "Epoch [289/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0003876583\n",
      "Epoch [289/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003873836\n",
      "Epoch [289/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003871090\n",
      "Epoch [289/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003868344\n",
      "Epoch [289/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003865599\n",
      "Epoch [289/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003862854\n",
      "Epoch [289/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0003860109\n",
      "Epoch [289/500], Batch [80/110], Train Loss: 0.0039, Val Loss: 0.0011, LR: 0.0003857364\n",
      "Epoch [289/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0007, LR: 0.0003854621\n",
      "Epoch [289/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0003851877\n",
      "Epoch [289/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003849134\n",
      "Epoch [290/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003846391\n",
      "Epoch [290/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003843649\n",
      "Epoch [290/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003840907\n",
      "Epoch [290/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0003838165\n",
      "Epoch [290/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0003835424\n",
      "Epoch [290/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003832683\n",
      "Epoch [290/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003829943\n",
      "Epoch [290/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003827203\n",
      "Epoch [290/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0003824463\n",
      "Epoch [290/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0003821724\n",
      "Epoch [290/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0003818985\n",
      "Epoch [291/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003816247\n",
      "Epoch [291/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0003813509\n",
      "Epoch [291/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003810771\n",
      "Epoch [291/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003808034\n",
      "Epoch [291/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003805297\n",
      "Epoch [291/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0003802561\n",
      "Epoch [291/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0003799825\n",
      "Epoch [291/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0003797089\n",
      "Epoch [291/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0003794354\n",
      "Epoch [291/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003791619\n",
      "Epoch [291/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0003788885\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 291: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 192.45 MB\n",
      "Epoch [292/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003786151\n",
      "Epoch [292/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0003783417\n",
      "Epoch [292/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0006, LR: 0.0003780684\n",
      "Epoch [292/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003777952\n",
      "Epoch [292/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003775219\n",
      "Epoch [292/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003772488\n",
      "Epoch [292/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0005, LR: 0.0003769756\n",
      "Epoch [292/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003767025\n",
      "Epoch [292/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003764294\n",
      "Epoch [292/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0003761564\n",
      "Epoch [292/500], Batch [110/110], Train Loss: 0.0103, Val Loss: 0.0018, LR: 0.0003758835\n",
      "Epoch [293/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0003756105\n",
      "Epoch [293/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003753376\n",
      "Epoch [293/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003750648\n",
      "Epoch [293/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003747920\n",
      "Epoch [293/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003745192\n",
      "Epoch [293/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0003742465\n",
      "Epoch [293/500], Batch [70/110], Train Loss: 0.0077, Val Loss: 0.0012, LR: 0.0003739738\n",
      "Epoch [293/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0017, LR: 0.0003737012\n",
      "Epoch [293/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0003734286\n",
      "Epoch [293/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003731560\n",
      "Epoch [293/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003728835\n",
      "Epoch [294/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003726111\n",
      "Epoch [294/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003723386\n",
      "Epoch [294/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003720663\n",
      "Epoch [294/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0003717939\n",
      "Epoch [294/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003715216\n",
      "Epoch [294/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003712494\n",
      "Epoch [294/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003709772\n",
      "Epoch [294/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003707050\n",
      "Epoch [294/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003704329\n",
      "Epoch [294/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003701608\n",
      "Epoch [294/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003698888\n",
      "Epoch [295/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0003696168\n",
      "Epoch [295/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003693449\n",
      "Epoch [295/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003690730\n",
      "Epoch [295/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003688011\n",
      "Epoch [295/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003685293\n",
      "Epoch [295/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003682575\n",
      "Epoch [295/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003679858\n",
      "Epoch [295/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003677142\n",
      "Epoch [295/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003674425\n",
      "Epoch [295/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0003671709\n",
      "Epoch [295/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0003668994\n",
      "Epoch [296/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0003666279\n",
      "Epoch [296/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0003663565\n",
      "Epoch [296/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003660851\n",
      "Epoch [296/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003658137\n",
      "Epoch [296/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003655424\n",
      "Epoch [296/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0003652711\n",
      "Epoch [296/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0003649999\n",
      "Epoch [296/500], Batch [80/110], Train Loss: 0.0008, Val Loss: 0.0005, LR: 0.0003647287\n",
      "Epoch [296/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003644576\n",
      "Epoch [296/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0005, LR: 0.0003641865\n",
      "Epoch [296/500], Batch [110/110], Train Loss: 0.0057, Val Loss: 0.0015, LR: 0.0003639155\n",
      "Epoch [297/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0003636445\n",
      "Epoch [297/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003633735\n",
      "Epoch [297/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0003631026\n",
      "Epoch [297/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0009, LR: 0.0003628318\n",
      "Epoch [297/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003625610\n",
      "Epoch [297/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003622902\n",
      "Epoch [297/500], Batch [70/110], Train Loss: 0.0018, Val Loss: 0.0002, LR: 0.0003620195\n",
      "Epoch [297/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003617488\n",
      "Epoch [297/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003614782\n",
      "Epoch [297/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003612076\n",
      "Epoch [297/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003609371\n",
      "Epoch [298/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003606666\n",
      "Epoch [298/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003603962\n",
      "Epoch [298/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003601258\n",
      "Epoch [298/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0003598555\n",
      "Epoch [298/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0003595852\n",
      "Epoch [298/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0004, LR: 0.0003593149\n",
      "Epoch [298/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0008, LR: 0.0003590447\n",
      "Epoch [298/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003587746\n",
      "Epoch [298/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003585045\n",
      "Epoch [298/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003582344\n",
      "Epoch [298/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0003, LR: 0.0003579644\n",
      "Epoch [299/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0007, LR: 0.0003576945\n",
      "Epoch [299/500], Batch [20/110], Train Loss: 0.0055, Val Loss: 0.0013, LR: 0.0003574246\n",
      "Epoch [299/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0003571547\n",
      "Epoch [299/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003568849\n",
      "Epoch [299/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0003566151\n",
      "Epoch [299/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003563454\n",
      "Epoch [299/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003560757\n",
      "Epoch [299/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003558061\n",
      "Epoch [299/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003555365\n",
      "Epoch [299/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003552670\n",
      "Epoch [299/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003549975\n",
      "Epoch [300/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003547281\n",
      "Epoch [300/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003544587\n",
      "Epoch [300/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003541894\n",
      "Epoch [300/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003539201\n",
      "Epoch [300/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003536509\n",
      "Epoch [300/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0003533817\n",
      "Epoch [300/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003531126\n",
      "Epoch [300/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003528435\n",
      "Epoch [300/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003525745\n",
      "Epoch [300/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003523055\n",
      "Epoch [300/500], Batch [110/110], Train Loss: 0.0049, Val Loss: 0.0014, LR: 0.0003520366\n",
      "Epoch [301/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0003517677\n",
      "Epoch [301/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003514989\n",
      "Epoch [301/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0007, LR: 0.0003512301\n",
      "Epoch [301/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003509614\n",
      "Epoch [301/500], Batch [50/110], Train Loss: 0.0088, Val Loss: 0.0003, LR: 0.0003506927\n",
      "Epoch [301/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003504241\n",
      "Epoch [301/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003501555\n",
      "Epoch [301/500], Batch [80/110], Train Loss: 0.0025, Val Loss: 0.0004, LR: 0.0003498870\n",
      "Epoch [301/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0008, LR: 0.0003496185\n",
      "Epoch [301/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0003493500\n",
      "Epoch [301/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003490817\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 301: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.043 sec , Memory Usage: 192.45 MB\n",
      "Epoch [302/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003488133\n",
      "Epoch [302/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003485451\n",
      "Epoch [302/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003482769\n",
      "Epoch [302/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003480087\n",
      "Epoch [302/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0003477406\n",
      "Epoch [302/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003474725\n",
      "Epoch [302/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003472045\n",
      "Epoch [302/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003469365\n",
      "Epoch [302/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003466686\n",
      "Epoch [302/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003464007\n",
      "Epoch [302/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003461329\n",
      "Epoch [303/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0003458652\n",
      "Epoch [303/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003455974\n",
      "Epoch [303/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003453298\n",
      "Epoch [303/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003450622\n",
      "Epoch [303/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003447946\n",
      "Epoch [303/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003445271\n",
      "Epoch [303/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003442597\n",
      "Epoch [303/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003439923\n",
      "Epoch [303/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003437250\n",
      "Epoch [303/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003434577\n",
      "Epoch [303/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003431904\n",
      "Epoch [304/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003429232\n",
      "Epoch [304/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003426561\n",
      "Epoch [304/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0003423890\n",
      "Epoch [304/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0003421220\n",
      "Epoch [304/500], Batch [50/110], Train Loss: 0.0063, Val Loss: 0.0006, LR: 0.0003418550\n",
      "Epoch [304/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003415881\n",
      "Epoch [304/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0003413213\n",
      "Epoch [304/500], Batch [80/110], Train Loss: 0.0632, Val Loss: 0.0002, LR: 0.0003410544\n",
      "Epoch [304/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0003407877\n",
      "Epoch [304/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0003405210\n",
      "Epoch [304/500], Batch [110/110], Train Loss: 0.0102, Val Loss: 0.0020, LR: 0.0003402543\n",
      "Epoch [305/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0003399877\n",
      "Epoch [305/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003397212\n",
      "Epoch [305/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003394547\n",
      "Epoch [305/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003391883\n",
      "Epoch [305/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0003389219\n",
      "Epoch [305/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0003386556\n",
      "Epoch [305/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003383893\n",
      "Epoch [305/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003381231\n",
      "Epoch [305/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003378569\n",
      "Epoch [305/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0003375908\n",
      "Epoch [305/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003373247\n",
      "Epoch [306/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003370587\n",
      "Epoch [306/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0003367928\n",
      "Epoch [306/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003365269\n",
      "Epoch [306/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003362611\n",
      "Epoch [306/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0003359953\n",
      "Epoch [306/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0003357296\n",
      "Epoch [306/500], Batch [70/110], Train Loss: 0.0204, Val Loss: 0.0023, LR: 0.0003354639\n",
      "Epoch [306/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0003351983\n",
      "Epoch [306/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003349327\n",
      "Epoch [306/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003346672\n",
      "Epoch [306/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003344018\n",
      "Epoch [307/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003341364\n",
      "Epoch [307/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003338710\n",
      "Epoch [307/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003336057\n",
      "Epoch [307/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0003333405\n",
      "Epoch [307/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0008, LR: 0.0003330753\n",
      "Epoch [307/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003328102\n",
      "Epoch [307/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003325452\n",
      "Epoch [307/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0003322802\n",
      "Epoch [307/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003320152\n",
      "Epoch [307/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003317503\n",
      "Epoch [307/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003314855\n",
      "Epoch [308/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0003312207\n",
      "Epoch [308/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003309560\n",
      "Epoch [308/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003306914\n",
      "Epoch [308/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003304268\n",
      "Epoch [308/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003301622\n",
      "Epoch [308/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003298977\n",
      "Epoch [308/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0003296333\n",
      "Epoch [308/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0003293689\n",
      "Epoch [308/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003291046\n",
      "Epoch [308/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003288403\n",
      "Epoch [308/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003285761\n",
      "Epoch [309/500], Batch [10/110], Train Loss: 0.0111, Val Loss: 0.0007, LR: 0.0003283120\n",
      "Epoch [309/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003280479\n",
      "Epoch [309/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0010, LR: 0.0003277839\n",
      "Epoch [309/500], Batch [40/110], Train Loss: 0.0014, Val Loss: 0.0012, LR: 0.0003275199\n",
      "Epoch [309/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0003272560\n",
      "Epoch [309/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003269921\n",
      "Epoch [309/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003267283\n",
      "Epoch [309/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003264646\n",
      "Epoch [309/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003262009\n",
      "Epoch [309/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003259373\n",
      "Epoch [309/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003256737\n",
      "Epoch [310/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003254102\n",
      "Epoch [310/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0003251467\n",
      "Epoch [310/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0003, LR: 0.0003248833\n",
      "Epoch [310/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003246200\n",
      "Epoch [310/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003243567\n",
      "Epoch [310/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003240935\n",
      "Epoch [310/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003238304\n",
      "Epoch [310/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003235673\n",
      "Epoch [310/500], Batch [90/110], Train Loss: 0.0044, Val Loss: 0.0001, LR: 0.0003233042\n",
      "Epoch [310/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003230413\n",
      "Epoch [310/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003227783\n",
      "Epoch [311/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003225155\n",
      "Epoch [311/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003222527\n",
      "Epoch [311/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003219900\n",
      "Epoch [311/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003217273\n",
      "Epoch [311/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003214647\n",
      "Epoch [311/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003212021\n",
      "Epoch [311/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003209396\n",
      "Epoch [311/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003206772\n",
      "Epoch [311/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003204148\n",
      "Epoch [311/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0003201525\n",
      "Epoch [311/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0009, LR: 0.0003198902\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 311: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 192.39 MB\n",
      "Epoch [312/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0003196280\n",
      "Epoch [312/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003193659\n",
      "Epoch [312/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003191038\n",
      "Epoch [312/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003188418\n",
      "Epoch [312/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003185798\n",
      "Epoch [312/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003183179\n",
      "Epoch [312/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0003180561\n",
      "Epoch [312/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003177943\n",
      "Epoch [312/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003175326\n",
      "Epoch [312/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003172709\n",
      "Epoch [312/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003170093\n",
      "Epoch [313/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003167478\n",
      "Epoch [313/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0003164864\n",
      "Epoch [313/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0003162249\n",
      "Epoch [313/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0003159636\n",
      "Epoch [313/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003157023\n",
      "Epoch [313/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003154411\n",
      "Epoch [313/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003151799\n",
      "Epoch [313/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003149188\n",
      "Epoch [313/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003146578\n",
      "Epoch [313/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0003143968\n",
      "Epoch [313/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0003141359\n",
      "Epoch [314/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0003138751\n",
      "Epoch [314/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0003136143\n",
      "Epoch [314/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0003133536\n",
      "Epoch [314/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0003130929\n",
      "Epoch [314/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003128323\n",
      "Epoch [314/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003125718\n",
      "Epoch [314/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0005, LR: 0.0003123113\n",
      "Epoch [314/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003120509\n",
      "Epoch [314/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003117905\n",
      "Epoch [314/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003115303\n",
      "Epoch [314/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0004, LR: 0.0003112700\n",
      "Epoch [315/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0003110099\n",
      "Epoch [315/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003107498\n",
      "Epoch [315/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003104898\n",
      "Epoch [315/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003102298\n",
      "Epoch [315/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003099699\n",
      "Epoch [315/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003097100\n",
      "Epoch [315/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003094503\n",
      "Epoch [315/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003091905\n",
      "Epoch [315/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0003089309\n",
      "Epoch [315/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0003086713\n",
      "Epoch [315/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003084118\n",
      "Epoch [316/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003081523\n",
      "Epoch [316/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0005, LR: 0.0003078929\n",
      "Epoch [316/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003076336\n",
      "Epoch [316/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003073744\n",
      "Epoch [316/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003071152\n",
      "Epoch [316/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003068560\n",
      "Epoch [316/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003065969\n",
      "Epoch [316/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0003063379\n",
      "Epoch [316/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0003060790\n",
      "Epoch [316/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003058201\n",
      "Epoch [316/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003055613\n",
      "Epoch [317/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003053026\n",
      "Epoch [317/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003050439\n",
      "Epoch [317/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003047853\n",
      "Epoch [317/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003045267\n",
      "Epoch [317/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003042682\n",
      "Epoch [317/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003040098\n",
      "Epoch [317/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0003037515\n",
      "Epoch [317/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003034932\n",
      "Epoch [317/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0003032350\n",
      "Epoch [317/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0005, LR: 0.0003029768\n",
      "Epoch [317/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003027187\n",
      "Epoch [318/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0003024607\n",
      "Epoch [318/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0003022027\n",
      "Epoch [318/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003019448\n",
      "Epoch [318/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003016870\n",
      "Epoch [318/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003014292\n",
      "Epoch [318/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003011716\n",
      "Epoch [318/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003009139\n",
      "Epoch [318/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003006564\n",
      "Epoch [318/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003003989\n",
      "Epoch [318/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003001414\n",
      "Epoch [318/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002998841\n",
      "Epoch [319/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0004, LR: 0.0002996268\n",
      "Epoch [319/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002993696\n",
      "Epoch [319/500], Batch [30/110], Train Loss: 0.0060, Val Loss: 0.0010, LR: 0.0002991124\n",
      "Epoch [319/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0002988553\n",
      "Epoch [319/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002985983\n",
      "Epoch [319/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002983413\n",
      "Epoch [319/500], Batch [70/110], Train Loss: 0.0044, Val Loss: 0.0002, LR: 0.0002980844\n",
      "Epoch [319/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002978276\n",
      "Epoch [319/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0002975709\n",
      "Epoch [319/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002973142\n",
      "Epoch [319/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002970576\n",
      "Epoch [320/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002968010\n",
      "Epoch [320/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002965445\n",
      "Epoch [320/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002962881\n",
      "Epoch [320/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002960318\n",
      "Epoch [320/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002957755\n",
      "Epoch [320/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002955193\n",
      "Epoch [320/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002952631\n",
      "Epoch [320/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002950071\n",
      "Epoch [320/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002947511\n",
      "Epoch [320/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0011, LR: 0.0002944951\n",
      "Epoch [320/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0002942393\n",
      "Epoch [321/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0002939835\n",
      "Epoch [321/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002937277\n",
      "Epoch [321/500], Batch [30/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0002934721\n",
      "Epoch [321/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002932165\n",
      "Epoch [321/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002929609\n",
      "Epoch [321/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002927055\n",
      "Epoch [321/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002924501\n",
      "Epoch [321/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0002921948\n",
      "Epoch [321/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002919395\n",
      "Epoch [321/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002916844\n",
      "Epoch [321/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002914293\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 321: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.044 sec , Memory Usage: 192.39 MB\n",
      "Epoch [322/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002911742\n",
      "Epoch [322/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002909193\n",
      "Epoch [322/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002906644\n",
      "Epoch [322/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002904095\n",
      "Epoch [322/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0002901548\n",
      "Epoch [322/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002899001\n",
      "Epoch [322/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002896455\n",
      "Epoch [322/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0005, LR: 0.0002893909\n",
      "Epoch [322/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0002891364\n",
      "Epoch [322/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002888820\n",
      "Epoch [322/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002886277\n",
      "Epoch [323/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002883734\n",
      "Epoch [323/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0002881192\n",
      "Epoch [323/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002878651\n",
      "Epoch [323/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002876111\n",
      "Epoch [323/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002873571\n",
      "Epoch [323/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002871032\n",
      "Epoch [323/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002868493\n",
      "Epoch [323/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002865956\n",
      "Epoch [323/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0002863419\n",
      "Epoch [323/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0002860882\n",
      "Epoch [323/500], Batch [110/110], Train Loss: 0.0009, Val Loss: 0.0004, LR: 0.0002858347\n",
      "Epoch [324/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002855812\n",
      "Epoch [324/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0002853278\n",
      "Epoch [324/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002850744\n",
      "Epoch [324/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002848212\n",
      "Epoch [324/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0002845680\n",
      "Epoch [324/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002843149\n",
      "Epoch [324/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002840618\n",
      "Epoch [324/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002838088\n",
      "Epoch [324/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002835559\n",
      "Epoch [324/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002833031\n",
      "Epoch [324/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002830503\n",
      "Epoch [325/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002827976\n",
      "Epoch [325/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002825450\n",
      "Epoch [325/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002822925\n",
      "Epoch [325/500], Batch [40/110], Train Loss: 0.0068, Val Loss: 0.0002, LR: 0.0002820400\n",
      "Epoch [325/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002817876\n",
      "Epoch [325/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002815352\n",
      "Epoch [325/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002812830\n",
      "Epoch [325/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002810308\n",
      "Epoch [325/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002807787\n",
      "Epoch [325/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002805267\n",
      "Epoch [325/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0002802747\n",
      "Epoch [326/500], Batch [10/110], Train Loss: 0.0078, Val Loss: 0.0002, LR: 0.0002800228\n",
      "Epoch [326/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002797710\n",
      "Epoch [326/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002795193\n",
      "Epoch [326/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002792676\n",
      "Epoch [326/500], Batch [50/110], Train Loss: 0.0477, Val Loss: 0.0005, LR: 0.0002790160\n",
      "Epoch [326/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0007, LR: 0.0002787645\n",
      "Epoch [326/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002785130\n",
      "Epoch [326/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002782616\n",
      "Epoch [326/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0007, LR: 0.0002780103\n",
      "Epoch [326/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002777591\n",
      "Epoch [326/500], Batch [110/110], Train Loss: 0.0043, Val Loss: 0.0007, LR: 0.0002775080\n",
      "Epoch [327/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002772569\n",
      "Epoch [327/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0002770059\n",
      "Epoch [327/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002767550\n",
      "Epoch [327/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002765041\n",
      "Epoch [327/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002762533\n",
      "Epoch [327/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0002760026\n",
      "Epoch [327/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002757520\n",
      "Epoch [327/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002755014\n",
      "Epoch [327/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002752510\n",
      "Epoch [327/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002750005\n",
      "Epoch [327/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002747502\n",
      "Epoch [328/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002745000\n",
      "Epoch [328/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002742498\n",
      "Epoch [328/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0002739997\n",
      "Epoch [328/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0021, LR: 0.0002737496\n",
      "Epoch [328/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0002734997\n",
      "Epoch [328/500], Batch [60/110], Train Loss: 0.0019, Val Loss: 0.0006, LR: 0.0002732498\n",
      "Epoch [328/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002730000\n",
      "Epoch [328/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0006, LR: 0.0002727503\n",
      "Epoch [328/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002725006\n",
      "Epoch [328/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002722511\n",
      "Epoch [328/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002720016\n",
      "Epoch [329/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0002717521\n",
      "Epoch [329/500], Batch [20/110], Train Loss: 0.0029, Val Loss: 0.0004, LR: 0.0002715028\n",
      "Epoch [329/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002712535\n",
      "Epoch [329/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002710043\n",
      "Epoch [329/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002707552\n",
      "Epoch [329/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002705062\n",
      "Epoch [329/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002702572\n",
      "Epoch [329/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002700083\n",
      "Epoch [329/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002697595\n",
      "Epoch [329/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0002695107\n",
      "Epoch [329/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0002692621\n",
      "Epoch [330/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002690135\n",
      "Epoch [330/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002687650\n",
      "Epoch [330/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002685166\n",
      "Epoch [330/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002682682\n",
      "Epoch [330/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002680199\n",
      "Epoch [330/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002677718\n",
      "Epoch [330/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002675236\n",
      "Epoch [330/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002672756\n",
      "Epoch [330/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002670276\n",
      "Epoch [330/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002667797\n",
      "Epoch [330/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002665319\n",
      "Epoch [331/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002662842\n",
      "Epoch [331/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002660365\n",
      "Epoch [331/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002657890\n",
      "Epoch [331/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002655415\n",
      "Epoch [331/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002652941\n",
      "Epoch [331/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002650467\n",
      "Epoch [331/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002647995\n",
      "Epoch [331/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002645523\n",
      "Epoch [331/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002643052\n",
      "Epoch [331/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002640581\n",
      "Epoch [331/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002638112\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 331: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 192.39 MB\n",
      "Epoch [332/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002635643\n",
      "Epoch [332/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0002633175\n",
      "Epoch [332/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002630708\n",
      "Epoch [332/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002628242\n",
      "Epoch [332/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002625776\n",
      "Epoch [332/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0002623312\n",
      "Epoch [332/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002620848\n",
      "Epoch [332/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002618384\n",
      "Epoch [332/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002615922\n",
      "Epoch [332/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002613460\n",
      "Epoch [332/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002611000\n",
      "Epoch [333/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002608540\n",
      "Epoch [333/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002606080\n",
      "Epoch [333/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002603622\n",
      "Epoch [333/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002601164\n",
      "Epoch [333/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002598708\n",
      "Epoch [333/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002596252\n",
      "Epoch [333/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0011, LR: 0.0002593796\n",
      "Epoch [333/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0002591342\n",
      "Epoch [333/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002588889\n",
      "Epoch [333/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002586436\n",
      "Epoch [333/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002583984\n",
      "Epoch [334/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002581533\n",
      "Epoch [334/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002579082\n",
      "Epoch [334/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002576633\n",
      "Epoch [334/500], Batch [40/110], Train Loss: 0.0170, Val Loss: 0.0003, LR: 0.0002574184\n",
      "Epoch [334/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002571736\n",
      "Epoch [334/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002569289\n",
      "Epoch [334/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002566842\n",
      "Epoch [334/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002564397\n",
      "Epoch [334/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002561952\n",
      "Epoch [334/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002559508\n",
      "Epoch [334/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0002557065\n",
      "Epoch [335/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002554623\n",
      "Epoch [335/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002552181\n",
      "Epoch [335/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002549741\n",
      "Epoch [335/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002547301\n",
      "Epoch [335/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002544862\n",
      "Epoch [335/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002542424\n",
      "Epoch [335/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002539986\n",
      "Epoch [335/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002537550\n",
      "Epoch [335/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002535114\n",
      "Epoch [335/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002532679\n",
      "Epoch [335/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002530245\n",
      "Epoch [336/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0005, LR: 0.0002527812\n",
      "Epoch [336/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002525379\n",
      "Epoch [336/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002522948\n",
      "Epoch [336/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0011, LR: 0.0002520517\n",
      "Epoch [336/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0002518087\n",
      "Epoch [336/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002515658\n",
      "Epoch [336/500], Batch [70/110], Train Loss: 0.0071, Val Loss: 0.0003, LR: 0.0002513229\n",
      "Epoch [336/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002510802\n",
      "Epoch [336/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002508375\n",
      "Epoch [336/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0002505949\n",
      "Epoch [336/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002503524\n",
      "Epoch [337/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002501100\n",
      "Epoch [337/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002498677\n",
      "Epoch [337/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002496254\n",
      "Epoch [337/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002493833\n",
      "Epoch [337/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002491412\n",
      "Epoch [337/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002488992\n",
      "Epoch [337/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002486573\n",
      "Epoch [337/500], Batch [80/110], Train Loss: 0.0013, Val Loss: 0.0003, LR: 0.0002484154\n",
      "Epoch [337/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002481737\n",
      "Epoch [337/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002479320\n",
      "Epoch [337/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002476904\n",
      "Epoch [338/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002474489\n",
      "Epoch [338/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0002472075\n",
      "Epoch [338/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002469662\n",
      "Epoch [338/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002467249\n",
      "Epoch [338/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002464838\n",
      "Epoch [338/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002462427\n",
      "Epoch [338/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002460017\n",
      "Epoch [338/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0002457608\n",
      "Epoch [338/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002455200\n",
      "Epoch [338/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0002452792\n",
      "Epoch [338/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002450386\n",
      "Epoch [339/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002447980\n",
      "Epoch [339/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002445575\n",
      "Epoch [339/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002443171\n",
      "Epoch [339/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002440768\n",
      "Epoch [339/500], Batch [50/110], Train Loss: 0.0022, Val Loss: 0.0001, LR: 0.0002438366\n",
      "Epoch [339/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002435964\n",
      "Epoch [339/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002433563\n",
      "Epoch [339/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002431164\n",
      "Epoch [339/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002428765\n",
      "Epoch [339/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0003, LR: 0.0002426367\n",
      "Epoch [339/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002423970\n",
      "Epoch [340/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002421573\n",
      "Epoch [340/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002419178\n",
      "Epoch [340/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002416783\n",
      "Epoch [340/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002414389\n",
      "Epoch [340/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002411997\n",
      "Epoch [340/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002409605\n",
      "Epoch [340/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002407213\n",
      "Epoch [340/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002404823\n",
      "Epoch [340/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002402434\n",
      "Epoch [340/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002400045\n",
      "Epoch [340/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002397657\n",
      "Epoch [341/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002395271\n",
      "Epoch [341/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002392885\n",
      "Epoch [341/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002390499\n",
      "Epoch [341/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0010, LR: 0.0002388115\n",
      "Epoch [341/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0002385732\n",
      "Epoch [341/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0002383349\n",
      "Epoch [341/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0002380968\n",
      "Epoch [341/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002378587\n",
      "Epoch [341/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002376207\n",
      "Epoch [341/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002373828\n",
      "Epoch [341/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002371450\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 341: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 192.40 MB\n",
      "Epoch [342/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002369073\n",
      "Epoch [342/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002366696\n",
      "Epoch [342/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002364321\n",
      "Epoch [342/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002361946\n",
      "Epoch [342/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002359572\n",
      "Epoch [342/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002357199\n",
      "Epoch [342/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002354827\n",
      "Epoch [342/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002352456\n",
      "Epoch [342/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002350086\n",
      "Epoch [342/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002347716\n",
      "Epoch [342/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002345348\n",
      "Epoch [343/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002342980\n",
      "Epoch [343/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002340614\n",
      "Epoch [343/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002338248\n",
      "Epoch [343/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002335883\n",
      "Epoch [343/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002333519\n",
      "Epoch [343/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002331156\n",
      "Epoch [343/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002328793\n",
      "Epoch [343/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002326432\n",
      "Epoch [343/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002324071\n",
      "Epoch [343/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0002321712\n",
      "Epoch [343/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002319353\n",
      "Epoch [344/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002316995\n",
      "Epoch [344/500], Batch [20/110], Train Loss: 0.0015, Val Loss: 0.0003, LR: 0.0002314638\n",
      "Epoch [344/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002312282\n",
      "Epoch [344/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002309927\n",
      "Epoch [344/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0002307573\n",
      "Epoch [344/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0002305219\n",
      "Epoch [344/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002302867\n",
      "Epoch [344/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002300515\n",
      "Epoch [344/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0002298164\n",
      "Epoch [344/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002295815\n",
      "Epoch [344/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002293466\n",
      "Epoch [345/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002291118\n",
      "Epoch [345/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002288771\n",
      "Epoch [345/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002286424\n",
      "Epoch [345/500], Batch [40/110], Train Loss: 0.0281, Val Loss: 0.0001, LR: 0.0002284079\n",
      "Epoch [345/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002281735\n",
      "Epoch [345/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002279391\n",
      "Epoch [345/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002277049\n",
      "Epoch [345/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0002274707\n",
      "Epoch [345/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0002272366\n",
      "Epoch [345/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0005, LR: 0.0002270026\n",
      "Epoch [345/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002267687\n",
      "Epoch [346/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002265349\n",
      "Epoch [346/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002263012\n",
      "Epoch [346/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002260676\n",
      "Epoch [346/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0002258340\n",
      "Epoch [346/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002256006\n",
      "Epoch [346/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002253673\n",
      "Epoch [346/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002251340\n",
      "Epoch [346/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002249008\n",
      "Epoch [346/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002246678\n",
      "Epoch [346/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002244348\n",
      "Epoch [346/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002242019\n",
      "Epoch [347/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002239691\n",
      "Epoch [347/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002237364\n",
      "Epoch [347/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0002235037\n",
      "Epoch [347/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002232712\n",
      "Epoch [347/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0004, LR: 0.0002230388\n",
      "Epoch [347/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002228064\n",
      "Epoch [347/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002225742\n",
      "Epoch [347/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0002223420\n",
      "Epoch [347/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002221100\n",
      "Epoch [347/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002218780\n",
      "Epoch [347/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0002216461\n",
      "Epoch [348/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0006, LR: 0.0002214143\n",
      "Epoch [348/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002211826\n",
      "Epoch [348/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002209510\n",
      "Epoch [348/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002207195\n",
      "Epoch [348/500], Batch [50/110], Train Loss: 0.0035, Val Loss: 0.0002, LR: 0.0002204881\n",
      "Epoch [348/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002202567\n",
      "Epoch [348/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0009, LR: 0.0002200255\n",
      "Epoch [348/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0009, LR: 0.0002197944\n",
      "Epoch [348/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002195633\n",
      "Epoch [348/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002193324\n",
      "Epoch [348/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002191015\n",
      "Epoch [349/500], Batch [10/110], Train Loss: 0.0055, Val Loss: 0.0003, LR: 0.0002188707\n",
      "Epoch [349/500], Batch [20/110], Train Loss: 0.0059, Val Loss: 0.0017, LR: 0.0002186401\n",
      "Epoch [349/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0002184095\n",
      "Epoch [349/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002181790\n",
      "Epoch [349/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002179486\n",
      "Epoch [349/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002177183\n",
      "Epoch [349/500], Batch [70/110], Train Loss: 0.0217, Val Loss: 0.0001, LR: 0.0002174881\n",
      "Epoch [349/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002172580\n",
      "Epoch [349/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0002170280\n",
      "Epoch [349/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002167980\n",
      "Epoch [349/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002165682\n",
      "Epoch [350/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002163385\n",
      "Epoch [350/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002161088\n",
      "Epoch [350/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002158793\n",
      "Epoch [350/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0002156498\n",
      "Epoch [350/500], Batch [50/110], Train Loss: 0.0064, Val Loss: 0.0019, LR: 0.0002154205\n",
      "Epoch [350/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0002151912\n",
      "Epoch [350/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0002149620\n",
      "Epoch [350/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002147330\n",
      "Epoch [350/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002145040\n",
      "Epoch [350/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002142751\n",
      "Epoch [350/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002140463\n",
      "Epoch [351/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002138176\n",
      "Epoch [351/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002135890\n",
      "Epoch [351/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002133605\n",
      "Epoch [351/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002131321\n",
      "Epoch [351/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002129038\n",
      "Epoch [351/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002126755\n",
      "Epoch [351/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002124474\n",
      "Epoch [351/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002122194\n",
      "Epoch [351/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002119915\n",
      "Epoch [351/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002117636\n",
      "Epoch [351/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002115359\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 351: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.030 sec , Memory Usage: 192.41 MB\n",
      "Epoch [352/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002113082\n",
      "Epoch [352/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0002110807\n",
      "Epoch [352/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002108532\n",
      "Epoch [352/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002106259\n",
      "Epoch [352/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002103986\n",
      "Epoch [352/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0002101714\n",
      "Epoch [352/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0002099444\n",
      "Epoch [352/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0002097174\n",
      "Epoch [352/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002094905\n",
      "Epoch [352/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002092637\n",
      "Epoch [352/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002090370\n",
      "Epoch [353/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002088104\n",
      "Epoch [353/500], Batch [20/110], Train Loss: 0.0027, Val Loss: 0.0005, LR: 0.0002085840\n",
      "Epoch [353/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002083576\n",
      "Epoch [353/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0002081313\n",
      "Epoch [353/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002079051\n",
      "Epoch [353/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0002076789\n",
      "Epoch [353/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0004, LR: 0.0002074529\n",
      "Epoch [353/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002072270\n",
      "Epoch [353/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002070012\n",
      "Epoch [353/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0002067755\n",
      "Epoch [353/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002065499\n",
      "Epoch [354/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002063244\n",
      "Epoch [354/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002060989\n",
      "Epoch [354/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0002058736\n",
      "Epoch [354/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002056484\n",
      "Epoch [354/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0002054232\n",
      "Epoch [354/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002051982\n",
      "Epoch [354/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002049733\n",
      "Epoch [354/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002047484\n",
      "Epoch [354/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002045237\n",
      "Epoch [354/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002042990\n",
      "Epoch [354/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002040745\n",
      "Epoch [355/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0002038501\n",
      "Epoch [355/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002036257\n",
      "Epoch [355/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002034015\n",
      "Epoch [355/500], Batch [40/110], Train Loss: 0.0018, Val Loss: 0.0005, LR: 0.0002031773\n",
      "Epoch [355/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002029533\n",
      "Epoch [355/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002027293\n",
      "Epoch [355/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002025054\n",
      "Epoch [355/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002022817\n",
      "Epoch [355/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002020580\n",
      "Epoch [355/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002018345\n",
      "Epoch [355/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002016110\n",
      "Epoch [356/500], Batch [10/110], Train Loss: 0.0023, Val Loss: 0.0004, LR: 0.0002013876\n",
      "Epoch [356/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002011644\n",
      "Epoch [356/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002009412\n",
      "Epoch [356/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002007182\n",
      "Epoch [356/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002004952\n",
      "Epoch [356/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002002723\n",
      "Epoch [356/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0002000496\n",
      "Epoch [356/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001998269\n",
      "Epoch [356/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001996043\n",
      "Epoch [356/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001993819\n",
      "Epoch [356/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001991595\n",
      "Epoch [357/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001989372\n",
      "Epoch [357/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001987151\n",
      "Epoch [357/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0001984930\n",
      "Epoch [357/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001982710\n",
      "Epoch [357/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001980492\n",
      "Epoch [357/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001978274\n",
      "Epoch [357/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0003, LR: 0.0001976057\n",
      "Epoch [357/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001973841\n",
      "Epoch [357/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001971627\n",
      "Epoch [357/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001969413\n",
      "Epoch [357/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0001967200\n",
      "Epoch [358/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0022, LR: 0.0001964989\n",
      "Epoch [358/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001962778\n",
      "Epoch [358/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001960569\n",
      "Epoch [358/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001958360\n",
      "Epoch [358/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001956152\n",
      "Epoch [358/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001953946\n",
      "Epoch [358/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001951740\n",
      "Epoch [358/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001949535\n",
      "Epoch [358/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001947332\n",
      "Epoch [358/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001945129\n",
      "Epoch [358/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001942928\n",
      "Epoch [359/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001940727\n",
      "Epoch [359/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001938528\n",
      "Epoch [359/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001936329\n",
      "Epoch [359/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001934132\n",
      "Epoch [359/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001931935\n",
      "Epoch [359/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001929740\n",
      "Epoch [359/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001927545\n",
      "Epoch [359/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001925352\n",
      "Epoch [359/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0001923159\n",
      "Epoch [359/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001920968\n",
      "Epoch [359/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001918778\n",
      "Epoch [360/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001916588\n",
      "Epoch [360/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001914400\n",
      "Epoch [360/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001912213\n",
      "Epoch [360/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001910026\n",
      "Epoch [360/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001907841\n",
      "Epoch [360/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001905657\n",
      "Epoch [360/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001903474\n",
      "Epoch [360/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001901292\n",
      "Epoch [360/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001899110\n",
      "Epoch [360/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001896930\n",
      "Epoch [360/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001894751\n",
      "Epoch [361/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001892573\n",
      "Epoch [361/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001890396\n",
      "Epoch [361/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001888220\n",
      "Epoch [361/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001886045\n",
      "Epoch [361/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001883871\n",
      "Epoch [361/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001881698\n",
      "Epoch [361/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001879526\n",
      "Epoch [361/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0001877356\n",
      "Epoch [361/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001875186\n",
      "Epoch [361/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001873017\n",
      "Epoch [361/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001870849\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 361: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.036 sec , Memory Usage: 192.41 MB\n",
      "Epoch [362/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001868683\n",
      "Epoch [362/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001866517\n",
      "Epoch [362/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001864352\n",
      "Epoch [362/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001862189\n",
      "Epoch [362/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0001860026\n",
      "Epoch [362/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001857865\n",
      "Epoch [362/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001855704\n",
      "Epoch [362/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001853545\n",
      "Epoch [362/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0001851387\n",
      "Epoch [362/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0001849229\n",
      "Epoch [362/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0001847073\n",
      "Epoch [363/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0001844918\n",
      "Epoch [363/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001842764\n",
      "Epoch [363/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001840610\n",
      "Epoch [363/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001838458\n",
      "Epoch [363/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001836307\n",
      "Epoch [363/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001834157\n",
      "Epoch [363/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001832008\n",
      "Epoch [363/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001829860\n",
      "Epoch [363/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001827714\n",
      "Epoch [363/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001825568\n",
      "Epoch [363/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001823423\n",
      "Epoch [364/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001821279\n",
      "Epoch [364/500], Batch [20/110], Train Loss: 0.0011, Val Loss: 0.0003, LR: 0.0001819137\n",
      "Epoch [364/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001816995\n",
      "Epoch [364/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001814855\n",
      "Epoch [364/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0001812715\n",
      "Epoch [364/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0001810577\n",
      "Epoch [364/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0001808439\n",
      "Epoch [364/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0001806303\n",
      "Epoch [364/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0001804168\n",
      "Epoch [364/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0001802034\n",
      "Epoch [364/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001799901\n",
      "Epoch [365/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001797768\n",
      "Epoch [365/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001795637\n",
      "Epoch [365/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001793507\n",
      "Epoch [365/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001791379\n",
      "Epoch [365/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001789251\n",
      "Epoch [365/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001787124\n",
      "Epoch [365/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0001784998\n",
      "Epoch [365/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0001782874\n",
      "Epoch [365/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0001780750\n",
      "Epoch [365/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001778628\n",
      "Epoch [365/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001776506\n",
      "Epoch [366/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001774386\n",
      "Epoch [366/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001772267\n",
      "Epoch [366/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001770148\n",
      "Epoch [366/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0001768031\n",
      "Epoch [366/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0001765915\n",
      "Epoch [366/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0001763800\n",
      "Epoch [366/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0001761686\n",
      "Epoch [366/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0001759573\n",
      "Epoch [366/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0001757462\n",
      "Epoch [366/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0001755351\n",
      "Epoch [366/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0013, LR: 0.0001753241\n",
      "Epoch [367/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0001751133\n",
      "Epoch [367/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0001749025\n",
      "Epoch [367/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001746919\n",
      "Epoch [367/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001744813\n",
      "Epoch [367/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001742709\n",
      "Epoch [367/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001740606\n",
      "Epoch [367/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001738504\n",
      "Epoch [367/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001736403\n",
      "Epoch [367/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0002, LR: 0.0001734303\n",
      "Epoch [367/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001732204\n",
      "Epoch [367/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001730106\n",
      "Epoch [368/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001728010\n",
      "Epoch [368/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001725914\n",
      "Epoch [368/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001723820\n",
      "Epoch [368/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001721726\n",
      "Epoch [368/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001719634\n",
      "Epoch [368/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001717543\n",
      "Epoch [368/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001715452\n",
      "Epoch [368/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001713363\n",
      "Epoch [368/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001711275\n",
      "Epoch [368/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001709188\n",
      "Epoch [368/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001707103\n",
      "Epoch [369/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001705018\n",
      "Epoch [369/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001702934\n",
      "Epoch [369/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001700852\n",
      "Epoch [369/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0001698770\n",
      "Epoch [369/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001696690\n",
      "Epoch [369/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001694611\n",
      "Epoch [369/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001692532\n",
      "Epoch [369/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001690455\n",
      "Epoch [369/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001688379\n",
      "Epoch [369/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001686305\n",
      "Epoch [369/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001684231\n",
      "Epoch [370/500], Batch [10/110], Train Loss: 0.0034, Val Loss: 0.0006, LR: 0.0001682158\n",
      "Epoch [370/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0001680087\n",
      "Epoch [370/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0001678016\n",
      "Epoch [370/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001675947\n",
      "Epoch [370/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001673878\n",
      "Epoch [370/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001671811\n",
      "Epoch [370/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001669745\n",
      "Epoch [370/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001667680\n",
      "Epoch [370/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001665616\n",
      "Epoch [370/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001663553\n",
      "Epoch [370/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001661492\n",
      "Epoch [371/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001659431\n",
      "Epoch [371/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001657372\n",
      "Epoch [371/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001655313\n",
      "Epoch [371/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001653256\n",
      "Epoch [371/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001651200\n",
      "Epoch [371/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001649145\n",
      "Epoch [371/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001647091\n",
      "Epoch [371/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001645038\n",
      "Epoch [371/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001642987\n",
      "Epoch [371/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001640936\n",
      "Epoch [371/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001638887\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 371: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.038 sec , Memory Usage: 192.43 MB\n",
      "Epoch [372/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001636838\n",
      "Epoch [372/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001634791\n",
      "Epoch [372/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001632745\n",
      "Epoch [372/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0001630700\n",
      "Epoch [372/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001628656\n",
      "Epoch [372/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0001626613\n",
      "Epoch [372/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001624572\n",
      "Epoch [372/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001622531\n",
      "Epoch [372/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001620492\n",
      "Epoch [372/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001618453\n",
      "Epoch [372/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001616416\n",
      "Epoch [373/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001614380\n",
      "Epoch [373/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001612345\n",
      "Epoch [373/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0001610311\n",
      "Epoch [373/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001608279\n",
      "Epoch [373/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001606247\n",
      "Epoch [373/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001604217\n",
      "Epoch [373/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001602187\n",
      "Epoch [373/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001600159\n",
      "Epoch [373/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0001598132\n",
      "Epoch [373/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0001596106\n",
      "Epoch [373/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001594081\n",
      "Epoch [374/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001592057\n",
      "Epoch [374/500], Batch [20/110], Train Loss: 0.0011, Val Loss: 0.0003, LR: 0.0001590035\n",
      "Epoch [374/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001588013\n",
      "Epoch [374/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001585993\n",
      "Epoch [374/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001583974\n",
      "Epoch [374/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001581956\n",
      "Epoch [374/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0002, LR: 0.0001579939\n",
      "Epoch [374/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001577923\n",
      "Epoch [374/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001575909\n",
      "Epoch [374/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001573895\n",
      "Epoch [374/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001571883\n",
      "Epoch [375/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001569871\n",
      "Epoch [375/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001567861\n",
      "Epoch [375/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001565852\n",
      "Epoch [375/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001563844\n",
      "Epoch [375/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001561838\n",
      "Epoch [375/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001559832\n",
      "Epoch [375/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001557828\n",
      "Epoch [375/500], Batch [80/110], Train Loss: 0.0187, Val Loss: 0.0004, LR: 0.0001555824\n",
      "Epoch [375/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001553822\n",
      "Epoch [375/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001551821\n",
      "Epoch [375/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001549821\n",
      "Epoch [376/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001547823\n",
      "Epoch [376/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001545825\n",
      "Epoch [376/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001543829\n",
      "Epoch [376/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001541833\n",
      "Epoch [376/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001539839\n",
      "Epoch [376/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001537846\n",
      "Epoch [376/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001535854\n",
      "Epoch [376/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0001533864\n",
      "Epoch [376/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0001531874\n",
      "Epoch [376/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0001529886\n",
      "Epoch [376/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0001527898\n",
      "Epoch [377/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0001525912\n",
      "Epoch [377/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0001523927\n",
      "Epoch [377/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0001521943\n",
      "Epoch [377/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001519961\n",
      "Epoch [377/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001517979\n",
      "Epoch [377/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0001515999\n",
      "Epoch [377/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0001514020\n",
      "Epoch [377/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001512042\n",
      "Epoch [377/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001510065\n",
      "Epoch [377/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001508089\n",
      "Epoch [377/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001506114\n",
      "Epoch [378/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001504141\n",
      "Epoch [378/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001502169\n",
      "Epoch [378/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001500198\n",
      "Epoch [378/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001498228\n",
      "Epoch [378/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001496259\n",
      "Epoch [378/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001494291\n",
      "Epoch [378/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001492325\n",
      "Epoch [378/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001490359\n",
      "Epoch [378/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001488395\n",
      "Epoch [378/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001486432\n",
      "Epoch [378/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001484470\n",
      "Epoch [379/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001482510\n",
      "Epoch [379/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001480550\n",
      "Epoch [379/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0001478592\n",
      "Epoch [379/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0001476635\n",
      "Epoch [379/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0001474679\n",
      "Epoch [379/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0001472724\n",
      "Epoch [379/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0001470770\n",
      "Epoch [379/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001468818\n",
      "Epoch [379/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001466866\n",
      "Epoch [379/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001464916\n",
      "Epoch [379/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001462967\n",
      "Epoch [380/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0001461019\n",
      "Epoch [380/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0001459073\n",
      "Epoch [380/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0001457127\n",
      "Epoch [380/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001455183\n",
      "Epoch [380/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001453240\n",
      "Epoch [380/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001451298\n",
      "Epoch [380/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001449357\n",
      "Epoch [380/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001447417\n",
      "Epoch [380/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001445479\n",
      "Epoch [380/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001443541\n",
      "Epoch [380/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001441605\n",
      "Epoch [381/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001439670\n",
      "Epoch [381/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0001437737\n",
      "Epoch [381/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001435804\n",
      "Epoch [381/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001433873\n",
      "Epoch [381/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001431942\n",
      "Epoch [381/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001430013\n",
      "Epoch [381/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001428086\n",
      "Epoch [381/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001426159\n",
      "Epoch [381/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001424233\n",
      "Epoch [381/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001422309\n",
      "Epoch [381/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001420386\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 381: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 192.43 MB\n",
      "Epoch [382/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001418464\n",
      "Epoch [382/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001416543\n",
      "Epoch [382/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001414624\n",
      "Epoch [382/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001412705\n",
      "Epoch [382/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001410788\n",
      "Epoch [382/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001408872\n",
      "Epoch [382/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001406957\n",
      "Epoch [382/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001405044\n",
      "Epoch [382/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001403131\n",
      "Epoch [382/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001401220\n",
      "Epoch [382/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001399310\n",
      "Epoch [383/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001397401\n",
      "Epoch [383/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001395494\n",
      "Epoch [383/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001393587\n",
      "Epoch [383/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001391682\n",
      "Epoch [383/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001389778\n",
      "Epoch [383/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001387875\n",
      "Epoch [383/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001385973\n",
      "Epoch [383/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001384073\n",
      "Epoch [383/500], Batch [90/110], Train Loss: 0.0013, Val Loss: 0.0005, LR: 0.0001382173\n",
      "Epoch [383/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001380275\n",
      "Epoch [383/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001378378\n",
      "Epoch [384/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001376483\n",
      "Epoch [384/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0001374588\n",
      "Epoch [384/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001372695\n",
      "Epoch [384/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001370803\n",
      "Epoch [384/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001368912\n",
      "Epoch [384/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001367022\n",
      "Epoch [384/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001365133\n",
      "Epoch [384/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001363246\n",
      "Epoch [384/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001361360\n",
      "Epoch [384/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001359475\n",
      "Epoch [384/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001357591\n",
      "Epoch [385/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001355709\n",
      "Epoch [385/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001353828\n",
      "Epoch [385/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001351947\n",
      "Epoch [385/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001350069\n",
      "Epoch [385/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001348191\n",
      "Epoch [385/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001346314\n",
      "Epoch [385/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001344439\n",
      "Epoch [385/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001342565\n",
      "Epoch [385/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001340692\n",
      "Epoch [385/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001338821\n",
      "Epoch [385/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001336950\n",
      "Epoch [386/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001335081\n",
      "Epoch [386/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001333213\n",
      "Epoch [386/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001331346\n",
      "Epoch [386/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001329481\n",
      "Epoch [386/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001327616\n",
      "Epoch [386/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001325753\n",
      "Epoch [386/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001323891\n",
      "Epoch [386/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001322031\n",
      "Epoch [386/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001320171\n",
      "Epoch [386/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001318313\n",
      "Epoch [386/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001316456\n",
      "Epoch [387/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001314600\n",
      "Epoch [387/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001312745\n",
      "Epoch [387/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001310892\n",
      "Epoch [387/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001309040\n",
      "Epoch [387/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001307189\n",
      "Epoch [387/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001305339\n",
      "Epoch [387/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001303490\n",
      "Epoch [387/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001301643\n",
      "Epoch [387/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001299797\n",
      "Epoch [387/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001297952\n",
      "Epoch [387/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001296109\n",
      "Epoch [388/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001294266\n",
      "Epoch [388/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001292425\n",
      "Epoch [388/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001290585\n",
      "Epoch [388/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001288746\n",
      "Epoch [388/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001286909\n",
      "Epoch [388/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001285072\n",
      "Epoch [388/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001283237\n",
      "Epoch [388/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001281404\n",
      "Epoch [388/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001279571\n",
      "Epoch [388/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0001277740\n",
      "Epoch [388/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0001275910\n",
      "Epoch [389/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0001274081\n",
      "Epoch [389/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001272253\n",
      "Epoch [389/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001270427\n",
      "Epoch [389/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001268601\n",
      "Epoch [389/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001266777\n",
      "Epoch [389/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001264955\n",
      "Epoch [389/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001263133\n",
      "Epoch [389/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001261313\n",
      "Epoch [389/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001259494\n",
      "Epoch [389/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001257676\n",
      "Epoch [389/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001255860\n",
      "Epoch [390/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001254044\n",
      "Epoch [390/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001252230\n",
      "Epoch [390/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001250417\n",
      "Epoch [390/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001248606\n",
      "Epoch [390/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001246795\n",
      "Epoch [390/500], Batch [60/110], Train Loss: 0.0113, Val Loss: 0.0003, LR: 0.0001244986\n",
      "Epoch [390/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001243178\n",
      "Epoch [390/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001241372\n",
      "Epoch [390/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001239566\n",
      "Epoch [390/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001237762\n",
      "Epoch [390/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001235959\n",
      "Epoch [391/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001234158\n",
      "Epoch [391/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001232357\n",
      "Epoch [391/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001230558\n",
      "Epoch [391/500], Batch [40/110], Train Loss: 0.0032, Val Loss: 0.0002, LR: 0.0001228760\n",
      "Epoch [391/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001226964\n",
      "Epoch [391/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001225168\n",
      "Epoch [391/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001223374\n",
      "Epoch [391/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001221581\n",
      "Epoch [391/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001219789\n",
      "Epoch [391/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001217999\n",
      "Epoch [391/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001216210\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 391: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.052 sec , Memory Usage: 192.43 MB\n",
      "Epoch [392/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001214422\n",
      "Epoch [392/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001212635\n",
      "Epoch [392/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0001210850\n",
      "Epoch [392/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001209066\n",
      "Epoch [392/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001207283\n",
      "Epoch [392/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001205501\n",
      "Epoch [392/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001203721\n",
      "Epoch [392/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001201942\n",
      "Epoch [392/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001200164\n",
      "Epoch [392/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001198387\n",
      "Epoch [392/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001196612\n",
      "Epoch [393/500], Batch [10/110], Train Loss: 0.0205, Val Loss: 0.0001, LR: 0.0001194837\n",
      "Epoch [393/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001193065\n",
      "Epoch [393/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001191293\n",
      "Epoch [393/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001189523\n",
      "Epoch [393/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001187754\n",
      "Epoch [393/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001185986\n",
      "Epoch [393/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001184219\n",
      "Epoch [393/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001182454\n",
      "Epoch [393/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001180690\n",
      "Epoch [393/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001178927\n",
      "Epoch [393/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001177165\n",
      "Epoch [394/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001175405\n",
      "Epoch [394/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001173646\n",
      "Epoch [394/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001171889\n",
      "Epoch [394/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001170132\n",
      "Epoch [394/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0001168377\n",
      "Epoch [394/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0001166623\n",
      "Epoch [394/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001164870\n",
      "Epoch [394/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001163119\n",
      "Epoch [394/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001161369\n",
      "Epoch [394/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001159620\n",
      "Epoch [394/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001157872\n",
      "Epoch [395/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001156126\n",
      "Epoch [395/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001154381\n",
      "Epoch [395/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001152637\n",
      "Epoch [395/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001150895\n",
      "Epoch [395/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001149153\n",
      "Epoch [395/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001147413\n",
      "Epoch [395/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001145675\n",
      "Epoch [395/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001143937\n",
      "Epoch [395/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001142201\n",
      "Epoch [395/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001140466\n",
      "Epoch [395/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001138733\n",
      "Epoch [396/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0005, LR: 0.0001137000\n",
      "Epoch [396/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0001135269\n",
      "Epoch [396/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0001133540\n",
      "Epoch [396/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001131811\n",
      "Epoch [396/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001130084\n",
      "Epoch [396/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001128358\n",
      "Epoch [396/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001126633\n",
      "Epoch [396/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001124910\n",
      "Epoch [396/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001123188\n",
      "Epoch [396/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001121467\n",
      "Epoch [396/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001119748\n",
      "Epoch [397/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001118029\n",
      "Epoch [397/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001116312\n",
      "Epoch [397/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001114597\n",
      "Epoch [397/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001112882\n",
      "Epoch [397/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001111169\n",
      "Epoch [397/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0001109457\n",
      "Epoch [397/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001107747\n",
      "Epoch [397/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001106038\n",
      "Epoch [397/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001104330\n",
      "Epoch [397/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001102623\n",
      "Epoch [397/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001100918\n",
      "Epoch [398/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001099213\n",
      "Epoch [398/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001097511\n",
      "Epoch [398/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001095809\n",
      "Epoch [398/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0001094109\n",
      "Epoch [398/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001092410\n",
      "Epoch [398/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001090712\n",
      "Epoch [398/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001089016\n",
      "Epoch [398/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001087321\n",
      "Epoch [398/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001085627\n",
      "Epoch [398/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001083935\n",
      "Epoch [398/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001082243\n",
      "Epoch [399/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001080554\n",
      "Epoch [399/500], Batch [20/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0001078865\n",
      "Epoch [399/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001077178\n",
      "Epoch [399/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001075492\n",
      "Epoch [399/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001073807\n",
      "Epoch [399/500], Batch [60/110], Train Loss: 0.0060, Val Loss: 0.0002, LR: 0.0001072124\n",
      "Epoch [399/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001070441\n",
      "Epoch [399/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0001068761\n",
      "Epoch [399/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001067081\n",
      "Epoch [399/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001065403\n",
      "Epoch [399/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001063726\n",
      "Epoch [400/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001062050\n",
      "Epoch [400/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001060376\n",
      "Epoch [400/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001058703\n",
      "Epoch [400/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001057031\n",
      "Epoch [400/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001055361\n",
      "Epoch [400/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001053692\n",
      "Epoch [400/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001052024\n",
      "Epoch [400/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001050358\n",
      "Epoch [400/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001048692\n",
      "Epoch [400/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001047028\n",
      "Epoch [400/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001045366\n",
      "Epoch [401/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001043705\n",
      "Epoch [401/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001042045\n",
      "Epoch [401/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001040386\n",
      "Epoch [401/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0001038729\n",
      "Epoch [401/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0001037073\n",
      "Epoch [401/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0001035418\n",
      "Epoch [401/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001033764\n",
      "Epoch [401/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001032112\n",
      "Epoch [401/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001030462\n",
      "Epoch [401/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001028812\n",
      "Epoch [401/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0001027164\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 401: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.036 sec , Memory Usage: 192.45 MB\n",
      "Epoch [402/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001025517\n",
      "Epoch [402/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001023871\n",
      "Epoch [402/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001022227\n",
      "Epoch [402/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001020584\n",
      "Epoch [402/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001018943\n",
      "Epoch [402/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001017302\n",
      "Epoch [402/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001015663\n",
      "Epoch [402/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001014026\n",
      "Epoch [402/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001012389\n",
      "Epoch [402/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001010754\n",
      "Epoch [402/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001009121\n",
      "Epoch [403/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001007488\n",
      "Epoch [403/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001005857\n",
      "Epoch [403/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001004227\n",
      "Epoch [403/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0001002599\n",
      "Epoch [403/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0001000972\n",
      "Epoch [403/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0000999346\n",
      "Epoch [403/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000997722\n",
      "Epoch [403/500], Batch [80/110], Train Loss: 0.0031, Val Loss: 0.0004, LR: 0.0000996099\n",
      "Epoch [403/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000994477\n",
      "Epoch [403/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000992856\n",
      "Epoch [403/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000991237\n",
      "Epoch [404/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0000989619\n",
      "Epoch [404/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000988003\n",
      "Epoch [404/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000986387\n",
      "Epoch [404/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0000984774\n",
      "Epoch [404/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0000983161\n",
      "Epoch [404/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0000981550\n",
      "Epoch [404/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0000979940\n",
      "Epoch [404/500], Batch [80/110], Train Loss: 0.0030, Val Loss: 0.0010, LR: 0.0000978331\n",
      "Epoch [404/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0000976724\n",
      "Epoch [404/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000975118\n",
      "Epoch [404/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000973514\n",
      "Epoch [405/500], Batch [10/110], Train Loss: 0.0027, Val Loss: 0.0005, LR: 0.0000971910\n",
      "Epoch [405/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000970308\n",
      "Epoch [405/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000968708\n",
      "Epoch [405/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000967109\n",
      "Epoch [405/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000965511\n",
      "Epoch [405/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0005, LR: 0.0000963914\n",
      "Epoch [405/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000962319\n",
      "Epoch [405/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000960725\n",
      "Epoch [405/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0000959132\n",
      "Epoch [405/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000957541\n",
      "Epoch [405/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000955951\n",
      "Epoch [406/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0000954363\n",
      "Epoch [406/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000952775\n",
      "Epoch [406/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0006, LR: 0.0000951189\n",
      "Epoch [406/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000949605\n",
      "Epoch [406/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000948022\n",
      "Epoch [406/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000946440\n",
      "Epoch [406/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0000944859\n",
      "Epoch [406/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000943280\n",
      "Epoch [406/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000941702\n",
      "Epoch [406/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000940125\n",
      "Epoch [406/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000938550\n",
      "Epoch [407/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000936976\n",
      "Epoch [407/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000935404\n",
      "Epoch [407/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000933833\n",
      "Epoch [407/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000932263\n",
      "Epoch [407/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000930694\n",
      "Epoch [407/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0000929127\n",
      "Epoch [407/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000927561\n",
      "Epoch [407/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000925997\n",
      "Epoch [407/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000924434\n",
      "Epoch [407/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000922872\n",
      "Epoch [407/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000921312\n",
      "Epoch [408/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000919753\n",
      "Epoch [408/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000918195\n",
      "Epoch [408/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0000916639\n",
      "Epoch [408/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000915084\n",
      "Epoch [408/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000913530\n",
      "Epoch [408/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0000911978\n",
      "Epoch [408/500], Batch [70/110], Train Loss: 0.0014, Val Loss: 0.0004, LR: 0.0000910427\n",
      "Epoch [408/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000908877\n",
      "Epoch [408/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000907329\n",
      "Epoch [408/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0002, LR: 0.0000905782\n",
      "Epoch [408/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0000904236\n",
      "Epoch [409/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000902692\n",
      "Epoch [409/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000901149\n",
      "Epoch [409/500], Batch [30/110], Train Loss: 0.0070, Val Loss: 0.0002, LR: 0.0000899608\n",
      "Epoch [409/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000898067\n",
      "Epoch [409/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000896529\n",
      "Epoch [409/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000894991\n",
      "Epoch [409/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000893455\n",
      "Epoch [409/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000891920\n",
      "Epoch [409/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000890387\n",
      "Epoch [409/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000888855\n",
      "Epoch [409/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0000887324\n",
      "Epoch [410/500], Batch [10/110], Train Loss: 0.0010, Val Loss: 0.0005, LR: 0.0000885795\n",
      "Epoch [410/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000884267\n",
      "Epoch [410/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000882740\n",
      "Epoch [410/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0000881215\n",
      "Epoch [410/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000879691\n",
      "Epoch [410/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0000878169\n",
      "Epoch [410/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0000876648\n",
      "Epoch [410/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0000875128\n",
      "Epoch [410/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0000873610\n",
      "Epoch [410/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0000872092\n",
      "Epoch [410/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000870577\n",
      "Epoch [411/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000869062\n",
      "Epoch [411/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000867549\n",
      "Epoch [411/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000866038\n",
      "Epoch [411/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000864528\n",
      "Epoch [411/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000863019\n",
      "Epoch [411/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000861511\n",
      "Epoch [411/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000860005\n",
      "Epoch [411/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000858500\n",
      "Epoch [411/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000856997\n",
      "Epoch [411/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000855495\n",
      "Epoch [411/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000853994\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 411: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.037 sec , Memory Usage: 192.45 MB\n",
      "Epoch [412/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000852495\n",
      "Epoch [412/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000850997\n",
      "Epoch [412/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000849500\n",
      "Epoch [412/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000848005\n",
      "Epoch [412/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000846511\n",
      "Epoch [412/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000845019\n",
      "Epoch [412/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000843528\n",
      "Epoch [412/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000842038\n",
      "Epoch [412/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000840550\n",
      "Epoch [412/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000839063\n",
      "Epoch [412/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000837577\n",
      "Epoch [413/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000836093\n",
      "Epoch [413/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000834610\n",
      "Epoch [413/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000833129\n",
      "Epoch [413/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000831649\n",
      "Epoch [413/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000830170\n",
      "Epoch [413/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000828693\n",
      "Epoch [413/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000827217\n",
      "Epoch [413/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000825742\n",
      "Epoch [413/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000824269\n",
      "Epoch [413/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000822797\n",
      "Epoch [413/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0004, LR: 0.0000821327\n",
      "Epoch [414/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0005, LR: 0.0000819858\n",
      "Epoch [414/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000818390\n",
      "Epoch [414/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000816924\n",
      "Epoch [414/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000815459\n",
      "Epoch [414/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000813995\n",
      "Epoch [414/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000812533\n",
      "Epoch [414/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000811072\n",
      "Epoch [414/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000809613\n",
      "Epoch [414/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000808155\n",
      "Epoch [414/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000806698\n",
      "Epoch [414/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000805243\n",
      "Epoch [415/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000803789\n",
      "Epoch [415/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000802337\n",
      "Epoch [415/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000800886\n",
      "Epoch [415/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000799436\n",
      "Epoch [415/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000797988\n",
      "Epoch [415/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000796541\n",
      "Epoch [415/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000795095\n",
      "Epoch [415/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000793651\n",
      "Epoch [415/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000792208\n",
      "Epoch [415/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000790767\n",
      "Epoch [415/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000789327\n",
      "Epoch [416/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000787888\n",
      "Epoch [416/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000786451\n",
      "Epoch [416/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000785015\n",
      "Epoch [416/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000783581\n",
      "Epoch [416/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000782148\n",
      "Epoch [416/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000780716\n",
      "Epoch [416/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000779286\n",
      "Epoch [416/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000777857\n",
      "Epoch [416/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000776430\n",
      "Epoch [416/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000775004\n",
      "Epoch [416/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000773579\n",
      "Epoch [417/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000772156\n",
      "Epoch [417/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000770734\n",
      "Epoch [417/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000769313\n",
      "Epoch [417/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000767894\n",
      "Epoch [417/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000766477\n",
      "Epoch [417/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0000765060\n",
      "Epoch [417/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000763646\n",
      "Epoch [417/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000762232\n",
      "Epoch [417/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000760820\n",
      "Epoch [417/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0000759409\n",
      "Epoch [417/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000758000\n",
      "Epoch [418/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000756592\n",
      "Epoch [418/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000755186\n",
      "Epoch [418/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000753781\n",
      "Epoch [418/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000752377\n",
      "Epoch [418/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000750975\n",
      "Epoch [418/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000749574\n",
      "Epoch [418/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0005, LR: 0.0000748174\n",
      "Epoch [418/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000746776\n",
      "Epoch [418/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000745380\n",
      "Epoch [418/500], Batch [100/110], Train Loss: 0.0086, Val Loss: 0.0006, LR: 0.0000743984\n",
      "Epoch [418/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000742590\n",
      "Epoch [419/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000741198\n",
      "Epoch [419/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000739807\n",
      "Epoch [419/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000738417\n",
      "Epoch [419/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000737029\n",
      "Epoch [419/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000735642\n",
      "Epoch [419/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000734257\n",
      "Epoch [419/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000732873\n",
      "Epoch [419/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000731490\n",
      "Epoch [419/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000730109\n",
      "Epoch [419/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000728729\n",
      "Epoch [419/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000727351\n",
      "Epoch [420/500], Batch [10/110], Train Loss: 0.0039, Val Loss: 0.0002, LR: 0.0000725974\n",
      "Epoch [420/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000724598\n",
      "Epoch [420/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000723224\n",
      "Epoch [420/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000721851\n",
      "Epoch [420/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000720480\n",
      "Epoch [420/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000719110\n",
      "Epoch [420/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000717742\n",
      "Epoch [420/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000716375\n",
      "Epoch [420/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000715009\n",
      "Epoch [420/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000713645\n",
      "Epoch [420/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000712282\n",
      "Epoch [421/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000710921\n",
      "Epoch [421/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000709561\n",
      "Epoch [421/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000708202\n",
      "Epoch [421/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000706845\n",
      "Epoch [421/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000705489\n",
      "Epoch [421/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000704135\n",
      "Epoch [421/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000702782\n",
      "Epoch [421/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000701430\n",
      "Epoch [421/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000700080\n",
      "Epoch [421/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000698732\n",
      "Epoch [421/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000697384\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 421: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 192.43 MB\n",
      "Epoch [422/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000696038\n",
      "Epoch [422/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000694694\n",
      "Epoch [422/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000693351\n",
      "Epoch [422/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000692010\n",
      "Epoch [422/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000690669\n",
      "Epoch [422/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000689331\n",
      "Epoch [422/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000687993\n",
      "Epoch [422/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000686657\n",
      "Epoch [422/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000685323\n",
      "Epoch [422/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000683990\n",
      "Epoch [422/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000682658\n",
      "Epoch [423/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000681328\n",
      "Epoch [423/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000680000\n",
      "Epoch [423/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000678672\n",
      "Epoch [423/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000677346\n",
      "Epoch [423/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000676022\n",
      "Epoch [423/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000674699\n",
      "Epoch [423/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000673377\n",
      "Epoch [423/500], Batch [80/110], Train Loss: 0.0035, Val Loss: 0.0001, LR: 0.0000672057\n",
      "Epoch [423/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000670738\n",
      "Epoch [423/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000669421\n",
      "Epoch [423/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000668105\n",
      "Epoch [424/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000666791\n",
      "Epoch [424/500], Batch [20/110], Train Loss: 0.0115, Val Loss: 0.0002, LR: 0.0000665478\n",
      "Epoch [424/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000664166\n",
      "Epoch [424/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000662856\n",
      "Epoch [424/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000661547\n",
      "Epoch [424/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000660240\n",
      "Epoch [424/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000658934\n",
      "Epoch [424/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000657629\n",
      "Epoch [424/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000656326\n",
      "Epoch [424/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000655025\n",
      "Epoch [424/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000653725\n",
      "Epoch [425/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000652426\n",
      "Epoch [425/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000651129\n",
      "Epoch [425/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000649833\n",
      "Epoch [425/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000648538\n",
      "Epoch [425/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000647245\n",
      "Epoch [425/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0000645954\n",
      "Epoch [425/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0000644664\n",
      "Epoch [425/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0000643375\n",
      "Epoch [425/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0000642088\n",
      "Epoch [425/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0000640802\n",
      "Epoch [425/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0000639518\n",
      "Epoch [426/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000638235\n",
      "Epoch [426/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0006, LR: 0.0000636953\n",
      "Epoch [426/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0000635673\n",
      "Epoch [426/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000634395\n",
      "Epoch [426/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0000633118\n",
      "Epoch [426/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0000631842\n",
      "Epoch [426/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000630568\n",
      "Epoch [426/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000629295\n",
      "Epoch [426/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000628023\n",
      "Epoch [426/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000626753\n",
      "Epoch [426/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000625485\n",
      "Epoch [427/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000624218\n",
      "Epoch [427/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000622952\n",
      "Epoch [427/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000621688\n",
      "Epoch [427/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000620425\n",
      "Epoch [427/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000619164\n",
      "Epoch [427/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000617904\n",
      "Epoch [427/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000616646\n",
      "Epoch [427/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000615389\n",
      "Epoch [427/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000614134\n",
      "Epoch [427/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000612879\n",
      "Epoch [427/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000611627\n",
      "Epoch [428/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000610376\n",
      "Epoch [428/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000609126\n",
      "Epoch [428/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000607878\n",
      "Epoch [428/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000606631\n",
      "Epoch [428/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000605386\n",
      "Epoch [428/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000604142\n",
      "Epoch [428/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000602899\n",
      "Epoch [428/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000601658\n",
      "Epoch [428/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000600419\n",
      "Epoch [428/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000599181\n",
      "Epoch [428/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000597944\n",
      "Epoch [429/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000596709\n",
      "Epoch [429/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000595475\n",
      "Epoch [429/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000594243\n",
      "Epoch [429/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000593012\n",
      "Epoch [429/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000591783\n",
      "Epoch [429/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000590555\n",
      "Epoch [429/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000589328\n",
      "Epoch [429/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000588103\n",
      "Epoch [429/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000586880\n",
      "Epoch [429/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000585658\n",
      "Epoch [429/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000584437\n",
      "Epoch [430/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000583218\n",
      "Epoch [430/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000582000\n",
      "Epoch [430/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000580784\n",
      "Epoch [430/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000579569\n",
      "Epoch [430/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000578356\n",
      "Epoch [430/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000577144\n",
      "Epoch [430/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000575933\n",
      "Epoch [430/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000574724\n",
      "Epoch [430/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000573517\n",
      "Epoch [430/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000572311\n",
      "Epoch [430/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000571106\n",
      "Epoch [431/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000569903\n",
      "Epoch [431/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000568701\n",
      "Epoch [431/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000567501\n",
      "Epoch [431/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000566302\n",
      "Epoch [431/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000565105\n",
      "Epoch [431/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000563909\n",
      "Epoch [431/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000562715\n",
      "Epoch [431/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000561522\n",
      "Epoch [431/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000560331\n",
      "Epoch [431/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0000559141\n",
      "Epoch [431/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000557952\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 431: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.039 sec , Memory Usage: 192.46 MB\n",
      "Epoch [432/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000556765\n",
      "Epoch [432/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000555580\n",
      "Epoch [432/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000554395\n",
      "Epoch [432/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000553213\n",
      "Epoch [432/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000552032\n",
      "Epoch [432/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000550852\n",
      "Epoch [432/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000549674\n",
      "Epoch [432/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000548497\n",
      "Epoch [432/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000547322\n",
      "Epoch [432/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000546148\n",
      "Epoch [432/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000544975\n",
      "Epoch [433/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000543805\n",
      "Epoch [433/500], Batch [20/110], Train Loss: 0.0024, Val Loss: 0.0001, LR: 0.0000542635\n",
      "Epoch [433/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000541467\n",
      "Epoch [433/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000540301\n",
      "Epoch [433/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000539136\n",
      "Epoch [433/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000537972\n",
      "Epoch [433/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000536810\n",
      "Epoch [433/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000535650\n",
      "Epoch [433/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000534490\n",
      "Epoch [433/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000533333\n",
      "Epoch [433/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000532177\n",
      "Epoch [434/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000531022\n",
      "Epoch [434/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000529869\n",
      "Epoch [434/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000528717\n",
      "Epoch [434/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000527567\n",
      "Epoch [434/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000526418\n",
      "Epoch [434/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000525271\n",
      "Epoch [434/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000524125\n",
      "Epoch [434/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000522980\n",
      "Epoch [434/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000521838\n",
      "Epoch [434/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000520696\n",
      "Epoch [434/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000519556\n",
      "Epoch [435/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000518418\n",
      "Epoch [435/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000517281\n",
      "Epoch [435/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000516145\n",
      "Epoch [435/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000515011\n",
      "Epoch [435/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000513879\n",
      "Epoch [435/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000512748\n",
      "Epoch [435/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000511618\n",
      "Epoch [435/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000510490\n",
      "Epoch [435/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000509363\n",
      "Epoch [435/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000508238\n",
      "Epoch [435/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000507115\n",
      "Epoch [436/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000505992\n",
      "Epoch [436/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000504872\n",
      "Epoch [436/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000503753\n",
      "Epoch [436/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000502635\n",
      "Epoch [436/500], Batch [50/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0000501519\n",
      "Epoch [436/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000500404\n",
      "Epoch [436/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000499291\n",
      "Epoch [436/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000498179\n",
      "Epoch [436/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000497068\n",
      "Epoch [436/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000495960\n",
      "Epoch [436/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000494852\n",
      "Epoch [437/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000493747\n",
      "Epoch [437/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000492642\n",
      "Epoch [437/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000491539\n",
      "Epoch [437/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000490438\n",
      "Epoch [437/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000489338\n",
      "Epoch [437/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0000488240\n",
      "Epoch [437/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000487143\n",
      "Epoch [437/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000486047\n",
      "Epoch [437/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000484953\n",
      "Epoch [437/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000483861\n",
      "Epoch [437/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000482770\n",
      "Epoch [438/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000481680\n",
      "Epoch [438/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000480592\n",
      "Epoch [438/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000479506\n",
      "Epoch [438/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000478421\n",
      "Epoch [438/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000477338\n",
      "Epoch [438/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000476255\n",
      "Epoch [438/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000475175\n",
      "Epoch [438/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000474096\n",
      "Epoch [438/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000473018\n",
      "Epoch [438/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000471942\n",
      "Epoch [438/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000470868\n",
      "Epoch [439/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000469795\n",
      "Epoch [439/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000468723\n",
      "Epoch [439/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000467653\n",
      "Epoch [439/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000466585\n",
      "Epoch [439/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000465518\n",
      "Epoch [439/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000464452\n",
      "Epoch [439/500], Batch [70/110], Train Loss: 0.0025, Val Loss: 0.0002, LR: 0.0000463388\n",
      "Epoch [439/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000462325\n",
      "Epoch [439/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000461264\n",
      "Epoch [439/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000460205\n",
      "Epoch [439/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000459146\n",
      "Epoch [440/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000458090\n",
      "Epoch [440/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000457035\n",
      "Epoch [440/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000455981\n",
      "Epoch [440/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0000454929\n",
      "Epoch [440/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000453878\n",
      "Epoch [440/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000452829\n",
      "Epoch [440/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000451782\n",
      "Epoch [440/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000450736\n",
      "Epoch [440/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0003, LR: 0.0000449691\n",
      "Epoch [440/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0000448648\n",
      "Epoch [440/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000447606\n",
      "Epoch [441/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000446566\n",
      "Epoch [441/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000445528\n",
      "Epoch [441/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000444491\n",
      "Epoch [441/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000443455\n",
      "Epoch [441/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000442421\n",
      "Epoch [441/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000441388\n",
      "Epoch [441/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000440357\n",
      "Epoch [441/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000439328\n",
      "Epoch [441/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0000438300\n",
      "Epoch [441/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000437273\n",
      "Epoch [441/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0000436248\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 441: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.025 sec , Memory Usage: 192.48 MB\n",
      "Epoch [442/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000435224\n",
      "Epoch [442/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000434202\n",
      "Epoch [442/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000433182\n",
      "Epoch [442/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000432163\n",
      "Epoch [442/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000431145\n",
      "Epoch [442/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000430129\n",
      "Epoch [442/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000429115\n",
      "Epoch [442/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000428102\n",
      "Epoch [442/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000427090\n",
      "Epoch [442/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0000426080\n",
      "Epoch [442/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0004, LR: 0.0000425072\n",
      "Epoch [443/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000424065\n",
      "Epoch [443/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000423059\n",
      "Epoch [443/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000422055\n",
      "Epoch [443/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000421053\n",
      "Epoch [443/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000420052\n",
      "Epoch [443/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000419053\n",
      "Epoch [443/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000418055\n",
      "Epoch [443/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000417058\n",
      "Epoch [443/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000416063\n",
      "Epoch [443/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000415070\n",
      "Epoch [443/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000414078\n",
      "Epoch [444/500], Batch [10/110], Train Loss: 0.0081, Val Loss: 0.0002, LR: 0.0000413088\n",
      "Epoch [444/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000412099\n",
      "Epoch [444/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000411112\n",
      "Epoch [444/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000410126\n",
      "Epoch [444/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000409141\n",
      "Epoch [444/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0000408159\n",
      "Epoch [444/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000407177\n",
      "Epoch [444/500], Batch [80/110], Train Loss: 0.0015, Val Loss: 0.0002, LR: 0.0000406198\n",
      "Epoch [444/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000405219\n",
      "Epoch [444/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000404243\n",
      "Epoch [444/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000403267\n",
      "Epoch [445/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000402294\n",
      "Epoch [445/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000401322\n",
      "Epoch [445/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000400351\n",
      "Epoch [445/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000399382\n",
      "Epoch [445/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000398414\n",
      "Epoch [445/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000397448\n",
      "Epoch [445/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000396483\n",
      "Epoch [445/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000395520\n",
      "Epoch [445/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000394559\n",
      "Epoch [445/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0000393599\n",
      "Epoch [445/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000392640\n",
      "Epoch [446/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000391683\n",
      "Epoch [446/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000390728\n",
      "Epoch [446/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000389774\n",
      "Epoch [446/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000388821\n",
      "Epoch [446/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000387870\n",
      "Epoch [446/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000386921\n",
      "Epoch [446/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000385973\n",
      "Epoch [446/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000385027\n",
      "Epoch [446/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000384082\n",
      "Epoch [446/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000383139\n",
      "Epoch [446/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000382197\n",
      "Epoch [447/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000381257\n",
      "Epoch [447/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000380318\n",
      "Epoch [447/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000379381\n",
      "Epoch [447/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000378445\n",
      "Epoch [447/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000377511\n",
      "Epoch [447/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000376578\n",
      "Epoch [447/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000375647\n",
      "Epoch [447/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000374717\n",
      "Epoch [447/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000373789\n",
      "Epoch [447/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000372863\n",
      "Epoch [447/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000371938\n",
      "Epoch [448/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000371014\n",
      "Epoch [448/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000370092\n",
      "Epoch [448/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000369172\n",
      "Epoch [448/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000368253\n",
      "Epoch [448/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000367336\n",
      "Epoch [448/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000366420\n",
      "Epoch [448/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000365505\n",
      "Epoch [448/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000364593\n",
      "Epoch [448/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000363681\n",
      "Epoch [448/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000362772\n",
      "Epoch [448/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000361863\n",
      "Epoch [449/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0004, LR: 0.0000360957\n",
      "Epoch [449/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000360052\n",
      "Epoch [449/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000359148\n",
      "Epoch [449/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000358246\n",
      "Epoch [449/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000357345\n",
      "Epoch [449/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000356446\n",
      "Epoch [449/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000355549\n",
      "Epoch [449/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000354653\n",
      "Epoch [449/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000353758\n",
      "Epoch [449/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000352865\n",
      "Epoch [449/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000351974\n",
      "Epoch [450/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000351084\n",
      "Epoch [450/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000350196\n",
      "Epoch [450/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000349309\n",
      "Epoch [450/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000348424\n",
      "Epoch [450/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000347540\n",
      "Epoch [450/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000346658\n",
      "Epoch [450/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0000345777\n",
      "Epoch [450/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000344898\n",
      "Epoch [450/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000344021\n",
      "Epoch [450/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000343145\n",
      "Epoch [450/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000342270\n",
      "Epoch [451/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000341397\n",
      "Epoch [451/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000340526\n",
      "Epoch [451/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000339656\n",
      "Epoch [451/500], Batch [40/110], Train Loss: 0.0013, Val Loss: 0.0003, LR: 0.0000338788\n",
      "Epoch [451/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0003, LR: 0.0000337921\n",
      "Epoch [451/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000337056\n",
      "Epoch [451/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000336192\n",
      "Epoch [451/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000335330\n",
      "Epoch [451/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000334469\n",
      "Epoch [451/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000333610\n",
      "Epoch [451/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000332752\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 451: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.043 sec , Memory Usage: 192.48 MB\n",
      "Epoch [452/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000331896\n",
      "Epoch [452/500], Batch [20/110], Train Loss: 0.0014, Val Loss: 0.0003, LR: 0.0000331042\n",
      "Epoch [452/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000330189\n",
      "Epoch [452/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000329337\n",
      "Epoch [452/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000328487\n",
      "Epoch [452/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000327639\n",
      "Epoch [452/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000326792\n",
      "Epoch [452/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000325947\n",
      "Epoch [452/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000325103\n",
      "Epoch [452/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000324261\n",
      "Epoch [452/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000323421\n",
      "Epoch [453/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000322581\n",
      "Epoch [453/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000321744\n",
      "Epoch [453/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000320908\n",
      "Epoch [453/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000320073\n",
      "Epoch [453/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000319240\n",
      "Epoch [453/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000318409\n",
      "Epoch [453/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000317579\n",
      "Epoch [453/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000316751\n",
      "Epoch [453/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000315924\n",
      "Epoch [453/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000315099\n",
      "Epoch [453/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000314275\n",
      "Epoch [454/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000313453\n",
      "Epoch [454/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000312633\n",
      "Epoch [454/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000311814\n",
      "Epoch [454/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000310996\n",
      "Epoch [454/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000310180\n",
      "Epoch [454/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000309366\n",
      "Epoch [454/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000308553\n",
      "Epoch [454/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0000307742\n",
      "Epoch [454/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000306932\n",
      "Epoch [454/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0000306124\n",
      "Epoch [454/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000305317\n",
      "Epoch [455/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000304512\n",
      "Epoch [455/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000303708\n",
      "Epoch [455/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000302906\n",
      "Epoch [455/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000302106\n",
      "Epoch [455/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000301307\n",
      "Epoch [455/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000300510\n",
      "Epoch [455/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000299714\n",
      "Epoch [455/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000298920\n",
      "Epoch [455/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000298127\n",
      "Epoch [455/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000297336\n",
      "Epoch [455/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000296546\n",
      "Epoch [456/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000295758\n",
      "Epoch [456/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000294972\n",
      "Epoch [456/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000294187\n",
      "Epoch [456/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000293403\n",
      "Epoch [456/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000292622\n",
      "Epoch [456/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000291841\n",
      "Epoch [456/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000291062\n",
      "Epoch [456/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000290285\n",
      "Epoch [456/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000289510\n",
      "Epoch [456/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000288736\n",
      "Epoch [456/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000287963\n",
      "Epoch [457/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000287192\n",
      "Epoch [457/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000286423\n",
      "Epoch [457/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000285655\n",
      "Epoch [457/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000284888\n",
      "Epoch [457/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000284124\n",
      "Epoch [457/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000283360\n",
      "Epoch [457/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000282599\n",
      "Epoch [457/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000281839\n",
      "Epoch [457/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000281080\n",
      "Epoch [457/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000280323\n",
      "Epoch [457/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000279568\n",
      "Epoch [458/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000278814\n",
      "Epoch [458/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000278062\n",
      "Epoch [458/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000277311\n",
      "Epoch [458/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000276562\n",
      "Epoch [458/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000275814\n",
      "Epoch [458/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000275068\n",
      "Epoch [458/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000274323\n",
      "Epoch [458/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000273580\n",
      "Epoch [458/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000272839\n",
      "Epoch [458/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000272099\n",
      "Epoch [458/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000271361\n",
      "Epoch [459/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000270624\n",
      "Epoch [459/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000269889\n",
      "Epoch [459/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000269155\n",
      "Epoch [459/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000268423\n",
      "Epoch [459/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000267693\n",
      "Epoch [459/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000266964\n",
      "Epoch [459/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000266237\n",
      "Epoch [459/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000265511\n",
      "Epoch [459/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000264786\n",
      "Epoch [459/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000264064\n",
      "Epoch [459/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000263343\n",
      "Epoch [460/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000262623\n",
      "Epoch [460/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000261905\n",
      "Epoch [460/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000261189\n",
      "Epoch [460/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000260474\n",
      "Epoch [460/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000259760\n",
      "Epoch [460/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000259049\n",
      "Epoch [460/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000258338\n",
      "Epoch [460/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000257630\n",
      "Epoch [460/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000256923\n",
      "Epoch [460/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000256217\n",
      "Epoch [460/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000255513\n",
      "Epoch [461/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000254811\n",
      "Epoch [461/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000254110\n",
      "Epoch [461/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000253411\n",
      "Epoch [461/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000252713\n",
      "Epoch [461/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000252017\n",
      "Epoch [461/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000251323\n",
      "Epoch [461/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000250630\n",
      "Epoch [461/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000249938\n",
      "Epoch [461/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000249248\n",
      "Epoch [461/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000248560\n",
      "Epoch [461/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000247873\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 461: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.025 sec , Memory Usage: 192.48 MB\n",
      "Epoch [462/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000247188\n",
      "Epoch [462/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000246505\n",
      "Epoch [462/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000245823\n",
      "Epoch [462/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000245142\n",
      "Epoch [462/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000244463\n",
      "Epoch [462/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000243786\n",
      "Epoch [462/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000243110\n",
      "Epoch [462/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000242436\n",
      "Epoch [462/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000241763\n",
      "Epoch [462/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000241092\n",
      "Epoch [462/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000240423\n",
      "Epoch [463/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000239755\n",
      "Epoch [463/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000239089\n",
      "Epoch [463/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000238424\n",
      "Epoch [463/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000237761\n",
      "Epoch [463/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000237099\n",
      "Epoch [463/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000236439\n",
      "Epoch [463/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000235781\n",
      "Epoch [463/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000235124\n",
      "Epoch [463/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000234468\n",
      "Epoch [463/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000233815\n",
      "Epoch [463/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000233162\n",
      "Epoch [464/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000232512\n",
      "Epoch [464/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000231863\n",
      "Epoch [464/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000231215\n",
      "Epoch [464/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000230569\n",
      "Epoch [464/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000229925\n",
      "Epoch [464/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000229282\n",
      "Epoch [464/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000228641\n",
      "Epoch [464/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000228001\n",
      "Epoch [464/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000227363\n",
      "Epoch [464/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000226727\n",
      "Epoch [464/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000226092\n",
      "Epoch [465/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000225459\n",
      "Epoch [465/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000224827\n",
      "Epoch [465/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000224197\n",
      "Epoch [465/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000223568\n",
      "Epoch [465/500], Batch [50/110], Train Loss: 0.0066, Val Loss: 0.0002, LR: 0.0000222941\n",
      "Epoch [465/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000222316\n",
      "Epoch [465/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000221692\n",
      "Epoch [465/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000221069\n",
      "Epoch [465/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000220449\n",
      "Epoch [465/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000219830\n",
      "Epoch [465/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000219212\n",
      "Epoch [466/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000218596\n",
      "Epoch [466/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000217982\n",
      "Epoch [466/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000217369\n",
      "Epoch [466/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000216757\n",
      "Epoch [466/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000216148\n",
      "Epoch [466/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000215540\n",
      "Epoch [466/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000214933\n",
      "Epoch [466/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000214328\n",
      "Epoch [466/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000213725\n",
      "Epoch [466/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000213123\n",
      "Epoch [466/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000212523\n",
      "Epoch [467/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000211924\n",
      "Epoch [467/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000211327\n",
      "Epoch [467/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000210732\n",
      "Epoch [467/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000210138\n",
      "Epoch [467/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000209545\n",
      "Epoch [467/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000208955\n",
      "Epoch [467/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000208365\n",
      "Epoch [467/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000207778\n",
      "Epoch [467/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000207192\n",
      "Epoch [467/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000206607\n",
      "Epoch [467/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000206025\n",
      "Epoch [468/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000205443\n",
      "Epoch [468/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000204864\n",
      "Epoch [468/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000204285\n",
      "Epoch [468/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000203709\n",
      "Epoch [468/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000203134\n",
      "Epoch [468/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000202561\n",
      "Epoch [468/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000201989\n",
      "Epoch [468/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000201419\n",
      "Epoch [468/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000200850\n",
      "Epoch [468/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000200283\n",
      "Epoch [468/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000199717\n",
      "Epoch [469/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000199154\n",
      "Epoch [469/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000198591\n",
      "Epoch [469/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000198031\n",
      "Epoch [469/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000197471\n",
      "Epoch [469/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000196914\n",
      "Epoch [469/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000196358\n",
      "Epoch [469/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000195804\n",
      "Epoch [469/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000195251\n",
      "Epoch [469/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000194700\n",
      "Epoch [469/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000194150\n",
      "Epoch [469/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000193602\n",
      "Epoch [470/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0000193055\n",
      "Epoch [470/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000192511\n",
      "Epoch [470/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000191967\n",
      "Epoch [470/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000191426\n",
      "Epoch [470/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000190886\n",
      "Epoch [470/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000190347\n",
      "Epoch [470/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000189810\n",
      "Epoch [470/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000189275\n",
      "Epoch [470/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000188741\n",
      "Epoch [470/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000188209\n",
      "Epoch [470/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000187678\n",
      "Epoch [471/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000187149\n",
      "Epoch [471/500], Batch [20/110], Train Loss: 0.0011, Val Loss: 0.0003, LR: 0.0000186622\n",
      "Epoch [471/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000186096\n",
      "Epoch [471/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000185572\n",
      "Epoch [471/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000185049\n",
      "Epoch [471/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000184528\n",
      "Epoch [471/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0000184008\n",
      "Epoch [471/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000183490\n",
      "Epoch [471/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000182974\n",
      "Epoch [471/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000182459\n",
      "Epoch [471/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000181946\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 471: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.037 sec , Memory Usage: 192.50 MB\n",
      "Epoch [472/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000181435\n",
      "Epoch [472/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000180925\n",
      "Epoch [472/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000180416\n",
      "Epoch [472/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000179910\n",
      "Epoch [472/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000179404\n",
      "Epoch [472/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000178901\n",
      "Epoch [472/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000178399\n",
      "Epoch [472/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000177898\n",
      "Epoch [472/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000177399\n",
      "Epoch [472/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000176902\n",
      "Epoch [472/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000176407\n",
      "Epoch [473/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000175912\n",
      "Epoch [473/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000175420\n",
      "Epoch [473/500], Batch [30/110], Train Loss: 0.0056, Val Loss: 0.0002, LR: 0.0000174929\n",
      "Epoch [473/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000174440\n",
      "Epoch [473/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000173952\n",
      "Epoch [473/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000173466\n",
      "Epoch [473/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000172981\n",
      "Epoch [473/500], Batch [80/110], Train Loss: 0.0082, Val Loss: 0.0002, LR: 0.0000172499\n",
      "Epoch [473/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000172017\n",
      "Epoch [473/500], Batch [100/110], Train Loss: 0.0106, Val Loss: 0.0002, LR: 0.0000171537\n",
      "Epoch [473/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000171059\n",
      "Epoch [474/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000170583\n",
      "Epoch [474/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000170108\n",
      "Epoch [474/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000169634\n",
      "Epoch [474/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000169163\n",
      "Epoch [474/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000168692\n",
      "Epoch [474/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000168224\n",
      "Epoch [474/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000167757\n",
      "Epoch [474/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000167291\n",
      "Epoch [474/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000166827\n",
      "Epoch [474/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000166365\n",
      "Epoch [474/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000165905\n",
      "Epoch [475/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000165446\n",
      "Epoch [475/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000164988\n",
      "Epoch [475/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000164532\n",
      "Epoch [475/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000164078\n",
      "Epoch [475/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000163625\n",
      "Epoch [475/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000163174\n",
      "Epoch [475/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000162725\n",
      "Epoch [475/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000162277\n",
      "Epoch [475/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000161831\n",
      "Epoch [475/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000161386\n",
      "Epoch [475/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000160943\n",
      "Epoch [476/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000160501\n",
      "Epoch [476/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000160061\n",
      "Epoch [476/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000159623\n",
      "Epoch [476/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000159186\n",
      "Epoch [476/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000158751\n",
      "Epoch [476/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000158318\n",
      "Epoch [476/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000157886\n",
      "Epoch [476/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000157455\n",
      "Epoch [476/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0003, LR: 0.0000157027\n",
      "Epoch [476/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000156599\n",
      "Epoch [476/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000156174\n",
      "Epoch [477/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000155750\n",
      "Epoch [477/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000155328\n",
      "Epoch [477/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000154907\n",
      "Epoch [477/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000154488\n",
      "Epoch [477/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000154070\n",
      "Epoch [477/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000153654\n",
      "Epoch [477/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000153240\n",
      "Epoch [477/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000152827\n",
      "Epoch [477/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000152416\n",
      "Epoch [477/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000152006\n",
      "Epoch [477/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000151598\n",
      "Epoch [478/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000151192\n",
      "Epoch [478/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000150787\n",
      "Epoch [478/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000150384\n",
      "Epoch [478/500], Batch [40/110], Train Loss: 0.0078, Val Loss: 0.0002, LR: 0.0000149982\n",
      "Epoch [478/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000149582\n",
      "Epoch [478/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000149184\n",
      "Epoch [478/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000148787\n",
      "Epoch [478/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000148392\n",
      "Epoch [478/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000147998\n",
      "Epoch [478/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000147606\n",
      "Epoch [478/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000147216\n",
      "Epoch [479/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000146827\n",
      "Epoch [479/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000146440\n",
      "Epoch [479/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000146054\n",
      "Epoch [479/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000145670\n",
      "Epoch [479/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000145288\n",
      "Epoch [479/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000144907\n",
      "Epoch [479/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000144528\n",
      "Epoch [479/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000144150\n",
      "Epoch [479/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000143774\n",
      "Epoch [479/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000143400\n",
      "Epoch [479/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000143027\n",
      "Epoch [480/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000142656\n",
      "Epoch [480/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000142286\n",
      "Epoch [480/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000141918\n",
      "Epoch [480/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000141552\n",
      "Epoch [480/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000141187\n",
      "Epoch [480/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000140824\n",
      "Epoch [480/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000140463\n",
      "Epoch [480/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000140103\n",
      "Epoch [480/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000139744\n",
      "Epoch [480/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000139387\n",
      "Epoch [480/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000139032\n",
      "Epoch [481/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000138679\n",
      "Epoch [481/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000138327\n",
      "Epoch [481/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000137976\n",
      "Epoch [481/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000137628\n",
      "Epoch [481/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000137280\n",
      "Epoch [481/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000136935\n",
      "Epoch [481/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000136591\n",
      "Epoch [481/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000136249\n",
      "Epoch [481/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000135908\n",
      "Epoch [481/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000135569\n",
      "Epoch [481/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000135231\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 481: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.035 sec , Memory Usage: 192.50 MB\n",
      "Epoch [482/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000134895\n",
      "Epoch [482/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000134561\n",
      "Epoch [482/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000134228\n",
      "Epoch [482/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0003, LR: 0.0000133897\n",
      "Epoch [482/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000133567\n",
      "Epoch [482/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000133240\n",
      "Epoch [482/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000132913\n",
      "Epoch [482/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0003, LR: 0.0000132589\n",
      "Epoch [482/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000132265\n",
      "Epoch [482/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000131944\n",
      "Epoch [482/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000131624\n",
      "Epoch [483/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000131306\n",
      "Epoch [483/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000130989\n",
      "Epoch [483/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000130674\n",
      "Epoch [483/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000130360\n",
      "Epoch [483/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000130049\n",
      "Epoch [483/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000129738\n",
      "Epoch [483/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000129430\n",
      "Epoch [483/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000129123\n",
      "Epoch [483/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0000128817\n",
      "Epoch [483/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000128513\n",
      "Epoch [483/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000128211\n",
      "Epoch [484/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000127910\n",
      "Epoch [484/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000127611\n",
      "Epoch [484/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000127314\n",
      "Epoch [484/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000127018\n",
      "Epoch [484/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000126724\n",
      "Epoch [484/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0000126431\n",
      "Epoch [484/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000126140\n",
      "Epoch [484/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000125851\n",
      "Epoch [484/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000125563\n",
      "Epoch [484/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000125277\n",
      "Epoch [484/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000124992\n",
      "Epoch [485/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000124710\n",
      "Epoch [485/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000124428\n",
      "Epoch [485/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000124148\n",
      "Epoch [485/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000123870\n",
      "Epoch [485/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000123594\n",
      "Epoch [485/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000123319\n",
      "Epoch [485/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000123045\n",
      "Epoch [485/500], Batch [80/110], Train Loss: 0.0074, Val Loss: 0.0002, LR: 0.0000122774\n",
      "Epoch [485/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000122504\n",
      "Epoch [485/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0000122235\n",
      "Epoch [485/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000121968\n",
      "Epoch [486/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000121703\n",
      "Epoch [486/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000121439\n",
      "Epoch [486/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000121177\n",
      "Epoch [486/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000120917\n",
      "Epoch [486/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000120658\n",
      "Epoch [486/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000120401\n",
      "Epoch [486/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000120145\n",
      "Epoch [486/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000119891\n",
      "Epoch [486/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000119639\n",
      "Epoch [486/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000119388\n",
      "Epoch [486/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000119139\n",
      "Epoch [487/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000118891\n",
      "Epoch [487/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000118645\n",
      "Epoch [487/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000118401\n",
      "Epoch [487/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000118158\n",
      "Epoch [487/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000117917\n",
      "Epoch [487/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000117677\n",
      "Epoch [487/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000117439\n",
      "Epoch [487/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000117203\n",
      "Epoch [487/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000116968\n",
      "Epoch [487/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000116735\n",
      "Epoch [487/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000116504\n",
      "Epoch [488/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000116274\n",
      "Epoch [488/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000116045\n",
      "Epoch [488/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000115819\n",
      "Epoch [488/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000115594\n",
      "Epoch [488/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000115370\n",
      "Epoch [488/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000115148\n",
      "Epoch [488/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000114928\n",
      "Epoch [488/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000114710\n",
      "Epoch [488/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000114493\n",
      "Epoch [488/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000114277\n",
      "Epoch [488/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000114063\n",
      "Epoch [489/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000113851\n",
      "Epoch [489/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000113641\n",
      "Epoch [489/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000113432\n",
      "Epoch [489/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000113224\n",
      "Epoch [489/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000113019\n",
      "Epoch [489/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000112815\n",
      "Epoch [489/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000112612\n",
      "Epoch [489/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000112411\n",
      "Epoch [489/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000112212\n",
      "Epoch [489/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000112014\n",
      "Epoch [489/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000111818\n",
      "Epoch [490/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000111624\n",
      "Epoch [490/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000111431\n",
      "Epoch [490/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000111240\n",
      "Epoch [490/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000111050\n",
      "Epoch [490/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000110862\n",
      "Epoch [490/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000110676\n",
      "Epoch [490/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000110491\n",
      "Epoch [490/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000110308\n",
      "Epoch [490/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000110126\n",
      "Epoch [490/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000109946\n",
      "Epoch [490/500], Batch [110/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0000109768\n",
      "Epoch [491/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000109591\n",
      "Epoch [491/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000109416\n",
      "Epoch [491/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000109242\n",
      "Epoch [491/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000109070\n",
      "Epoch [491/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000108900\n",
      "Epoch [491/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000108731\n",
      "Epoch [491/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000108564\n",
      "Epoch [491/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000108399\n",
      "Epoch [491/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000108235\n",
      "Epoch [491/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000108073\n",
      "Epoch [491/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000107912\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 491: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.030 sec , Memory Usage: 192.52 MB\n",
      "Epoch [492/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000107753\n",
      "Epoch [492/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000107596\n",
      "Epoch [492/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000107440\n",
      "Epoch [492/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000107286\n",
      "Epoch [492/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000107133\n",
      "Epoch [492/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000106983\n",
      "Epoch [492/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000106833\n",
      "Epoch [492/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000106686\n",
      "Epoch [492/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000106539\n",
      "Epoch [492/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000106395\n",
      "Epoch [492/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000106252\n",
      "Epoch [493/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000106111\n",
      "Epoch [493/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000105971\n",
      "Epoch [493/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000105833\n",
      "Epoch [493/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000105697\n",
      "Epoch [493/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000105562\n",
      "Epoch [493/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000105429\n",
      "Epoch [493/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000105297\n",
      "Epoch [493/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000105167\n",
      "Epoch [493/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000105039\n",
      "Epoch [493/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000104912\n",
      "Epoch [493/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000104787\n",
      "Epoch [494/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000104663\n",
      "Epoch [494/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000104542\n",
      "Epoch [494/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000104421\n",
      "Epoch [494/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000104303\n",
      "Epoch [494/500], Batch [50/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0000104186\n",
      "Epoch [494/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000104070\n",
      "Epoch [494/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000103956\n",
      "Epoch [494/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000103844\n",
      "Epoch [494/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000103733\n",
      "Epoch [494/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000103624\n",
      "Epoch [494/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000103517\n",
      "Epoch [495/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000103411\n",
      "Epoch [495/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000103307\n",
      "Epoch [495/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000103205\n",
      "Epoch [495/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000103104\n",
      "Epoch [495/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000103004\n",
      "Epoch [495/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000102907\n",
      "Epoch [495/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000102811\n",
      "Epoch [495/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000102716\n",
      "Epoch [495/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000102623\n",
      "Epoch [495/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000102532\n",
      "Epoch [495/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000102443\n",
      "Epoch [496/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000102355\n",
      "Epoch [496/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000102268\n",
      "Epoch [496/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000102183\n",
      "Epoch [496/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000102100\n",
      "Epoch [496/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000102019\n",
      "Epoch [496/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000101939\n",
      "Epoch [496/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000101860\n",
      "Epoch [496/500], Batch [80/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0000101784\n",
      "Epoch [496/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000101709\n",
      "Epoch [496/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000101635\n",
      "Epoch [496/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000101563\n",
      "Epoch [497/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000101493\n",
      "Epoch [497/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000101424\n",
      "Epoch [497/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000101357\n",
      "Epoch [497/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000101292\n",
      "Epoch [497/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000101228\n",
      "Epoch [497/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000101166\n",
      "Epoch [497/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000101105\n",
      "Epoch [497/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000101047\n",
      "Epoch [497/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100989\n",
      "Epoch [497/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100933\n",
      "Epoch [497/500], Batch [110/110], Train Loss: 0.0020, Val Loss: 0.0002, LR: 0.0000100879\n",
      "Epoch [498/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100827\n",
      "Epoch [498/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100776\n",
      "Epoch [498/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100727\n",
      "Epoch [498/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100679\n",
      "Epoch [498/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100633\n",
      "Epoch [498/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100589\n",
      "Epoch [498/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0002, LR: 0.0000100546\n",
      "Epoch [498/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100505\n",
      "Epoch [498/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000100465\n",
      "Epoch [498/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100427\n",
      "Epoch [498/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100391\n",
      "Epoch [499/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100356\n",
      "Epoch [499/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100323\n",
      "Epoch [499/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000100292\n",
      "Epoch [499/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100262\n",
      "Epoch [499/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100233\n",
      "Epoch [499/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000100207\n",
      "Epoch [499/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100182\n",
      "Epoch [499/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100158\n",
      "Epoch [499/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100136\n",
      "Epoch [499/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100116\n",
      "Epoch [499/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100098\n",
      "Epoch [500/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100081\n",
      "Epoch [500/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100065\n",
      "Epoch [500/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100052\n",
      "Epoch [500/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100040\n",
      "Epoch [500/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100029\n",
      "Epoch [500/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100020\n",
      "Epoch [500/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100013\n",
      "Epoch [500/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100007\n",
      "Epoch [500/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100003\n",
      "Epoch [500/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100001\n",
      "Epoch [500/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0000100000\n",
      "Confusion Matrix:\n",
      "[[652   0]\n",
      " [  0 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       652\n",
      "           1    1.00000   1.00000   1.00000       848\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.035 sec , Memory Usage: 195.75 MB\n",
      "=========== SEED: 2025 , FOLD: 4/4, D: cpu ===========\n",
      " Label\n",
      "1    5551\n",
      "0    4449\n",
      "Name: count, dtype: int64\n",
      "=========== TP: 2,879 ===========\n",
      "Epoch [1/500], Batch [10/110], Train Loss: 0.6755, Val Loss: 0.6862, LR: 0.0009999999\n",
      "Epoch [1/500], Batch [20/110], Train Loss: 0.6834, Val Loss: 0.6802, LR: 0.0009999997\n",
      "Epoch [1/500], Batch [30/110], Train Loss: 0.6356, Val Loss: 0.6694, LR: 0.0009999993\n",
      "Epoch [1/500], Batch [40/110], Train Loss: 0.6318, Val Loss: 0.6455, LR: 0.0009999987\n",
      "Epoch [1/500], Batch [50/110], Train Loss: 0.5585, Val Loss: 0.6081, LR: 0.0009999980\n",
      "Epoch [1/500], Batch [60/110], Train Loss: 0.5955, Val Loss: 0.5608, LR: 0.0009999971\n",
      "Epoch [1/500], Batch [70/110], Train Loss: 0.5647, Val Loss: 0.5230, LR: 0.0009999960\n",
      "Epoch [1/500], Batch [80/110], Train Loss: 0.4318, Val Loss: 0.4776, LR: 0.0009999948\n",
      "Epoch [1/500], Batch [90/110], Train Loss: 0.3681, Val Loss: 0.4131, LR: 0.0009999935\n",
      "Epoch [1/500], Batch [100/110], Train Loss: 0.3599, Val Loss: 0.3378, LR: 0.0009999919\n",
      "Epoch [1/500], Batch [110/110], Train Loss: 0.2501, Val Loss: 0.2634, LR: 0.0009999902\n",
      "Confusion Matrix:\n",
      "[[546  85]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.86529   0.92778       631\n",
      "           1    0.91090   1.00000   0.95337       869\n",
      "\n",
      "    accuracy                        0.94333      1500\n",
      "   macro avg    0.95545   0.93265   0.94058      1500\n",
      "weighted avg    0.94838   0.94333   0.94261      1500\n",
      "\n",
      "Total Errors: 85\n",
      "Index: 20, Predicted: 1, Actual: 0\n",
      "Index: 35, Predicted: 1, Actual: 0\n",
      "Index: 64, Predicted: 1, Actual: 0\n",
      "Index: 91, Predicted: 1, Actual: 0\n",
      "Index: 116, Predicted: 1, Actual: 0\n",
      "Epoch 1: OK- Accuracy: 0.94333, Precision: 0.91090, Recall: 1.00000, F1: 0.95337, ROC AUC: 0.93265, AUPR (PR-AUC): 0.91090, Sensitivity: 1.00000, Specificity: 0.86529, Far: 0.1347068145800317, False Positive Rate (FPR): 0.13471, False Negative Rate (FNR): 0.00000, Runtime: 0.048 sec , Memory Usage: 310.46 MB\n",
      "Epoch [2/500], Batch [10/110], Train Loss: 0.2663, Val Loss: 0.2052, LR: 0.0009999884\n",
      "Epoch [2/500], Batch [20/110], Train Loss: 0.1993, Val Loss: 0.1504, LR: 0.0009999864\n",
      "Epoch [2/500], Batch [30/110], Train Loss: 0.1792, Val Loss: 0.1223, LR: 0.0009999842\n",
      "Epoch [2/500], Batch [40/110], Train Loss: 0.0923, Val Loss: 0.1062, LR: 0.0009999818\n",
      "Epoch [2/500], Batch [50/110], Train Loss: 0.1458, Val Loss: 0.0978, LR: 0.0009999793\n",
      "Epoch [2/500], Batch [60/110], Train Loss: 0.2162, Val Loss: 0.1086, LR: 0.0009999767\n",
      "Epoch [2/500], Batch [70/110], Train Loss: 0.2116, Val Loss: 0.0829, LR: 0.0009999738\n",
      "Epoch [2/500], Batch [80/110], Train Loss: 0.1747, Val Loss: 0.0985, LR: 0.0009999708\n",
      "Epoch [2/500], Batch [90/110], Train Loss: 0.2065, Val Loss: 0.0862, LR: 0.0009999677\n",
      "Epoch [2/500], Batch [100/110], Train Loss: 0.1328, Val Loss: 0.0935, LR: 0.0009999644\n",
      "Epoch [2/500], Batch [110/110], Train Loss: 0.0657, Val Loss: 0.0732, LR: 0.0009999609\n",
      "Epoch [3/500], Batch [10/110], Train Loss: 0.0111, Val Loss: 0.0895, LR: 0.0009999573\n",
      "Epoch [3/500], Batch [20/110], Train Loss: 0.2541, Val Loss: 0.0647, LR: 0.0009999535\n",
      "Epoch [3/500], Batch [30/110], Train Loss: 0.0217, Val Loss: 0.0641, LR: 0.0009999495\n",
      "Epoch [3/500], Batch [40/110], Train Loss: 0.0357, Val Loss: 0.0616, LR: 0.0009999454\n",
      "Epoch [3/500], Batch [50/110], Train Loss: 0.0123, Val Loss: 0.0692, LR: 0.0009999411\n",
      "Epoch [3/500], Batch [60/110], Train Loss: 0.0099, Val Loss: 0.0610, LR: 0.0009999367\n",
      "Epoch [3/500], Batch [70/110], Train Loss: 0.0851, Val Loss: 0.0571, LR: 0.0009999321\n",
      "Epoch [3/500], Batch [80/110], Train Loss: 0.0739, Val Loss: 0.0673, LR: 0.0009999273\n",
      "Epoch [3/500], Batch [90/110], Train Loss: 0.0185, Val Loss: 0.0546, LR: 0.0009999224\n",
      "Epoch [3/500], Batch [100/110], Train Loss: 0.1285, Val Loss: 0.0544, LR: 0.0009999173\n",
      "Epoch [3/500], Batch [110/110], Train Loss: 0.0326, Val Loss: 0.0696, LR: 0.0009999121\n",
      "Epoch [4/500], Batch [10/110], Train Loss: 0.0186, Val Loss: 0.0546, LR: 0.0009999067\n",
      "Epoch [4/500], Batch [20/110], Train Loss: 0.0132, Val Loss: 0.0592, LR: 0.0009999011\n",
      "Epoch [4/500], Batch [30/110], Train Loss: 0.1070, Val Loss: 0.0618, LR: 0.0009998953\n",
      "Epoch [4/500], Batch [40/110], Train Loss: 0.0362, Val Loss: 0.0506, LR: 0.0009998895\n",
      "Epoch [4/500], Batch [50/110], Train Loss: 0.0611, Val Loss: 0.0763, LR: 0.0009998834\n",
      "Epoch [4/500], Batch [60/110], Train Loss: 0.0449, Val Loss: 0.0485, LR: 0.0009998772\n",
      "Epoch [4/500], Batch [70/110], Train Loss: 0.0427, Val Loss: 0.0474, LR: 0.0009998708\n",
      "Epoch [4/500], Batch [80/110], Train Loss: 0.0167, Val Loss: 0.0553, LR: 0.0009998643\n",
      "Epoch [4/500], Batch [90/110], Train Loss: 0.0825, Val Loss: 0.0459, LR: 0.0009998576\n",
      "Epoch [4/500], Batch [100/110], Train Loss: 0.0132, Val Loss: 0.0495, LR: 0.0009998507\n",
      "Epoch [4/500], Batch [110/110], Train Loss: 0.0792, Val Loss: 0.0461, LR: 0.0009998437\n",
      "Epoch [5/500], Batch [10/110], Train Loss: 0.0101, Val Loss: 0.0450, LR: 0.0009998365\n",
      "Epoch [5/500], Batch [20/110], Train Loss: 0.0851, Val Loss: 0.0442, LR: 0.0009998291\n",
      "Epoch [5/500], Batch [30/110], Train Loss: 0.0990, Val Loss: 0.0493, LR: 0.0009998216\n",
      "Epoch [5/500], Batch [40/110], Train Loss: 0.0076, Val Loss: 0.0458, LR: 0.0009998140\n",
      "Epoch [5/500], Batch [50/110], Train Loss: 0.1815, Val Loss: 0.0564, LR: 0.0009998061\n",
      "Epoch [5/500], Batch [60/110], Train Loss: 0.1754, Val Loss: 0.0414, LR: 0.0009997981\n",
      "Epoch [5/500], Batch [70/110], Train Loss: 0.1073, Val Loss: 0.0445, LR: 0.0009997900\n",
      "Epoch [5/500], Batch [80/110], Train Loss: 0.0380, Val Loss: 0.0399, LR: 0.0009997817\n",
      "Epoch [5/500], Batch [90/110], Train Loss: 0.0088, Val Loss: 0.0579, LR: 0.0009997732\n",
      "Epoch [5/500], Batch [100/110], Train Loss: 0.0897, Val Loss: 0.0397, LR: 0.0009997645\n",
      "Epoch [5/500], Batch [110/110], Train Loss: 0.0087, Val Loss: 0.0378, LR: 0.0009997557\n",
      "Epoch [6/500], Batch [10/110], Train Loss: 0.0066, Val Loss: 0.0512, LR: 0.0009997468\n",
      "Epoch [6/500], Batch [20/110], Train Loss: 0.2260, Val Loss: 0.0419, LR: 0.0009997377\n",
      "Epoch [6/500], Batch [30/110], Train Loss: 0.0048, Val Loss: 0.0403, LR: 0.0009997284\n",
      "Epoch [6/500], Batch [40/110], Train Loss: 0.0050, Val Loss: 0.0390, LR: 0.0009997189\n",
      "Epoch [6/500], Batch [50/110], Train Loss: 0.0118, Val Loss: 0.0373, LR: 0.0009997093\n",
      "Epoch [6/500], Batch [60/110], Train Loss: 0.0154, Val Loss: 0.0352, LR: 0.0009996996\n",
      "Epoch [6/500], Batch [70/110], Train Loss: 0.0074, Val Loss: 0.0364, LR: 0.0009996896\n",
      "Epoch [6/500], Batch [80/110], Train Loss: 0.0103, Val Loss: 0.0358, LR: 0.0009996795\n",
      "Epoch [6/500], Batch [90/110], Train Loss: 0.0112, Val Loss: 0.0362, LR: 0.0009996693\n",
      "Epoch [6/500], Batch [100/110], Train Loss: 0.0274, Val Loss: 0.0333, LR: 0.0009996589\n",
      "Epoch [6/500], Batch [110/110], Train Loss: 0.0071, Val Loss: 0.0562, LR: 0.0009996483\n",
      "Epoch [7/500], Batch [10/110], Train Loss: 0.0043, Val Loss: 0.0434, LR: 0.0009996376\n",
      "Epoch [7/500], Batch [20/110], Train Loss: 0.0088, Val Loss: 0.0357, LR: 0.0009996267\n",
      "Epoch [7/500], Batch [30/110], Train Loss: 0.1061, Val Loss: 0.0418, LR: 0.0009996156\n",
      "Epoch [7/500], Batch [40/110], Train Loss: 0.0254, Val Loss: 0.0358, LR: 0.0009996044\n",
      "Epoch [7/500], Batch [50/110], Train Loss: 0.0316, Val Loss: 0.0400, LR: 0.0009995930\n",
      "Epoch [7/500], Batch [60/110], Train Loss: 0.0126, Val Loss: 0.0356, LR: 0.0009995814\n",
      "Epoch [7/500], Batch [70/110], Train Loss: 0.0087, Val Loss: 0.0326, LR: 0.0009995697\n",
      "Epoch [7/500], Batch [80/110], Train Loss: 0.3112, Val Loss: 0.0419, LR: 0.0009995579\n",
      "Epoch [7/500], Batch [90/110], Train Loss: 0.0832, Val Loss: 0.0325, LR: 0.0009995458\n",
      "Epoch [7/500], Batch [100/110], Train Loss: 0.0329, Val Loss: 0.0342, LR: 0.0009995337\n",
      "Epoch [7/500], Batch [110/110], Train Loss: 0.0045, Val Loss: 0.0340, LR: 0.0009995213\n",
      "Epoch [8/500], Batch [10/110], Train Loss: 0.0765, Val Loss: 0.0291, LR: 0.0009995088\n",
      "Epoch [8/500], Batch [20/110], Train Loss: 0.0281, Val Loss: 0.0296, LR: 0.0009994961\n",
      "Epoch [8/500], Batch [30/110], Train Loss: 0.0228, Val Loss: 0.0307, LR: 0.0009994833\n",
      "Epoch [8/500], Batch [40/110], Train Loss: 0.1251, Val Loss: 0.0331, LR: 0.0009994703\n",
      "Epoch [8/500], Batch [50/110], Train Loss: 0.0103, Val Loss: 0.0315, LR: 0.0009994571\n",
      "Epoch [8/500], Batch [60/110], Train Loss: 0.0300, Val Loss: 0.0302, LR: 0.0009994438\n",
      "Epoch [8/500], Batch [70/110], Train Loss: 0.0030, Val Loss: 0.0459, LR: 0.0009994303\n",
      "Epoch [8/500], Batch [80/110], Train Loss: 0.0116, Val Loss: 0.0316, LR: 0.0009994167\n",
      "Epoch [8/500], Batch [90/110], Train Loss: 0.0553, Val Loss: 0.0273, LR: 0.0009994029\n",
      "Epoch [8/500], Batch [100/110], Train Loss: 0.0260, Val Loss: 0.0345, LR: 0.0009993889\n",
      "Epoch [8/500], Batch [110/110], Train Loss: 0.0159, Val Loss: 0.0264, LR: 0.0009993748\n",
      "Epoch [9/500], Batch [10/110], Train Loss: 0.0030, Val Loss: 0.0380, LR: 0.0009993605\n",
      "Epoch [9/500], Batch [20/110], Train Loss: 0.1696, Val Loss: 0.0280, LR: 0.0009993461\n",
      "Epoch [9/500], Batch [30/110], Train Loss: 0.0019, Val Loss: 0.0267, LR: 0.0009993314\n",
      "Epoch [9/500], Batch [40/110], Train Loss: 0.0295, Val Loss: 0.0445, LR: 0.0009993167\n",
      "Epoch [9/500], Batch [50/110], Train Loss: 0.0041, Val Loss: 0.0326, LR: 0.0009993017\n",
      "Epoch [9/500], Batch [60/110], Train Loss: 0.2607, Val Loss: 0.0260, LR: 0.0009992867\n",
      "Epoch [9/500], Batch [70/110], Train Loss: 0.0014, Val Loss: 0.0347, LR: 0.0009992714\n",
      "Epoch [9/500], Batch [80/110], Train Loss: 0.0040, Val Loss: 0.0263, LR: 0.0009992560\n",
      "Epoch [9/500], Batch [90/110], Train Loss: 0.0921, Val Loss: 0.0271, LR: 0.0009992404\n",
      "Epoch [9/500], Batch [100/110], Train Loss: 0.0134, Val Loss: 0.0328, LR: 0.0009992247\n",
      "Epoch [9/500], Batch [110/110], Train Loss: 0.0184, Val Loss: 0.0303, LR: 0.0009992088\n",
      "Epoch [10/500], Batch [10/110], Train Loss: 0.0589, Val Loss: 0.0250, LR: 0.0009991927\n",
      "Epoch [10/500], Batch [20/110], Train Loss: 0.0180, Val Loss: 0.0270, LR: 0.0009991765\n",
      "Epoch [10/500], Batch [30/110], Train Loss: 0.0130, Val Loss: 0.0285, LR: 0.0009991601\n",
      "Epoch [10/500], Batch [40/110], Train Loss: 0.0136, Val Loss: 0.0268, LR: 0.0009991436\n",
      "Epoch [10/500], Batch [50/110], Train Loss: 0.0298, Val Loss: 0.0279, LR: 0.0009991269\n",
      "Epoch [10/500], Batch [60/110], Train Loss: 0.2370, Val Loss: 0.0297, LR: 0.0009991100\n",
      "Epoch [10/500], Batch [70/110], Train Loss: 0.0568, Val Loss: 0.0245, LR: 0.0009990930\n",
      "Epoch [10/500], Batch [80/110], Train Loss: 0.0815, Val Loss: 0.0308, LR: 0.0009990758\n",
      "Epoch [10/500], Batch [90/110], Train Loss: 0.0600, Val Loss: 0.0246, LR: 0.0009990584\n",
      "Epoch [10/500], Batch [100/110], Train Loss: 0.0131, Val Loss: 0.0299, LR: 0.0009990409\n",
      "Epoch [10/500], Batch [110/110], Train Loss: 0.0799, Val Loss: 0.0297, LR: 0.0009990232\n",
      "Epoch [11/500], Batch [10/110], Train Loss: 0.0069, Val Loss: 0.0243, LR: 0.0009990054\n",
      "Epoch [11/500], Batch [20/110], Train Loss: 0.0051, Val Loss: 0.0247, LR: 0.0009989874\n",
      "Epoch [11/500], Batch [30/110], Train Loss: 0.1765, Val Loss: 0.0415, LR: 0.0009989692\n",
      "Epoch [11/500], Batch [40/110], Train Loss: 0.2889, Val Loss: 0.0241, LR: 0.0009989509\n",
      "Epoch [11/500], Batch [50/110], Train Loss: 0.0795, Val Loss: 0.0309, LR: 0.0009989324\n",
      "Epoch [11/500], Batch [60/110], Train Loss: 0.1053, Val Loss: 0.0377, LR: 0.0009989138\n",
      "Epoch [11/500], Batch [70/110], Train Loss: 0.0594, Val Loss: 0.0244, LR: 0.0009988950\n",
      "Epoch [11/500], Batch [80/110], Train Loss: 0.1411, Val Loss: 0.0260, LR: 0.0009988760\n",
      "Epoch [11/500], Batch [90/110], Train Loss: 0.0036, Val Loss: 0.0340, LR: 0.0009988569\n",
      "Epoch [11/500], Batch [100/110], Train Loss: 0.0532, Val Loss: 0.0255, LR: 0.0009988376\n",
      "Epoch [11/500], Batch [110/110], Train Loss: 0.0066, Val Loss: 0.0234, LR: 0.0009988182\n",
      "Confusion Matrix:\n",
      "[[621  10]\n",
      " [  8 861]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98728   0.98415   0.98571       631\n",
      "           1    0.98852   0.99079   0.98966       869\n",
      "\n",
      "    accuracy                        0.98800      1500\n",
      "   macro avg    0.98790   0.98747   0.98768      1500\n",
      "weighted avg    0.98800   0.98800   0.98800      1500\n",
      "\n",
      "Total Errors: 18\n",
      "Index: 88, Predicted: 0, Actual: 1\n",
      "Index: 246, Predicted: 1, Actual: 0\n",
      "Index: 281, Predicted: 1, Actual: 0\n",
      "Index: 283, Predicted: 1, Actual: 0\n",
      "Index: 434, Predicted: 1, Actual: 0\n",
      "Epoch 11: OK- Accuracy: 0.98800, Precision: 0.98852, Recall: 0.99079, F1: 0.98966, ROC AUC: 0.98747, AUPR (PR-AUC): 0.98475, Sensitivity: 0.99079, Specificity: 0.98415, Far: 0.01584786053882726, False Positive Rate (FPR): 0.01585, False Negative Rate (FNR): 0.00921, Runtime: 0.046 sec , Memory Usage: 310.47 MB\n",
      "Epoch [12/500], Batch [10/110], Train Loss: 0.0054, Val Loss: 0.0261, LR: 0.0009987986\n",
      "Epoch [12/500], Batch [20/110], Train Loss: 0.0791, Val Loss: 0.0275, LR: 0.0009987788\n",
      "Epoch [12/500], Batch [30/110], Train Loss: 0.0022, Val Loss: 0.0270, LR: 0.0009987589\n",
      "Epoch [12/500], Batch [40/110], Train Loss: 0.0622, Val Loss: 0.0224, LR: 0.0009987388\n",
      "Epoch [12/500], Batch [50/110], Train Loss: 0.0043, Val Loss: 0.0295, LR: 0.0009987185\n",
      "Epoch [12/500], Batch [60/110], Train Loss: 0.0409, Val Loss: 0.0225, LR: 0.0009986981\n",
      "Epoch [12/500], Batch [70/110], Train Loss: 0.0017, Val Loss: 0.0277, LR: 0.0009986776\n",
      "Epoch [12/500], Batch [80/110], Train Loss: 0.0012, Val Loss: 0.0289, LR: 0.0009986568\n",
      "Epoch [12/500], Batch [90/110], Train Loss: 0.0476, Val Loss: 0.0227, LR: 0.0009986359\n",
      "Epoch [12/500], Batch [100/110], Train Loss: 0.0059, Val Loss: 0.0244, LR: 0.0009986149\n",
      "Epoch [12/500], Batch [110/110], Train Loss: 0.0901, Val Loss: 0.0345, LR: 0.0009985937\n",
      "Epoch [13/500], Batch [10/110], Train Loss: 0.0100, Val Loss: 0.0375, LR: 0.0009985723\n",
      "Epoch [13/500], Batch [20/110], Train Loss: 0.0011, Val Loss: 0.0312, LR: 0.0009985507\n",
      "Epoch [13/500], Batch [30/110], Train Loss: 0.0025, Val Loss: 0.0283, LR: 0.0009985290\n",
      "Epoch [13/500], Batch [40/110], Train Loss: 0.0023, Val Loss: 0.0272, LR: 0.0009985072\n",
      "Epoch [13/500], Batch [50/110], Train Loss: 0.0015, Val Loss: 0.0214, LR: 0.0009984852\n",
      "Epoch [13/500], Batch [60/110], Train Loss: 0.0044, Val Loss: 0.0261, LR: 0.0009984630\n",
      "Epoch [13/500], Batch [70/110], Train Loss: 0.0141, Val Loss: 0.0247, LR: 0.0009984406\n",
      "Epoch [13/500], Batch [80/110], Train Loss: 0.0250, Val Loss: 0.0212, LR: 0.0009984181\n",
      "Epoch [13/500], Batch [90/110], Train Loss: 0.3045, Val Loss: 0.0274, LR: 0.0009983955\n",
      "Epoch [13/500], Batch [100/110], Train Loss: 0.0134, Val Loss: 0.0211, LR: 0.0009983726\n",
      "Epoch [13/500], Batch [110/110], Train Loss: 0.0169, Val Loss: 0.0239, LR: 0.0009983496\n",
      "Epoch [14/500], Batch [10/110], Train Loss: 0.0015, Val Loss: 0.0217, LR: 0.0009983265\n",
      "Epoch [14/500], Batch [20/110], Train Loss: 0.0054, Val Loss: 0.0212, LR: 0.0009983032\n",
      "Epoch [14/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0230, LR: 0.0009982797\n",
      "Epoch [14/500], Batch [40/110], Train Loss: 0.0065, Val Loss: 0.0239, LR: 0.0009982561\n",
      "Epoch [14/500], Batch [50/110], Train Loss: 0.0996, Val Loss: 0.0291, LR: 0.0009982323\n",
      "Epoch [14/500], Batch [60/110], Train Loss: 0.0776, Val Loss: 0.0209, LR: 0.0009982083\n",
      "Epoch [14/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0214, LR: 0.0009981842\n",
      "Epoch [14/500], Batch [80/110], Train Loss: 0.0465, Val Loss: 0.0223, LR: 0.0009981599\n",
      "Epoch [14/500], Batch [90/110], Train Loss: 0.0110, Val Loss: 0.0231, LR: 0.0009981355\n",
      "Epoch [14/500], Batch [100/110], Train Loss: 0.0361, Val Loss: 0.0236, LR: 0.0009981109\n",
      "Epoch [14/500], Batch [110/110], Train Loss: 0.0025, Val Loss: 0.0261, LR: 0.0009980861\n",
      "Epoch [15/500], Batch [10/110], Train Loss: 0.0908, Val Loss: 0.0206, LR: 0.0009980612\n",
      "Epoch [15/500], Batch [20/110], Train Loss: 0.0024, Val Loss: 0.0242, LR: 0.0009980361\n",
      "Epoch [15/500], Batch [30/110], Train Loss: 0.0080, Val Loss: 0.0299, LR: 0.0009980109\n",
      "Epoch [15/500], Batch [40/110], Train Loss: 0.0068, Val Loss: 0.0208, LR: 0.0009979855\n",
      "Epoch [15/500], Batch [50/110], Train Loss: 0.0064, Val Loss: 0.0211, LR: 0.0009979599\n",
      "Epoch [15/500], Batch [60/110], Train Loss: 0.0883, Val Loss: 0.0249, LR: 0.0009979342\n",
      "Epoch [15/500], Batch [70/110], Train Loss: 0.0708, Val Loss: 0.0264, LR: 0.0009979083\n",
      "Epoch [15/500], Batch [80/110], Train Loss: 0.0110, Val Loss: 0.0218, LR: 0.0009978823\n",
      "Epoch [15/500], Batch [90/110], Train Loss: 0.2906, Val Loss: 0.0201, LR: 0.0009978561\n",
      "Epoch [15/500], Batch [100/110], Train Loss: 0.0038, Val Loss: 0.0246, LR: 0.0009978297\n",
      "Epoch [15/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0196, LR: 0.0009978032\n",
      "Epoch [16/500], Batch [10/110], Train Loss: 0.0083, Val Loss: 0.0236, LR: 0.0009977765\n",
      "Epoch [16/500], Batch [20/110], Train Loss: 0.0954, Val Loss: 0.0213, LR: 0.0009977496\n",
      "Epoch [16/500], Batch [30/110], Train Loss: 0.0195, Val Loss: 0.0209, LR: 0.0009977226\n",
      "Epoch [16/500], Batch [40/110], Train Loss: 0.0022, Val Loss: 0.0203, LR: 0.0009976955\n",
      "Epoch [16/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0250, LR: 0.0009976681\n",
      "Epoch [16/500], Batch [60/110], Train Loss: 0.0587, Val Loss: 0.0199, LR: 0.0009976406\n",
      "Epoch [16/500], Batch [70/110], Train Loss: 0.0029, Val Loss: 0.0196, LR: 0.0009976130\n",
      "Epoch [16/500], Batch [80/110], Train Loss: 0.0535, Val Loss: 0.0257, LR: 0.0009975852\n",
      "Epoch [16/500], Batch [90/110], Train Loss: 0.0489, Val Loss: 0.0205, LR: 0.0009975572\n",
      "Epoch [16/500], Batch [100/110], Train Loss: 0.1013, Val Loss: 0.0198, LR: 0.0009975290\n",
      "Epoch [16/500], Batch [110/110], Train Loss: 0.0043, Val Loss: 0.0199, LR: 0.0009975008\n",
      "Epoch [17/500], Batch [10/110], Train Loss: 0.0215, Val Loss: 0.0207, LR: 0.0009974723\n",
      "Epoch [17/500], Batch [20/110], Train Loss: 0.0175, Val Loss: 0.0227, LR: 0.0009974437\n",
      "Epoch [17/500], Batch [30/110], Train Loss: 0.0115, Val Loss: 0.0263, LR: 0.0009974149\n",
      "Epoch [17/500], Batch [40/110], Train Loss: 0.0439, Val Loss: 0.0184, LR: 0.0009973860\n",
      "Epoch [17/500], Batch [50/110], Train Loss: 0.0312, Val Loss: 0.0184, LR: 0.0009973569\n",
      "Epoch [17/500], Batch [60/110], Train Loss: 0.3007, Val Loss: 0.0192, LR: 0.0009973276\n",
      "Epoch [17/500], Batch [70/110], Train Loss: 0.0017, Val Loss: 0.0191, LR: 0.0009972982\n",
      "Epoch [17/500], Batch [80/110], Train Loss: 0.1682, Val Loss: 0.0222, LR: 0.0009972686\n",
      "Epoch [17/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0247, LR: 0.0009972389\n",
      "Epoch [17/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0208, LR: 0.0009972090\n",
      "Epoch [17/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0228, LR: 0.0009971789\n",
      "Epoch [18/500], Batch [10/110], Train Loss: 0.0016, Val Loss: 0.0193, LR: 0.0009971487\n",
      "Epoch [18/500], Batch [20/110], Train Loss: 0.0029, Val Loss: 0.0242, LR: 0.0009971183\n",
      "Epoch [18/500], Batch [30/110], Train Loss: 0.0033, Val Loss: 0.0238, LR: 0.0009970877\n",
      "Epoch [18/500], Batch [40/110], Train Loss: 0.0038, Val Loss: 0.0184, LR: 0.0009970570\n",
      "Epoch [18/500], Batch [50/110], Train Loss: 0.0049, Val Loss: 0.0262, LR: 0.0009970262\n",
      "Epoch [18/500], Batch [60/110], Train Loss: 0.0013, Val Loss: 0.0195, LR: 0.0009969951\n",
      "Epoch [18/500], Batch [70/110], Train Loss: 0.0528, Val Loss: 0.0201, LR: 0.0009969640\n",
      "Epoch [18/500], Batch [80/110], Train Loss: 0.0165, Val Loss: 0.0218, LR: 0.0009969326\n",
      "Epoch [18/500], Batch [90/110], Train Loss: 0.0433, Val Loss: 0.0202, LR: 0.0009969011\n",
      "Epoch [18/500], Batch [100/110], Train Loss: 0.0052, Val Loss: 0.0188, LR: 0.0009968694\n",
      "Epoch [18/500], Batch [110/110], Train Loss: 0.0209, Val Loss: 0.0195, LR: 0.0009968376\n",
      "Epoch [19/500], Batch [10/110], Train Loss: 0.0313, Val Loss: 0.0185, LR: 0.0009968056\n",
      "Epoch [19/500], Batch [20/110], Train Loss: 0.0908, Val Loss: 0.0208, LR: 0.0009967735\n",
      "Epoch [19/500], Batch [30/110], Train Loss: 0.0266, Val Loss: 0.0188, LR: 0.0009967411\n",
      "Epoch [19/500], Batch [40/110], Train Loss: 0.0013, Val Loss: 0.0192, LR: 0.0009967087\n",
      "Epoch [19/500], Batch [50/110], Train Loss: 0.0026, Val Loss: 0.0310, LR: 0.0009966760\n",
      "Epoch [19/500], Batch [60/110], Train Loss: 0.0036, Val Loss: 0.0184, LR: 0.0009966433\n",
      "Epoch [19/500], Batch [70/110], Train Loss: 0.0033, Val Loss: 0.0186, LR: 0.0009966103\n",
      "Epoch [19/500], Batch [80/110], Train Loss: 0.0012, Val Loss: 0.0217, LR: 0.0009965772\n",
      "Epoch [19/500], Batch [90/110], Train Loss: 0.0021, Val Loss: 0.0184, LR: 0.0009965439\n",
      "Epoch [19/500], Batch [100/110], Train Loss: 0.0139, Val Loss: 0.0230, LR: 0.0009965105\n",
      "Epoch [19/500], Batch [110/110], Train Loss: 0.0033, Val Loss: 0.0193, LR: 0.0009964769\n",
      "Epoch [20/500], Batch [10/110], Train Loss: 0.0365, Val Loss: 0.0190, LR: 0.0009964431\n",
      "Epoch [20/500], Batch [20/110], Train Loss: 0.0025, Val Loss: 0.0184, LR: 0.0009964092\n",
      "Epoch [20/500], Batch [30/110], Train Loss: 0.0096, Val Loss: 0.0195, LR: 0.0009963751\n",
      "Epoch [20/500], Batch [40/110], Train Loss: 0.0023, Val Loss: 0.0198, LR: 0.0009963409\n",
      "Epoch [20/500], Batch [50/110], Train Loss: 0.0045, Val Loss: 0.0207, LR: 0.0009963065\n",
      "Epoch [20/500], Batch [60/110], Train Loss: 0.0392, Val Loss: 0.0199, LR: 0.0009962720\n",
      "Epoch [20/500], Batch [70/110], Train Loss: 0.0147, Val Loss: 0.0259, LR: 0.0009962372\n",
      "Epoch [20/500], Batch [80/110], Train Loss: 0.0035, Val Loss: 0.0210, LR: 0.0009962024\n",
      "Epoch [20/500], Batch [90/110], Train Loss: 0.0647, Val Loss: 0.0176, LR: 0.0009961673\n",
      "Epoch [20/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0177, LR: 0.0009961321\n",
      "Epoch [20/500], Batch [110/110], Train Loss: 0.0545, Val Loss: 0.0210, LR: 0.0009960968\n",
      "Epoch [21/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0178, LR: 0.0009960613\n",
      "Epoch [21/500], Batch [20/110], Train Loss: 0.2602, Val Loss: 0.0236, LR: 0.0009960256\n",
      "Epoch [21/500], Batch [30/110], Train Loss: 0.0060, Val Loss: 0.0190, LR: 0.0009959897\n",
      "Epoch [21/500], Batch [40/110], Train Loss: 0.0288, Val Loss: 0.0180, LR: 0.0009959537\n",
      "Epoch [21/500], Batch [50/110], Train Loss: 0.0388, Val Loss: 0.0202, LR: 0.0009959176\n",
      "Epoch [21/500], Batch [60/110], Train Loss: 0.0335, Val Loss: 0.0196, LR: 0.0009958813\n",
      "Epoch [21/500], Batch [70/110], Train Loss: 0.0573, Val Loss: 0.0191, LR: 0.0009958448\n",
      "Epoch [21/500], Batch [80/110], Train Loss: 0.2373, Val Loss: 0.0173, LR: 0.0009958082\n",
      "Epoch [21/500], Batch [90/110], Train Loss: 0.0336, Val Loss: 0.0169, LR: 0.0009957714\n",
      "Epoch [21/500], Batch [100/110], Train Loss: 0.0379, Val Loss: 0.0171, LR: 0.0009957344\n",
      "Epoch [21/500], Batch [110/110], Train Loss: 0.0206, Val Loss: 0.0175, LR: 0.0009956973\n",
      "Confusion Matrix:\n",
      "[[628   3]\n",
      " [ 12 857]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98125   0.99525   0.98820       631\n",
      "           1    0.99651   0.98619   0.99132       869\n",
      "\n",
      "    accuracy                        0.99000      1500\n",
      "   macro avg    0.98888   0.99072   0.98976      1500\n",
      "weighted avg    0.99009   0.99000   0.99001      1500\n",
      "\n",
      "Total Errors: 15\n",
      "Index: 88, Predicted: 0, Actual: 1\n",
      "Index: 283, Predicted: 1, Actual: 0\n",
      "Index: 434, Predicted: 1, Actual: 0\n",
      "Index: 477, Predicted: 0, Actual: 1\n",
      "Index: 606, Predicted: 0, Actual: 1\n",
      "Epoch 21: OK- Accuracy: 0.99000, Precision: 0.99651, Recall: 0.98619, F1: 0.99132, ROC AUC: 0.99072, AUPR (PR-AUC): 0.99075, Sensitivity: 0.98619, Specificity: 0.99525, Far: 0.004754358161648178, False Positive Rate (FPR): 0.00475, False Negative Rate (FNR): 0.01381, Runtime: 0.053 sec , Memory Usage: 310.48 MB\n",
      "Epoch [22/500], Batch [10/110], Train Loss: 0.0400, Val Loss: 0.0187, LR: 0.0009956600\n",
      "Epoch [22/500], Batch [20/110], Train Loss: 0.0080, Val Loss: 0.0177, LR: 0.0009956226\n",
      "Epoch [22/500], Batch [30/110], Train Loss: 0.0392, Val Loss: 0.0169, LR: 0.0009955850\n",
      "Epoch [22/500], Batch [40/110], Train Loss: 0.0201, Val Loss: 0.0168, LR: 0.0009955472\n",
      "Epoch [22/500], Batch [50/110], Train Loss: 0.0085, Val Loss: 0.0169, LR: 0.0009955093\n",
      "Epoch [22/500], Batch [60/110], Train Loss: 0.0525, Val Loss: 0.0202, LR: 0.0009954712\n",
      "Epoch [22/500], Batch [70/110], Train Loss: 0.0023, Val Loss: 0.0172, LR: 0.0009954330\n",
      "Epoch [22/500], Batch [80/110], Train Loss: 0.0013, Val Loss: 0.0171, LR: 0.0009953946\n",
      "Epoch [22/500], Batch [90/110], Train Loss: 0.0485, Val Loss: 0.0198, LR: 0.0009953560\n",
      "Epoch [22/500], Batch [100/110], Train Loss: 0.0016, Val Loss: 0.0177, LR: 0.0009953173\n",
      "Epoch [22/500], Batch [110/110], Train Loss: 0.0925, Val Loss: 0.0194, LR: 0.0009952784\n",
      "Epoch [23/500], Batch [10/110], Train Loss: 0.0443, Val Loss: 0.0196, LR: 0.0009952394\n",
      "Epoch [23/500], Batch [20/110], Train Loss: 0.0451, Val Loss: 0.0205, LR: 0.0009952002\n",
      "Epoch [23/500], Batch [30/110], Train Loss: 0.0170, Val Loss: 0.0197, LR: 0.0009951608\n",
      "Epoch [23/500], Batch [40/110], Train Loss: 0.0061, Val Loss: 0.0171, LR: 0.0009951213\n",
      "Epoch [23/500], Batch [50/110], Train Loss: 0.0014, Val Loss: 0.0171, LR: 0.0009950816\n",
      "Epoch [23/500], Batch [60/110], Train Loss: 0.0630, Val Loss: 0.0195, LR: 0.0009950418\n",
      "Epoch [23/500], Batch [70/110], Train Loss: 0.0297, Val Loss: 0.0167, LR: 0.0009950018\n",
      "Epoch [23/500], Batch [80/110], Train Loss: 0.0206, Val Loss: 0.0167, LR: 0.0009949616\n",
      "Epoch [23/500], Batch [90/110], Train Loss: 0.0400, Val Loss: 0.0182, LR: 0.0009949213\n",
      "Epoch [23/500], Batch [100/110], Train Loss: 0.0044, Val Loss: 0.0190, LR: 0.0009948808\n",
      "Epoch [23/500], Batch [110/110], Train Loss: 0.0125, Val Loss: 0.0171, LR: 0.0009948402\n",
      "Epoch [24/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0204, LR: 0.0009947994\n",
      "Epoch [24/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0171, LR: 0.0009947584\n",
      "Epoch [24/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0175, LR: 0.0009947173\n",
      "Epoch [24/500], Batch [40/110], Train Loss: 0.0697, Val Loss: 0.0179, LR: 0.0009946760\n",
      "Epoch [24/500], Batch [50/110], Train Loss: 0.0216, Val Loss: 0.0172, LR: 0.0009946346\n",
      "Epoch [24/500], Batch [60/110], Train Loss: 0.0301, Val Loss: 0.0235, LR: 0.0009945930\n",
      "Epoch [24/500], Batch [70/110], Train Loss: 0.0017, Val Loss: 0.0281, LR: 0.0009945512\n",
      "Epoch [24/500], Batch [80/110], Train Loss: 0.0299, Val Loss: 0.0178, LR: 0.0009945093\n",
      "Epoch [24/500], Batch [90/110], Train Loss: 0.0138, Val Loss: 0.0173, LR: 0.0009944672\n",
      "Epoch [24/500], Batch [100/110], Train Loss: 0.0032, Val Loss: 0.0171, LR: 0.0009944250\n",
      "Epoch [24/500], Batch [110/110], Train Loss: 0.0343, Val Loss: 0.0173, LR: 0.0009943826\n",
      "Epoch [25/500], Batch [10/110], Train Loss: 0.0612, Val Loss: 0.0171, LR: 0.0009943401\n",
      "Epoch [25/500], Batch [20/110], Train Loss: 0.2616, Val Loss: 0.0180, LR: 0.0009942973\n",
      "Epoch [25/500], Batch [30/110], Train Loss: 0.0084, Val Loss: 0.0173, LR: 0.0009942545\n",
      "Epoch [25/500], Batch [40/110], Train Loss: 0.0459, Val Loss: 0.0212, LR: 0.0009942114\n",
      "Epoch [25/500], Batch [50/110], Train Loss: 0.0027, Val Loss: 0.0162, LR: 0.0009941682\n",
      "Epoch [25/500], Batch [60/110], Train Loss: 0.0474, Val Loss: 0.0190, LR: 0.0009941249\n",
      "Epoch [25/500], Batch [70/110], Train Loss: 0.0737, Val Loss: 0.0259, LR: 0.0009940814\n",
      "Epoch [25/500], Batch [80/110], Train Loss: 0.0008, Val Loss: 0.0161, LR: 0.0009940377\n",
      "Epoch [25/500], Batch [90/110], Train Loss: 0.1273, Val Loss: 0.0187, LR: 0.0009939939\n",
      "Epoch [25/500], Batch [100/110], Train Loss: 0.0749, Val Loss: 0.0239, LR: 0.0009939499\n",
      "Epoch [25/500], Batch [110/110], Train Loss: 0.0355, Val Loss: 0.0180, LR: 0.0009939057\n",
      "Epoch [26/500], Batch [10/110], Train Loss: 0.0549, Val Loss: 0.0172, LR: 0.0009938614\n",
      "Epoch [26/500], Batch [20/110], Train Loss: 0.0011, Val Loss: 0.0198, LR: 0.0009938169\n",
      "Epoch [26/500], Batch [30/110], Train Loss: 0.0276, Val Loss: 0.0173, LR: 0.0009937723\n",
      "Epoch [26/500], Batch [40/110], Train Loss: 0.0166, Val Loss: 0.0161, LR: 0.0009937275\n",
      "Epoch [26/500], Batch [50/110], Train Loss: 0.0011, Val Loss: 0.0219, LR: 0.0009936826\n",
      "Epoch [26/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0164, LR: 0.0009936375\n",
      "Epoch [26/500], Batch [70/110], Train Loss: 0.0311, Val Loss: 0.0165, LR: 0.0009935922\n",
      "Epoch [26/500], Batch [80/110], Train Loss: 0.0620, Val Loss: 0.0237, LR: 0.0009935468\n",
      "Epoch [26/500], Batch [90/110], Train Loss: 0.0201, Val Loss: 0.0178, LR: 0.0009935012\n",
      "Epoch [26/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0168, LR: 0.0009934554\n",
      "Epoch [26/500], Batch [110/110], Train Loss: 0.0015, Val Loss: 0.0172, LR: 0.0009934095\n",
      "Epoch [27/500], Batch [10/110], Train Loss: 0.0596, Val Loss: 0.0216, LR: 0.0009933635\n",
      "Epoch [27/500], Batch [20/110], Train Loss: 0.0557, Val Loss: 0.0163, LR: 0.0009933173\n",
      "Epoch [27/500], Batch [30/110], Train Loss: 0.0012, Val Loss: 0.0163, LR: 0.0009932709\n",
      "Epoch [27/500], Batch [40/110], Train Loss: 0.0525, Val Loss: 0.0210, LR: 0.0009932243\n",
      "Epoch [27/500], Batch [50/110], Train Loss: 0.0212, Val Loss: 0.0182, LR: 0.0009931776\n",
      "Epoch [27/500], Batch [60/110], Train Loss: 0.0180, Val Loss: 0.0170, LR: 0.0009931308\n",
      "Epoch [27/500], Batch [70/110], Train Loss: 0.0464, Val Loss: 0.0163, LR: 0.0009930837\n",
      "Epoch [27/500], Batch [80/110], Train Loss: 0.0372, Val Loss: 0.0176, LR: 0.0009930366\n",
      "Epoch [27/500], Batch [90/110], Train Loss: 0.0077, Val Loss: 0.0170, LR: 0.0009929892\n",
      "Epoch [27/500], Batch [100/110], Train Loss: 0.0115, Val Loss: 0.0165, LR: 0.0009929417\n",
      "Epoch [27/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0210, LR: 0.0009928941\n",
      "Epoch [28/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0166, LR: 0.0009928463\n",
      "Epoch [28/500], Batch [20/110], Train Loss: 0.0599, Val Loss: 0.0168, LR: 0.0009927983\n",
      "Epoch [28/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0160, LR: 0.0009927501\n",
      "Epoch [28/500], Batch [40/110], Train Loss: 0.0020, Val Loss: 0.0172, LR: 0.0009927019\n",
      "Epoch [28/500], Batch [50/110], Train Loss: 0.0246, Val Loss: 0.0177, LR: 0.0009926534\n",
      "Epoch [28/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0156, LR: 0.0009926048\n",
      "Epoch [28/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0263, LR: 0.0009925560\n",
      "Epoch [28/500], Batch [80/110], Train Loss: 0.0023, Val Loss: 0.0175, LR: 0.0009925071\n",
      "Epoch [28/500], Batch [90/110], Train Loss: 0.0605, Val Loss: 0.0173, LR: 0.0009924580\n",
      "Epoch [28/500], Batch [100/110], Train Loss: 0.0985, Val Loss: 0.0162, LR: 0.0009924088\n",
      "Epoch [28/500], Batch [110/110], Train Loss: 0.0707, Val Loss: 0.0173, LR: 0.0009923593\n",
      "Epoch [29/500], Batch [10/110], Train Loss: 0.0055, Val Loss: 0.0172, LR: 0.0009923098\n",
      "Epoch [29/500], Batch [20/110], Train Loss: 0.0027, Val Loss: 0.0196, LR: 0.0009922601\n",
      "Epoch [29/500], Batch [30/110], Train Loss: 0.0186, Val Loss: 0.0188, LR: 0.0009922102\n",
      "Epoch [29/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0180, LR: 0.0009921601\n",
      "Epoch [29/500], Batch [50/110], Train Loss: 0.0024, Val Loss: 0.0162, LR: 0.0009921099\n",
      "Epoch [29/500], Batch [60/110], Train Loss: 0.0054, Val Loss: 0.0175, LR: 0.0009920596\n",
      "Epoch [29/500], Batch [70/110], Train Loss: 0.0089, Val Loss: 0.0170, LR: 0.0009920090\n",
      "Epoch [29/500], Batch [80/110], Train Loss: 0.0216, Val Loss: 0.0190, LR: 0.0009919584\n",
      "Epoch [29/500], Batch [90/110], Train Loss: 0.0374, Val Loss: 0.0168, LR: 0.0009919075\n",
      "Epoch [29/500], Batch [100/110], Train Loss: 0.0159, Val Loss: 0.0165, LR: 0.0009918565\n",
      "Epoch [29/500], Batch [110/110], Train Loss: 0.0009, Val Loss: 0.0158, LR: 0.0009918054\n",
      "Epoch [30/500], Batch [10/110], Train Loss: 0.0068, Val Loss: 0.0192, LR: 0.0009917541\n",
      "Epoch [30/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0169, LR: 0.0009917026\n",
      "Epoch [30/500], Batch [30/110], Train Loss: 0.2591, Val Loss: 0.0172, LR: 0.0009916510\n",
      "Epoch [30/500], Batch [40/110], Train Loss: 0.0117, Val Loss: 0.0180, LR: 0.0009915992\n",
      "Epoch [30/500], Batch [50/110], Train Loss: 0.0169, Val Loss: 0.0176, LR: 0.0009915472\n",
      "Epoch [30/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0159, LR: 0.0009914951\n",
      "Epoch [30/500], Batch [70/110], Train Loss: 0.0470, Val Loss: 0.0160, LR: 0.0009914428\n",
      "Epoch [30/500], Batch [80/110], Train Loss: 0.0118, Val Loss: 0.0189, LR: 0.0009913904\n",
      "Epoch [30/500], Batch [90/110], Train Loss: 0.0154, Val Loss: 0.0184, LR: 0.0009913378\n",
      "Epoch [30/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0159, LR: 0.0009912851\n",
      "Epoch [30/500], Batch [110/110], Train Loss: 0.1111, Val Loss: 0.0158, LR: 0.0009912322\n",
      "Epoch [31/500], Batch [10/110], Train Loss: 0.0473, Val Loss: 0.0192, LR: 0.0009911791\n",
      "Epoch [31/500], Batch [20/110], Train Loss: 0.0623, Val Loss: 0.0217, LR: 0.0009911259\n",
      "Epoch [31/500], Batch [30/110], Train Loss: 0.0305, Val Loss: 0.0157, LR: 0.0009910725\n",
      "Epoch [31/500], Batch [40/110], Train Loss: 0.0719, Val Loss: 0.0168, LR: 0.0009910190\n",
      "Epoch [31/500], Batch [50/110], Train Loss: 0.0349, Val Loss: 0.0161, LR: 0.0009909653\n",
      "Epoch [31/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0199, LR: 0.0009909114\n",
      "Epoch [31/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0202, LR: 0.0009908574\n",
      "Epoch [31/500], Batch [80/110], Train Loss: 0.0439, Val Loss: 0.0169, LR: 0.0009908033\n",
      "Epoch [31/500], Batch [90/110], Train Loss: 0.0396, Val Loss: 0.0160, LR: 0.0009907489\n",
      "Epoch [31/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0167, LR: 0.0009906945\n",
      "Epoch [31/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0161, LR: 0.0009906398\n",
      "Confusion Matrix:\n",
      "[[626   5]\n",
      " [  5 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99208   0.99208   0.99208       631\n",
      "           1    0.99425   0.99425   0.99425       869\n",
      "\n",
      "    accuracy                        0.99333      1500\n",
      "   macro avg    0.99316   0.99316   0.99316      1500\n",
      "weighted avg    0.99333   0.99333   0.99333      1500\n",
      "\n",
      "Total Errors: 10\n",
      "Index: 88, Predicted: 0, Actual: 1\n",
      "Index: 246, Predicted: 1, Actual: 0\n",
      "Index: 283, Predicted: 1, Actual: 0\n",
      "Index: 434, Predicted: 1, Actual: 0\n",
      "Index: 477, Predicted: 0, Actual: 1\n",
      "Epoch 31: OK- Accuracy: 0.99333, Precision: 0.99425, Recall: 0.99425, F1: 0.99425, ROC AUC: 0.99316, AUPR (PR-AUC): 0.99186, Sensitivity: 0.99425, Specificity: 0.99208, Far: 0.00792393026941363, False Positive Rate (FPR): 0.00792, False Negative Rate (FNR): 0.00575, Runtime: 0.040 sec , Memory Usage: 310.48 MB\n",
      "Epoch [32/500], Batch [10/110], Train Loss: 0.0123, Val Loss: 0.0189, LR: 0.0009905850\n",
      "Epoch [32/500], Batch [20/110], Train Loss: 0.0062, Val Loss: 0.0158, LR: 0.0009905300\n",
      "Epoch [32/500], Batch [30/110], Train Loss: 0.0045, Val Loss: 0.0158, LR: 0.0009904749\n",
      "Epoch [32/500], Batch [40/110], Train Loss: 0.0183, Val Loss: 0.0167, LR: 0.0009904196\n",
      "Epoch [32/500], Batch [50/110], Train Loss: 0.0195, Val Loss: 0.0157, LR: 0.0009903642\n",
      "Epoch [32/500], Batch [60/110], Train Loss: 0.0194, Val Loss: 0.0182, LR: 0.0009903086\n",
      "Epoch [32/500], Batch [70/110], Train Loss: 0.0151, Val Loss: 0.0200, LR: 0.0009902529\n",
      "Epoch [32/500], Batch [80/110], Train Loss: 0.2646, Val Loss: 0.0157, LR: 0.0009901969\n",
      "Epoch [32/500], Batch [90/110], Train Loss: 0.0026, Val Loss: 0.0181, LR: 0.0009901409\n",
      "Epoch [32/500], Batch [100/110], Train Loss: 0.0450, Val Loss: 0.0175, LR: 0.0009900846\n",
      "Epoch [32/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0161, LR: 0.0009900283\n",
      "Epoch [33/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0155, LR: 0.0009899717\n",
      "Epoch [33/500], Batch [20/110], Train Loss: 0.0247, Val Loss: 0.0155, LR: 0.0009899150\n",
      "Epoch [33/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0153, LR: 0.0009898581\n",
      "Epoch [33/500], Batch [40/110], Train Loss: 0.0013, Val Loss: 0.0150, LR: 0.0009898011\n",
      "Epoch [33/500], Batch [50/110], Train Loss: 0.0077, Val Loss: 0.0166, LR: 0.0009897439\n",
      "Epoch [33/500], Batch [60/110], Train Loss: 0.0315, Val Loss: 0.0161, LR: 0.0009896866\n",
      "Epoch [33/500], Batch [70/110], Train Loss: 0.0292, Val Loss: 0.0148, LR: 0.0009896291\n",
      "Epoch [33/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0170, LR: 0.0009895715\n",
      "Epoch [33/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0148, LR: 0.0009895136\n",
      "Epoch [33/500], Batch [100/110], Train Loss: 0.0015, Val Loss: 0.0179, LR: 0.0009894557\n",
      "Epoch [33/500], Batch [110/110], Train Loss: 0.0275, Val Loss: 0.0186, LR: 0.0009893975\n",
      "Epoch [34/500], Batch [10/110], Train Loss: 0.0281, Val Loss: 0.0164, LR: 0.0009893393\n",
      "Epoch [34/500], Batch [20/110], Train Loss: 0.0725, Val Loss: 0.0159, LR: 0.0009892808\n",
      "Epoch [34/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0152, LR: 0.0009892222\n",
      "Epoch [34/500], Batch [40/110], Train Loss: 0.0189, Val Loss: 0.0177, LR: 0.0009891635\n",
      "Epoch [34/500], Batch [50/110], Train Loss: 0.0211, Val Loss: 0.0151, LR: 0.0009891045\n",
      "Epoch [34/500], Batch [60/110], Train Loss: 0.0400, Val Loss: 0.0156, LR: 0.0009890455\n",
      "Epoch [34/500], Batch [70/110], Train Loss: 0.1158, Val Loss: 0.0150, LR: 0.0009889862\n",
      "Epoch [34/500], Batch [80/110], Train Loss: 0.0029, Val Loss: 0.0152, LR: 0.0009889268\n",
      "Epoch [34/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0167, LR: 0.0009888673\n",
      "Epoch [34/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0156, LR: 0.0009888076\n",
      "Epoch [34/500], Batch [110/110], Train Loss: 0.0298, Val Loss: 0.0157, LR: 0.0009887477\n",
      "Epoch [35/500], Batch [10/110], Train Loss: 0.0409, Val Loss: 0.0155, LR: 0.0009886877\n",
      "Epoch [35/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0153, LR: 0.0009886275\n",
      "Epoch [35/500], Batch [30/110], Train Loss: 0.0032, Val Loss: 0.0150, LR: 0.0009885672\n",
      "Epoch [35/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0149, LR: 0.0009885067\n",
      "Epoch [35/500], Batch [50/110], Train Loss: 0.0386, Val Loss: 0.0155, LR: 0.0009884460\n",
      "Epoch [35/500], Batch [60/110], Train Loss: 0.0749, Val Loss: 0.0172, LR: 0.0009883852\n",
      "Epoch [35/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0156, LR: 0.0009883243\n",
      "Epoch [35/500], Batch [80/110], Train Loss: 0.0300, Val Loss: 0.0161, LR: 0.0009882631\n",
      "Epoch [35/500], Batch [90/110], Train Loss: 0.0237, Val Loss: 0.0163, LR: 0.0009882018\n",
      "Epoch [35/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0183, LR: 0.0009881404\n",
      "Epoch [35/500], Batch [110/110], Train Loss: 0.0108, Val Loss: 0.0150, LR: 0.0009880788\n",
      "Epoch [36/500], Batch [10/110], Train Loss: 0.0157, Val Loss: 0.0155, LR: 0.0009880170\n",
      "Epoch [36/500], Batch [20/110], Train Loss: 0.0065, Val Loss: 0.0196, LR: 0.0009879551\n",
      "Epoch [36/500], Batch [30/110], Train Loss: 0.0246, Val Loss: 0.0159, LR: 0.0009878931\n",
      "Epoch [36/500], Batch [40/110], Train Loss: 0.0047, Val Loss: 0.0147, LR: 0.0009878308\n",
      "Epoch [36/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0164, LR: 0.0009877684\n",
      "Epoch [36/500], Batch [60/110], Train Loss: 0.0606, Val Loss: 0.0245, LR: 0.0009877059\n",
      "Epoch [36/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0151, LR: 0.0009876432\n",
      "Epoch [36/500], Batch [80/110], Train Loss: 0.0473, Val Loss: 0.0156, LR: 0.0009875803\n",
      "Epoch [36/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0193, LR: 0.0009875173\n",
      "Epoch [36/500], Batch [100/110], Train Loss: 0.0021, Val Loss: 0.0182, LR: 0.0009874541\n",
      "Epoch [36/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0162, LR: 0.0009873908\n",
      "Epoch [37/500], Batch [10/110], Train Loss: 0.0024, Val Loss: 0.0158, LR: 0.0009873273\n",
      "Epoch [37/500], Batch [20/110], Train Loss: 0.0123, Val Loss: 0.0160, LR: 0.0009872637\n",
      "Epoch [37/500], Batch [30/110], Train Loss: 0.0236, Val Loss: 0.0152, LR: 0.0009871999\n",
      "Epoch [37/500], Batch [40/110], Train Loss: 0.0016, Val Loss: 0.0152, LR: 0.0009871359\n",
      "Epoch [37/500], Batch [50/110], Train Loss: 0.0762, Val Loss: 0.0151, LR: 0.0009870718\n",
      "Epoch [37/500], Batch [60/110], Train Loss: 0.0620, Val Loss: 0.0162, LR: 0.0009870075\n",
      "Epoch [37/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0175, LR: 0.0009869431\n",
      "Epoch [37/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0185, LR: 0.0009868785\n",
      "Epoch [37/500], Batch [90/110], Train Loss: 0.0052, Val Loss: 0.0190, LR: 0.0009868137\n",
      "Epoch [37/500], Batch [100/110], Train Loss: 0.0949, Val Loss: 0.0184, LR: 0.0009867488\n",
      "Epoch [37/500], Batch [110/110], Train Loss: 0.0425, Val Loss: 0.0145, LR: 0.0009866838\n",
      "Epoch [38/500], Batch [10/110], Train Loss: 0.0009, Val Loss: 0.0152, LR: 0.0009866185\n",
      "Epoch [38/500], Batch [20/110], Train Loss: 0.0010, Val Loss: 0.0210, LR: 0.0009865532\n",
      "Epoch [38/500], Batch [30/110], Train Loss: 0.0023, Val Loss: 0.0154, LR: 0.0009864876\n",
      "Epoch [38/500], Batch [40/110], Train Loss: 0.0348, Val Loss: 0.0154, LR: 0.0009864219\n",
      "Epoch [38/500], Batch [50/110], Train Loss: 0.0011, Val Loss: 0.0163, LR: 0.0009863561\n",
      "Epoch [38/500], Batch [60/110], Train Loss: 0.0540, Val Loss: 0.0156, LR: 0.0009862901\n",
      "Epoch [38/500], Batch [70/110], Train Loss: 0.0146, Val Loss: 0.0152, LR: 0.0009862239\n",
      "Epoch [38/500], Batch [80/110], Train Loss: 0.0164, Val Loss: 0.0176, LR: 0.0009861576\n",
      "Epoch [38/500], Batch [90/110], Train Loss: 0.0128, Val Loss: 0.0182, LR: 0.0009860911\n",
      "Epoch [38/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0152, LR: 0.0009860245\n",
      "Epoch [38/500], Batch [110/110], Train Loss: 0.0728, Val Loss: 0.0156, LR: 0.0009859577\n",
      "Epoch [39/500], Batch [10/110], Train Loss: 0.0356, Val Loss: 0.0150, LR: 0.0009858908\n",
      "Epoch [39/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0151, LR: 0.0009858237\n",
      "Epoch [39/500], Batch [30/110], Train Loss: 0.0394, Val Loss: 0.0151, LR: 0.0009857564\n",
      "Epoch [39/500], Batch [40/110], Train Loss: 0.2645, Val Loss: 0.0161, LR: 0.0009856890\n",
      "Epoch [39/500], Batch [50/110], Train Loss: 0.0158, Val Loss: 0.0145, LR: 0.0009856214\n",
      "Epoch [39/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0145, LR: 0.0009855537\n",
      "Epoch [39/500], Batch [70/110], Train Loss: 0.0058, Val Loss: 0.0151, LR: 0.0009854858\n",
      "Epoch [39/500], Batch [80/110], Train Loss: 0.0010, Val Loss: 0.0195, LR: 0.0009854177\n",
      "Epoch [39/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0159, LR: 0.0009853495\n",
      "Epoch [39/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0173, LR: 0.0009852812\n",
      "Epoch [39/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0172, LR: 0.0009852127\n",
      "Epoch [40/500], Batch [10/110], Train Loss: 0.0009, Val Loss: 0.0217, LR: 0.0009851440\n",
      "Epoch [40/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0151, LR: 0.0009850752\n",
      "Epoch [40/500], Batch [30/110], Train Loss: 0.0229, Val Loss: 0.0170, LR: 0.0009850062\n",
      "Epoch [40/500], Batch [40/110], Train Loss: 0.0519, Val Loss: 0.0165, LR: 0.0009849370\n",
      "Epoch [40/500], Batch [50/110], Train Loss: 0.0607, Val Loss: 0.0163, LR: 0.0009848677\n",
      "Epoch [40/500], Batch [60/110], Train Loss: 0.0031, Val Loss: 0.0172, LR: 0.0009847983\n",
      "Epoch [40/500], Batch [70/110], Train Loss: 0.0260, Val Loss: 0.0159, LR: 0.0009847287\n",
      "Epoch [40/500], Batch [80/110], Train Loss: 0.0119, Val Loss: 0.0153, LR: 0.0009846589\n",
      "Epoch [40/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0150, LR: 0.0009845890\n",
      "Epoch [40/500], Batch [100/110], Train Loss: 0.0195, Val Loss: 0.0151, LR: 0.0009845189\n",
      "Epoch [40/500], Batch [110/110], Train Loss: 0.0122, Val Loss: 0.0151, LR: 0.0009844487\n",
      "Epoch [41/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0170, LR: 0.0009843783\n",
      "Epoch [41/500], Batch [20/110], Train Loss: 0.0346, Val Loss: 0.0149, LR: 0.0009843077\n",
      "Epoch [41/500], Batch [30/110], Train Loss: 0.0768, Val Loss: 0.0144, LR: 0.0009842370\n",
      "Epoch [41/500], Batch [40/110], Train Loss: 0.0563, Val Loss: 0.0142, LR: 0.0009841662\n",
      "Epoch [41/500], Batch [50/110], Train Loss: 0.0640, Val Loss: 0.0167, LR: 0.0009840951\n",
      "Epoch [41/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0162, LR: 0.0009840240\n",
      "Epoch [41/500], Batch [70/110], Train Loss: 0.0029, Val Loss: 0.0143, LR: 0.0009839526\n",
      "Epoch [41/500], Batch [80/110], Train Loss: 0.1493, Val Loss: 0.0154, LR: 0.0009838811\n",
      "Epoch [41/500], Batch [90/110], Train Loss: 0.0082, Val Loss: 0.0147, LR: 0.0009838095\n",
      "Epoch [41/500], Batch [100/110], Train Loss: 0.0447, Val Loss: 0.0169, LR: 0.0009837377\n",
      "Epoch [41/500], Batch [110/110], Train Loss: 0.0175, Val Loss: 0.0144, LR: 0.0009836657\n",
      "Confusion Matrix:\n",
      "[[625   6]\n",
      " [  3 866]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99522   0.99049   0.99285       631\n",
      "           1    0.99312   0.99655   0.99483       869\n",
      "\n",
      "    accuracy                        0.99400      1500\n",
      "   macro avg    0.99417   0.99352   0.99384      1500\n",
      "weighted avg    0.99400   0.99400   0.99400      1500\n",
      "\n",
      "Total Errors: 9\n",
      "Index: 246, Predicted: 1, Actual: 0\n",
      "Index: 283, Predicted: 1, Actual: 0\n",
      "Index: 434, Predicted: 1, Actual: 0\n",
      "Index: 738, Predicted: 1, Actual: 0\n",
      "Index: 811, Predicted: 1, Actual: 0\n",
      "Epoch 41: OK- Accuracy: 0.99400, Precision: 0.99312, Recall: 0.99655, F1: 0.99483, ROC AUC: 0.99352, AUPR (PR-AUC): 0.99169, Sensitivity: 0.99655, Specificity: 0.99049, Far: 0.009508716323296355, False Positive Rate (FPR): 0.00951, False Negative Rate (FNR): 0.00345, Runtime: 0.031 sec , Memory Usage: 309.97 MB\n",
      "Epoch [42/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0141, LR: 0.0009835936\n",
      "Epoch [42/500], Batch [20/110], Train Loss: 0.1068, Val Loss: 0.0146, LR: 0.0009835214\n",
      "Epoch [42/500], Batch [30/110], Train Loss: 0.0162, Val Loss: 0.0148, LR: 0.0009834489\n",
      "Epoch [42/500], Batch [40/110], Train Loss: 0.0011, Val Loss: 0.0162, LR: 0.0009833763\n",
      "Epoch [42/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0160, LR: 0.0009833036\n",
      "Epoch [42/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0144, LR: 0.0009832307\n",
      "Epoch [42/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0211, LR: 0.0009831577\n",
      "Epoch [42/500], Batch [80/110], Train Loss: 0.0037, Val Loss: 0.0196, LR: 0.0009830845\n",
      "Epoch [42/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0163, LR: 0.0009830111\n",
      "Epoch [42/500], Batch [100/110], Train Loss: 0.0594, Val Loss: 0.0153, LR: 0.0009829376\n",
      "Epoch [42/500], Batch [110/110], Train Loss: 0.0939, Val Loss: 0.0222, LR: 0.0009828639\n",
      "Epoch [43/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0197, LR: 0.0009827901\n",
      "Epoch [43/500], Batch [20/110], Train Loss: 0.0189, Val Loss: 0.0139, LR: 0.0009827161\n",
      "Epoch [43/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0142, LR: 0.0009826420\n",
      "Epoch [43/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0144, LR: 0.0009825677\n",
      "Epoch [43/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0172, LR: 0.0009824932\n",
      "Epoch [43/500], Batch [60/110], Train Loss: 0.0189, Val Loss: 0.0165, LR: 0.0009824186\n",
      "Epoch [43/500], Batch [70/110], Train Loss: 0.0339, Val Loss: 0.0159, LR: 0.0009823438\n",
      "Epoch [43/500], Batch [80/110], Train Loss: 0.0011, Val Loss: 0.0139, LR: 0.0009822689\n",
      "Epoch [43/500], Batch [90/110], Train Loss: 0.0016, Val Loss: 0.0158, LR: 0.0009821938\n",
      "Epoch [43/500], Batch [100/110], Train Loss: 0.0494, Val Loss: 0.0143, LR: 0.0009821186\n",
      "Epoch [43/500], Batch [110/110], Train Loss: 0.0387, Val Loss: 0.0142, LR: 0.0009820432\n",
      "Epoch [44/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0176, LR: 0.0009819677\n",
      "Epoch [44/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0143, LR: 0.0009818920\n",
      "Epoch [44/500], Batch [30/110], Train Loss: 0.0529, Val Loss: 0.0141, LR: 0.0009818161\n",
      "Epoch [44/500], Batch [40/110], Train Loss: 0.0284, Val Loss: 0.0157, LR: 0.0009817401\n",
      "Epoch [44/500], Batch [50/110], Train Loss: 0.0568, Val Loss: 0.0142, LR: 0.0009816640\n",
      "Epoch [44/500], Batch [60/110], Train Loss: 0.0015, Val Loss: 0.0138, LR: 0.0009815876\n",
      "Epoch [44/500], Batch [70/110], Train Loss: 0.0014, Val Loss: 0.0140, LR: 0.0009815112\n",
      "Epoch [44/500], Batch [80/110], Train Loss: 0.0316, Val Loss: 0.0161, LR: 0.0009814345\n",
      "Epoch [44/500], Batch [90/110], Train Loss: 0.0084, Val Loss: 0.0144, LR: 0.0009813577\n",
      "Epoch [44/500], Batch [100/110], Train Loss: 0.0399, Val Loss: 0.0138, LR: 0.0009812808\n",
      "Epoch [44/500], Batch [110/110], Train Loss: 0.0425, Val Loss: 0.0152, LR: 0.0009812037\n",
      "Epoch [45/500], Batch [10/110], Train Loss: 0.0135, Val Loss: 0.0155, LR: 0.0009811264\n",
      "Epoch [45/500], Batch [20/110], Train Loss: 0.0202, Val Loss: 0.0138, LR: 0.0009810490\n",
      "Epoch [45/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0154, LR: 0.0009809715\n",
      "Epoch [45/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0156, LR: 0.0009808938\n",
      "Epoch [45/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0141, LR: 0.0009808159\n",
      "Epoch [45/500], Batch [60/110], Train Loss: 0.0316, Val Loss: 0.0139, LR: 0.0009807378\n",
      "Epoch [45/500], Batch [70/110], Train Loss: 0.0429, Val Loss: 0.0148, LR: 0.0009806597\n",
      "Epoch [45/500], Batch [80/110], Train Loss: 0.0119, Val Loss: 0.0138, LR: 0.0009805813\n",
      "Epoch [45/500], Batch [90/110], Train Loss: 0.0611, Val Loss: 0.0143, LR: 0.0009805028\n",
      "Epoch [45/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0147, LR: 0.0009804242\n",
      "Epoch [45/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0187, LR: 0.0009803454\n",
      "Epoch [46/500], Batch [10/110], Train Loss: 0.0619, Val Loss: 0.0162, LR: 0.0009802664\n",
      "Epoch [46/500], Batch [20/110], Train Loss: 0.1281, Val Loss: 0.0153, LR: 0.0009801873\n",
      "Epoch [46/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0146, LR: 0.0009801080\n",
      "Epoch [46/500], Batch [40/110], Train Loss: 0.0807, Val Loss: 0.0147, LR: 0.0009800286\n",
      "Epoch [46/500], Batch [50/110], Train Loss: 0.0305, Val Loss: 0.0153, LR: 0.0009799490\n",
      "Epoch [46/500], Batch [60/110], Train Loss: 0.0359, Val Loss: 0.0147, LR: 0.0009798693\n",
      "Epoch [46/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0143, LR: 0.0009797894\n",
      "Epoch [46/500], Batch [80/110], Train Loss: 0.0010, Val Loss: 0.0172, LR: 0.0009797094\n",
      "Epoch [46/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0152, LR: 0.0009796292\n",
      "Epoch [46/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0155, LR: 0.0009795488\n",
      "Epoch [46/500], Batch [110/110], Train Loss: 0.1383, Val Loss: 0.0139, LR: 0.0009794683\n",
      "Epoch [47/500], Batch [10/110], Train Loss: 0.0559, Val Loss: 0.0139, LR: 0.0009793876\n",
      "Epoch [47/500], Batch [20/110], Train Loss: 0.0292, Val Loss: 0.0140, LR: 0.0009793068\n",
      "Epoch [47/500], Batch [30/110], Train Loss: 0.0124, Val Loss: 0.0190, LR: 0.0009792258\n",
      "Epoch [47/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0139, LR: 0.0009791447\n",
      "Epoch [47/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0139, LR: 0.0009790634\n",
      "Epoch [47/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0143, LR: 0.0009789820\n",
      "Epoch [47/500], Batch [70/110], Train Loss: 0.0148, Val Loss: 0.0145, LR: 0.0009789004\n",
      "Epoch [47/500], Batch [80/110], Train Loss: 0.0684, Val Loss: 0.0187, LR: 0.0009788186\n",
      "Epoch [47/500], Batch [90/110], Train Loss: 0.0669, Val Loss: 0.0216, LR: 0.0009787367\n",
      "Epoch [47/500], Batch [100/110], Train Loss: 0.0180, Val Loss: 0.0162, LR: 0.0009786547\n",
      "Epoch [47/500], Batch [110/110], Train Loss: 0.0435, Val Loss: 0.0140, LR: 0.0009785725\n",
      "Epoch [48/500], Batch [10/110], Train Loss: 0.0047, Val Loss: 0.0140, LR: 0.0009784901\n",
      "Epoch [48/500], Batch [20/110], Train Loss: 0.0137, Val Loss: 0.0195, LR: 0.0009784076\n",
      "Epoch [48/500], Batch [30/110], Train Loss: 0.1009, Val Loss: 0.0151, LR: 0.0009783249\n",
      "Epoch [48/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0150, LR: 0.0009782421\n",
      "Epoch [48/500], Batch [50/110], Train Loss: 0.0034, Val Loss: 0.0143, LR: 0.0009781591\n",
      "Epoch [48/500], Batch [60/110], Train Loss: 0.0524, Val Loss: 0.0144, LR: 0.0009780760\n",
      "Epoch [48/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0153, LR: 0.0009779927\n",
      "Epoch [48/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0136, LR: 0.0009779092\n",
      "Epoch [48/500], Batch [90/110], Train Loss: 0.0468, Val Loss: 0.0133, LR: 0.0009778256\n",
      "Epoch [48/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0184, LR: 0.0009777419\n",
      "Epoch [48/500], Batch [110/110], Train Loss: 0.1066, Val Loss: 0.0160, LR: 0.0009776579\n",
      "Epoch [49/500], Batch [10/110], Train Loss: 0.0010, Val Loss: 0.0132, LR: 0.0009775739\n",
      "Epoch [49/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0131, LR: 0.0009774897\n",
      "Epoch [49/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0175, LR: 0.0009774053\n",
      "Epoch [49/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0224, LR: 0.0009773208\n",
      "Epoch [49/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0138, LR: 0.0009772361\n",
      "Epoch [49/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0131, LR: 0.0009771513\n",
      "Epoch [49/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0133, LR: 0.0009770663\n",
      "Epoch [49/500], Batch [80/110], Train Loss: 0.0140, Val Loss: 0.0173, LR: 0.0009769811\n",
      "Epoch [49/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0177, LR: 0.0009768958\n",
      "Epoch [49/500], Batch [100/110], Train Loss: 0.0254, Val Loss: 0.0138, LR: 0.0009768104\n",
      "Epoch [49/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0136, LR: 0.0009767248\n",
      "Epoch [50/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0155, LR: 0.0009766390\n",
      "Epoch [50/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0143, LR: 0.0009765531\n",
      "Epoch [50/500], Batch [30/110], Train Loss: 0.0184, Val Loss: 0.0144, LR: 0.0009764670\n",
      "Epoch [50/500], Batch [40/110], Train Loss: 0.0009, Val Loss: 0.0135, LR: 0.0009763808\n",
      "Epoch [50/500], Batch [50/110], Train Loss: 0.0700, Val Loss: 0.0136, LR: 0.0009762944\n",
      "Epoch [50/500], Batch [60/110], Train Loss: 0.0313, Val Loss: 0.0137, LR: 0.0009762079\n",
      "Epoch [50/500], Batch [70/110], Train Loss: 0.0139, Val Loss: 0.0173, LR: 0.0009761212\n",
      "Epoch [50/500], Batch [80/110], Train Loss: 0.0025, Val Loss: 0.0167, LR: 0.0009760344\n",
      "Epoch [50/500], Batch [90/110], Train Loss: 0.0235, Val Loss: 0.0147, LR: 0.0009759474\n",
      "Epoch [50/500], Batch [100/110], Train Loss: 0.0014, Val Loss: 0.0144, LR: 0.0009758603\n",
      "Epoch [50/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0155, LR: 0.0009757730\n",
      "Epoch [51/500], Batch [10/110], Train Loss: 0.0010, Val Loss: 0.0137, LR: 0.0009756855\n",
      "Epoch [51/500], Batch [20/110], Train Loss: 0.0317, Val Loss: 0.0136, LR: 0.0009755979\n",
      "Epoch [51/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0152, LR: 0.0009755102\n",
      "Epoch [51/500], Batch [40/110], Train Loss: 0.0222, Val Loss: 0.0135, LR: 0.0009754223\n",
      "Epoch [51/500], Batch [50/110], Train Loss: 0.0819, Val Loss: 0.0139, LR: 0.0009753342\n",
      "Epoch [51/500], Batch [60/110], Train Loss: 0.0315, Val Loss: 0.0136, LR: 0.0009752460\n",
      "Epoch [51/500], Batch [70/110], Train Loss: 0.0049, Val Loss: 0.0218, LR: 0.0009751576\n",
      "Epoch [51/500], Batch [80/110], Train Loss: 0.0032, Val Loss: 0.0133, LR: 0.0009750691\n",
      "Epoch [51/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0137, LR: 0.0009749804\n",
      "Epoch [51/500], Batch [100/110], Train Loss: 0.0024, Val Loss: 0.0133, LR: 0.0009748916\n",
      "Epoch [51/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0131, LR: 0.0009748026\n",
      "Confusion Matrix:\n",
      "[[628   3]\n",
      " [ 11 858]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98279   0.99525   0.98898       631\n",
      "           1    0.99652   0.98734   0.99191       869\n",
      "\n",
      "    accuracy                        0.99067      1500\n",
      "   macro avg    0.98965   0.99129   0.99044      1500\n",
      "weighted avg    0.99074   0.99067   0.99067      1500\n",
      "\n",
      "Total Errors: 14\n",
      "Index: 88, Predicted: 0, Actual: 1\n",
      "Index: 283, Predicted: 1, Actual: 0\n",
      "Index: 434, Predicted: 1, Actual: 0\n",
      "Index: 477, Predicted: 0, Actual: 1\n",
      "Index: 606, Predicted: 0, Actual: 1\n",
      "Epoch 51: OK- Accuracy: 0.99067, Precision: 0.99652, Recall: 0.98734, F1: 0.99191, ROC AUC: 0.99129, AUPR (PR-AUC): 0.99123, Sensitivity: 0.98734, Specificity: 0.99525, Far: 0.004754358161648178, False Positive Rate (FPR): 0.00475, False Negative Rate (FNR): 0.01266, Runtime: 0.031 sec , Memory Usage: 309.99 MB\n",
      "Epoch [52/500], Batch [10/110], Train Loss: 0.0256, Val Loss: 0.0153, LR: 0.0009747135\n",
      "Epoch [52/500], Batch [20/110], Train Loss: 0.0194, Val Loss: 0.0179, LR: 0.0009746242\n",
      "Epoch [52/500], Batch [30/110], Train Loss: 0.0172, Val Loss: 0.0164, LR: 0.0009745347\n",
      "Epoch [52/500], Batch [40/110], Train Loss: 0.0226, Val Loss: 0.0146, LR: 0.0009744451\n",
      "Epoch [52/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0134, LR: 0.0009743554\n",
      "Epoch [52/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0136, LR: 0.0009742655\n",
      "Epoch [52/500], Batch [70/110], Train Loss: 0.0262, Val Loss: 0.0136, LR: 0.0009741754\n",
      "Epoch [52/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0136, LR: 0.0009740852\n",
      "Epoch [52/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0150, LR: 0.0009739948\n",
      "Epoch [52/500], Batch [100/110], Train Loss: 0.0600, Val Loss: 0.0148, LR: 0.0009739043\n",
      "Epoch [52/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0152, LR: 0.0009738137\n",
      "Epoch [53/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0139, LR: 0.0009737228\n",
      "Epoch [53/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0137, LR: 0.0009736319\n",
      "Epoch [53/500], Batch [30/110], Train Loss: 0.0047, Val Loss: 0.0153, LR: 0.0009735407\n",
      "Epoch [53/500], Batch [40/110], Train Loss: 0.0013, Val Loss: 0.0133, LR: 0.0009734495\n",
      "Epoch [53/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0136, LR: 0.0009733580\n",
      "Epoch [53/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0147, LR: 0.0009732664\n",
      "Epoch [53/500], Batch [70/110], Train Loss: 0.0175, Val Loss: 0.0173, LR: 0.0009731747\n",
      "Epoch [53/500], Batch [80/110], Train Loss: 0.0362, Val Loss: 0.0134, LR: 0.0009730828\n",
      "Epoch [53/500], Batch [90/110], Train Loss: 0.0231, Val Loss: 0.0152, LR: 0.0009729908\n",
      "Epoch [53/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0149, LR: 0.0009728986\n",
      "Epoch [53/500], Batch [110/110], Train Loss: 0.0046, Val Loss: 0.0137, LR: 0.0009728062\n",
      "Epoch [54/500], Batch [10/110], Train Loss: 0.0010, Val Loss: 0.0135, LR: 0.0009727137\n",
      "Epoch [54/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0154, LR: 0.0009726211\n",
      "Epoch [54/500], Batch [30/110], Train Loss: 0.0580, Val Loss: 0.0134, LR: 0.0009725283\n",
      "Epoch [54/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0143, LR: 0.0009724353\n",
      "Epoch [54/500], Batch [50/110], Train Loss: 0.0014, Val Loss: 0.0140, LR: 0.0009723422\n",
      "Epoch [54/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0143, LR: 0.0009722489\n",
      "Epoch [54/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0154, LR: 0.0009721555\n",
      "Epoch [54/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0146, LR: 0.0009720619\n",
      "Epoch [54/500], Batch [90/110], Train Loss: 0.0489, Val Loss: 0.0131, LR: 0.0009719682\n",
      "Epoch [54/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0128, LR: 0.0009718743\n",
      "Epoch [54/500], Batch [110/110], Train Loss: 0.0351, Val Loss: 0.0163, LR: 0.0009717803\n",
      "Epoch [55/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0158, LR: 0.0009716861\n",
      "Epoch [55/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0134, LR: 0.0009715918\n",
      "Epoch [55/500], Batch [30/110], Train Loss: 0.0244, Val Loss: 0.0133, LR: 0.0009714973\n",
      "Epoch [55/500], Batch [40/110], Train Loss: 0.0310, Val Loss: 0.0131, LR: 0.0009714027\n",
      "Epoch [55/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0125, LR: 0.0009713079\n",
      "Epoch [55/500], Batch [60/110], Train Loss: 0.0552, Val Loss: 0.0135, LR: 0.0009712130\n",
      "Epoch [55/500], Batch [70/110], Train Loss: 0.0020, Val Loss: 0.0140, LR: 0.0009711179\n",
      "Epoch [55/500], Batch [80/110], Train Loss: 0.0399, Val Loss: 0.0175, LR: 0.0009710226\n",
      "Epoch [55/500], Batch [90/110], Train Loss: 0.0475, Val Loss: 0.0128, LR: 0.0009709272\n",
      "Epoch [55/500], Batch [100/110], Train Loss: 0.0319, Val Loss: 0.0128, LR: 0.0009708317\n",
      "Epoch [55/500], Batch [110/110], Train Loss: 0.0017, Val Loss: 0.0145, LR: 0.0009707360\n",
      "Epoch [56/500], Batch [10/110], Train Loss: 0.0167, Val Loss: 0.0131, LR: 0.0009706401\n",
      "Epoch [56/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0161, LR: 0.0009705441\n",
      "Epoch [56/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0132, LR: 0.0009704480\n",
      "Epoch [56/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0123, LR: 0.0009703517\n",
      "Epoch [56/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0141, LR: 0.0009702552\n",
      "Epoch [56/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0126, LR: 0.0009701586\n",
      "Epoch [56/500], Batch [70/110], Train Loss: 0.0023, Val Loss: 0.0127, LR: 0.0009700618\n",
      "Epoch [56/500], Batch [80/110], Train Loss: 0.0038, Val Loss: 0.0151, LR: 0.0009699649\n",
      "Epoch [56/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0157, LR: 0.0009698678\n",
      "Epoch [56/500], Batch [100/110], Train Loss: 0.0012, Val Loss: 0.0149, LR: 0.0009697706\n",
      "Epoch [56/500], Batch [110/110], Train Loss: 0.0549, Val Loss: 0.0122, LR: 0.0009696733\n",
      "Epoch [57/500], Batch [10/110], Train Loss: 0.0029, Val Loss: 0.0119, LR: 0.0009695757\n",
      "Epoch [57/500], Batch [20/110], Train Loss: 0.0016, Val Loss: 0.0137, LR: 0.0009694781\n",
      "Epoch [57/500], Batch [30/110], Train Loss: 0.0411, Val Loss: 0.0130, LR: 0.0009693802\n",
      "Epoch [57/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0124, LR: 0.0009692823\n",
      "Epoch [57/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0129, LR: 0.0009691841\n",
      "Epoch [57/500], Batch [60/110], Train Loss: 0.1756, Val Loss: 0.0122, LR: 0.0009690859\n",
      "Epoch [57/500], Batch [70/110], Train Loss: 0.0027, Val Loss: 0.0125, LR: 0.0009689874\n",
      "Epoch [57/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0130, LR: 0.0009688888\n",
      "Epoch [57/500], Batch [90/110], Train Loss: 0.0434, Val Loss: 0.0125, LR: 0.0009687901\n",
      "Epoch [57/500], Batch [100/110], Train Loss: 0.0311, Val Loss: 0.0126, LR: 0.0009686912\n",
      "Epoch [57/500], Batch [110/110], Train Loss: 0.0370, Val Loss: 0.0125, LR: 0.0009685922\n",
      "Epoch [58/500], Batch [10/110], Train Loss: 0.0373, Val Loss: 0.0122, LR: 0.0009684930\n",
      "Epoch [58/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0134, LR: 0.0009683937\n",
      "Epoch [58/500], Batch [30/110], Train Loss: 0.0264, Val Loss: 0.0128, LR: 0.0009682942\n",
      "Epoch [58/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0148, LR: 0.0009681945\n",
      "Epoch [58/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0123, LR: 0.0009680947\n",
      "Epoch [58/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0123, LR: 0.0009679948\n",
      "Epoch [58/500], Batch [70/110], Train Loss: 0.0016, Val Loss: 0.0151, LR: 0.0009678947\n",
      "Epoch [58/500], Batch [80/110], Train Loss: 0.0036, Val Loss: 0.0128, LR: 0.0009677945\n",
      "Epoch [58/500], Batch [90/110], Train Loss: 0.0113, Val Loss: 0.0184, LR: 0.0009676941\n",
      "Epoch [58/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0133, LR: 0.0009675935\n",
      "Epoch [58/500], Batch [110/110], Train Loss: 0.0938, Val Loss: 0.0122, LR: 0.0009674928\n",
      "Epoch [59/500], Batch [10/110], Train Loss: 0.0023, Val Loss: 0.0118, LR: 0.0009673920\n",
      "Epoch [59/500], Batch [20/110], Train Loss: 0.0123, Val Loss: 0.0143, LR: 0.0009672910\n",
      "Epoch [59/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0118, LR: 0.0009671898\n",
      "Epoch [59/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0120, LR: 0.0009670885\n",
      "Epoch [59/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0121, LR: 0.0009669871\n",
      "Epoch [59/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0128, LR: 0.0009668855\n",
      "Epoch [59/500], Batch [70/110], Train Loss: 0.0581, Val Loss: 0.0139, LR: 0.0009667837\n",
      "Epoch [59/500], Batch [80/110], Train Loss: 0.0219, Val Loss: 0.0134, LR: 0.0009666818\n",
      "Epoch [59/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0117, LR: 0.0009665798\n",
      "Epoch [59/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0118, LR: 0.0009664776\n",
      "Epoch [59/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0159, LR: 0.0009663752\n",
      "Epoch [60/500], Batch [10/110], Train Loss: 0.0261, Val Loss: 0.0130, LR: 0.0009662727\n",
      "Epoch [60/500], Batch [20/110], Train Loss: 0.0520, Val Loss: 0.0132, LR: 0.0009661700\n",
      "Epoch [60/500], Batch [30/110], Train Loss: 0.0810, Val Loss: 0.0151, LR: 0.0009660672\n",
      "Epoch [60/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0117, LR: 0.0009659643\n",
      "Epoch [60/500], Batch [50/110], Train Loss: 0.0200, Val Loss: 0.0119, LR: 0.0009658612\n",
      "Epoch [60/500], Batch [60/110], Train Loss: 0.0176, Val Loss: 0.0135, LR: 0.0009657579\n",
      "Epoch [60/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0122, LR: 0.0009656545\n",
      "Epoch [60/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0121, LR: 0.0009655509\n",
      "Epoch [60/500], Batch [90/110], Train Loss: 0.0454, Val Loss: 0.0145, LR: 0.0009654472\n",
      "Epoch [60/500], Batch [100/110], Train Loss: 0.0015, Val Loss: 0.0115, LR: 0.0009653434\n",
      "Epoch [60/500], Batch [110/110], Train Loss: 0.0619, Val Loss: 0.0122, LR: 0.0009652394\n",
      "Epoch [61/500], Batch [10/110], Train Loss: 0.0010, Val Loss: 0.0128, LR: 0.0009651352\n",
      "Epoch [61/500], Batch [20/110], Train Loss: 0.0318, Val Loss: 0.0116, LR: 0.0009650309\n",
      "Epoch [61/500], Batch [30/110], Train Loss: 0.0223, Val Loss: 0.0123, LR: 0.0009649264\n",
      "Epoch [61/500], Batch [40/110], Train Loss: 0.0010, Val Loss: 0.0130, LR: 0.0009648218\n",
      "Epoch [61/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0122, LR: 0.0009647171\n",
      "Epoch [61/500], Batch [60/110], Train Loss: 0.0036, Val Loss: 0.0120, LR: 0.0009646122\n",
      "Epoch [61/500], Batch [70/110], Train Loss: 0.0522, Val Loss: 0.0147, LR: 0.0009645071\n",
      "Epoch [61/500], Batch [80/110], Train Loss: 0.0441, Val Loss: 0.0115, LR: 0.0009644019\n",
      "Epoch [61/500], Batch [90/110], Train Loss: 0.0383, Val Loss: 0.0118, LR: 0.0009642965\n",
      "Epoch [61/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0113, LR: 0.0009641910\n",
      "Epoch [61/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0114, LR: 0.0009640854\n",
      "Confusion Matrix:\n",
      "[[628   3]\n",
      " [ 12 857]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98125   0.99525   0.98820       631\n",
      "           1    0.99651   0.98619   0.99132       869\n",
      "\n",
      "    accuracy                        0.99000      1500\n",
      "   macro avg    0.98888   0.99072   0.98976      1500\n",
      "weighted avg    0.99009   0.99000   0.99001      1500\n",
      "\n",
      "Total Errors: 15\n",
      "Index: 88, Predicted: 0, Actual: 1\n",
      "Index: 283, Predicted: 1, Actual: 0\n",
      "Index: 434, Predicted: 1, Actual: 0\n",
      "Index: 477, Predicted: 0, Actual: 1\n",
      "Index: 606, Predicted: 0, Actual: 1\n",
      "Epoch 61: OK- Accuracy: 0.99000, Precision: 0.99651, Recall: 0.98619, F1: 0.99132, ROC AUC: 0.99072, AUPR (PR-AUC): 0.99075, Sensitivity: 0.98619, Specificity: 0.99525, Far: 0.004754358161648178, False Positive Rate (FPR): 0.00475, False Negative Rate (FNR): 0.01381, Runtime: 0.031 sec , Memory Usage: 310.00 MB\n",
      "Epoch [62/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0127, LR: 0.0009639795\n",
      "Epoch [62/500], Batch [20/110], Train Loss: 0.0110, Val Loss: 0.0143, LR: 0.0009638736\n",
      "Epoch [62/500], Batch [30/110], Train Loss: 0.0333, Val Loss: 0.0115, LR: 0.0009637675\n",
      "Epoch [62/500], Batch [40/110], Train Loss: 0.0017, Val Loss: 0.0127, LR: 0.0009636612\n",
      "Epoch [62/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0126, LR: 0.0009635548\n",
      "Epoch [62/500], Batch [60/110], Train Loss: 0.0026, Val Loss: 0.0121, LR: 0.0009634482\n",
      "Epoch [62/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0117, LR: 0.0009633415\n",
      "Epoch [62/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0116, LR: 0.0009632347\n",
      "Epoch [62/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0116, LR: 0.0009631277\n",
      "Epoch [62/500], Batch [100/110], Train Loss: 0.0019, Val Loss: 0.0111, LR: 0.0009630205\n",
      "Epoch [62/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0148, LR: 0.0009629132\n",
      "Epoch [63/500], Batch [10/110], Train Loss: 0.0571, Val Loss: 0.0137, LR: 0.0009628058\n",
      "Epoch [63/500], Batch [20/110], Train Loss: 0.0022, Val Loss: 0.0120, LR: 0.0009626982\n",
      "Epoch [63/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0160, LR: 0.0009625904\n",
      "Epoch [63/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0121, LR: 0.0009624825\n",
      "Epoch [63/500], Batch [50/110], Train Loss: 0.0011, Val Loss: 0.0122, LR: 0.0009623745\n",
      "Epoch [63/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0130, LR: 0.0009622662\n",
      "Epoch [63/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0153, LR: 0.0009621579\n",
      "Epoch [63/500], Batch [80/110], Train Loss: 0.0008, Val Loss: 0.0117, LR: 0.0009620494\n",
      "Epoch [63/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0117, LR: 0.0009619408\n",
      "Epoch [63/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0145, LR: 0.0009618320\n",
      "Epoch [63/500], Batch [110/110], Train Loss: 0.0130, Val Loss: 0.0147, LR: 0.0009617230\n",
      "Epoch [64/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0119, LR: 0.0009616139\n",
      "Epoch [64/500], Batch [20/110], Train Loss: 0.0040, Val Loss: 0.0122, LR: 0.0009615047\n",
      "Epoch [64/500], Batch [30/110], Train Loss: 0.0321, Val Loss: 0.0130, LR: 0.0009613953\n",
      "Epoch [64/500], Batch [40/110], Train Loss: 0.0288, Val Loss: 0.0135, LR: 0.0009612857\n",
      "Epoch [64/500], Batch [50/110], Train Loss: 0.0242, Val Loss: 0.0127, LR: 0.0009611760\n",
      "Epoch [64/500], Batch [60/110], Train Loss: 0.0528, Val Loss: 0.0121, LR: 0.0009610662\n",
      "Epoch [64/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0135, LR: 0.0009609562\n",
      "Epoch [64/500], Batch [80/110], Train Loss: 0.0316, Val Loss: 0.0112, LR: 0.0009608461\n",
      "Epoch [64/500], Batch [90/110], Train Loss: 0.0425, Val Loss: 0.0118, LR: 0.0009607358\n",
      "Epoch [64/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0110, LR: 0.0009606253\n",
      "Epoch [64/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0137, LR: 0.0009605148\n",
      "Epoch [65/500], Batch [10/110], Train Loss: 0.0013, Val Loss: 0.0123, LR: 0.0009604040\n",
      "Epoch [65/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0125, LR: 0.0009602932\n",
      "Epoch [65/500], Batch [30/110], Train Loss: 0.0249, Val Loss: 0.0117, LR: 0.0009601821\n",
      "Epoch [65/500], Batch [40/110], Train Loss: 0.0055, Val Loss: 0.0114, LR: 0.0009600709\n",
      "Epoch [65/500], Batch [50/110], Train Loss: 0.0189, Val Loss: 0.0114, LR: 0.0009599596\n",
      "Epoch [65/500], Batch [60/110], Train Loss: 0.0039, Val Loss: 0.0111, LR: 0.0009598481\n",
      "Epoch [65/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0110, LR: 0.0009597365\n",
      "Epoch [65/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0138, LR: 0.0009596247\n",
      "Epoch [65/500], Batch [90/110], Train Loss: 0.0206, Val Loss: 0.0126, LR: 0.0009595128\n",
      "Epoch [65/500], Batch [100/110], Train Loss: 0.0785, Val Loss: 0.0107, LR: 0.0009594008\n",
      "Epoch [65/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0118, LR: 0.0009592885\n",
      "Epoch [66/500], Batch [10/110], Train Loss: 0.2533, Val Loss: 0.0132, LR: 0.0009591762\n",
      "Epoch [66/500], Batch [20/110], Train Loss: 0.0013, Val Loss: 0.0113, LR: 0.0009590637\n",
      "Epoch [66/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0111, LR: 0.0009589510\n",
      "Epoch [66/500], Batch [40/110], Train Loss: 0.0457, Val Loss: 0.0104, LR: 0.0009588382\n",
      "Epoch [66/500], Batch [50/110], Train Loss: 0.0222, Val Loss: 0.0112, LR: 0.0009587252\n",
      "Epoch [66/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0115, LR: 0.0009586121\n",
      "Epoch [66/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0121, LR: 0.0009584989\n",
      "Epoch [66/500], Batch [80/110], Train Loss: 0.0181, Val Loss: 0.0109, LR: 0.0009583855\n",
      "Epoch [66/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0119, LR: 0.0009582719\n",
      "Epoch [66/500], Batch [100/110], Train Loss: 0.0330, Val Loss: 0.0119, LR: 0.0009581582\n",
      "Epoch [66/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0107, LR: 0.0009580444\n",
      "Epoch [67/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0111, LR: 0.0009579304\n",
      "Epoch [67/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0156, LR: 0.0009578162\n",
      "Epoch [67/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0103, LR: 0.0009577020\n",
      "Epoch [67/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0104, LR: 0.0009575875\n",
      "Epoch [67/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0117, LR: 0.0009574729\n",
      "Epoch [67/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0111, LR: 0.0009573582\n",
      "Epoch [67/500], Batch [70/110], Train Loss: 0.1000, Val Loss: 0.0108, LR: 0.0009572433\n",
      "Epoch [67/500], Batch [80/110], Train Loss: 0.0500, Val Loss: 0.0110, LR: 0.0009571283\n",
      "Epoch [67/500], Batch [90/110], Train Loss: 0.0094, Val Loss: 0.0107, LR: 0.0009570131\n",
      "Epoch [67/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0110, LR: 0.0009568978\n",
      "Epoch [67/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0132, LR: 0.0009567823\n",
      "Epoch [68/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0105, LR: 0.0009566667\n",
      "Epoch [68/500], Batch [20/110], Train Loss: 0.0144, Val Loss: 0.0106, LR: 0.0009565510\n",
      "Epoch [68/500], Batch [30/110], Train Loss: 0.0529, Val Loss: 0.0124, LR: 0.0009564350\n",
      "Epoch [68/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0109, LR: 0.0009563190\n",
      "Epoch [68/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0109, LR: 0.0009562028\n",
      "Epoch [68/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0114, LR: 0.0009560864\n",
      "Epoch [68/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0127, LR: 0.0009559699\n",
      "Epoch [68/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0107, LR: 0.0009558533\n",
      "Epoch [68/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0103, LR: 0.0009557365\n",
      "Epoch [68/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0136, LR: 0.0009556195\n",
      "Epoch [68/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0143, LR: 0.0009555025\n",
      "Epoch [69/500], Batch [10/110], Train Loss: 0.0080, Val Loss: 0.0104, LR: 0.0009553852\n",
      "Epoch [69/500], Batch [20/110], Train Loss: 0.2476, Val Loss: 0.0121, LR: 0.0009552678\n",
      "Epoch [69/500], Batch [30/110], Train Loss: 0.0168, Val Loss: 0.0129, LR: 0.0009551503\n",
      "Epoch [69/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0108, LR: 0.0009550326\n",
      "Epoch [69/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0122, LR: 0.0009549148\n",
      "Epoch [69/500], Batch [60/110], Train Loss: 0.0225, Val Loss: 0.0115, LR: 0.0009547968\n",
      "Epoch [69/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0125, LR: 0.0009546787\n",
      "Epoch [69/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0111, LR: 0.0009545605\n",
      "Epoch [69/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0110, LR: 0.0009544420\n",
      "Epoch [69/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0112, LR: 0.0009543235\n",
      "Epoch [69/500], Batch [110/110], Train Loss: 0.0132, Val Loss: 0.0134, LR: 0.0009542048\n",
      "Epoch [70/500], Batch [10/110], Train Loss: 0.0675, Val Loss: 0.0125, LR: 0.0009540859\n",
      "Epoch [70/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0108, LR: 0.0009539669\n",
      "Epoch [70/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0120, LR: 0.0009538478\n",
      "Epoch [70/500], Batch [40/110], Train Loss: 0.0215, Val Loss: 0.0117, LR: 0.0009537285\n",
      "Epoch [70/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0109, LR: 0.0009536091\n",
      "Epoch [70/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0103, LR: 0.0009534895\n",
      "Epoch [70/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0104, LR: 0.0009533698\n",
      "Epoch [70/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0104, LR: 0.0009532499\n",
      "Epoch [70/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0104, LR: 0.0009531299\n",
      "Epoch [70/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0101, LR: 0.0009530097\n",
      "Epoch [70/500], Batch [110/110], Train Loss: 0.1019, Val Loss: 0.0155, LR: 0.0009528894\n",
      "Epoch [71/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0143, LR: 0.0009527689\n",
      "Epoch [71/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0108, LR: 0.0009526483\n",
      "Epoch [71/500], Batch [30/110], Train Loss: 0.0058, Val Loss: 0.0108, LR: 0.0009525276\n",
      "Epoch [71/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0108, LR: 0.0009524067\n",
      "Epoch [71/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0109, LR: 0.0009522856\n",
      "Epoch [71/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0108, LR: 0.0009521644\n",
      "Epoch [71/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0101, LR: 0.0009520431\n",
      "Epoch [71/500], Batch [80/110], Train Loss: 0.0270, Val Loss: 0.0107, LR: 0.0009519216\n",
      "Epoch [71/500], Batch [90/110], Train Loss: 0.0602, Val Loss: 0.0137, LR: 0.0009518000\n",
      "Epoch [71/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0118, LR: 0.0009516782\n",
      "Epoch [71/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0100, LR: 0.0009515563\n",
      "Confusion Matrix:\n",
      "[[625   6]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99049   0.99522       631\n",
      "           1    0.99314   1.00000   0.99656       869\n",
      "\n",
      "    accuracy                        0.99600      1500\n",
      "   macro avg    0.99657   0.99525   0.99589      1500\n",
      "weighted avg    0.99603   0.99600   0.99600      1500\n",
      "\n",
      "Total Errors: 6\n",
      "Index: 246, Predicted: 1, Actual: 0\n",
      "Index: 283, Predicted: 1, Actual: 0\n",
      "Index: 434, Predicted: 1, Actual: 0\n",
      "Index: 738, Predicted: 1, Actual: 0\n",
      "Index: 811, Predicted: 1, Actual: 0\n",
      "Epoch 71: OK- Accuracy: 0.99600, Precision: 0.99314, Recall: 1.00000, F1: 0.99656, ROC AUC: 0.99525, AUPR (PR-AUC): 0.99314, Sensitivity: 1.00000, Specificity: 0.99049, Far: 0.009508716323296355, False Positive Rate (FPR): 0.00951, False Negative Rate (FNR): 0.00000, Runtime: 0.047 sec , Memory Usage: 310.01 MB\n",
      "Epoch [72/500], Batch [10/110], Train Loss: 0.0357, Val Loss: 0.0092, LR: 0.0009514342\n",
      "Epoch [72/500], Batch [20/110], Train Loss: 0.2260, Val Loss: 0.0095, LR: 0.0009513120\n",
      "Epoch [72/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0110, LR: 0.0009511897\n",
      "Epoch [72/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0106, LR: 0.0009510672\n",
      "Epoch [72/500], Batch [50/110], Train Loss: 0.0235, Val Loss: 0.0094, LR: 0.0009509445\n",
      "Epoch [72/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0093, LR: 0.0009508217\n",
      "Epoch [72/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0121, LR: 0.0009506988\n",
      "Epoch [72/500], Batch [80/110], Train Loss: 0.0284, Val Loss: 0.0108, LR: 0.0009505757\n",
      "Epoch [72/500], Batch [90/110], Train Loss: 0.0390, Val Loss: 0.0131, LR: 0.0009504525\n",
      "Epoch [72/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0112, LR: 0.0009503291\n",
      "Epoch [72/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0095, LR: 0.0009502056\n",
      "Epoch [73/500], Batch [10/110], Train Loss: 0.0469, Val Loss: 0.0090, LR: 0.0009500819\n",
      "Epoch [73/500], Batch [20/110], Train Loss: 0.0171, Val Loss: 0.0110, LR: 0.0009499581\n",
      "Epoch [73/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0139, LR: 0.0009498342\n",
      "Epoch [73/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0105, LR: 0.0009497101\n",
      "Epoch [73/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0095, LR: 0.0009495858\n",
      "Epoch [73/500], Batch [60/110], Train Loss: 0.0019, Val Loss: 0.0091, LR: 0.0009494614\n",
      "Epoch [73/500], Batch [70/110], Train Loss: 0.0653, Val Loss: 0.0098, LR: 0.0009493369\n",
      "Epoch [73/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0120, LR: 0.0009492122\n",
      "Epoch [73/500], Batch [90/110], Train Loss: 0.0047, Val Loss: 0.0122, LR: 0.0009490874\n",
      "Epoch [73/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0093, LR: 0.0009489624\n",
      "Epoch [73/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0094, LR: 0.0009488373\n",
      "Epoch [74/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0140, LR: 0.0009487121\n",
      "Epoch [74/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0120, LR: 0.0009485866\n",
      "Epoch [74/500], Batch [30/110], Train Loss: 0.0355, Val Loss: 0.0097, LR: 0.0009484611\n",
      "Epoch [74/500], Batch [40/110], Train Loss: 0.2918, Val Loss: 0.0096, LR: 0.0009483354\n",
      "Epoch [74/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0104, LR: 0.0009482096\n",
      "Epoch [74/500], Batch [60/110], Train Loss: 0.0967, Val Loss: 0.0103, LR: 0.0009480836\n",
      "Epoch [74/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0094, LR: 0.0009479575\n",
      "Epoch [74/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0088, LR: 0.0009478312\n",
      "Epoch [74/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0104, LR: 0.0009477048\n",
      "Epoch [74/500], Batch [100/110], Train Loss: 0.0070, Val Loss: 0.0125, LR: 0.0009475782\n",
      "Epoch [74/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0090, LR: 0.0009474515\n",
      "Epoch [75/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0084, LR: 0.0009473247\n",
      "Epoch [75/500], Batch [20/110], Train Loss: 0.0433, Val Loss: 0.0108, LR: 0.0009471977\n",
      "Epoch [75/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0094, LR: 0.0009470705\n",
      "Epoch [75/500], Batch [40/110], Train Loss: 0.0175, Val Loss: 0.0089, LR: 0.0009469432\n",
      "Epoch [75/500], Batch [50/110], Train Loss: 0.0260, Val Loss: 0.0111, LR: 0.0009468158\n",
      "Epoch [75/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0109, LR: 0.0009466882\n",
      "Epoch [75/500], Batch [70/110], Train Loss: 0.0473, Val Loss: 0.0090, LR: 0.0009465605\n",
      "Epoch [75/500], Batch [80/110], Train Loss: 0.0115, Val Loss: 0.0092, LR: 0.0009464327\n",
      "Epoch [75/500], Batch [90/110], Train Loss: 0.0030, Val Loss: 0.0097, LR: 0.0009463047\n",
      "Epoch [75/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0110, LR: 0.0009461765\n",
      "Epoch [75/500], Batch [110/110], Train Loss: 0.0168, Val Loss: 0.0104, LR: 0.0009460482\n",
      "Epoch [76/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0114, LR: 0.0009459198\n",
      "Epoch [76/500], Batch [20/110], Train Loss: 0.0128, Val Loss: 0.0126, LR: 0.0009457912\n",
      "Epoch [76/500], Batch [30/110], Train Loss: 0.0493, Val Loss: 0.0109, LR: 0.0009456625\n",
      "Epoch [76/500], Batch [40/110], Train Loss: 0.0209, Val Loss: 0.0086, LR: 0.0009455336\n",
      "Epoch [76/500], Batch [50/110], Train Loss: 0.0484, Val Loss: 0.0087, LR: 0.0009454046\n",
      "Epoch [76/500], Batch [60/110], Train Loss: 0.0181, Val Loss: 0.0105, LR: 0.0009452755\n",
      "Epoch [76/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0112, LR: 0.0009451462\n",
      "Epoch [76/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0009450167\n",
      "Epoch [76/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0105, LR: 0.0009448871\n",
      "Epoch [76/500], Batch [100/110], Train Loss: 0.0422, Val Loss: 0.0087, LR: 0.0009447574\n",
      "Epoch [76/500], Batch [110/110], Train Loss: 0.0029, Val Loss: 0.0103, LR: 0.0009446275\n",
      "Epoch [77/500], Batch [10/110], Train Loss: 0.0015, Val Loss: 0.0094, LR: 0.0009444975\n",
      "Epoch [77/500], Batch [20/110], Train Loss: 0.0250, Val Loss: 0.0092, LR: 0.0009443674\n",
      "Epoch [77/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0101, LR: 0.0009442371\n",
      "Epoch [77/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0092, LR: 0.0009441066\n",
      "Epoch [77/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0102, LR: 0.0009439760\n",
      "Epoch [77/500], Batch [60/110], Train Loss: 0.0096, Val Loss: 0.0093, LR: 0.0009438453\n",
      "Epoch [77/500], Batch [70/110], Train Loss: 0.0266, Val Loss: 0.0084, LR: 0.0009437144\n",
      "Epoch [77/500], Batch [80/110], Train Loss: 0.1077, Val Loss: 0.0088, LR: 0.0009435834\n",
      "Epoch [77/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0097, LR: 0.0009434522\n",
      "Epoch [77/500], Batch [100/110], Train Loss: 0.0111, Val Loss: 0.0076, LR: 0.0009433209\n",
      "Epoch [77/500], Batch [110/110], Train Loss: 0.0051, Val Loss: 0.0119, LR: 0.0009431895\n",
      "Epoch [78/500], Batch [10/110], Train Loss: 0.0588, Val Loss: 0.0108, LR: 0.0009430579\n",
      "Epoch [78/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0009429262\n",
      "Epoch [78/500], Batch [30/110], Train Loss: 0.0110, Val Loss: 0.0098, LR: 0.0009427943\n",
      "Epoch [78/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0087, LR: 0.0009426623\n",
      "Epoch [78/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0083, LR: 0.0009425301\n",
      "Epoch [78/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0119, LR: 0.0009423978\n",
      "Epoch [78/500], Batch [70/110], Train Loss: 0.0120, Val Loss: 0.0107, LR: 0.0009422654\n",
      "Epoch [78/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0093, LR: 0.0009421328\n",
      "Epoch [78/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0009420000\n",
      "Epoch [78/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0084, LR: 0.0009418672\n",
      "Epoch [78/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0128, LR: 0.0009417342\n",
      "Epoch [79/500], Batch [10/110], Train Loss: 0.0089, Val Loss: 0.0139, LR: 0.0009416010\n",
      "Epoch [79/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0009414677\n",
      "Epoch [79/500], Batch [30/110], Train Loss: 0.0422, Val Loss: 0.0077, LR: 0.0009413343\n",
      "Epoch [79/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0009412007\n",
      "Epoch [79/500], Batch [50/110], Train Loss: 0.0040, Val Loss: 0.0110, LR: 0.0009410669\n",
      "Epoch [79/500], Batch [60/110], Train Loss: 0.0293, Val Loss: 0.0078, LR: 0.0009409331\n",
      "Epoch [79/500], Batch [70/110], Train Loss: 0.0094, Val Loss: 0.0078, LR: 0.0009407990\n",
      "Epoch [79/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0081, LR: 0.0009406649\n",
      "Epoch [79/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0009405306\n",
      "Epoch [79/500], Batch [100/110], Train Loss: 0.0381, Val Loss: 0.0085, LR: 0.0009403962\n",
      "Epoch [79/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0081, LR: 0.0009402616\n",
      "Epoch [80/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0101, LR: 0.0009401268\n",
      "Epoch [80/500], Batch [20/110], Train Loss: 0.0189, Val Loss: 0.0100, LR: 0.0009399920\n",
      "Epoch [80/500], Batch [30/110], Train Loss: 0.0328, Val Loss: 0.0077, LR: 0.0009398570\n",
      "Epoch [80/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0102, LR: 0.0009397218\n",
      "Epoch [80/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0082, LR: 0.0009395865\n",
      "Epoch [80/500], Batch [60/110], Train Loss: 0.0213, Val Loss: 0.0083, LR: 0.0009394511\n",
      "Epoch [80/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0080, LR: 0.0009393155\n",
      "Epoch [80/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0084, LR: 0.0009391798\n",
      "Epoch [80/500], Batch [90/110], Train Loss: 0.0211, Val Loss: 0.0089, LR: 0.0009390439\n",
      "Epoch [80/500], Batch [100/110], Train Loss: 0.0256, Val Loss: 0.0083, LR: 0.0009389079\n",
      "Epoch [80/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0009387718\n",
      "Epoch [81/500], Batch [10/110], Train Loss: 0.0245, Val Loss: 0.0082, LR: 0.0009386355\n",
      "Epoch [81/500], Batch [20/110], Train Loss: 0.0168, Val Loss: 0.0093, LR: 0.0009384991\n",
      "Epoch [81/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0081, LR: 0.0009383625\n",
      "Epoch [81/500], Batch [40/110], Train Loss: 0.0744, Val Loss: 0.0076, LR: 0.0009382258\n",
      "Epoch [81/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0093, LR: 0.0009380890\n",
      "Epoch [81/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0097, LR: 0.0009379520\n",
      "Epoch [81/500], Batch [70/110], Train Loss: 0.0063, Val Loss: 0.0083, LR: 0.0009378149\n",
      "Epoch [81/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0009376776\n",
      "Epoch [81/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0098, LR: 0.0009375402\n",
      "Epoch [81/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0079, LR: 0.0009374026\n",
      "Epoch [81/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0078, LR: 0.0009372649\n",
      "Confusion Matrix:\n",
      "[[628   3]\n",
      " [ 11 858]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98279   0.99525   0.98898       631\n",
      "           1    0.99652   0.98734   0.99191       869\n",
      "\n",
      "    accuracy                        0.99067      1500\n",
      "   macro avg    0.98965   0.99129   0.99044      1500\n",
      "weighted avg    0.99074   0.99067   0.99067      1500\n",
      "\n",
      "Total Errors: 14\n",
      "Index: 88, Predicted: 0, Actual: 1\n",
      "Index: 283, Predicted: 1, Actual: 0\n",
      "Index: 434, Predicted: 1, Actual: 0\n",
      "Index: 477, Predicted: 0, Actual: 1\n",
      "Index: 606, Predicted: 0, Actual: 1\n",
      "Epoch 81: OK- Accuracy: 0.99067, Precision: 0.99652, Recall: 0.98734, F1: 0.99191, ROC AUC: 0.99129, AUPR (PR-AUC): 0.99123, Sensitivity: 0.98734, Specificity: 0.99525, Far: 0.004754358161648178, False Positive Rate (FPR): 0.00475, False Negative Rate (FNR): 0.01266, Runtime: 0.047 sec , Memory Usage: 310.01 MB\n",
      "Epoch [82/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0074, LR: 0.0009371271\n",
      "Epoch [82/500], Batch [20/110], Train Loss: 0.0181, Val Loss: 0.0108, LR: 0.0009369891\n",
      "Epoch [82/500], Batch [30/110], Train Loss: 0.0448, Val Loss: 0.0081, LR: 0.0009368510\n",
      "Epoch [82/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0073, LR: 0.0009367127\n",
      "Epoch [82/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0085, LR: 0.0009365743\n",
      "Epoch [82/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0009364358\n",
      "Epoch [82/500], Batch [70/110], Train Loss: 0.0174, Val Loss: 0.0115, LR: 0.0009362971\n",
      "Epoch [82/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0084, LR: 0.0009361583\n",
      "Epoch [82/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0079, LR: 0.0009360193\n",
      "Epoch [82/500], Batch [100/110], Train Loss: 0.0032, Val Loss: 0.0077, LR: 0.0009358802\n",
      "Epoch [82/500], Batch [110/110], Train Loss: 0.0215, Val Loss: 0.0081, LR: 0.0009357410\n",
      "Epoch [83/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0009356016\n",
      "Epoch [83/500], Batch [20/110], Train Loss: 0.0268, Val Loss: 0.0073, LR: 0.0009354620\n",
      "Epoch [83/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0075, LR: 0.0009353224\n",
      "Epoch [83/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0077, LR: 0.0009351826\n",
      "Epoch [83/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0009350426\n",
      "Epoch [83/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0089, LR: 0.0009349025\n",
      "Epoch [83/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0086, LR: 0.0009347623\n",
      "Epoch [83/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0080, LR: 0.0009346219\n",
      "Epoch [83/500], Batch [90/110], Train Loss: 0.0331, Val Loss: 0.0079, LR: 0.0009344814\n",
      "Epoch [83/500], Batch [100/110], Train Loss: 0.0023, Val Loss: 0.0095, LR: 0.0009343408\n",
      "Epoch [83/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0076, LR: 0.0009342000\n",
      "Epoch [84/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0074, LR: 0.0009340591\n",
      "Epoch [84/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0125, LR: 0.0009339180\n",
      "Epoch [84/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0113, LR: 0.0009337768\n",
      "Epoch [84/500], Batch [40/110], Train Loss: 0.0200, Val Loss: 0.0079, LR: 0.0009336354\n",
      "Epoch [84/500], Batch [50/110], Train Loss: 0.0333, Val Loss: 0.0076, LR: 0.0009334940\n",
      "Epoch [84/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0078, LR: 0.0009333523\n",
      "Epoch [84/500], Batch [70/110], Train Loss: 0.0418, Val Loss: 0.0080, LR: 0.0009332106\n",
      "Epoch [84/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0009330687\n",
      "Epoch [84/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0083, LR: 0.0009329266\n",
      "Epoch [84/500], Batch [100/110], Train Loss: 0.0196, Val Loss: 0.0085, LR: 0.0009327844\n",
      "Epoch [84/500], Batch [110/110], Train Loss: 0.0173, Val Loss: 0.0074, LR: 0.0009326421\n",
      "Epoch [85/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0074, LR: 0.0009324996\n",
      "Epoch [85/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0076, LR: 0.0009323570\n",
      "Epoch [85/500], Batch [30/110], Train Loss: 0.0392, Val Loss: 0.0076, LR: 0.0009322143\n",
      "Epoch [85/500], Batch [40/110], Train Loss: 0.0018, Val Loss: 0.0080, LR: 0.0009320714\n",
      "Epoch [85/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0152, LR: 0.0009319284\n",
      "Epoch [85/500], Batch [60/110], Train Loss: 0.0140, Val Loss: 0.0114, LR: 0.0009317852\n",
      "Epoch [85/500], Batch [70/110], Train Loss: 0.0192, Val Loss: 0.0090, LR: 0.0009316419\n",
      "Epoch [85/500], Batch [80/110], Train Loss: 0.0353, Val Loss: 0.0070, LR: 0.0009314985\n",
      "Epoch [85/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0082, LR: 0.0009313549\n",
      "Epoch [85/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0074, LR: 0.0009312112\n",
      "Epoch [85/500], Batch [110/110], Train Loss: 0.0196, Val Loss: 0.0076, LR: 0.0009310673\n",
      "Epoch [86/500], Batch [10/110], Train Loss: 0.0023, Val Loss: 0.0073, LR: 0.0009309233\n",
      "Epoch [86/500], Batch [20/110], Train Loss: 0.0202, Val Loss: 0.0079, LR: 0.0009307792\n",
      "Epoch [86/500], Batch [30/110], Train Loss: 0.1075, Val Loss: 0.0116, LR: 0.0009306349\n",
      "Epoch [86/500], Batch [40/110], Train Loss: 0.0463, Val Loss: 0.0083, LR: 0.0009304905\n",
      "Epoch [86/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0009303459\n",
      "Epoch [86/500], Batch [60/110], Train Loss: 0.0022, Val Loss: 0.0093, LR: 0.0009302012\n",
      "Epoch [86/500], Batch [70/110], Train Loss: 0.0542, Val Loss: 0.0082, LR: 0.0009300564\n",
      "Epoch [86/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0071, LR: 0.0009299114\n",
      "Epoch [86/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0088, LR: 0.0009297663\n",
      "Epoch [86/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0115, LR: 0.0009296211\n",
      "Epoch [86/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0096, LR: 0.0009294757\n",
      "Epoch [87/500], Batch [10/110], Train Loss: 0.0275, Val Loss: 0.0076, LR: 0.0009293302\n",
      "Epoch [87/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0087, LR: 0.0009291845\n",
      "Epoch [87/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0009290387\n",
      "Epoch [87/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0074, LR: 0.0009288928\n",
      "Epoch [87/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0087, LR: 0.0009287467\n",
      "Epoch [87/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0108, LR: 0.0009286005\n",
      "Epoch [87/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0090, LR: 0.0009284541\n",
      "Epoch [87/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0072, LR: 0.0009283076\n",
      "Epoch [87/500], Batch [90/110], Train Loss: 0.0385, Val Loss: 0.0067, LR: 0.0009281610\n",
      "Epoch [87/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0089, LR: 0.0009280142\n",
      "Epoch [87/500], Batch [110/110], Train Loss: 0.0324, Val Loss: 0.0086, LR: 0.0009278673\n",
      "Epoch [88/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0073, LR: 0.0009277203\n",
      "Epoch [88/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0009275731\n",
      "Epoch [88/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0072, LR: 0.0009274258\n",
      "Epoch [88/500], Batch [40/110], Train Loss: 0.0059, Val Loss: 0.0095, LR: 0.0009272783\n",
      "Epoch [88/500], Batch [50/110], Train Loss: 0.0112, Val Loss: 0.0101, LR: 0.0009271307\n",
      "Epoch [88/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0071, LR: 0.0009269830\n",
      "Epoch [88/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0072, LR: 0.0009268351\n",
      "Epoch [88/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0088, LR: 0.0009266871\n",
      "Epoch [88/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0009265390\n",
      "Epoch [88/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0064, LR: 0.0009263907\n",
      "Epoch [88/500], Batch [110/110], Train Loss: 0.0634, Val Loss: 0.0063, LR: 0.0009262423\n",
      "Epoch [89/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0064, LR: 0.0009260937\n",
      "Epoch [89/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0066, LR: 0.0009259450\n",
      "Epoch [89/500], Batch [30/110], Train Loss: 0.0418, Val Loss: 0.0072, LR: 0.0009257962\n",
      "Epoch [89/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0084, LR: 0.0009256472\n",
      "Epoch [89/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0071, LR: 0.0009254981\n",
      "Epoch [89/500], Batch [60/110], Train Loss: 0.0645, Val Loss: 0.0069, LR: 0.0009253489\n",
      "Epoch [89/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0065, LR: 0.0009251995\n",
      "Epoch [89/500], Batch [80/110], Train Loss: 0.0153, Val Loss: 0.0082, LR: 0.0009250500\n",
      "Epoch [89/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0073, LR: 0.0009249003\n",
      "Epoch [89/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0009247505\n",
      "Epoch [89/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0009246006\n",
      "Epoch [90/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0070, LR: 0.0009244505\n",
      "Epoch [90/500], Batch [20/110], Train Loss: 0.0080, Val Loss: 0.0066, LR: 0.0009243003\n",
      "Epoch [90/500], Batch [30/110], Train Loss: 0.0020, Val Loss: 0.0075, LR: 0.0009241500\n",
      "Epoch [90/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0117, LR: 0.0009239995\n",
      "Epoch [90/500], Batch [50/110], Train Loss: 0.0126, Val Loss: 0.0139, LR: 0.0009238489\n",
      "Epoch [90/500], Batch [60/110], Train Loss: 0.0155, Val Loss: 0.0091, LR: 0.0009236981\n",
      "Epoch [90/500], Batch [70/110], Train Loss: 0.0252, Val Loss: 0.0074, LR: 0.0009235472\n",
      "Epoch [90/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0068, LR: 0.0009233962\n",
      "Epoch [90/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0069, LR: 0.0009232451\n",
      "Epoch [90/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0009230938\n",
      "Epoch [90/500], Batch [110/110], Train Loss: 0.1245, Val Loss: 0.0093, LR: 0.0009229423\n",
      "Epoch [91/500], Batch [10/110], Train Loss: 0.0173, Val Loss: 0.0069, LR: 0.0009227908\n",
      "Epoch [91/500], Batch [20/110], Train Loss: 0.0021, Val Loss: 0.0074, LR: 0.0009226390\n",
      "Epoch [91/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0079, LR: 0.0009224872\n",
      "Epoch [91/500], Batch [40/110], Train Loss: 0.0245, Val Loss: 0.0072, LR: 0.0009223352\n",
      "Epoch [91/500], Batch [50/110], Train Loss: 0.0057, Val Loss: 0.0069, LR: 0.0009221831\n",
      "Epoch [91/500], Batch [60/110], Train Loss: 0.0143, Val Loss: 0.0083, LR: 0.0009220309\n",
      "Epoch [91/500], Batch [70/110], Train Loss: 0.0352, Val Loss: 0.0071, LR: 0.0009218785\n",
      "Epoch [91/500], Batch [80/110], Train Loss: 0.0305, Val Loss: 0.0066, LR: 0.0009217260\n",
      "Epoch [91/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0066, LR: 0.0009215733\n",
      "Epoch [91/500], Batch [100/110], Train Loss: 0.0093, Val Loss: 0.0077, LR: 0.0009214205\n",
      "Epoch [91/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0009212676\n",
      "Confusion Matrix:\n",
      "[[624   7]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.98891   0.99442       631\n",
      "           1    0.99201   1.00000   0.99599       869\n",
      "\n",
      "    accuracy                        0.99533      1500\n",
      "   macro avg    0.99600   0.99445   0.99521      1500\n",
      "weighted avg    0.99537   0.99533   0.99533      1500\n",
      "\n",
      "Total Errors: 7\n",
      "Index: 246, Predicted: 1, Actual: 0\n",
      "Index: 265, Predicted: 1, Actual: 0\n",
      "Index: 283, Predicted: 1, Actual: 0\n",
      "Index: 434, Predicted: 1, Actual: 0\n",
      "Index: 738, Predicted: 1, Actual: 0\n",
      "Epoch 91: OK- Accuracy: 0.99533, Precision: 0.99201, Recall: 1.00000, F1: 0.99599, ROC AUC: 0.99445, AUPR (PR-AUC): 0.99201, Sensitivity: 1.00000, Specificity: 0.98891, Far: 0.011093502377179081, False Positive Rate (FPR): 0.01109, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 310.01 MB\n",
      "Epoch [92/500], Batch [10/110], Train Loss: 0.0032, Val Loss: 0.0069, LR: 0.0009211145\n",
      "Epoch [92/500], Batch [20/110], Train Loss: 0.0498, Val Loss: 0.0072, LR: 0.0009209613\n",
      "Epoch [92/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0079, LR: 0.0009208080\n",
      "Epoch [92/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0074, LR: 0.0009206545\n",
      "Epoch [92/500], Batch [50/110], Train Loss: 0.0572, Val Loss: 0.0072, LR: 0.0009205009\n",
      "Epoch [92/500], Batch [60/110], Train Loss: 0.0188, Val Loss: 0.0074, LR: 0.0009203471\n",
      "Epoch [92/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0079, LR: 0.0009201933\n",
      "Epoch [92/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0078, LR: 0.0009200392\n",
      "Epoch [92/500], Batch [90/110], Train Loss: 0.0120, Val Loss: 0.0073, LR: 0.0009198851\n",
      "Epoch [92/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0068, LR: 0.0009197308\n",
      "Epoch [92/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0104, LR: 0.0009195764\n",
      "Epoch [93/500], Batch [10/110], Train Loss: 0.0381, Val Loss: 0.0071, LR: 0.0009194218\n",
      "Epoch [93/500], Batch [20/110], Train Loss: 0.0026, Val Loss: 0.0071, LR: 0.0009192671\n",
      "Epoch [93/500], Batch [30/110], Train Loss: 0.0123, Val Loss: 0.0073, LR: 0.0009191123\n",
      "Epoch [93/500], Batch [40/110], Train Loss: 0.0350, Val Loss: 0.0070, LR: 0.0009189573\n",
      "Epoch [93/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0076, LR: 0.0009188022\n",
      "Epoch [93/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0072, LR: 0.0009186470\n",
      "Epoch [93/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0073, LR: 0.0009184916\n",
      "Epoch [93/500], Batch [80/110], Train Loss: 0.0102, Val Loss: 0.0077, LR: 0.0009183361\n",
      "Epoch [93/500], Batch [90/110], Train Loss: 0.0706, Val Loss: 0.0062, LR: 0.0009181805\n",
      "Epoch [93/500], Batch [100/110], Train Loss: 0.0020, Val Loss: 0.0089, LR: 0.0009180247\n",
      "Epoch [93/500], Batch [110/110], Train Loss: 0.0359, Val Loss: 0.0071, LR: 0.0009178688\n",
      "Epoch [94/500], Batch [10/110], Train Loss: 0.2376, Val Loss: 0.0059, LR: 0.0009177128\n",
      "Epoch [94/500], Batch [20/110], Train Loss: 0.0207, Val Loss: 0.0059, LR: 0.0009175566\n",
      "Epoch [94/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0064, LR: 0.0009174003\n",
      "Epoch [94/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0090, LR: 0.0009172439\n",
      "Epoch [94/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0073, LR: 0.0009170873\n",
      "Epoch [94/500], Batch [60/110], Train Loss: 0.0326, Val Loss: 0.0068, LR: 0.0009169306\n",
      "Epoch [94/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0068, LR: 0.0009167737\n",
      "Epoch [94/500], Batch [80/110], Train Loss: 0.0485, Val Loss: 0.0063, LR: 0.0009166167\n",
      "Epoch [94/500], Batch [90/110], Train Loss: 0.0133, Val Loss: 0.0071, LR: 0.0009164596\n",
      "Epoch [94/500], Batch [100/110], Train Loss: 0.0415, Val Loss: 0.0064, LR: 0.0009163024\n",
      "Epoch [94/500], Batch [110/110], Train Loss: 0.0237, Val Loss: 0.0061, LR: 0.0009161450\n",
      "Epoch [95/500], Batch [10/110], Train Loss: 0.0364, Val Loss: 0.0073, LR: 0.0009159875\n",
      "Epoch [95/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0077, LR: 0.0009158298\n",
      "Epoch [95/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0104, LR: 0.0009156720\n",
      "Epoch [95/500], Batch [40/110], Train Loss: 0.0041, Val Loss: 0.0064, LR: 0.0009155141\n",
      "Epoch [95/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0066, LR: 0.0009153560\n",
      "Epoch [95/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0080, LR: 0.0009151978\n",
      "Epoch [95/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0066, LR: 0.0009150395\n",
      "Epoch [95/500], Batch [80/110], Train Loss: 0.0353, Val Loss: 0.0064, LR: 0.0009148811\n",
      "Epoch [95/500], Batch [90/110], Train Loss: 0.0076, Val Loss: 0.0071, LR: 0.0009147225\n",
      "Epoch [95/500], Batch [100/110], Train Loss: 0.0222, Val Loss: 0.0062, LR: 0.0009145637\n",
      "Epoch [95/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0009144049\n",
      "Epoch [96/500], Batch [10/110], Train Loss: 0.0129, Val Loss: 0.0065, LR: 0.0009142459\n",
      "Epoch [96/500], Batch [20/110], Train Loss: 0.0071, Val Loss: 0.0106, LR: 0.0009140868\n",
      "Epoch [96/500], Batch [30/110], Train Loss: 0.0272, Val Loss: 0.0062, LR: 0.0009139275\n",
      "Epoch [96/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0061, LR: 0.0009137681\n",
      "Epoch [96/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0068, LR: 0.0009136086\n",
      "Epoch [96/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0060, LR: 0.0009134489\n",
      "Epoch [96/500], Batch [70/110], Train Loss: 0.0788, Val Loss: 0.0060, LR: 0.0009132891\n",
      "Epoch [96/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0009131292\n",
      "Epoch [96/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0060, LR: 0.0009129692\n",
      "Epoch [96/500], Batch [100/110], Train Loss: 0.0076, Val Loss: 0.0073, LR: 0.0009128090\n",
      "Epoch [96/500], Batch [110/110], Train Loss: 0.0092, Val Loss: 0.0058, LR: 0.0009126486\n",
      "Epoch [97/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0057, LR: 0.0009124882\n",
      "Epoch [97/500], Batch [20/110], Train Loss: 0.0141, Val Loss: 0.0075, LR: 0.0009123276\n",
      "Epoch [97/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0064, LR: 0.0009121669\n",
      "Epoch [97/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0060, LR: 0.0009120060\n",
      "Epoch [97/500], Batch [50/110], Train Loss: 0.0016, Val Loss: 0.0085, LR: 0.0009118450\n",
      "Epoch [97/500], Batch [60/110], Train Loss: 0.0125, Val Loss: 0.0074, LR: 0.0009116839\n",
      "Epoch [97/500], Batch [70/110], Train Loss: 0.0164, Val Loss: 0.0066, LR: 0.0009115226\n",
      "Epoch [97/500], Batch [80/110], Train Loss: 0.0451, Val Loss: 0.0063, LR: 0.0009113613\n",
      "Epoch [97/500], Batch [90/110], Train Loss: 0.0382, Val Loss: 0.0071, LR: 0.0009111997\n",
      "Epoch [97/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0071, LR: 0.0009110381\n",
      "Epoch [97/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0069, LR: 0.0009108763\n",
      "Epoch [98/500], Batch [10/110], Train Loss: 0.0009, Val Loss: 0.0068, LR: 0.0009107144\n",
      "Epoch [98/500], Batch [20/110], Train Loss: 0.0172, Val Loss: 0.0068, LR: 0.0009105523\n",
      "Epoch [98/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0085, LR: 0.0009103901\n",
      "Epoch [98/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0082, LR: 0.0009102278\n",
      "Epoch [98/500], Batch [50/110], Train Loss: 0.0086, Val Loss: 0.0076, LR: 0.0009100654\n",
      "Epoch [98/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0062, LR: 0.0009099028\n",
      "Epoch [98/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0057, LR: 0.0009097401\n",
      "Epoch [98/500], Batch [80/110], Train Loss: 0.0272, Val Loss: 0.0062, LR: 0.0009095773\n",
      "Epoch [98/500], Batch [90/110], Train Loss: 0.0212, Val Loss: 0.0057, LR: 0.0009094143\n",
      "Epoch [98/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0055, LR: 0.0009092512\n",
      "Epoch [98/500], Batch [110/110], Train Loss: 0.0182, Val Loss: 0.0062, LR: 0.0009090879\n",
      "Epoch [99/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0063, LR: 0.0009089246\n",
      "Epoch [99/500], Batch [20/110], Train Loss: 0.0115, Val Loss: 0.0063, LR: 0.0009087611\n",
      "Epoch [99/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0055, LR: 0.0009085974\n",
      "Epoch [99/500], Batch [40/110], Train Loss: 0.0054, Val Loss: 0.0059, LR: 0.0009084337\n",
      "Epoch [99/500], Batch [50/110], Train Loss: 0.0014, Val Loss: 0.0069, LR: 0.0009082698\n",
      "Epoch [99/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0060, LR: 0.0009081057\n",
      "Epoch [99/500], Batch [70/110], Train Loss: 0.0064, Val Loss: 0.0061, LR: 0.0009079416\n",
      "Epoch [99/500], Batch [80/110], Train Loss: 0.0145, Val Loss: 0.0067, LR: 0.0009077773\n",
      "Epoch [99/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0056, LR: 0.0009076129\n",
      "Epoch [99/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0054, LR: 0.0009074483\n",
      "Epoch [99/500], Batch [110/110], Train Loss: 0.0332, Val Loss: 0.0071, LR: 0.0009072836\n",
      "Epoch [100/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0100, LR: 0.0009071188\n",
      "Epoch [100/500], Batch [20/110], Train Loss: 0.0205, Val Loss: 0.0052, LR: 0.0009069538\n",
      "Epoch [100/500], Batch [30/110], Train Loss: 0.0225, Val Loss: 0.0065, LR: 0.0009067888\n",
      "Epoch [100/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0058, LR: 0.0009066236\n",
      "Epoch [100/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0080, LR: 0.0009064582\n",
      "Epoch [100/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0071, LR: 0.0009062927\n",
      "Epoch [100/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0009061271\n",
      "Epoch [100/500], Batch [80/110], Train Loss: 0.0087, Val Loss: 0.0057, LR: 0.0009059614\n",
      "Epoch [100/500], Batch [90/110], Train Loss: 0.0022, Val Loss: 0.0053, LR: 0.0009057955\n",
      "Epoch [100/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0053, LR: 0.0009056295\n",
      "Epoch [100/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0067, LR: 0.0009054634\n",
      "Epoch [101/500], Batch [10/110], Train Loss: 0.0186, Val Loss: 0.0092, LR: 0.0009052972\n",
      "Epoch [101/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0064, LR: 0.0009051308\n",
      "Epoch [101/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0009049642\n",
      "Epoch [101/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0009047976\n",
      "Epoch [101/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0052, LR: 0.0009046308\n",
      "Epoch [101/500], Batch [60/110], Train Loss: 0.0160, Val Loss: 0.0051, LR: 0.0009044639\n",
      "Epoch [101/500], Batch [70/110], Train Loss: 0.0344, Val Loss: 0.0053, LR: 0.0009042969\n",
      "Epoch [101/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0055, LR: 0.0009041297\n",
      "Epoch [101/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0063, LR: 0.0009039624\n",
      "Epoch [101/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0059, LR: 0.0009037950\n",
      "Epoch [101/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0053, LR: 0.0009036274\n",
      "Confusion Matrix:\n",
      "[[628   3]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99525   0.99762       631\n",
      "           1    0.99656   1.00000   0.99828       869\n",
      "\n",
      "    accuracy                        0.99800      1500\n",
      "   macro avg    0.99828   0.99762   0.99795      1500\n",
      "weighted avg    0.99801   0.99800   0.99800      1500\n",
      "\n",
      "Total Errors: 3\n",
      "Index: 283, Predicted: 1, Actual: 0\n",
      "Index: 434, Predicted: 1, Actual: 0\n",
      "Index: 738, Predicted: 1, Actual: 0\n",
      "Epoch 101: OK- Accuracy: 0.99800, Precision: 0.99656, Recall: 1.00000, F1: 0.99828, ROC AUC: 0.99762, AUPR (PR-AUC): 0.99656, Sensitivity: 1.00000, Specificity: 0.99525, Far: 0.004754358161648178, False Positive Rate (FPR): 0.00475, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 310.01 MB\n",
      "Epoch [102/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0068, LR: 0.0009034597\n",
      "Epoch [102/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0079, LR: 0.0009032919\n",
      "Epoch [102/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0062, LR: 0.0009031239\n",
      "Epoch [102/500], Batch [40/110], Train Loss: 0.0017, Val Loss: 0.0064, LR: 0.0009029559\n",
      "Epoch [102/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0061, LR: 0.0009027876\n",
      "Epoch [102/500], Batch [60/110], Train Loss: 0.0022, Val Loss: 0.0058, LR: 0.0009026193\n",
      "Epoch [102/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0059, LR: 0.0009024508\n",
      "Epoch [102/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0076, LR: 0.0009022822\n",
      "Epoch [102/500], Batch [90/110], Train Loss: 0.0132, Val Loss: 0.0054, LR: 0.0009021135\n",
      "Epoch [102/500], Batch [100/110], Train Loss: 0.0333, Val Loss: 0.0053, LR: 0.0009019446\n",
      "Epoch [102/500], Batch [110/110], Train Loss: 0.0058, Val Loss: 0.0054, LR: 0.0009017757\n",
      "Epoch [103/500], Batch [10/110], Train Loss: 0.0373, Val Loss: 0.0052, LR: 0.0009016065\n",
      "Epoch [103/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0052, LR: 0.0009014373\n",
      "Epoch [103/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0052, LR: 0.0009012679\n",
      "Epoch [103/500], Batch [40/110], Train Loss: 0.0323, Val Loss: 0.0053, LR: 0.0009010984\n",
      "Epoch [103/500], Batch [50/110], Train Loss: 0.0108, Val Loss: 0.0057, LR: 0.0009009288\n",
      "Epoch [103/500], Batch [60/110], Train Loss: 0.0419, Val Loss: 0.0052, LR: 0.0009007590\n",
      "Epoch [103/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0009005891\n",
      "Epoch [103/500], Batch [80/110], Train Loss: 0.0309, Val Loss: 0.0094, LR: 0.0009004191\n",
      "Epoch [103/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0078, LR: 0.0009002489\n",
      "Epoch [103/500], Batch [100/110], Train Loss: 0.0330, Val Loss: 0.0062, LR: 0.0009000787\n",
      "Epoch [103/500], Batch [110/110], Train Loss: 0.0314, Val Loss: 0.0056, LR: 0.0008999082\n",
      "Epoch [104/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0060, LR: 0.0008997377\n",
      "Epoch [104/500], Batch [20/110], Train Loss: 0.0010, Val Loss: 0.0072, LR: 0.0008995670\n",
      "Epoch [104/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0008993962\n",
      "Epoch [104/500], Batch [40/110], Train Loss: 0.0202, Val Loss: 0.0083, LR: 0.0008992253\n",
      "Epoch [104/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0008990543\n",
      "Epoch [104/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0061, LR: 0.0008988831\n",
      "Epoch [104/500], Batch [70/110], Train Loss: 0.0177, Val Loss: 0.0055, LR: 0.0008987118\n",
      "Epoch [104/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0008985403\n",
      "Epoch [104/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0058, LR: 0.0008983688\n",
      "Epoch [104/500], Batch [100/110], Train Loss: 0.0107, Val Loss: 0.0055, LR: 0.0008981971\n",
      "Epoch [104/500], Batch [110/110], Train Loss: 0.0017, Val Loss: 0.0059, LR: 0.0008980252\n",
      "Epoch [105/500], Batch [10/110], Train Loss: 0.0373, Val Loss: 0.0068, LR: 0.0008978533\n",
      "Epoch [105/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0072, LR: 0.0008976812\n",
      "Epoch [105/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0008975090\n",
      "Epoch [105/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0053, LR: 0.0008973367\n",
      "Epoch [105/500], Batch [50/110], Train Loss: 0.1317, Val Loss: 0.0077, LR: 0.0008971642\n",
      "Epoch [105/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0008969916\n",
      "Epoch [105/500], Batch [70/110], Train Loss: 0.0210, Val Loss: 0.0064, LR: 0.0008968189\n",
      "Epoch [105/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0053, LR: 0.0008966460\n",
      "Epoch [105/500], Batch [90/110], Train Loss: 0.0017, Val Loss: 0.0053, LR: 0.0008964731\n",
      "Epoch [105/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0008963000\n",
      "Epoch [105/500], Batch [110/110], Train Loss: 0.0266, Val Loss: 0.0054, LR: 0.0008961267\n",
      "Epoch [106/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0050, LR: 0.0008959534\n",
      "Epoch [106/500], Batch [20/110], Train Loss: 0.0268, Val Loss: 0.0050, LR: 0.0008957799\n",
      "Epoch [106/500], Batch [30/110], Train Loss: 0.0105, Val Loss: 0.0065, LR: 0.0008956063\n",
      "Epoch [106/500], Batch [40/110], Train Loss: 0.0010, Val Loss: 0.0052, LR: 0.0008954325\n",
      "Epoch [106/500], Batch [50/110], Train Loss: 0.0252, Val Loss: 0.0049, LR: 0.0008952587\n",
      "Epoch [106/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0072, LR: 0.0008950847\n",
      "Epoch [106/500], Batch [70/110], Train Loss: 0.0111, Val Loss: 0.0052, LR: 0.0008949105\n",
      "Epoch [106/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0008947363\n",
      "Epoch [106/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0051, LR: 0.0008945619\n",
      "Epoch [106/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0008943874\n",
      "Epoch [106/500], Batch [110/110], Train Loss: 0.0601, Val Loss: 0.0065, LR: 0.0008942128\n",
      "Epoch [107/500], Batch [10/110], Train Loss: 0.0072, Val Loss: 0.0057, LR: 0.0008940380\n",
      "Epoch [107/500], Batch [20/110], Train Loss: 0.0129, Val Loss: 0.0051, LR: 0.0008938631\n",
      "Epoch [107/500], Batch [30/110], Train Loss: 0.0246, Val Loss: 0.0050, LR: 0.0008936881\n",
      "Epoch [107/500], Batch [40/110], Train Loss: 0.0072, Val Loss: 0.0050, LR: 0.0008935130\n",
      "Epoch [107/500], Batch [50/110], Train Loss: 0.0096, Val Loss: 0.0062, LR: 0.0008933377\n",
      "Epoch [107/500], Batch [60/110], Train Loss: 0.0243, Val Loss: 0.0062, LR: 0.0008931623\n",
      "Epoch [107/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0058, LR: 0.0008929868\n",
      "Epoch [107/500], Batch [80/110], Train Loss: 0.0465, Val Loss: 0.0052, LR: 0.0008928111\n",
      "Epoch [107/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0051, LR: 0.0008926354\n",
      "Epoch [107/500], Batch [100/110], Train Loss: 0.0277, Val Loss: 0.0059, LR: 0.0008924595\n",
      "Epoch [107/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0054, LR: 0.0008922835\n",
      "Epoch [108/500], Batch [10/110], Train Loss: 0.0018, Val Loss: 0.0073, LR: 0.0008921073\n",
      "Epoch [108/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0008919310\n",
      "Epoch [108/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0008917546\n",
      "Epoch [108/500], Batch [40/110], Train Loss: 0.0129, Val Loss: 0.0063, LR: 0.0008915781\n",
      "Epoch [108/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0063, LR: 0.0008914014\n",
      "Epoch [108/500], Batch [60/110], Train Loss: 0.0267, Val Loss: 0.0074, LR: 0.0008912246\n",
      "Epoch [108/500], Batch [70/110], Train Loss: 0.0122, Val Loss: 0.0072, LR: 0.0008910477\n",
      "Epoch [108/500], Batch [80/110], Train Loss: 0.0055, Val Loss: 0.0053, LR: 0.0008908707\n",
      "Epoch [108/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0051, LR: 0.0008906935\n",
      "Epoch [108/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0052, LR: 0.0008905163\n",
      "Epoch [108/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0008903388\n",
      "Epoch [109/500], Batch [10/110], Train Loss: 0.0084, Val Loss: 0.0067, LR: 0.0008901613\n",
      "Epoch [109/500], Batch [20/110], Train Loss: 0.0199, Val Loss: 0.0050, LR: 0.0008899836\n",
      "Epoch [109/500], Batch [30/110], Train Loss: 0.0034, Val Loss: 0.0062, LR: 0.0008898058\n",
      "Epoch [109/500], Batch [40/110], Train Loss: 0.0198, Val Loss: 0.0064, LR: 0.0008896279\n",
      "Epoch [109/500], Batch [50/110], Train Loss: 0.0489, Val Loss: 0.0081, LR: 0.0008894499\n",
      "Epoch [109/500], Batch [60/110], Train Loss: 0.0431, Val Loss: 0.0063, LR: 0.0008892717\n",
      "Epoch [109/500], Batch [70/110], Train Loss: 0.0038, Val Loss: 0.0051, LR: 0.0008890934\n",
      "Epoch [109/500], Batch [80/110], Train Loss: 0.0178, Val Loss: 0.0057, LR: 0.0008889150\n",
      "Epoch [109/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0054, LR: 0.0008887365\n",
      "Epoch [109/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0053, LR: 0.0008885578\n",
      "Epoch [109/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0050, LR: 0.0008883790\n",
      "Epoch [110/500], Batch [10/110], Train Loss: 0.0306, Val Loss: 0.0057, LR: 0.0008882001\n",
      "Epoch [110/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0066, LR: 0.0008880211\n",
      "Epoch [110/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0047, LR: 0.0008878419\n",
      "Epoch [110/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0055, LR: 0.0008876626\n",
      "Epoch [110/500], Batch [50/110], Train Loss: 0.0074, Val Loss: 0.0048, LR: 0.0008874832\n",
      "Epoch [110/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0047, LR: 0.0008873036\n",
      "Epoch [110/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0008871240\n",
      "Epoch [110/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0047, LR: 0.0008869442\n",
      "Epoch [110/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0063, LR: 0.0008867643\n",
      "Epoch [110/500], Batch [100/110], Train Loss: 0.0123, Val Loss: 0.0059, LR: 0.0008865842\n",
      "Epoch [110/500], Batch [110/110], Train Loss: 0.0632, Val Loss: 0.0050, LR: 0.0008864041\n",
      "Epoch [111/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0051, LR: 0.0008862238\n",
      "Epoch [111/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0050, LR: 0.0008860434\n",
      "Epoch [111/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0052, LR: 0.0008858628\n",
      "Epoch [111/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0076, LR: 0.0008856822\n",
      "Epoch [111/500], Batch [50/110], Train Loss: 0.1930, Val Loss: 0.0075, LR: 0.0008855014\n",
      "Epoch [111/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0048, LR: 0.0008853205\n",
      "Epoch [111/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0008851394\n",
      "Epoch [111/500], Batch [80/110], Train Loss: 0.0058, Val Loss: 0.0058, LR: 0.0008849583\n",
      "Epoch [111/500], Batch [90/110], Train Loss: 0.0070, Val Loss: 0.0071, LR: 0.0008847770\n",
      "Epoch [111/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0045, LR: 0.0008845956\n",
      "Epoch [111/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0008844140\n",
      "Confusion Matrix:\n",
      "[[627   4]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99366   0.99682       631\n",
      "           1    0.99542   1.00000   0.99770       869\n",
      "\n",
      "    accuracy                        0.99733      1500\n",
      "   macro avg    0.99771   0.99683   0.99726      1500\n",
      "weighted avg    0.99735   0.99733   0.99733      1500\n",
      "\n",
      "Total Errors: 4\n",
      "Index: 283, Predicted: 1, Actual: 0\n",
      "Index: 434, Predicted: 1, Actual: 0\n",
      "Index: 738, Predicted: 1, Actual: 0\n",
      "Index: 1445, Predicted: 1, Actual: 0\n",
      "Epoch 111: OK- Accuracy: 0.99733, Precision: 0.99542, Recall: 1.00000, F1: 0.99770, ROC AUC: 0.99683, AUPR (PR-AUC): 0.99542, Sensitivity: 1.00000, Specificity: 0.99366, Far: 0.006339144215530904, False Positive Rate (FPR): 0.00634, False Negative Rate (FNR): 0.00000, Runtime: 0.038 sec , Memory Usage: 310.01 MB\n",
      "Epoch [112/500], Batch [10/110], Train Loss: 0.0362, Val Loss: 0.0046, LR: 0.0008842324\n",
      "Epoch [112/500], Batch [20/110], Train Loss: 0.0015, Val Loss: 0.0048, LR: 0.0008840506\n",
      "Epoch [112/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0008838687\n",
      "Epoch [112/500], Batch [40/110], Train Loss: 0.0090, Val Loss: 0.0049, LR: 0.0008836867\n",
      "Epoch [112/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0044, LR: 0.0008835045\n",
      "Epoch [112/500], Batch [60/110], Train Loss: 0.0160, Val Loss: 0.0044, LR: 0.0008833223\n",
      "Epoch [112/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0092, LR: 0.0008831399\n",
      "Epoch [112/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0062, LR: 0.0008829573\n",
      "Epoch [112/500], Batch [90/110], Train Loss: 0.0286, Val Loss: 0.0045, LR: 0.0008827747\n",
      "Epoch [112/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0048, LR: 0.0008825919\n",
      "Epoch [112/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0051, LR: 0.0008824090\n",
      "Epoch [113/500], Batch [10/110], Train Loss: 0.0117, Val Loss: 0.0056, LR: 0.0008822260\n",
      "Epoch [113/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0046, LR: 0.0008820429\n",
      "Epoch [113/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0065, LR: 0.0008818596\n",
      "Epoch [113/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0008816763\n",
      "Epoch [113/500], Batch [50/110], Train Loss: 0.0160, Val Loss: 0.0050, LR: 0.0008814928\n",
      "Epoch [113/500], Batch [60/110], Train Loss: 0.0071, Val Loss: 0.0047, LR: 0.0008813091\n",
      "Epoch [113/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0053, LR: 0.0008811254\n",
      "Epoch [113/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0054, LR: 0.0008809415\n",
      "Epoch [113/500], Batch [90/110], Train Loss: 0.0037, Val Loss: 0.0050, LR: 0.0008807575\n",
      "Epoch [113/500], Batch [100/110], Train Loss: 0.0176, Val Loss: 0.0049, LR: 0.0008805734\n",
      "Epoch [113/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0008803891\n",
      "Epoch [114/500], Batch [10/110], Train Loss: 0.0099, Val Loss: 0.0065, LR: 0.0008802048\n",
      "Epoch [114/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0051, LR: 0.0008800203\n",
      "Epoch [114/500], Batch [30/110], Train Loss: 0.0230, Val Loss: 0.0049, LR: 0.0008798357\n",
      "Epoch [114/500], Batch [40/110], Train Loss: 0.0068, Val Loss: 0.0049, LR: 0.0008796510\n",
      "Epoch [114/500], Batch [50/110], Train Loss: 0.0499, Val Loss: 0.0063, LR: 0.0008794661\n",
      "Epoch [114/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0008792811\n",
      "Epoch [114/500], Batch [70/110], Train Loss: 0.0382, Val Loss: 0.0051, LR: 0.0008790960\n",
      "Epoch [114/500], Batch [80/110], Train Loss: 0.0805, Val Loss: 0.0052, LR: 0.0008789108\n",
      "Epoch [114/500], Batch [90/110], Train Loss: 0.1052, Val Loss: 0.0063, LR: 0.0008787255\n",
      "Epoch [114/500], Batch [100/110], Train Loss: 0.0166, Val Loss: 0.0049, LR: 0.0008785400\n",
      "Epoch [114/500], Batch [110/110], Train Loss: 0.0029, Val Loss: 0.0046, LR: 0.0008783544\n",
      "Epoch [115/500], Batch [10/110], Train Loss: 0.0025, Val Loss: 0.0045, LR: 0.0008781687\n",
      "Epoch [115/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0062, LR: 0.0008779829\n",
      "Epoch [115/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0045, LR: 0.0008777969\n",
      "Epoch [115/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0044, LR: 0.0008776109\n",
      "Epoch [115/500], Batch [50/110], Train Loss: 0.0392, Val Loss: 0.0046, LR: 0.0008774247\n",
      "Epoch [115/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0048, LR: 0.0008772384\n",
      "Epoch [115/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0051, LR: 0.0008770519\n",
      "Epoch [115/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0088, LR: 0.0008768654\n",
      "Epoch [115/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0051, LR: 0.0008766787\n",
      "Epoch [115/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0071, LR: 0.0008764919\n",
      "Epoch [115/500], Batch [110/110], Train Loss: 0.0024, Val Loss: 0.0079, LR: 0.0008763050\n",
      "Epoch [116/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0066, LR: 0.0008761179\n",
      "Epoch [116/500], Batch [20/110], Train Loss: 0.0038, Val Loss: 0.0051, LR: 0.0008759308\n",
      "Epoch [116/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0048, LR: 0.0008757435\n",
      "Epoch [116/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0049, LR: 0.0008755561\n",
      "Epoch [116/500], Batch [50/110], Train Loss: 0.0131, Val Loss: 0.0051, LR: 0.0008753686\n",
      "Epoch [116/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0057, LR: 0.0008751809\n",
      "Epoch [116/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0062, LR: 0.0008749931\n",
      "Epoch [116/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0050, LR: 0.0008748053\n",
      "Epoch [116/500], Batch [90/110], Train Loss: 0.0140, Val Loss: 0.0050, LR: 0.0008746172\n",
      "Epoch [116/500], Batch [100/110], Train Loss: 0.0373, Val Loss: 0.0054, LR: 0.0008744291\n",
      "Epoch [116/500], Batch [110/110], Train Loss: 0.0142, Val Loss: 0.0047, LR: 0.0008742409\n",
      "Epoch [117/500], Batch [10/110], Train Loss: 0.1112, Val Loss: 0.0054, LR: 0.0008740525\n",
      "Epoch [117/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0048, LR: 0.0008738640\n",
      "Epoch [117/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0049, LR: 0.0008736754\n",
      "Epoch [117/500], Batch [40/110], Train Loss: 0.0140, Val Loss: 0.0064, LR: 0.0008734867\n",
      "Epoch [117/500], Batch [50/110], Train Loss: 0.0136, Val Loss: 0.0059, LR: 0.0008732978\n",
      "Epoch [117/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0008731088\n",
      "Epoch [117/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0041, LR: 0.0008729197\n",
      "Epoch [117/500], Batch [80/110], Train Loss: 0.0092, Val Loss: 0.0043, LR: 0.0008727305\n",
      "Epoch [117/500], Batch [90/110], Train Loss: 0.0250, Val Loss: 0.0040, LR: 0.0008725412\n",
      "Epoch [117/500], Batch [100/110], Train Loss: 0.0159, Val Loss: 0.0042, LR: 0.0008723517\n",
      "Epoch [117/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0047, LR: 0.0008721622\n",
      "Epoch [118/500], Batch [10/110], Train Loss: 0.0013, Val Loss: 0.0067, LR: 0.0008719725\n",
      "Epoch [118/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0059, LR: 0.0008717827\n",
      "Epoch [118/500], Batch [30/110], Train Loss: 0.0213, Val Loss: 0.0045, LR: 0.0008715927\n",
      "Epoch [118/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0047, LR: 0.0008714027\n",
      "Epoch [118/500], Batch [50/110], Train Loss: 0.0168, Val Loss: 0.0049, LR: 0.0008712125\n",
      "Epoch [118/500], Batch [60/110], Train Loss: 0.0488, Val Loss: 0.0045, LR: 0.0008710222\n",
      "Epoch [118/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0040, LR: 0.0008708318\n",
      "Epoch [118/500], Batch [80/110], Train Loss: 0.0030, Val Loss: 0.0045, LR: 0.0008706413\n",
      "Epoch [118/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0039, LR: 0.0008704506\n",
      "Epoch [118/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0038, LR: 0.0008702599\n",
      "Epoch [118/500], Batch [110/110], Train Loss: 0.0240, Val Loss: 0.0038, LR: 0.0008700690\n",
      "Epoch [119/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0049, LR: 0.0008698780\n",
      "Epoch [119/500], Batch [20/110], Train Loss: 0.0142, Val Loss: 0.0050, LR: 0.0008696869\n",
      "Epoch [119/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0040, LR: 0.0008694956\n",
      "Epoch [119/500], Batch [40/110], Train Loss: 0.0069, Val Loss: 0.0041, LR: 0.0008693043\n",
      "Epoch [119/500], Batch [50/110], Train Loss: 0.0082, Val Loss: 0.0054, LR: 0.0008691128\n",
      "Epoch [119/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0091, LR: 0.0008689212\n",
      "Epoch [119/500], Batch [70/110], Train Loss: 0.0020, Val Loss: 0.0047, LR: 0.0008687295\n",
      "Epoch [119/500], Batch [80/110], Train Loss: 0.0095, Val Loss: 0.0039, LR: 0.0008685376\n",
      "Epoch [119/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0038, LR: 0.0008683457\n",
      "Epoch [119/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0008681536\n",
      "Epoch [119/500], Batch [110/110], Train Loss: 0.0040, Val Loss: 0.0037, LR: 0.0008679614\n",
      "Epoch [120/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0043, LR: 0.0008677691\n",
      "Epoch [120/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0041, LR: 0.0008675767\n",
      "Epoch [120/500], Batch [30/110], Train Loss: 0.0138, Val Loss: 0.0039, LR: 0.0008673841\n",
      "Epoch [120/500], Batch [40/110], Train Loss: 0.0106, Val Loss: 0.0039, LR: 0.0008671914\n",
      "Epoch [120/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0056, LR: 0.0008669987\n",
      "Epoch [120/500], Batch [60/110], Train Loss: 0.0318, Val Loss: 0.0052, LR: 0.0008668058\n",
      "Epoch [120/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0062, LR: 0.0008666127\n",
      "Epoch [120/500], Batch [80/110], Train Loss: 0.0013, Val Loss: 0.0044, LR: 0.0008664196\n",
      "Epoch [120/500], Batch [90/110], Train Loss: 0.0442, Val Loss: 0.0053, LR: 0.0008662263\n",
      "Epoch [120/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0079, LR: 0.0008660330\n",
      "Epoch [120/500], Batch [110/110], Train Loss: 0.0046, Val Loss: 0.0057, LR: 0.0008658395\n",
      "Epoch [121/500], Batch [10/110], Train Loss: 0.2067, Val Loss: 0.0067, LR: 0.0008656459\n",
      "Epoch [121/500], Batch [20/110], Train Loss: 0.0189, Val Loss: 0.0043, LR: 0.0008654521\n",
      "Epoch [121/500], Batch [30/110], Train Loss: 0.0383, Val Loss: 0.0039, LR: 0.0008652583\n",
      "Epoch [121/500], Batch [40/110], Train Loss: 0.0406, Val Loss: 0.0064, LR: 0.0008650643\n",
      "Epoch [121/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0065, LR: 0.0008648702\n",
      "Epoch [121/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0008646760\n",
      "Epoch [121/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0047, LR: 0.0008644817\n",
      "Epoch [121/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0008642873\n",
      "Epoch [121/500], Batch [90/110], Train Loss: 0.0397, Val Loss: 0.0071, LR: 0.0008640927\n",
      "Epoch [121/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0064, LR: 0.0008638981\n",
      "Epoch [121/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0042, LR: 0.0008637033\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  1 868]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99842   1.00000   0.99921       631\n",
      "           1    1.00000   0.99885   0.99942       869\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99921   0.99942   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 1495, Predicted: 0, Actual: 1\n",
      "Epoch 121: OK- Accuracy: 0.99933, Precision: 1.00000, Recall: 0.99885, F1: 0.99942, ROC AUC: 0.99942, AUPR (PR-AUC): 0.99952, Sensitivity: 0.99885, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00115, Runtime: 0.031 sec , Memory Usage: 310.01 MB\n",
      "Epoch [122/500], Batch [10/110], Train Loss: 0.0024, Val Loss: 0.0043, LR: 0.0008635084\n",
      "Epoch [122/500], Batch [20/110], Train Loss: 0.0210, Val Loss: 0.0041, LR: 0.0008633134\n",
      "Epoch [122/500], Batch [30/110], Train Loss: 0.0086, Val Loss: 0.0058, LR: 0.0008631182\n",
      "Epoch [122/500], Batch [40/110], Train Loss: 0.0104, Val Loss: 0.0042, LR: 0.0008629230\n",
      "Epoch [122/500], Batch [50/110], Train Loss: 0.0032, Val Loss: 0.0038, LR: 0.0008627276\n",
      "Epoch [122/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0038, LR: 0.0008625321\n",
      "Epoch [122/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0008623365\n",
      "Epoch [122/500], Batch [80/110], Train Loss: 0.1405, Val Loss: 0.0034, LR: 0.0008621408\n",
      "Epoch [122/500], Batch [90/110], Train Loss: 0.0393, Val Loss: 0.0037, LR: 0.0008619450\n",
      "Epoch [122/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0037, LR: 0.0008617490\n",
      "Epoch [122/500], Batch [110/110], Train Loss: 0.0027, Val Loss: 0.0044, LR: 0.0008615530\n",
      "Epoch [123/500], Batch [10/110], Train Loss: 0.0021, Val Loss: 0.0040, LR: 0.0008613568\n",
      "Epoch [123/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0039, LR: 0.0008611605\n",
      "Epoch [123/500], Batch [30/110], Train Loss: 0.0155, Val Loss: 0.0040, LR: 0.0008609641\n",
      "Epoch [123/500], Batch [40/110], Train Loss: 0.0058, Val Loss: 0.0054, LR: 0.0008607675\n",
      "Epoch [123/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0042, LR: 0.0008605709\n",
      "Epoch [123/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0008603741\n",
      "Epoch [123/500], Batch [70/110], Train Loss: 0.0022, Val Loss: 0.0041, LR: 0.0008601772\n",
      "Epoch [123/500], Batch [80/110], Train Loss: 0.0053, Val Loss: 0.0055, LR: 0.0008599802\n",
      "Epoch [123/500], Batch [90/110], Train Loss: 0.1028, Val Loss: 0.0099, LR: 0.0008597831\n",
      "Epoch [123/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0041, LR: 0.0008595859\n",
      "Epoch [123/500], Batch [110/110], Train Loss: 0.0317, Val Loss: 0.0037, LR: 0.0008593886\n",
      "Epoch [124/500], Batch [10/110], Train Loss: 0.0052, Val Loss: 0.0056, LR: 0.0008591911\n",
      "Epoch [124/500], Batch [20/110], Train Loss: 0.0145, Val Loss: 0.0036, LR: 0.0008589935\n",
      "Epoch [124/500], Batch [30/110], Train Loss: 0.0299, Val Loss: 0.0037, LR: 0.0008587958\n",
      "Epoch [124/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0048, LR: 0.0008585980\n",
      "Epoch [124/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0008584001\n",
      "Epoch [124/500], Batch [60/110], Train Loss: 0.0119, Val Loss: 0.0035, LR: 0.0008582021\n",
      "Epoch [124/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0008580039\n",
      "Epoch [124/500], Batch [80/110], Train Loss: 0.1254, Val Loss: 0.0037, LR: 0.0008578057\n",
      "Epoch [124/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0038, LR: 0.0008576073\n",
      "Epoch [124/500], Batch [100/110], Train Loss: 0.0102, Val Loss: 0.0036, LR: 0.0008574088\n",
      "Epoch [124/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0041, LR: 0.0008572102\n",
      "Epoch [125/500], Batch [10/110], Train Loss: 0.0304, Val Loss: 0.0045, LR: 0.0008570114\n",
      "Epoch [125/500], Batch [20/110], Train Loss: 0.0034, Val Loss: 0.0058, LR: 0.0008568126\n",
      "Epoch [125/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0035, LR: 0.0008566136\n",
      "Epoch [125/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0008564146\n",
      "Epoch [125/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0008562154\n",
      "Epoch [125/500], Batch [60/110], Train Loss: 0.0149, Val Loss: 0.0037, LR: 0.0008560161\n",
      "Epoch [125/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0034, LR: 0.0008558167\n",
      "Epoch [125/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0036, LR: 0.0008556171\n",
      "Epoch [125/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0056, LR: 0.0008554175\n",
      "Epoch [125/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0040, LR: 0.0008552177\n",
      "Epoch [125/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0045, LR: 0.0008550179\n",
      "Epoch [126/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0008548179\n",
      "Epoch [126/500], Batch [20/110], Train Loss: 0.0152, Val Loss: 0.0036, LR: 0.0008546178\n",
      "Epoch [126/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0039, LR: 0.0008544176\n",
      "Epoch [126/500], Batch [40/110], Train Loss: 0.0014, Val Loss: 0.0046, LR: 0.0008542172\n",
      "Epoch [126/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0008540168\n",
      "Epoch [126/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0008538162\n",
      "Epoch [126/500], Batch [70/110], Train Loss: 0.0195, Val Loss: 0.0039, LR: 0.0008536156\n",
      "Epoch [126/500], Batch [80/110], Train Loss: 0.0330, Val Loss: 0.0039, LR: 0.0008534148\n",
      "Epoch [126/500], Batch [90/110], Train Loss: 0.0046, Val Loss: 0.0038, LR: 0.0008532139\n",
      "Epoch [126/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0008530129\n",
      "Epoch [126/500], Batch [110/110], Train Loss: 0.0630, Val Loss: 0.0035, LR: 0.0008528117\n",
      "Epoch [127/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0008526105\n",
      "Epoch [127/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0008524091\n",
      "Epoch [127/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0008522077\n",
      "Epoch [127/500], Batch [40/110], Train Loss: 0.0074, Val Loss: 0.0044, LR: 0.0008520061\n",
      "Epoch [127/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0035, LR: 0.0008518044\n",
      "Epoch [127/500], Batch [60/110], Train Loss: 0.0427, Val Loss: 0.0048, LR: 0.0008516026\n",
      "Epoch [127/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0032, LR: 0.0008514007\n",
      "Epoch [127/500], Batch [80/110], Train Loss: 0.0669, Val Loss: 0.0049, LR: 0.0008511987\n",
      "Epoch [127/500], Batch [90/110], Train Loss: 0.0069, Val Loss: 0.0047, LR: 0.0008509965\n",
      "Epoch [127/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0008507943\n",
      "Epoch [127/500], Batch [110/110], Train Loss: 0.0017, Val Loss: 0.0034, LR: 0.0008505919\n",
      "Epoch [128/500], Batch [10/110], Train Loss: 0.0080, Val Loss: 0.0036, LR: 0.0008503894\n",
      "Epoch [128/500], Batch [20/110], Train Loss: 0.0136, Val Loss: 0.0039, LR: 0.0008501868\n",
      "Epoch [128/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0039, LR: 0.0008499841\n",
      "Epoch [128/500], Batch [40/110], Train Loss: 0.0074, Val Loss: 0.0040, LR: 0.0008497813\n",
      "Epoch [128/500], Batch [50/110], Train Loss: 0.0073, Val Loss: 0.0033, LR: 0.0008495783\n",
      "Epoch [128/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0035, LR: 0.0008493753\n",
      "Epoch [128/500], Batch [70/110], Train Loss: 0.0220, Val Loss: 0.0042, LR: 0.0008491721\n",
      "Epoch [128/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0008489689\n",
      "Epoch [128/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0045, LR: 0.0008487655\n",
      "Epoch [128/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0008485620\n",
      "Epoch [128/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0008483584\n",
      "Epoch [129/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0008481547\n",
      "Epoch [129/500], Batch [20/110], Train Loss: 0.0037, Val Loss: 0.0046, LR: 0.0008479508\n",
      "Epoch [129/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0031, LR: 0.0008477469\n",
      "Epoch [129/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0008475428\n",
      "Epoch [129/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0008473387\n",
      "Epoch [129/500], Batch [60/110], Train Loss: 0.0150, Val Loss: 0.0035, LR: 0.0008471344\n",
      "Epoch [129/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0034, LR: 0.0008469300\n",
      "Epoch [129/500], Batch [80/110], Train Loss: 0.0250, Val Loss: 0.0051, LR: 0.0008467255\n",
      "Epoch [129/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0047, LR: 0.0008465209\n",
      "Epoch [129/500], Batch [100/110], Train Loss: 0.0466, Val Loss: 0.0031, LR: 0.0008463162\n",
      "Epoch [129/500], Batch [110/110], Train Loss: 0.0135, Val Loss: 0.0031, LR: 0.0008461113\n",
      "Epoch [130/500], Batch [10/110], Train Loss: 0.0036, Val Loss: 0.0041, LR: 0.0008459064\n",
      "Epoch [130/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0033, LR: 0.0008457013\n",
      "Epoch [130/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0032, LR: 0.0008454962\n",
      "Epoch [130/500], Batch [40/110], Train Loss: 0.0384, Val Loss: 0.0034, LR: 0.0008452909\n",
      "Epoch [130/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0008450855\n",
      "Epoch [130/500], Batch [60/110], Train Loss: 0.0192, Val Loss: 0.0039, LR: 0.0008448800\n",
      "Epoch [130/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0008446744\n",
      "Epoch [130/500], Batch [80/110], Train Loss: 0.0026, Val Loss: 0.0052, LR: 0.0008444687\n",
      "Epoch [130/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0008442628\n",
      "Epoch [130/500], Batch [100/110], Train Loss: 0.0203, Val Loss: 0.0043, LR: 0.0008440569\n",
      "Epoch [130/500], Batch [110/110], Train Loss: 0.0052, Val Loss: 0.0044, LR: 0.0008438508\n",
      "Epoch [131/500], Batch [10/110], Train Loss: 0.0594, Val Loss: 0.0045, LR: 0.0008436447\n",
      "Epoch [131/500], Batch [20/110], Train Loss: 0.0164, Val Loss: 0.0039, LR: 0.0008434384\n",
      "Epoch [131/500], Batch [30/110], Train Loss: 0.0020, Val Loss: 0.0046, LR: 0.0008432320\n",
      "Epoch [131/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0043, LR: 0.0008430255\n",
      "Epoch [131/500], Batch [50/110], Train Loss: 0.0054, Val Loss: 0.0043, LR: 0.0008428189\n",
      "Epoch [131/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0008426122\n",
      "Epoch [131/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0008424053\n",
      "Epoch [131/500], Batch [80/110], Train Loss: 0.0010, Val Loss: 0.0033, LR: 0.0008421984\n",
      "Epoch [131/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0031, LR: 0.0008419913\n",
      "Epoch [131/500], Batch [100/110], Train Loss: 0.0167, Val Loss: 0.0031, LR: 0.0008417842\n",
      "Epoch [131/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0008415769\n",
      "Confusion Matrix:\n",
      "[[630   1]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99842   0.99921       631\n",
      "           1    0.99885   1.00000   0.99942       869\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99943   0.99921   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 1445, Predicted: 1, Actual: 0\n",
      "Epoch 131: OK- Accuracy: 0.99933, Precision: 0.99885, Recall: 1.00000, F1: 0.99942, ROC AUC: 0.99921, AUPR (PR-AUC): 0.99885, Sensitivity: 1.00000, Specificity: 0.99842, Far: 0.001584786053882726, False Positive Rate (FPR): 0.00158, False Negative Rate (FNR): 0.00000, Runtime: 0.036 sec , Memory Usage: 310.01 MB\n",
      "Epoch [132/500], Batch [10/110], Train Loss: 0.1359, Val Loss: 0.0046, LR: 0.0008413695\n",
      "Epoch [132/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0008411621\n",
      "Epoch [132/500], Batch [30/110], Train Loss: 0.0068, Val Loss: 0.0038, LR: 0.0008409545\n",
      "Epoch [132/500], Batch [40/110], Train Loss: 0.0182, Val Loss: 0.0031, LR: 0.0008407468\n",
      "Epoch [132/500], Batch [50/110], Train Loss: 0.0059, Val Loss: 0.0031, LR: 0.0008405389\n",
      "Epoch [132/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0033, LR: 0.0008403310\n",
      "Epoch [132/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0035, LR: 0.0008401230\n",
      "Epoch [132/500], Batch [80/110], Train Loss: 0.0319, Val Loss: 0.0043, LR: 0.0008399148\n",
      "Epoch [132/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0093, LR: 0.0008397066\n",
      "Epoch [132/500], Batch [100/110], Train Loss: 0.0042, Val Loss: 0.0080, LR: 0.0008394982\n",
      "Epoch [132/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0033, LR: 0.0008392897\n",
      "Epoch [133/500], Batch [10/110], Train Loss: 0.0284, Val Loss: 0.0029, LR: 0.0008390812\n",
      "Epoch [133/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0030, LR: 0.0008388725\n",
      "Epoch [133/500], Batch [30/110], Train Loss: 0.0085, Val Loss: 0.0035, LR: 0.0008386637\n",
      "Epoch [133/500], Batch [40/110], Train Loss: 0.0009, Val Loss: 0.0043, LR: 0.0008384548\n",
      "Epoch [133/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0008382457\n",
      "Epoch [133/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0008380366\n",
      "Epoch [133/500], Batch [70/110], Train Loss: 0.0317, Val Loss: 0.0031, LR: 0.0008378274\n",
      "Epoch [133/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0008376180\n",
      "Epoch [133/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0008374086\n",
      "Epoch [133/500], Batch [100/110], Train Loss: 0.0228, Val Loss: 0.0031, LR: 0.0008371990\n",
      "Epoch [133/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0008369894\n",
      "Epoch [134/500], Batch [10/110], Train Loss: 0.0037, Val Loss: 0.0035, LR: 0.0008367796\n",
      "Epoch [134/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0008365697\n",
      "Epoch [134/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0031, LR: 0.0008363597\n",
      "Epoch [134/500], Batch [40/110], Train Loss: 0.0159, Val Loss: 0.0035, LR: 0.0008361496\n",
      "Epoch [134/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0008359394\n",
      "Epoch [134/500], Batch [60/110], Train Loss: 0.0041, Val Loss: 0.0029, LR: 0.0008357291\n",
      "Epoch [134/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0057, LR: 0.0008355187\n",
      "Epoch [134/500], Batch [80/110], Train Loss: 0.0193, Val Loss: 0.0073, LR: 0.0008353081\n",
      "Epoch [134/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0030, LR: 0.0008350975\n",
      "Epoch [134/500], Batch [100/110], Train Loss: 0.0043, Val Loss: 0.0028, LR: 0.0008348867\n",
      "Epoch [134/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0038, LR: 0.0008346759\n",
      "Epoch [135/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0035, LR: 0.0008344649\n",
      "Epoch [135/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0036, LR: 0.0008342538\n",
      "Epoch [135/500], Batch [30/110], Train Loss: 0.0017, Val Loss: 0.0038, LR: 0.0008340427\n",
      "Epoch [135/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0055, LR: 0.0008338314\n",
      "Epoch [135/500], Batch [50/110], Train Loss: 0.0153, Val Loss: 0.0034, LR: 0.0008336200\n",
      "Epoch [135/500], Batch [60/110], Train Loss: 0.0027, Val Loss: 0.0040, LR: 0.0008334085\n",
      "Epoch [135/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0008331969\n",
      "Epoch [135/500], Batch [80/110], Train Loss: 0.0036, Val Loss: 0.0039, LR: 0.0008329852\n",
      "Epoch [135/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0008327733\n",
      "Epoch [135/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0008325614\n",
      "Epoch [135/500], Batch [110/110], Train Loss: 0.0068, Val Loss: 0.0038, LR: 0.0008323494\n",
      "Epoch [136/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0008321372\n",
      "Epoch [136/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0008319250\n",
      "Epoch [136/500], Batch [30/110], Train Loss: 0.0012, Val Loss: 0.0029, LR: 0.0008317126\n",
      "Epoch [136/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0008315002\n",
      "Epoch [136/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0008312876\n",
      "Epoch [136/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0008310749\n",
      "Epoch [136/500], Batch [70/110], Train Loss: 0.0040, Val Loss: 0.0034, LR: 0.0008308621\n",
      "Epoch [136/500], Batch [80/110], Train Loss: 0.0127, Val Loss: 0.0027, LR: 0.0008306493\n",
      "Epoch [136/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0027, LR: 0.0008304363\n",
      "Epoch [136/500], Batch [100/110], Train Loss: 0.0116, Val Loss: 0.0029, LR: 0.0008302232\n",
      "Epoch [136/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0039, LR: 0.0008300099\n",
      "Epoch [137/500], Batch [10/110], Train Loss: 0.0098, Val Loss: 0.0028, LR: 0.0008297966\n",
      "Epoch [137/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0008295832\n",
      "Epoch [137/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0026, LR: 0.0008293697\n",
      "Epoch [137/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0008291561\n",
      "Epoch [137/500], Batch [50/110], Train Loss: 0.0156, Val Loss: 0.0025, LR: 0.0008289423\n",
      "Epoch [137/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0027, LR: 0.0008287285\n",
      "Epoch [137/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0030, LR: 0.0008285145\n",
      "Epoch [137/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0008283005\n",
      "Epoch [137/500], Batch [90/110], Train Loss: 0.0097, Val Loss: 0.0029, LR: 0.0008280863\n",
      "Epoch [137/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0025, LR: 0.0008278721\n",
      "Epoch [137/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0008276577\n",
      "Epoch [138/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0039, LR: 0.0008274432\n",
      "Epoch [138/500], Batch [20/110], Train Loss: 0.0139, Val Loss: 0.0039, LR: 0.0008272286\n",
      "Epoch [138/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0008270140\n",
      "Epoch [138/500], Batch [40/110], Train Loss: 0.0224, Val Loss: 0.0025, LR: 0.0008267992\n",
      "Epoch [138/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0036, LR: 0.0008265843\n",
      "Epoch [138/500], Batch [60/110], Train Loss: 0.0058, Val Loss: 0.0039, LR: 0.0008263693\n",
      "Epoch [138/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0008261542\n",
      "Epoch [138/500], Batch [80/110], Train Loss: 0.0155, Val Loss: 0.0025, LR: 0.0008259390\n",
      "Epoch [138/500], Batch [90/110], Train Loss: 0.0065, Val Loss: 0.0040, LR: 0.0008257236\n",
      "Epoch [138/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0027, LR: 0.0008255082\n",
      "Epoch [138/500], Batch [110/110], Train Loss: 0.0281, Val Loss: 0.0041, LR: 0.0008252927\n",
      "Epoch [139/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0031, LR: 0.0008250771\n",
      "Epoch [139/500], Batch [20/110], Train Loss: 0.2894, Val Loss: 0.0043, LR: 0.0008248613\n",
      "Epoch [139/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0008246455\n",
      "Epoch [139/500], Batch [40/110], Train Loss: 0.0186, Val Loss: 0.0026, LR: 0.0008244296\n",
      "Epoch [139/500], Batch [50/110], Train Loss: 0.0401, Val Loss: 0.0027, LR: 0.0008242135\n",
      "Epoch [139/500], Batch [60/110], Train Loss: 0.0018, Val Loss: 0.0031, LR: 0.0008239974\n",
      "Epoch [139/500], Batch [70/110], Train Loss: 0.0659, Val Loss: 0.0043, LR: 0.0008237811\n",
      "Epoch [139/500], Batch [80/110], Train Loss: 0.0017, Val Loss: 0.0037, LR: 0.0008235648\n",
      "Epoch [139/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0008233483\n",
      "Epoch [139/500], Batch [100/110], Train Loss: 0.0036, Val Loss: 0.0029, LR: 0.0008231317\n",
      "Epoch [139/500], Batch [110/110], Train Loss: 0.0649, Val Loss: 0.0036, LR: 0.0008229151\n",
      "Epoch [140/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0008226983\n",
      "Epoch [140/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0008224814\n",
      "Epoch [140/500], Batch [30/110], Train Loss: 0.0209, Val Loss: 0.0044, LR: 0.0008222644\n",
      "Epoch [140/500], Batch [40/110], Train Loss: 0.2108, Val Loss: 0.0041, LR: 0.0008220474\n",
      "Epoch [140/500], Batch [50/110], Train Loss: 0.0021, Val Loss: 0.0033, LR: 0.0008218302\n",
      "Epoch [140/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0045, LR: 0.0008216129\n",
      "Epoch [140/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0054, LR: 0.0008213955\n",
      "Epoch [140/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0008211780\n",
      "Epoch [140/500], Batch [90/110], Train Loss: 0.0027, Val Loss: 0.0028, LR: 0.0008209604\n",
      "Epoch [140/500], Batch [100/110], Train Loss: 0.0098, Val Loss: 0.0035, LR: 0.0008207427\n",
      "Epoch [140/500], Batch [110/110], Train Loss: 0.0074, Val Loss: 0.0027, LR: 0.0008205249\n",
      "Epoch [141/500], Batch [10/110], Train Loss: 0.0015, Val Loss: 0.0024, LR: 0.0008203070\n",
      "Epoch [141/500], Batch [20/110], Train Loss: 0.0164, Val Loss: 0.0027, LR: 0.0008200890\n",
      "Epoch [141/500], Batch [30/110], Train Loss: 0.0196, Val Loss: 0.0031, LR: 0.0008198708\n",
      "Epoch [141/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0091, LR: 0.0008196526\n",
      "Epoch [141/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0061, LR: 0.0008194343\n",
      "Epoch [141/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0008192159\n",
      "Epoch [141/500], Batch [70/110], Train Loss: 0.0142, Val Loss: 0.0050, LR: 0.0008189974\n",
      "Epoch [141/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0008187787\n",
      "Epoch [141/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0044, LR: 0.0008185600\n",
      "Epoch [141/500], Batch [100/110], Train Loss: 0.0046, Val Loss: 0.0029, LR: 0.0008183412\n",
      "Epoch [141/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0008181222\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 141: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 310.01 MB\n",
      "Epoch [142/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0008179032\n",
      "Epoch [142/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0025, LR: 0.0008176841\n",
      "Epoch [142/500], Batch [30/110], Train Loss: 0.0067, Val Loss: 0.0031, LR: 0.0008174648\n",
      "Epoch [142/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0008172455\n",
      "Epoch [142/500], Batch [50/110], Train Loss: 0.0127, Val Loss: 0.0024, LR: 0.0008170260\n",
      "Epoch [142/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0037, LR: 0.0008168065\n",
      "Epoch [142/500], Batch [70/110], Train Loss: 0.0090, Val Loss: 0.0024, LR: 0.0008165868\n",
      "Epoch [142/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0042, LR: 0.0008163671\n",
      "Epoch [142/500], Batch [90/110], Train Loss: 0.0015, Val Loss: 0.0033, LR: 0.0008161472\n",
      "Epoch [142/500], Batch [100/110], Train Loss: 0.0111, Val Loss: 0.0066, LR: 0.0008159273\n",
      "Epoch [142/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0049, LR: 0.0008157072\n",
      "Epoch [143/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0008154871\n",
      "Epoch [143/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0008152668\n",
      "Epoch [143/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0008150465\n",
      "Epoch [143/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0008148260\n",
      "Epoch [143/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0055, LR: 0.0008146054\n",
      "Epoch [143/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0081, LR: 0.0008143848\n",
      "Epoch [143/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0061, LR: 0.0008141640\n",
      "Epoch [143/500], Batch [80/110], Train Loss: 0.0126, Val Loss: 0.0033, LR: 0.0008139431\n",
      "Epoch [143/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0036, LR: 0.0008137222\n",
      "Epoch [143/500], Batch [100/110], Train Loss: 0.0076, Val Loss: 0.0031, LR: 0.0008135011\n",
      "Epoch [143/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0033, LR: 0.0008132800\n",
      "Epoch [144/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0050, LR: 0.0008130587\n",
      "Epoch [144/500], Batch [20/110], Train Loss: 0.0042, Val Loss: 0.0048, LR: 0.0008128373\n",
      "Epoch [144/500], Batch [30/110], Train Loss: 0.0155, Val Loss: 0.0036, LR: 0.0008126159\n",
      "Epoch [144/500], Batch [40/110], Train Loss: 0.0050, Val Loss: 0.0036, LR: 0.0008123943\n",
      "Epoch [144/500], Batch [50/110], Train Loss: 0.0080, Val Loss: 0.0034, LR: 0.0008121726\n",
      "Epoch [144/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0040, LR: 0.0008119508\n",
      "Epoch [144/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0008117290\n",
      "Epoch [144/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0008115070\n",
      "Epoch [144/500], Batch [90/110], Train Loss: 0.0427, Val Loss: 0.0050, LR: 0.0008112849\n",
      "Epoch [144/500], Batch [100/110], Train Loss: 0.0468, Val Loss: 0.0050, LR: 0.0008110628\n",
      "Epoch [144/500], Batch [110/110], Train Loss: 0.0055, Val Loss: 0.0027, LR: 0.0008108405\n",
      "Epoch [145/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0008106181\n",
      "Epoch [145/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0008103957\n",
      "Epoch [145/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0008101731\n",
      "Epoch [145/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0026, LR: 0.0008099504\n",
      "Epoch [145/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0031, LR: 0.0008097277\n",
      "Epoch [145/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0008095048\n",
      "Epoch [145/500], Batch [70/110], Train Loss: 0.0033, Val Loss: 0.0040, LR: 0.0008092818\n",
      "Epoch [145/500], Batch [80/110], Train Loss: 0.0103, Val Loss: 0.0044, LR: 0.0008090588\n",
      "Epoch [145/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0008088356\n",
      "Epoch [145/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0029, LR: 0.0008086124\n",
      "Epoch [145/500], Batch [110/110], Train Loss: 0.0077, Val Loss: 0.0029, LR: 0.0008083890\n",
      "Epoch [146/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0034, LR: 0.0008081655\n",
      "Epoch [146/500], Batch [20/110], Train Loss: 0.0029, Val Loss: 0.0028, LR: 0.0008079420\n",
      "Epoch [146/500], Batch [30/110], Train Loss: 0.0045, Val Loss: 0.0031, LR: 0.0008077183\n",
      "Epoch [146/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0032, LR: 0.0008074946\n",
      "Epoch [146/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0008072707\n",
      "Epoch [146/500], Batch [60/110], Train Loss: 0.0021, Val Loss: 0.0031, LR: 0.0008070467\n",
      "Epoch [146/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0008068227\n",
      "Epoch [146/500], Batch [80/110], Train Loss: 0.0109, Val Loss: 0.0029, LR: 0.0008065985\n",
      "Epoch [146/500], Batch [90/110], Train Loss: 0.0074, Val Loss: 0.0027, LR: 0.0008063743\n",
      "Epoch [146/500], Batch [100/110], Train Loss: 0.0149, Val Loss: 0.0032, LR: 0.0008061499\n",
      "Epoch [146/500], Batch [110/110], Train Loss: 0.0157, Val Loss: 0.0022, LR: 0.0008059255\n",
      "Epoch [147/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0008057010\n",
      "Epoch [147/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0008054763\n",
      "Epoch [147/500], Batch [30/110], Train Loss: 0.0070, Val Loss: 0.0025, LR: 0.0008052516\n",
      "Epoch [147/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0029, LR: 0.0008050267\n",
      "Epoch [147/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0008048018\n",
      "Epoch [147/500], Batch [60/110], Train Loss: 0.0397, Val Loss: 0.0030, LR: 0.0008045768\n",
      "Epoch [147/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0023, LR: 0.0008043516\n",
      "Epoch [147/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0008041264\n",
      "Epoch [147/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0008039011\n",
      "Epoch [147/500], Batch [100/110], Train Loss: 0.0297, Val Loss: 0.0025, LR: 0.0008036756\n",
      "Epoch [147/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0008034501\n",
      "Epoch [148/500], Batch [10/110], Train Loss: 0.0050, Val Loss: 0.0022, LR: 0.0008032245\n",
      "Epoch [148/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0025, LR: 0.0008029988\n",
      "Epoch [148/500], Batch [30/110], Train Loss: 0.0056, Val Loss: 0.0030, LR: 0.0008027730\n",
      "Epoch [148/500], Batch [40/110], Train Loss: 0.0072, Val Loss: 0.0025, LR: 0.0008025471\n",
      "Epoch [148/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0008023211\n",
      "Epoch [148/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0008020949\n",
      "Epoch [148/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0008018687\n",
      "Epoch [148/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0008016424\n",
      "Epoch [148/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0021, LR: 0.0008014160\n",
      "Epoch [148/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0008011896\n",
      "Epoch [148/500], Batch [110/110], Train Loss: 0.0074, Val Loss: 0.0020, LR: 0.0008009630\n",
      "Epoch [149/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0008007363\n",
      "Epoch [149/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0023, LR: 0.0008005095\n",
      "Epoch [149/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0047, LR: 0.0008002826\n",
      "Epoch [149/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0043, LR: 0.0008000556\n",
      "Epoch [149/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0038, LR: 0.0007998286\n",
      "Epoch [149/500], Batch [60/110], Train Loss: 0.0015, Val Loss: 0.0024, LR: 0.0007996014\n",
      "Epoch [149/500], Batch [70/110], Train Loss: 0.0533, Val Loss: 0.0033, LR: 0.0007993741\n",
      "Epoch [149/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0007991468\n",
      "Epoch [149/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007989193\n",
      "Epoch [149/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007986918\n",
      "Epoch [149/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0007984641\n",
      "Epoch [150/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0007982364\n",
      "Epoch [150/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0007980085\n",
      "Epoch [150/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0007977806\n",
      "Epoch [150/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0007975526\n",
      "Epoch [150/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0048, LR: 0.0007973245\n",
      "Epoch [150/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0007970962\n",
      "Epoch [150/500], Batch [70/110], Train Loss: 0.0091, Val Loss: 0.0026, LR: 0.0007968679\n",
      "Epoch [150/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0007966395\n",
      "Epoch [150/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0007964110\n",
      "Epoch [150/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0007961824\n",
      "Epoch [150/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007959537\n",
      "Epoch [151/500], Batch [10/110], Train Loss: 0.0281, Val Loss: 0.0021, LR: 0.0007957249\n",
      "Epoch [151/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0007954960\n",
      "Epoch [151/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0033, LR: 0.0007952670\n",
      "Epoch [151/500], Batch [40/110], Train Loss: 0.0035, Val Loss: 0.0020, LR: 0.0007950380\n",
      "Epoch [151/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0007948088\n",
      "Epoch [151/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0020, LR: 0.0007945795\n",
      "Epoch [151/500], Batch [70/110], Train Loss: 0.0072, Val Loss: 0.0035, LR: 0.0007943502\n",
      "Epoch [151/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0042, LR: 0.0007941207\n",
      "Epoch [151/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0030, LR: 0.0007938912\n",
      "Epoch [151/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0007936615\n",
      "Epoch [151/500], Batch [110/110], Train Loss: 0.0131, Val Loss: 0.0029, LR: 0.0007934318\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 151: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.039 sec , Memory Usage: 305.20 MB\n",
      "Epoch [152/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0007932020\n",
      "Epoch [152/500], Batch [20/110], Train Loss: 0.0192, Val Loss: 0.0037, LR: 0.0007929720\n",
      "Epoch [152/500], Batch [30/110], Train Loss: 0.0037, Val Loss: 0.0028, LR: 0.0007927420\n",
      "Epoch [152/500], Batch [40/110], Train Loss: 0.0060, Val Loss: 0.0039, LR: 0.0007925119\n",
      "Epoch [152/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0068, LR: 0.0007922817\n",
      "Epoch [152/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0042, LR: 0.0007920514\n",
      "Epoch [152/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0069, LR: 0.0007918210\n",
      "Epoch [152/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0007915905\n",
      "Epoch [152/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0047, LR: 0.0007913599\n",
      "Epoch [152/500], Batch [100/110], Train Loss: 0.0063, Val Loss: 0.0047, LR: 0.0007911293\n",
      "Epoch [152/500], Batch [110/110], Train Loss: 0.0009, Val Loss: 0.0027, LR: 0.0007908985\n",
      "Epoch [153/500], Batch [10/110], Train Loss: 0.0121, Val Loss: 0.0022, LR: 0.0007906676\n",
      "Epoch [153/500], Batch [20/110], Train Loss: 0.0064, Val Loss: 0.0023, LR: 0.0007904367\n",
      "Epoch [153/500], Batch [30/110], Train Loss: 0.0054, Val Loss: 0.0026, LR: 0.0007902056\n",
      "Epoch [153/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0007899745\n",
      "Epoch [153/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007897433\n",
      "Epoch [153/500], Batch [60/110], Train Loss: 0.0045, Val Loss: 0.0024, LR: 0.0007895119\n",
      "Epoch [153/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0023, LR: 0.0007892805\n",
      "Epoch [153/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007890490\n",
      "Epoch [153/500], Batch [90/110], Train Loss: 0.0440, Val Loss: 0.0033, LR: 0.0007888174\n",
      "Epoch [153/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0026, LR: 0.0007885857\n",
      "Epoch [153/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007883539\n",
      "Epoch [154/500], Batch [10/110], Train Loss: 0.0098, Val Loss: 0.0022, LR: 0.0007881220\n",
      "Epoch [154/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007878900\n",
      "Epoch [154/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0007876580\n",
      "Epoch [154/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0007874258\n",
      "Epoch [154/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0053, LR: 0.0007871936\n",
      "Epoch [154/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0045, LR: 0.0007869612\n",
      "Epoch [154/500], Batch [70/110], Train Loss: 0.0042, Val Loss: 0.0024, LR: 0.0007867288\n",
      "Epoch [154/500], Batch [80/110], Train Loss: 0.0074, Val Loss: 0.0033, LR: 0.0007864963\n",
      "Epoch [154/500], Batch [90/110], Train Loss: 0.0045, Val Loss: 0.0028, LR: 0.0007862636\n",
      "Epoch [154/500], Batch [100/110], Train Loss: 0.0185, Val Loss: 0.0061, LR: 0.0007860309\n",
      "Epoch [154/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0007857981\n",
      "Epoch [155/500], Batch [10/110], Train Loss: 0.0065, Val Loss: 0.0022, LR: 0.0007855652\n",
      "Epoch [155/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007853322\n",
      "Epoch [155/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0023, LR: 0.0007850992\n",
      "Epoch [155/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0007848660\n",
      "Epoch [155/500], Batch [50/110], Train Loss: 0.0131, Val Loss: 0.0020, LR: 0.0007846327\n",
      "Epoch [155/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007843994\n",
      "Epoch [155/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0007841660\n",
      "Epoch [155/500], Batch [80/110], Train Loss: 0.0071, Val Loss: 0.0028, LR: 0.0007839324\n",
      "Epoch [155/500], Batch [90/110], Train Loss: 0.0035, Val Loss: 0.0028, LR: 0.0007836988\n",
      "Epoch [155/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007834651\n",
      "Epoch [155/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0007832313\n",
      "Epoch [156/500], Batch [10/110], Train Loss: 0.0069, Val Loss: 0.0024, LR: 0.0007829974\n",
      "Epoch [156/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0007827634\n",
      "Epoch [156/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0007825293\n",
      "Epoch [156/500], Batch [40/110], Train Loss: 0.0395, Val Loss: 0.0033, LR: 0.0007822951\n",
      "Epoch [156/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0007820609\n",
      "Epoch [156/500], Batch [60/110], Train Loss: 0.0059, Val Loss: 0.0029, LR: 0.0007818265\n",
      "Epoch [156/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0024, LR: 0.0007815921\n",
      "Epoch [156/500], Batch [80/110], Train Loss: 0.0107, Val Loss: 0.0023, LR: 0.0007813576\n",
      "Epoch [156/500], Batch [90/110], Train Loss: 0.0146, Val Loss: 0.0025, LR: 0.0007811229\n",
      "Epoch [156/500], Batch [100/110], Train Loss: 0.0025, Val Loss: 0.0021, LR: 0.0007808882\n",
      "Epoch [156/500], Batch [110/110], Train Loss: 0.0026, Val Loss: 0.0029, LR: 0.0007806534\n",
      "Epoch [157/500], Batch [10/110], Train Loss: 0.0032, Val Loss: 0.0031, LR: 0.0007804185\n",
      "Epoch [157/500], Batch [20/110], Train Loss: 0.0194, Val Loss: 0.0029, LR: 0.0007801836\n",
      "Epoch [157/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0025, LR: 0.0007799485\n",
      "Epoch [157/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0023, LR: 0.0007797133\n",
      "Epoch [157/500], Batch [50/110], Train Loss: 0.0026, Val Loss: 0.0035, LR: 0.0007794781\n",
      "Epoch [157/500], Batch [60/110], Train Loss: 0.0057, Val Loss: 0.0023, LR: 0.0007792427\n",
      "Epoch [157/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007790073\n",
      "Epoch [157/500], Batch [80/110], Train Loss: 0.0164, Val Loss: 0.0020, LR: 0.0007787718\n",
      "Epoch [157/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007785362\n",
      "Epoch [157/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007783005\n",
      "Epoch [157/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007780647\n",
      "Epoch [158/500], Batch [10/110], Train Loss: 0.0010, Val Loss: 0.0029, LR: 0.0007778288\n",
      "Epoch [158/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0007775929\n",
      "Epoch [158/500], Batch [30/110], Train Loss: 0.0029, Val Loss: 0.0019, LR: 0.0007773568\n",
      "Epoch [158/500], Batch [40/110], Train Loss: 0.0047, Val Loss: 0.0018, LR: 0.0007771207\n",
      "Epoch [158/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007768844\n",
      "Epoch [158/500], Batch [60/110], Train Loss: 0.0180, Val Loss: 0.0019, LR: 0.0007766481\n",
      "Epoch [158/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007764117\n",
      "Epoch [158/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007761752\n",
      "Epoch [158/500], Batch [90/110], Train Loss: 0.0018, Val Loss: 0.0020, LR: 0.0007759386\n",
      "Epoch [158/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0007757020\n",
      "Epoch [158/500], Batch [110/110], Train Loss: 0.0017, Val Loss: 0.0019, LR: 0.0007754652\n",
      "Epoch [159/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0030, LR: 0.0007752284\n",
      "Epoch [159/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0007749914\n",
      "Epoch [159/500], Batch [30/110], Train Loss: 0.2219, Val Loss: 0.0029, LR: 0.0007747544\n",
      "Epoch [159/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007745173\n",
      "Epoch [159/500], Batch [50/110], Train Loss: 0.0541, Val Loss: 0.0035, LR: 0.0007742801\n",
      "Epoch [159/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007740428\n",
      "Epoch [159/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0007738054\n",
      "Epoch [159/500], Batch [80/110], Train Loss: 0.0033, Val Loss: 0.0037, LR: 0.0007735679\n",
      "Epoch [159/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0035, LR: 0.0007733304\n",
      "Epoch [159/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007730927\n",
      "Epoch [159/500], Batch [110/110], Train Loss: 0.0058, Val Loss: 0.0020, LR: 0.0007728550\n",
      "Epoch [160/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007726172\n",
      "Epoch [160/500], Batch [20/110], Train Loss: 0.0034, Val Loss: 0.0021, LR: 0.0007723793\n",
      "Epoch [160/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007721413\n",
      "Epoch [160/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007719032\n",
      "Epoch [160/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007716651\n",
      "Epoch [160/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0007714268\n",
      "Epoch [160/500], Batch [70/110], Train Loss: 0.0023, Val Loss: 0.0025, LR: 0.0007711885\n",
      "Epoch [160/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007709501\n",
      "Epoch [160/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0007707115\n",
      "Epoch [160/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007704729\n",
      "Epoch [160/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0007702343\n",
      "Epoch [161/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0032, LR: 0.0007699955\n",
      "Epoch [161/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0007697566\n",
      "Epoch [161/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0007695177\n",
      "Epoch [161/500], Batch [40/110], Train Loss: 0.0095, Val Loss: 0.0048, LR: 0.0007692787\n",
      "Epoch [161/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0072, LR: 0.0007690395\n",
      "Epoch [161/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0007688003\n",
      "Epoch [161/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0007685611\n",
      "Epoch [161/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0007683217\n",
      "Epoch [161/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0007680822\n",
      "Epoch [161/500], Batch [100/110], Train Loss: 0.0125, Val Loss: 0.0024, LR: 0.0007678427\n",
      "Epoch [161/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007676030\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 161: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 305.20 MB\n",
      "Epoch [162/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0007673633\n",
      "Epoch [162/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007671235\n",
      "Epoch [162/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0034, LR: 0.0007668836\n",
      "Epoch [162/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0007666437\n",
      "Epoch [162/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007664036\n",
      "Epoch [162/500], Batch [60/110], Train Loss: 0.0237, Val Loss: 0.0032, LR: 0.0007661634\n",
      "Epoch [162/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0007659232\n",
      "Epoch [162/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0018, LR: 0.0007656829\n",
      "Epoch [162/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007654425\n",
      "Epoch [162/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007652020\n",
      "Epoch [162/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007649614\n",
      "Epoch [163/500], Batch [10/110], Train Loss: 0.0020, Val Loss: 0.0027, LR: 0.0007647208\n",
      "Epoch [163/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0007644800\n",
      "Epoch [163/500], Batch [30/110], Train Loss: 0.0168, Val Loss: 0.0023, LR: 0.0007642392\n",
      "Epoch [163/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007639983\n",
      "Epoch [163/500], Batch [50/110], Train Loss: 0.0012, Val Loss: 0.0016, LR: 0.0007637573\n",
      "Epoch [163/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0007635162\n",
      "Epoch [163/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0031, LR: 0.0007632751\n",
      "Epoch [163/500], Batch [80/110], Train Loss: 0.0052, Val Loss: 0.0028, LR: 0.0007630338\n",
      "Epoch [163/500], Batch [90/110], Train Loss: 0.0028, Val Loss: 0.0024, LR: 0.0007627925\n",
      "Epoch [163/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007625511\n",
      "Epoch [163/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0018, LR: 0.0007623096\n",
      "Epoch [164/500], Batch [10/110], Train Loss: 0.0038, Val Loss: 0.0016, LR: 0.0007620680\n",
      "Epoch [164/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007618263\n",
      "Epoch [164/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007615846\n",
      "Epoch [164/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0007613427\n",
      "Epoch [164/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007611008\n",
      "Epoch [164/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007608588\n",
      "Epoch [164/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007606167\n",
      "Epoch [164/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007603746\n",
      "Epoch [164/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007601323\n",
      "Epoch [164/500], Batch [100/110], Train Loss: 0.0062, Val Loss: 0.0014, LR: 0.0007598900\n",
      "Epoch [164/500], Batch [110/110], Train Loss: 0.0430, Val Loss: 0.0017, LR: 0.0007596476\n",
      "Epoch [165/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007594051\n",
      "Epoch [165/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007591625\n",
      "Epoch [165/500], Batch [30/110], Train Loss: 0.0094, Val Loss: 0.0023, LR: 0.0007589198\n",
      "Epoch [165/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0007586771\n",
      "Epoch [165/500], Batch [50/110], Train Loss: 0.0127, Val Loss: 0.0021, LR: 0.0007584342\n",
      "Epoch [165/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007581913\n",
      "Epoch [165/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007579483\n",
      "Epoch [165/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0007577052\n",
      "Epoch [165/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0007574621\n",
      "Epoch [165/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007572188\n",
      "Epoch [165/500], Batch [110/110], Train Loss: 0.0014, Val Loss: 0.0030, LR: 0.0007569755\n",
      "Epoch [166/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007567321\n",
      "Epoch [166/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007564886\n",
      "Epoch [166/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0007562450\n",
      "Epoch [166/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0016, LR: 0.0007560014\n",
      "Epoch [166/500], Batch [50/110], Train Loss: 0.0055, Val Loss: 0.0018, LR: 0.0007557576\n",
      "Epoch [166/500], Batch [60/110], Train Loss: 0.0023, Val Loss: 0.0024, LR: 0.0007555138\n",
      "Epoch [166/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007552699\n",
      "Epoch [166/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007550259\n",
      "Epoch [166/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007547819\n",
      "Epoch [166/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007545377\n",
      "Epoch [166/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0007542935\n",
      "Epoch [167/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0052, LR: 0.0007540492\n",
      "Epoch [167/500], Batch [20/110], Train Loss: 0.0054, Val Loss: 0.0019, LR: 0.0007538048\n",
      "Epoch [167/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0021, LR: 0.0007535603\n",
      "Epoch [167/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007533158\n",
      "Epoch [167/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0020, LR: 0.0007530711\n",
      "Epoch [167/500], Batch [60/110], Train Loss: 0.0046, Val Loss: 0.0022, LR: 0.0007528264\n",
      "Epoch [167/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007525816\n",
      "Epoch [167/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007523367\n",
      "Epoch [167/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007520918\n",
      "Epoch [167/500], Batch [100/110], Train Loss: 0.0013, Val Loss: 0.0023, LR: 0.0007518467\n",
      "Epoch [167/500], Batch [110/110], Train Loss: 0.0028, Val Loss: 0.0026, LR: 0.0007516016\n",
      "Epoch [168/500], Batch [10/110], Train Loss: 0.0044, Val Loss: 0.0022, LR: 0.0007513564\n",
      "Epoch [168/500], Batch [20/110], Train Loss: 0.0084, Val Loss: 0.0016, LR: 0.0007511111\n",
      "Epoch [168/500], Batch [30/110], Train Loss: 0.0257, Val Loss: 0.0017, LR: 0.0007508658\n",
      "Epoch [168/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007506204\n",
      "Epoch [168/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007503748\n",
      "Epoch [168/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007501292\n",
      "Epoch [168/500], Batch [70/110], Train Loss: 0.0041, Val Loss: 0.0022, LR: 0.0007498836\n",
      "Epoch [168/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0023, LR: 0.0007496378\n",
      "Epoch [168/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0018, LR: 0.0007493920\n",
      "Epoch [168/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0007491460\n",
      "Epoch [168/500], Batch [110/110], Train Loss: 0.0029, Val Loss: 0.0020, LR: 0.0007489000\n",
      "Epoch [169/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0022, LR: 0.0007486540\n",
      "Epoch [169/500], Batch [20/110], Train Loss: 0.0059, Val Loss: 0.0015, LR: 0.0007484078\n",
      "Epoch [169/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007481616\n",
      "Epoch [169/500], Batch [40/110], Train Loss: 0.0137, Val Loss: 0.0017, LR: 0.0007479152\n",
      "Epoch [169/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007476688\n",
      "Epoch [169/500], Batch [60/110], Train Loss: 0.0036, Val Loss: 0.0021, LR: 0.0007474224\n",
      "Epoch [169/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0029, LR: 0.0007471758\n",
      "Epoch [169/500], Batch [80/110], Train Loss: 0.0085, Val Loss: 0.0020, LR: 0.0007469292\n",
      "Epoch [169/500], Batch [90/110], Train Loss: 0.1685, Val Loss: 0.0048, LR: 0.0007466825\n",
      "Epoch [169/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007464357\n",
      "Epoch [169/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0041, LR: 0.0007461888\n",
      "Epoch [170/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0007459419\n",
      "Epoch [170/500], Batch [20/110], Train Loss: 0.0148, Val Loss: 0.0020, LR: 0.0007456948\n",
      "Epoch [170/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007454477\n",
      "Epoch [170/500], Batch [40/110], Train Loss: 0.0050, Val Loss: 0.0018, LR: 0.0007452005\n",
      "Epoch [170/500], Batch [50/110], Train Loss: 0.0023, Val Loss: 0.0020, LR: 0.0007449533\n",
      "Epoch [170/500], Batch [60/110], Train Loss: 0.0035, Val Loss: 0.0028, LR: 0.0007447059\n",
      "Epoch [170/500], Batch [70/110], Train Loss: 0.2227, Val Loss: 0.0033, LR: 0.0007444585\n",
      "Epoch [170/500], Batch [80/110], Train Loss: 0.0079, Val Loss: 0.0029, LR: 0.0007442110\n",
      "Epoch [170/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0007439635\n",
      "Epoch [170/500], Batch [100/110], Train Loss: 0.0029, Val Loss: 0.0018, LR: 0.0007437158\n",
      "Epoch [170/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0007434681\n",
      "Epoch [171/500], Batch [10/110], Train Loss: 0.0690, Val Loss: 0.0052, LR: 0.0007432203\n",
      "Epoch [171/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007429724\n",
      "Epoch [171/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0007427244\n",
      "Epoch [171/500], Batch [40/110], Train Loss: 0.0015, Val Loss: 0.0038, LR: 0.0007424764\n",
      "Epoch [171/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0060, LR: 0.0007422282\n",
      "Epoch [171/500], Batch [60/110], Train Loss: 0.0366, Val Loss: 0.0037, LR: 0.0007419801\n",
      "Epoch [171/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0045, LR: 0.0007417318\n",
      "Epoch [171/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0035, LR: 0.0007414834\n",
      "Epoch [171/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0033, LR: 0.0007412350\n",
      "Epoch [171/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0007409865\n",
      "Epoch [171/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0007407379\n",
      "Confusion Matrix:\n",
      "[[630   1]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.99842   0.99921       631\n",
      "           1    0.99885   1.00000   0.99942       869\n",
      "\n",
      "    accuracy                        0.99933      1500\n",
      "   macro avg    0.99943   0.99921   0.99932      1500\n",
      "weighted avg    0.99933   0.99933   0.99933      1500\n",
      "\n",
      "Total Errors: 1\n",
      "Index: 1445, Predicted: 1, Actual: 0\n",
      "Epoch 171: OK- Accuracy: 0.99933, Precision: 0.99885, Recall: 1.00000, F1: 0.99942, ROC AUC: 0.99921, AUPR (PR-AUC): 0.99885, Sensitivity: 1.00000, Specificity: 0.99842, Far: 0.001584786053882726, False Positive Rate (FPR): 0.00158, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 305.21 MB\n",
      "Epoch [172/500], Batch [10/110], Train Loss: 0.0033, Val Loss: 0.0031, LR: 0.0007404893\n",
      "Epoch [172/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0046, LR: 0.0007402405\n",
      "Epoch [172/500], Batch [30/110], Train Loss: 0.0332, Val Loss: 0.0065, LR: 0.0007399917\n",
      "Epoch [172/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0007397428\n",
      "Epoch [172/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007394938\n",
      "Epoch [172/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0007392448\n",
      "Epoch [172/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0007389957\n",
      "Epoch [172/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007387465\n",
      "Epoch [172/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0019, LR: 0.0007384972\n",
      "Epoch [172/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007382479\n",
      "Epoch [172/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007379984\n",
      "Epoch [173/500], Batch [10/110], Train Loss: 0.0030, Val Loss: 0.0024, LR: 0.0007377489\n",
      "Epoch [173/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007374994\n",
      "Epoch [173/500], Batch [30/110], Train Loss: 0.0045, Val Loss: 0.0013, LR: 0.0007372497\n",
      "Epoch [173/500], Batch [40/110], Train Loss: 0.0032, Val Loss: 0.0017, LR: 0.0007370000\n",
      "Epoch [173/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0007367502\n",
      "Epoch [173/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007365003\n",
      "Epoch [173/500], Batch [70/110], Train Loss: 0.0130, Val Loss: 0.0015, LR: 0.0007362504\n",
      "Epoch [173/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007360003\n",
      "Epoch [173/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007357502\n",
      "Epoch [173/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0007355000\n",
      "Epoch [173/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007352498\n",
      "Epoch [174/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007349995\n",
      "Epoch [174/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0007347490\n",
      "Epoch [174/500], Batch [30/110], Train Loss: 0.0288, Val Loss: 0.0017, LR: 0.0007344986\n",
      "Epoch [174/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0042, LR: 0.0007342480\n",
      "Epoch [174/500], Batch [50/110], Train Loss: 0.0027, Val Loss: 0.0020, LR: 0.0007339974\n",
      "Epoch [174/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007337467\n",
      "Epoch [174/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007334959\n",
      "Epoch [174/500], Batch [80/110], Train Loss: 0.0030, Val Loss: 0.0030, LR: 0.0007332450\n",
      "Epoch [174/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0065, LR: 0.0007329941\n",
      "Epoch [174/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007327431\n",
      "Epoch [174/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0007324920\n",
      "Epoch [175/500], Batch [10/110], Train Loss: 0.0028, Val Loss: 0.0017, LR: 0.0007322409\n",
      "Epoch [175/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0007319897\n",
      "Epoch [175/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0007317384\n",
      "Epoch [175/500], Batch [40/110], Train Loss: 0.0046, Val Loss: 0.0015, LR: 0.0007314870\n",
      "Epoch [175/500], Batch [50/110], Train Loss: 0.0081, Val Loss: 0.0015, LR: 0.0007312355\n",
      "Epoch [175/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0017, LR: 0.0007309840\n",
      "Epoch [175/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007307324\n",
      "Epoch [175/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0015, LR: 0.0007304807\n",
      "Epoch [175/500], Batch [90/110], Train Loss: 0.0166, Val Loss: 0.0015, LR: 0.0007302290\n",
      "Epoch [175/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007299772\n",
      "Epoch [175/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0007297253\n",
      "Epoch [176/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007294733\n",
      "Epoch [176/500], Batch [20/110], Train Loss: 0.0058, Val Loss: 0.0015, LR: 0.0007292213\n",
      "Epoch [176/500], Batch [30/110], Train Loss: 0.0051, Val Loss: 0.0015, LR: 0.0007289692\n",
      "Epoch [176/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007287170\n",
      "Epoch [176/500], Batch [50/110], Train Loss: 0.0129, Val Loss: 0.0018, LR: 0.0007284648\n",
      "Epoch [176/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007282124\n",
      "Epoch [176/500], Batch [70/110], Train Loss: 0.0025, Val Loss: 0.0023, LR: 0.0007279600\n",
      "Epoch [176/500], Batch [80/110], Train Loss: 0.0019, Val Loss: 0.0018, LR: 0.0007277075\n",
      "Epoch [176/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0021, LR: 0.0007274550\n",
      "Epoch [176/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007272024\n",
      "Epoch [176/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007269497\n",
      "Epoch [177/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007266969\n",
      "Epoch [177/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007264441\n",
      "Epoch [177/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007261912\n",
      "Epoch [177/500], Batch [40/110], Train Loss: 0.0069, Val Loss: 0.0019, LR: 0.0007259382\n",
      "Epoch [177/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007256851\n",
      "Epoch [177/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007254320\n",
      "Epoch [177/500], Batch [70/110], Train Loss: 0.0096, Val Loss: 0.0018, LR: 0.0007251788\n",
      "Epoch [177/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007249256\n",
      "Epoch [177/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0007246722\n",
      "Epoch [177/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007244188\n",
      "Epoch [177/500], Batch [110/110], Train Loss: 0.0006, Val Loss: 0.0014, LR: 0.0007241653\n",
      "Epoch [178/500], Batch [10/110], Train Loss: 0.0037, Val Loss: 0.0034, LR: 0.0007239118\n",
      "Epoch [178/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007236581\n",
      "Epoch [178/500], Batch [30/110], Train Loss: 0.0019, Val Loss: 0.0019, LR: 0.0007234044\n",
      "Epoch [178/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007231507\n",
      "Epoch [178/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007228968\n",
      "Epoch [178/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0017, LR: 0.0007226429\n",
      "Epoch [178/500], Batch [70/110], Train Loss: 0.0184, Val Loss: 0.0014, LR: 0.0007223889\n",
      "Epoch [178/500], Batch [80/110], Train Loss: 0.0111, Val Loss: 0.0014, LR: 0.0007221349\n",
      "Epoch [178/500], Batch [90/110], Train Loss: 0.0362, Val Loss: 0.0019, LR: 0.0007218808\n",
      "Epoch [178/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007216266\n",
      "Epoch [178/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007213723\n",
      "Epoch [179/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007211180\n",
      "Epoch [179/500], Batch [20/110], Train Loss: 0.0014, Val Loss: 0.0015, LR: 0.0007208636\n",
      "Epoch [179/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0031, LR: 0.0007206091\n",
      "Epoch [179/500], Batch [40/110], Train Loss: 0.0019, Val Loss: 0.0026, LR: 0.0007203545\n",
      "Epoch [179/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007200999\n",
      "Epoch [179/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0007198452\n",
      "Epoch [179/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007195905\n",
      "Epoch [179/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007193356\n",
      "Epoch [179/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007190807\n",
      "Epoch [179/500], Batch [100/110], Train Loss: 0.0090, Val Loss: 0.0017, LR: 0.0007188258\n",
      "Epoch [179/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007185707\n",
      "Epoch [180/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0007183156\n",
      "Epoch [180/500], Batch [20/110], Train Loss: 0.0033, Val Loss: 0.0022, LR: 0.0007180605\n",
      "Epoch [180/500], Batch [30/110], Train Loss: 0.0300, Val Loss: 0.0020, LR: 0.0007178052\n",
      "Epoch [180/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007175499\n",
      "Epoch [180/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007172945\n",
      "Epoch [180/500], Batch [60/110], Train Loss: 0.0056, Val Loss: 0.0018, LR: 0.0007170391\n",
      "Epoch [180/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007167835\n",
      "Epoch [180/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007165279\n",
      "Epoch [180/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0020, LR: 0.0007162723\n",
      "Epoch [180/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0007160165\n",
      "Epoch [180/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0007157607\n",
      "Epoch [181/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007155049\n",
      "Epoch [181/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0013, LR: 0.0007152489\n",
      "Epoch [181/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0013, LR: 0.0007149929\n",
      "Epoch [181/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0007147369\n",
      "Epoch [181/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007144807\n",
      "Epoch [181/500], Batch [60/110], Train Loss: 0.0031, Val Loss: 0.0018, LR: 0.0007142245\n",
      "Epoch [181/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007139682\n",
      "Epoch [181/500], Batch [80/110], Train Loss: 0.0026, Val Loss: 0.0015, LR: 0.0007137119\n",
      "Epoch [181/500], Batch [90/110], Train Loss: 0.0300, Val Loss: 0.0017, LR: 0.0007134555\n",
      "Epoch [181/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007131990\n",
      "Epoch [181/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0014, LR: 0.0007129424\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 181: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.047 sec , Memory Usage: 305.22 MB\n",
      "Epoch [182/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0052, LR: 0.0007126858\n",
      "Epoch [182/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0051, LR: 0.0007124291\n",
      "Epoch [182/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0007121724\n",
      "Epoch [182/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0017, LR: 0.0007119156\n",
      "Epoch [182/500], Batch [50/110], Train Loss: 0.0019, Val Loss: 0.0017, LR: 0.0007116587\n",
      "Epoch [182/500], Batch [60/110], Train Loss: 0.0297, Val Loss: 0.0019, LR: 0.0007114017\n",
      "Epoch [182/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0007111447\n",
      "Epoch [182/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0016, LR: 0.0007108876\n",
      "Epoch [182/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0020, LR: 0.0007106304\n",
      "Epoch [182/500], Batch [100/110], Train Loss: 0.0108, Val Loss: 0.0018, LR: 0.0007103732\n",
      "Epoch [182/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0007101159\n",
      "Epoch [183/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0020, LR: 0.0007098586\n",
      "Epoch [183/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0007096011\n",
      "Epoch [183/500], Batch [30/110], Train Loss: 0.0021, Val Loss: 0.0022, LR: 0.0007093436\n",
      "Epoch [183/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0018, LR: 0.0007090861\n",
      "Epoch [183/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0019, LR: 0.0007088284\n",
      "Epoch [183/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0017, LR: 0.0007085708\n",
      "Epoch [183/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0007083130\n",
      "Epoch [183/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0007080552\n",
      "Epoch [183/500], Batch [90/110], Train Loss: 0.0339, Val Loss: 0.0018, LR: 0.0007077973\n",
      "Epoch [183/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0015, LR: 0.0007075393\n",
      "Epoch [183/500], Batch [110/110], Train Loss: 0.0050, Val Loss: 0.0016, LR: 0.0007072813\n",
      "Epoch [184/500], Batch [10/110], Train Loss: 0.2128, Val Loss: 0.0017, LR: 0.0007070232\n",
      "Epoch [184/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007067650\n",
      "Epoch [184/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0007065068\n",
      "Epoch [184/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0026, LR: 0.0007062485\n",
      "Epoch [184/500], Batch [50/110], Train Loss: 0.0097, Val Loss: 0.0021, LR: 0.0007059902\n",
      "Epoch [184/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007057318\n",
      "Epoch [184/500], Batch [70/110], Train Loss: 0.0040, Val Loss: 0.0017, LR: 0.0007054733\n",
      "Epoch [184/500], Batch [80/110], Train Loss: 0.0067, Val Loss: 0.0014, LR: 0.0007052147\n",
      "Epoch [184/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007049561\n",
      "Epoch [184/500], Batch [100/110], Train Loss: 0.0025, Val Loss: 0.0017, LR: 0.0007046974\n",
      "Epoch [184/500], Batch [110/110], Train Loss: 0.0014, Val Loss: 0.0021, LR: 0.0007044387\n",
      "Epoch [185/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0007041799\n",
      "Epoch [185/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0021, LR: 0.0007039210\n",
      "Epoch [185/500], Batch [30/110], Train Loss: 0.0138, Val Loss: 0.0017, LR: 0.0007036621\n",
      "Epoch [185/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007034031\n",
      "Epoch [185/500], Batch [50/110], Train Loss: 0.0025, Val Loss: 0.0020, LR: 0.0007031440\n",
      "Epoch [185/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007028848\n",
      "Epoch [185/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0007026256\n",
      "Epoch [185/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0007023664\n",
      "Epoch [185/500], Batch [90/110], Train Loss: 0.0007, Val Loss: 0.0022, LR: 0.0007021071\n",
      "Epoch [185/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0007018477\n",
      "Epoch [185/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0007015882\n",
      "Epoch [186/500], Batch [10/110], Train Loss: 0.0092, Val Loss: 0.0023, LR: 0.0007013287\n",
      "Epoch [186/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0007010691\n",
      "Epoch [186/500], Batch [30/110], Train Loss: 0.2060, Val Loss: 0.0025, LR: 0.0007008095\n",
      "Epoch [186/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0007005497\n",
      "Epoch [186/500], Batch [50/110], Train Loss: 0.0064, Val Loss: 0.0018, LR: 0.0007002900\n",
      "Epoch [186/500], Batch [60/110], Train Loss: 0.0022, Val Loss: 0.0019, LR: 0.0007000301\n",
      "Epoch [186/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0006997702\n",
      "Epoch [186/500], Batch [80/110], Train Loss: 0.0009, Val Loss: 0.0022, LR: 0.0006995102\n",
      "Epoch [186/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0006992502\n",
      "Epoch [186/500], Batch [100/110], Train Loss: 0.0023, Val Loss: 0.0019, LR: 0.0006989901\n",
      "Epoch [186/500], Batch [110/110], Train Loss: 0.0026, Val Loss: 0.0020, LR: 0.0006987300\n",
      "Epoch [187/500], Batch [10/110], Train Loss: 0.0665, Val Loss: 0.0033, LR: 0.0006984697\n",
      "Epoch [187/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006982095\n",
      "Epoch [187/500], Batch [30/110], Train Loss: 0.0408, Val Loss: 0.0015, LR: 0.0006979491\n",
      "Epoch [187/500], Batch [40/110], Train Loss: 0.0110, Val Loss: 0.0037, LR: 0.0006976887\n",
      "Epoch [187/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0025, LR: 0.0006974282\n",
      "Epoch [187/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0028, LR: 0.0006971677\n",
      "Epoch [187/500], Batch [70/110], Train Loss: 0.0028, Val Loss: 0.0034, LR: 0.0006969071\n",
      "Epoch [187/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0040, LR: 0.0006966464\n",
      "Epoch [187/500], Batch [90/110], Train Loss: 0.0071, Val Loss: 0.0031, LR: 0.0006963857\n",
      "Epoch [187/500], Batch [100/110], Train Loss: 0.0036, Val Loss: 0.0032, LR: 0.0006961249\n",
      "Epoch [187/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0006958641\n",
      "Epoch [188/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0006956032\n",
      "Epoch [188/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006953422\n",
      "Epoch [188/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0023, LR: 0.0006950812\n",
      "Epoch [188/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0026, LR: 0.0006948201\n",
      "Epoch [188/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0006945589\n",
      "Epoch [188/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0029, LR: 0.0006942977\n",
      "Epoch [188/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0006940364\n",
      "Epoch [188/500], Batch [80/110], Train Loss: 0.0030, Val Loss: 0.0020, LR: 0.0006937751\n",
      "Epoch [188/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0025, LR: 0.0006935136\n",
      "Epoch [188/500], Batch [100/110], Train Loss: 0.0325, Val Loss: 0.0024, LR: 0.0006932522\n",
      "Epoch [188/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0006929907\n",
      "Epoch [189/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006927291\n",
      "Epoch [189/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006924674\n",
      "Epoch [189/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0006922057\n",
      "Epoch [189/500], Batch [40/110], Train Loss: 0.0054, Val Loss: 0.0022, LR: 0.0006919439\n",
      "Epoch [189/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0006916821\n",
      "Epoch [189/500], Batch [60/110], Train Loss: 0.0052, Val Loss: 0.0014, LR: 0.0006914202\n",
      "Epoch [189/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0022, LR: 0.0006911582\n",
      "Epoch [189/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0006908962\n",
      "Epoch [189/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006906341\n",
      "Epoch [189/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0013, LR: 0.0006903720\n",
      "Epoch [189/500], Batch [110/110], Train Loss: 0.0206, Val Loss: 0.0013, LR: 0.0006901098\n",
      "Epoch [190/500], Batch [10/110], Train Loss: 0.0028, Val Loss: 0.0014, LR: 0.0006898475\n",
      "Epoch [190/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006895852\n",
      "Epoch [190/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0006893228\n",
      "Epoch [190/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0030, LR: 0.0006890604\n",
      "Epoch [190/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0033, LR: 0.0006887979\n",
      "Epoch [190/500], Batch [60/110], Train Loss: 0.0057, Val Loss: 0.0032, LR: 0.0006885353\n",
      "Epoch [190/500], Batch [70/110], Train Loss: 0.0191, Val Loss: 0.0031, LR: 0.0006882727\n",
      "Epoch [190/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006880100\n",
      "Epoch [190/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006877473\n",
      "Epoch [190/500], Batch [100/110], Train Loss: 0.0029, Val Loss: 0.0013, LR: 0.0006874845\n",
      "Epoch [190/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006872217\n",
      "Epoch [191/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006869587\n",
      "Epoch [191/500], Batch [20/110], Train Loss: 0.0030, Val Loss: 0.0018, LR: 0.0006866958\n",
      "Epoch [191/500], Batch [30/110], Train Loss: 0.0270, Val Loss: 0.0016, LR: 0.0006864327\n",
      "Epoch [191/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0013, LR: 0.0006861696\n",
      "Epoch [191/500], Batch [50/110], Train Loss: 0.0123, Val Loss: 0.0015, LR: 0.0006859065\n",
      "Epoch [191/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006856433\n",
      "Epoch [191/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006853800\n",
      "Epoch [191/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006851167\n",
      "Epoch [191/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006848533\n",
      "Epoch [191/500], Batch [100/110], Train Loss: 0.0456, Val Loss: 0.0017, LR: 0.0006845898\n",
      "Epoch [191/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006843263\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 191: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 305.23 MB\n",
      "Epoch [192/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006840627\n",
      "Epoch [192/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006837991\n",
      "Epoch [192/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006835354\n",
      "Epoch [192/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006832717\n",
      "Epoch [192/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006830079\n",
      "Epoch [192/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0020, LR: 0.0006827440\n",
      "Epoch [192/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0012, LR: 0.0006824801\n",
      "Epoch [192/500], Batch [80/110], Train Loss: 0.0137, Val Loss: 0.0011, LR: 0.0006822161\n",
      "Epoch [192/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006819521\n",
      "Epoch [192/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006816880\n",
      "Epoch [192/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0012, LR: 0.0006814239\n",
      "Epoch [193/500], Batch [10/110], Train Loss: 0.0025, Val Loss: 0.0011, LR: 0.0006811597\n",
      "Epoch [193/500], Batch [20/110], Train Loss: 0.0090, Val Loss: 0.0014, LR: 0.0006808954\n",
      "Epoch [193/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006806311\n",
      "Epoch [193/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006803667\n",
      "Epoch [193/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006801023\n",
      "Epoch [193/500], Batch [60/110], Train Loss: 0.0071, Val Loss: 0.0010, LR: 0.0006798378\n",
      "Epoch [193/500], Batch [70/110], Train Loss: 0.0025, Val Loss: 0.0012, LR: 0.0006795732\n",
      "Epoch [193/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0006793086\n",
      "Epoch [193/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0023, LR: 0.0006790440\n",
      "Epoch [193/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006787793\n",
      "Epoch [193/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006785145\n",
      "Epoch [194/500], Batch [10/110], Train Loss: 0.0033, Val Loss: 0.0014, LR: 0.0006782497\n",
      "Epoch [194/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006779848\n",
      "Epoch [194/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0011, LR: 0.0006777198\n",
      "Epoch [194/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006774548\n",
      "Epoch [194/500], Batch [50/110], Train Loss: 0.0333, Val Loss: 0.0012, LR: 0.0006771898\n",
      "Epoch [194/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006769247\n",
      "Epoch [194/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0006766595\n",
      "Epoch [194/500], Batch [80/110], Train Loss: 0.0020, Val Loss: 0.0012, LR: 0.0006763943\n",
      "Epoch [194/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006761290\n",
      "Epoch [194/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006758636\n",
      "Epoch [194/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006755982\n",
      "Epoch [195/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0019, LR: 0.0006753328\n",
      "Epoch [195/500], Batch [20/110], Train Loss: 0.0010, Val Loss: 0.0030, LR: 0.0006750673\n",
      "Epoch [195/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006748017\n",
      "Epoch [195/500], Batch [40/110], Train Loss: 0.0051, Val Loss: 0.0011, LR: 0.0006745361\n",
      "Epoch [195/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0013, LR: 0.0006742704\n",
      "Epoch [195/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006740047\n",
      "Epoch [195/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006737389\n",
      "Epoch [195/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006734731\n",
      "Epoch [195/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006732072\n",
      "Epoch [195/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006729413\n",
      "Epoch [195/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006726753\n",
      "Epoch [196/500], Batch [10/110], Train Loss: 0.0043, Val Loss: 0.0011, LR: 0.0006724092\n",
      "Epoch [196/500], Batch [20/110], Train Loss: 0.0114, Val Loss: 0.0011, LR: 0.0006721431\n",
      "Epoch [196/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006718769\n",
      "Epoch [196/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0006716107\n",
      "Epoch [196/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006713444\n",
      "Epoch [196/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006710781\n",
      "Epoch [196/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0011, LR: 0.0006708117\n",
      "Epoch [196/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006705453\n",
      "Epoch [196/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0023, LR: 0.0006702788\n",
      "Epoch [196/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006700123\n",
      "Epoch [196/500], Batch [110/110], Train Loss: 0.0108, Val Loss: 0.0012, LR: 0.0006697457\n",
      "Epoch [197/500], Batch [10/110], Train Loss: 0.0035, Val Loss: 0.0018, LR: 0.0006694790\n",
      "Epoch [197/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006692123\n",
      "Epoch [197/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006689456\n",
      "Epoch [197/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006686787\n",
      "Epoch [197/500], Batch [50/110], Train Loss: 0.0012, Val Loss: 0.0026, LR: 0.0006684119\n",
      "Epoch [197/500], Batch [60/110], Train Loss: 0.0323, Val Loss: 0.0016, LR: 0.0006681450\n",
      "Epoch [197/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006678780\n",
      "Epoch [197/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006676110\n",
      "Epoch [197/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006673439\n",
      "Epoch [197/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006670768\n",
      "Epoch [197/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0006668096\n",
      "Epoch [198/500], Batch [10/110], Train Loss: 0.0048, Val Loss: 0.0012, LR: 0.0006665423\n",
      "Epoch [198/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0035, LR: 0.0006662750\n",
      "Epoch [198/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006660077\n",
      "Epoch [198/500], Batch [40/110], Train Loss: 0.0072, Val Loss: 0.0012, LR: 0.0006657403\n",
      "Epoch [198/500], Batch [50/110], Train Loss: 0.0229, Val Loss: 0.0011, LR: 0.0006654729\n",
      "Epoch [198/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006652054\n",
      "Epoch [198/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006649378\n",
      "Epoch [198/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006646702\n",
      "Epoch [198/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0006644026\n",
      "Epoch [198/500], Batch [100/110], Train Loss: 0.0016, Val Loss: 0.0037, LR: 0.0006641348\n",
      "Epoch [198/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0022, LR: 0.0006638671\n",
      "Epoch [199/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0038, LR: 0.0006635993\n",
      "Epoch [199/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0058, LR: 0.0006633314\n",
      "Epoch [199/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0036, LR: 0.0006630635\n",
      "Epoch [199/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0035, LR: 0.0006627955\n",
      "Epoch [199/500], Batch [50/110], Train Loss: 0.0070, Val Loss: 0.0040, LR: 0.0006625275\n",
      "Epoch [199/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0033, LR: 0.0006622594\n",
      "Epoch [199/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0029, LR: 0.0006619913\n",
      "Epoch [199/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0029, LR: 0.0006617231\n",
      "Epoch [199/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006614549\n",
      "Epoch [199/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006611867\n",
      "Epoch [199/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0006609183\n",
      "Epoch [200/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0011, LR: 0.0006606500\n",
      "Epoch [200/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006603815\n",
      "Epoch [200/500], Batch [30/110], Train Loss: 0.0043, Val Loss: 0.0011, LR: 0.0006601130\n",
      "Epoch [200/500], Batch [40/110], Train Loss: 0.0371, Val Loss: 0.0014, LR: 0.0006598445\n",
      "Epoch [200/500], Batch [50/110], Train Loss: 0.0354, Val Loss: 0.0014, LR: 0.0006595759\n",
      "Epoch [200/500], Batch [60/110], Train Loss: 0.0148, Val Loss: 0.0011, LR: 0.0006593073\n",
      "Epoch [200/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006590386\n",
      "Epoch [200/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006587699\n",
      "Epoch [200/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006585011\n",
      "Epoch [200/500], Batch [100/110], Train Loss: 0.0024, Val Loss: 0.0011, LR: 0.0006582323\n",
      "Epoch [200/500], Batch [110/110], Train Loss: 0.0078, Val Loss: 0.0010, LR: 0.0006579634\n",
      "Epoch [201/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006576945\n",
      "Epoch [201/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006574255\n",
      "Epoch [201/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006571565\n",
      "Epoch [201/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006568874\n",
      "Epoch [201/500], Batch [50/110], Train Loss: 0.0022, Val Loss: 0.0010, LR: 0.0006566183\n",
      "Epoch [201/500], Batch [60/110], Train Loss: 0.0127, Val Loss: 0.0011, LR: 0.0006563491\n",
      "Epoch [201/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006560799\n",
      "Epoch [201/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006558106\n",
      "Epoch [201/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006555413\n",
      "Epoch [201/500], Batch [100/110], Train Loss: 0.0075, Val Loss: 0.0010, LR: 0.0006552719\n",
      "Epoch [201/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006550025\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 201: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.047 sec , Memory Usage: 305.23 MB\n",
      "Epoch [202/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006547330\n",
      "Epoch [202/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006544635\n",
      "Epoch [202/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0014, LR: 0.0006541939\n",
      "Epoch [202/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0006539243\n",
      "Epoch [202/500], Batch [50/110], Train Loss: 0.0026, Val Loss: 0.0010, LR: 0.0006536546\n",
      "Epoch [202/500], Batch [60/110], Train Loss: 0.0092, Val Loss: 0.0015, LR: 0.0006533849\n",
      "Epoch [202/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006531151\n",
      "Epoch [202/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006528453\n",
      "Epoch [202/500], Batch [90/110], Train Loss: 0.0123, Val Loss: 0.0011, LR: 0.0006525754\n",
      "Epoch [202/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0010, LR: 0.0006523055\n",
      "Epoch [202/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006520356\n",
      "Epoch [203/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0006517656\n",
      "Epoch [203/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0018, LR: 0.0006514955\n",
      "Epoch [203/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0016, LR: 0.0006512254\n",
      "Epoch [203/500], Batch [40/110], Train Loss: 0.0010, Val Loss: 0.0015, LR: 0.0006509553\n",
      "Epoch [203/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006506851\n",
      "Epoch [203/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006504148\n",
      "Epoch [203/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0016, LR: 0.0006501445\n",
      "Epoch [203/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006498742\n",
      "Epoch [203/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0006496038\n",
      "Epoch [203/500], Batch [100/110], Train Loss: 0.0022, Val Loss: 0.0018, LR: 0.0006493334\n",
      "Epoch [203/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006490629\n",
      "Epoch [204/500], Batch [10/110], Train Loss: 0.0101, Val Loss: 0.0014, LR: 0.0006487924\n",
      "Epoch [204/500], Batch [20/110], Train Loss: 0.0040, Val Loss: 0.0012, LR: 0.0006485218\n",
      "Epoch [204/500], Batch [30/110], Train Loss: 0.0016, Val Loss: 0.0013, LR: 0.0006482512\n",
      "Epoch [204/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006479805\n",
      "Epoch [204/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006477098\n",
      "Epoch [204/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0006474390\n",
      "Epoch [204/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006471682\n",
      "Epoch [204/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006468974\n",
      "Epoch [204/500], Batch [90/110], Train Loss: 0.0183, Val Loss: 0.0010, LR: 0.0006466265\n",
      "Epoch [204/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006463555\n",
      "Epoch [204/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0006460845\n",
      "Epoch [205/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0011, LR: 0.0006458135\n",
      "Epoch [205/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0022, LR: 0.0006455424\n",
      "Epoch [205/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006452713\n",
      "Epoch [205/500], Batch [40/110], Train Loss: 0.0031, Val Loss: 0.0010, LR: 0.0006450001\n",
      "Epoch [205/500], Batch [50/110], Train Loss: 0.0018, Val Loss: 0.0010, LR: 0.0006447289\n",
      "Epoch [205/500], Batch [60/110], Train Loss: 0.0013, Val Loss: 0.0014, LR: 0.0006444576\n",
      "Epoch [205/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0010, LR: 0.0006441863\n",
      "Epoch [205/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006439149\n",
      "Epoch [205/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006436435\n",
      "Epoch [205/500], Batch [100/110], Train Loss: 0.0043, Val Loss: 0.0012, LR: 0.0006433721\n",
      "Epoch [205/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006431006\n",
      "Epoch [206/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006428291\n",
      "Epoch [206/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006425575\n",
      "Epoch [206/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006422858\n",
      "Epoch [206/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0011, LR: 0.0006420142\n",
      "Epoch [206/500], Batch [50/110], Train Loss: 0.0030, Val Loss: 0.0010, LR: 0.0006417425\n",
      "Epoch [206/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006414707\n",
      "Epoch [206/500], Batch [70/110], Train Loss: 0.0020, Val Loss: 0.0010, LR: 0.0006411989\n",
      "Epoch [206/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006409270\n",
      "Epoch [206/500], Batch [90/110], Train Loss: 0.0122, Val Loss: 0.0010, LR: 0.0006406551\n",
      "Epoch [206/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0011, LR: 0.0006403832\n",
      "Epoch [206/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006401112\n",
      "Epoch [207/500], Batch [10/110], Train Loss: 0.0049, Val Loss: 0.0017, LR: 0.0006398392\n",
      "Epoch [207/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006395671\n",
      "Epoch [207/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006392950\n",
      "Epoch [207/500], Batch [40/110], Train Loss: 0.0059, Val Loss: 0.0011, LR: 0.0006390228\n",
      "Epoch [207/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0010, LR: 0.0006387506\n",
      "Epoch [207/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006384784\n",
      "Epoch [207/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006382061\n",
      "Epoch [207/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006379337\n",
      "Epoch [207/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006376614\n",
      "Epoch [207/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006373889\n",
      "Epoch [207/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006371165\n",
      "Epoch [208/500], Batch [10/110], Train Loss: 0.1306, Val Loss: 0.0013, LR: 0.0006368440\n",
      "Epoch [208/500], Batch [20/110], Train Loss: 0.0245, Val Loss: 0.0031, LR: 0.0006365714\n",
      "Epoch [208/500], Batch [30/110], Train Loss: 0.0038, Val Loss: 0.0024, LR: 0.0006362988\n",
      "Epoch [208/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0029, LR: 0.0006360262\n",
      "Epoch [208/500], Batch [50/110], Train Loss: 0.0141, Val Loss: 0.0017, LR: 0.0006357535\n",
      "Epoch [208/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006354808\n",
      "Epoch [208/500], Batch [70/110], Train Loss: 0.0015, Val Loss: 0.0016, LR: 0.0006352080\n",
      "Epoch [208/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006349352\n",
      "Epoch [208/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0011, LR: 0.0006346624\n",
      "Epoch [208/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006343895\n",
      "Epoch [208/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006341165\n",
      "Epoch [209/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006338436\n",
      "Epoch [209/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0006335706\n",
      "Epoch [209/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0011, LR: 0.0006332975\n",
      "Epoch [209/500], Batch [40/110], Train Loss: 0.0281, Val Loss: 0.0016, LR: 0.0006330244\n",
      "Epoch [209/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006327512\n",
      "Epoch [209/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0006324781\n",
      "Epoch [209/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0028, LR: 0.0006322048\n",
      "Epoch [209/500], Batch [80/110], Train Loss: 0.0029, Val Loss: 0.0040, LR: 0.0006319316\n",
      "Epoch [209/500], Batch [90/110], Train Loss: 0.0015, Val Loss: 0.0031, LR: 0.0006316583\n",
      "Epoch [209/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0031, LR: 0.0006313849\n",
      "Epoch [209/500], Batch [110/110], Train Loss: 0.0332, Val Loss: 0.0036, LR: 0.0006311115\n",
      "Epoch [210/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0033, LR: 0.0006308381\n",
      "Epoch [210/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0037, LR: 0.0006305646\n",
      "Epoch [210/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0006302911\n",
      "Epoch [210/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0006300175\n",
      "Epoch [210/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006297439\n",
      "Epoch [210/500], Batch [60/110], Train Loss: 0.0020, Val Loss: 0.0011, LR: 0.0006294703\n",
      "Epoch [210/500], Batch [70/110], Train Loss: 0.0227, Val Loss: 0.0011, LR: 0.0006291966\n",
      "Epoch [210/500], Batch [80/110], Train Loss: 0.0022, Val Loss: 0.0012, LR: 0.0006289229\n",
      "Epoch [210/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006286491\n",
      "Epoch [210/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0014, LR: 0.0006283753\n",
      "Epoch [210/500], Batch [110/110], Train Loss: 0.0009, Val Loss: 0.0011, LR: 0.0006281015\n",
      "Epoch [211/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006278276\n",
      "Epoch [211/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006275537\n",
      "Epoch [211/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006272797\n",
      "Epoch [211/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0019, LR: 0.0006270057\n",
      "Epoch [211/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006267317\n",
      "Epoch [211/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006264576\n",
      "Epoch [211/500], Batch [70/110], Train Loss: 0.0022, Val Loss: 0.0011, LR: 0.0006261835\n",
      "Epoch [211/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006259093\n",
      "Epoch [211/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006256351\n",
      "Epoch [211/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006253609\n",
      "Epoch [211/500], Batch [110/110], Train Loss: 0.0084, Val Loss: 0.0011, LR: 0.0006250866\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 211: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 305.23 MB\n",
      "Epoch [212/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006248123\n",
      "Epoch [212/500], Batch [20/110], Train Loss: 0.0032, Val Loss: 0.0011, LR: 0.0006245379\n",
      "Epoch [212/500], Batch [30/110], Train Loss: 0.0042, Val Loss: 0.0011, LR: 0.0006242636\n",
      "Epoch [212/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0011, LR: 0.0006239891\n",
      "Epoch [212/500], Batch [50/110], Train Loss: 0.0140, Val Loss: 0.0010, LR: 0.0006237146\n",
      "Epoch [212/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0012, LR: 0.0006234401\n",
      "Epoch [212/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006231656\n",
      "Epoch [212/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006228910\n",
      "Epoch [212/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0011, LR: 0.0006226164\n",
      "Epoch [212/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006223417\n",
      "Epoch [212/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006220670\n",
      "Epoch [213/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0006217923\n",
      "Epoch [213/500], Batch [20/110], Train Loss: 0.0051, Val Loss: 0.0013, LR: 0.0006215175\n",
      "Epoch [213/500], Batch [30/110], Train Loss: 0.0264, Val Loss: 0.0017, LR: 0.0006212427\n",
      "Epoch [213/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006209678\n",
      "Epoch [213/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0021, LR: 0.0006206929\n",
      "Epoch [213/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0014, LR: 0.0006204180\n",
      "Epoch [213/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0006201430\n",
      "Epoch [213/500], Batch [80/110], Train Loss: 0.0012, Val Loss: 0.0021, LR: 0.0006198680\n",
      "Epoch [213/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0022, LR: 0.0006195930\n",
      "Epoch [213/500], Batch [100/110], Train Loss: 0.0024, Val Loss: 0.0022, LR: 0.0006193179\n",
      "Epoch [213/500], Batch [110/110], Train Loss: 0.0028, Val Loss: 0.0015, LR: 0.0006190428\n",
      "Epoch [214/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006187676\n",
      "Epoch [214/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006184924\n",
      "Epoch [214/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006182172\n",
      "Epoch [214/500], Batch [40/110], Train Loss: 0.0020, Val Loss: 0.0016, LR: 0.0006179419\n",
      "Epoch [214/500], Batch [50/110], Train Loss: 0.0016, Val Loss: 0.0013, LR: 0.0006176666\n",
      "Epoch [214/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006173913\n",
      "Epoch [214/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0015, LR: 0.0006171159\n",
      "Epoch [214/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0006168405\n",
      "Epoch [214/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0014, LR: 0.0006165650\n",
      "Epoch [214/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0018, LR: 0.0006162895\n",
      "Epoch [214/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0019, LR: 0.0006160140\n",
      "Epoch [215/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006157385\n",
      "Epoch [215/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0006154629\n",
      "Epoch [215/500], Batch [30/110], Train Loss: 0.0049, Val Loss: 0.0010, LR: 0.0006151872\n",
      "Epoch [215/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006149116\n",
      "Epoch [215/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0034, LR: 0.0006146359\n",
      "Epoch [215/500], Batch [60/110], Train Loss: 0.0025, Val Loss: 0.0008, LR: 0.0006143601\n",
      "Epoch [215/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0020, LR: 0.0006140844\n",
      "Epoch [215/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006138085\n",
      "Epoch [215/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006135327\n",
      "Epoch [215/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006132568\n",
      "Epoch [215/500], Batch [110/110], Train Loss: 0.0075, Val Loss: 0.0009, LR: 0.0006129809\n",
      "Epoch [216/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0006127050\n",
      "Epoch [216/500], Batch [20/110], Train Loss: 0.0025, Val Loss: 0.0011, LR: 0.0006124290\n",
      "Epoch [216/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0006121529\n",
      "Epoch [216/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006118769\n",
      "Epoch [216/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0006116008\n",
      "Epoch [216/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006113247\n",
      "Epoch [216/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006110485\n",
      "Epoch [216/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0006107723\n",
      "Epoch [216/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006104961\n",
      "Epoch [216/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006102198\n",
      "Epoch [216/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006099435\n",
      "Epoch [217/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006096672\n",
      "Epoch [217/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0009, LR: 0.0006093908\n",
      "Epoch [217/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006091144\n",
      "Epoch [217/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006088380\n",
      "Epoch [217/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006085615\n",
      "Epoch [217/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006082850\n",
      "Epoch [217/500], Batch [70/110], Train Loss: 0.0031, Val Loss: 0.0008, LR: 0.0006080085\n",
      "Epoch [217/500], Batch [80/110], Train Loss: 0.0022, Val Loss: 0.0013, LR: 0.0006077319\n",
      "Epoch [217/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006074553\n",
      "Epoch [217/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006071787\n",
      "Epoch [217/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006069020\n",
      "Epoch [218/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0006066253\n",
      "Epoch [218/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0006063486\n",
      "Epoch [218/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006060718\n",
      "Epoch [218/500], Batch [40/110], Train Loss: 0.0016, Val Loss: 0.0008, LR: 0.0006057950\n",
      "Epoch [218/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0006055181\n",
      "Epoch [218/500], Batch [60/110], Train Loss: 0.0075, Val Loss: 0.0008, LR: 0.0006052413\n",
      "Epoch [218/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006049644\n",
      "Epoch [218/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006046874\n",
      "Epoch [218/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006044105\n",
      "Epoch [218/500], Batch [100/110], Train Loss: 0.0092, Val Loss: 0.0008, LR: 0.0006041335\n",
      "Epoch [218/500], Batch [110/110], Train Loss: 0.0037, Val Loss: 0.0009, LR: 0.0006038564\n",
      "Epoch [219/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006035794\n",
      "Epoch [219/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0006033023\n",
      "Epoch [219/500], Batch [30/110], Train Loss: 0.0043, Val Loss: 0.0014, LR: 0.0006030252\n",
      "Epoch [219/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006027480\n",
      "Epoch [219/500], Batch [50/110], Train Loss: 0.0027, Val Loss: 0.0010, LR: 0.0006024708\n",
      "Epoch [219/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006021936\n",
      "Epoch [219/500], Batch [70/110], Train Loss: 0.0021, Val Loss: 0.0009, LR: 0.0006019163\n",
      "Epoch [219/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0006016390\n",
      "Epoch [219/500], Batch [90/110], Train Loss: 0.0016, Val Loss: 0.0010, LR: 0.0006013617\n",
      "Epoch [219/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0006010844\n",
      "Epoch [219/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0006008070\n",
      "Epoch [220/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0006005296\n",
      "Epoch [220/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0006002521\n",
      "Epoch [220/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0010, LR: 0.0005999747\n",
      "Epoch [220/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0005996972\n",
      "Epoch [220/500], Batch [50/110], Train Loss: 0.0007, Val Loss: 0.0009, LR: 0.0005994196\n",
      "Epoch [220/500], Batch [60/110], Train Loss: 0.0012, Val Loss: 0.0012, LR: 0.0005991420\n",
      "Epoch [220/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005988644\n",
      "Epoch [220/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0015, LR: 0.0005985868\n",
      "Epoch [220/500], Batch [90/110], Train Loss: 0.0019, Val Loss: 0.0014, LR: 0.0005983092\n",
      "Epoch [220/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0005980315\n",
      "Epoch [220/500], Batch [110/110], Train Loss: 0.0223, Val Loss: 0.0019, LR: 0.0005977538\n",
      "Epoch [221/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0005974760\n",
      "Epoch [221/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005971982\n",
      "Epoch [221/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005969204\n",
      "Epoch [221/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0010, LR: 0.0005966426\n",
      "Epoch [221/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005963647\n",
      "Epoch [221/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0024, LR: 0.0005960868\n",
      "Epoch [221/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005958089\n",
      "Epoch [221/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0018, LR: 0.0005955309\n",
      "Epoch [221/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005952529\n",
      "Epoch [221/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005949749\n",
      "Epoch [221/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005946969\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 221: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.047 sec , Memory Usage: 305.23 MB\n",
      "Epoch [222/500], Batch [10/110], Train Loss: 0.0048, Val Loss: 0.0025, LR: 0.0005944188\n",
      "Epoch [222/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0027, LR: 0.0005941407\n",
      "Epoch [222/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005938625\n",
      "Epoch [222/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005935844\n",
      "Epoch [222/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0008, LR: 0.0005933062\n",
      "Epoch [222/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0005930280\n",
      "Epoch [222/500], Batch [70/110], Train Loss: 0.0659, Val Loss: 0.0024, LR: 0.0005927497\n",
      "Epoch [222/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0034, LR: 0.0005924714\n",
      "Epoch [222/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0007, LR: 0.0005921931\n",
      "Epoch [222/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0005919148\n",
      "Epoch [222/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005916364\n",
      "Epoch [223/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005913580\n",
      "Epoch [223/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005910796\n",
      "Epoch [223/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0005908012\n",
      "Epoch [223/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0007, LR: 0.0005905227\n",
      "Epoch [223/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005902442\n",
      "Epoch [223/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005899656\n",
      "Epoch [223/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005896871\n",
      "Epoch [223/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0005894085\n",
      "Epoch [223/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005891299\n",
      "Epoch [223/500], Batch [100/110], Train Loss: 0.0026, Val Loss: 0.0009, LR: 0.0005888512\n",
      "Epoch [223/500], Batch [110/110], Train Loss: 0.0126, Val Loss: 0.0006, LR: 0.0005885726\n",
      "Epoch [224/500], Batch [10/110], Train Loss: 0.0086, Val Loss: 0.0006, LR: 0.0005882939\n",
      "Epoch [224/500], Batch [20/110], Train Loss: 0.0045, Val Loss: 0.0008, LR: 0.0005880151\n",
      "Epoch [224/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005877364\n",
      "Epoch [224/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0005874576\n",
      "Epoch [224/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005871788\n",
      "Epoch [224/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005869000\n",
      "Epoch [224/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005866211\n",
      "Epoch [224/500], Batch [80/110], Train Loss: 0.0012, Val Loss: 0.0007, LR: 0.0005863422\n",
      "Epoch [224/500], Batch [90/110], Train Loss: 0.0028, Val Loss: 0.0007, LR: 0.0005860633\n",
      "Epoch [224/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005857844\n",
      "Epoch [224/500], Batch [110/110], Train Loss: 0.0023, Val Loss: 0.0007, LR: 0.0005855054\n",
      "Epoch [225/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005852264\n",
      "Epoch [225/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0007, LR: 0.0005849474\n",
      "Epoch [225/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0010, LR: 0.0005846683\n",
      "Epoch [225/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005843893\n",
      "Epoch [225/500], Batch [50/110], Train Loss: 0.0010, Val Loss: 0.0008, LR: 0.0005841102\n",
      "Epoch [225/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0009, LR: 0.0005838311\n",
      "Epoch [225/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005835519\n",
      "Epoch [225/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005832727\n",
      "Epoch [225/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005829935\n",
      "Epoch [225/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005827143\n",
      "Epoch [225/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005824351\n",
      "Epoch [226/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0005821558\n",
      "Epoch [226/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0005818765\n",
      "Epoch [226/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0007, LR: 0.0005815972\n",
      "Epoch [226/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005813178\n",
      "Epoch [226/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005810384\n",
      "Epoch [226/500], Batch [60/110], Train Loss: 0.0189, Val Loss: 0.0007, LR: 0.0005807590\n",
      "Epoch [226/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005804796\n",
      "Epoch [226/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005802002\n",
      "Epoch [226/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0005799207\n",
      "Epoch [226/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005796412\n",
      "Epoch [226/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005793617\n",
      "Epoch [227/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005790821\n",
      "Epoch [227/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0016, LR: 0.0005788025\n",
      "Epoch [227/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005785230\n",
      "Epoch [227/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005782433\n",
      "Epoch [227/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005779637\n",
      "Epoch [227/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005776840\n",
      "Epoch [227/500], Batch [70/110], Train Loss: 0.0229, Val Loss: 0.0007, LR: 0.0005774043\n",
      "Epoch [227/500], Batch [80/110], Train Loss: 0.0054, Val Loss: 0.0006, LR: 0.0005771246\n",
      "Epoch [227/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0005768449\n",
      "Epoch [227/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005765651\n",
      "Epoch [227/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005762853\n",
      "Epoch [228/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005760055\n",
      "Epoch [228/500], Batch [20/110], Train Loss: 0.0329, Val Loss: 0.0018, LR: 0.0005757257\n",
      "Epoch [228/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005754458\n",
      "Epoch [228/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005751660\n",
      "Epoch [228/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005748861\n",
      "Epoch [228/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0013, LR: 0.0005746061\n",
      "Epoch [228/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005743262\n",
      "Epoch [228/500], Batch [80/110], Train Loss: 0.0019, Val Loss: 0.0009, LR: 0.0005740462\n",
      "Epoch [228/500], Batch [90/110], Train Loss: 0.0062, Val Loss: 0.0010, LR: 0.0005737662\n",
      "Epoch [228/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005734862\n",
      "Epoch [228/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0005732062\n",
      "Epoch [229/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005729261\n",
      "Epoch [229/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005726461\n",
      "Epoch [229/500], Batch [30/110], Train Loss: 0.0106, Val Loss: 0.0023, LR: 0.0005723660\n",
      "Epoch [229/500], Batch [40/110], Train Loss: 0.0044, Val Loss: 0.0007, LR: 0.0005720858\n",
      "Epoch [229/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005718057\n",
      "Epoch [229/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005715255\n",
      "Epoch [229/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005712453\n",
      "Epoch [229/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005709651\n",
      "Epoch [229/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005706849\n",
      "Epoch [229/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005704046\n",
      "Epoch [229/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005701244\n",
      "Epoch [230/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005698441\n",
      "Epoch [230/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005695637\n",
      "Epoch [230/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005692834\n",
      "Epoch [230/500], Batch [40/110], Train Loss: 0.0010, Val Loss: 0.0011, LR: 0.0005690030\n",
      "Epoch [230/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0005687227\n",
      "Epoch [230/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005684423\n",
      "Epoch [230/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0007, LR: 0.0005681618\n",
      "Epoch [230/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0005678814\n",
      "Epoch [230/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0012, LR: 0.0005676009\n",
      "Epoch [230/500], Batch [100/110], Train Loss: 0.0044, Val Loss: 0.0007, LR: 0.0005673205\n",
      "Epoch [230/500], Batch [110/110], Train Loss: 0.0318, Val Loss: 0.0011, LR: 0.0005670400\n",
      "Epoch [231/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0027, LR: 0.0005667594\n",
      "Epoch [231/500], Batch [20/110], Train Loss: 0.0032, Val Loss: 0.0006, LR: 0.0005664789\n",
      "Epoch [231/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005661983\n",
      "Epoch [231/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0005659177\n",
      "Epoch [231/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0015, LR: 0.0005656371\n",
      "Epoch [231/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005653565\n",
      "Epoch [231/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005650759\n",
      "Epoch [231/500], Batch [80/110], Train Loss: 0.0009, Val Loss: 0.0006, LR: 0.0005647952\n",
      "Epoch [231/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005645145\n",
      "Epoch [231/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005642338\n",
      "Epoch [231/500], Batch [110/110], Train Loss: 0.0024, Val Loss: 0.0008, LR: 0.0005639531\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 231: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 305.23 MB\n",
      "Epoch [232/500], Batch [10/110], Train Loss: 0.0216, Val Loss: 0.0006, LR: 0.0005636724\n",
      "Epoch [232/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005633916\n",
      "Epoch [232/500], Batch [30/110], Train Loss: 0.0026, Val Loss: 0.0013, LR: 0.0005631108\n",
      "Epoch [232/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005628300\n",
      "Epoch [232/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005625492\n",
      "Epoch [232/500], Batch [60/110], Train Loss: 0.0018, Val Loss: 0.0007, LR: 0.0005622684\n",
      "Epoch [232/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005619875\n",
      "Epoch [232/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0005617066\n",
      "Epoch [232/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005614257\n",
      "Epoch [232/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0005, LR: 0.0005611448\n",
      "Epoch [232/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0007, LR: 0.0005608639\n",
      "Epoch [233/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005605830\n",
      "Epoch [233/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005603020\n",
      "Epoch [233/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005600210\n",
      "Epoch [233/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005597400\n",
      "Epoch [233/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0005, LR: 0.0005594590\n",
      "Epoch [233/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005591780\n",
      "Epoch [233/500], Batch [70/110], Train Loss: 0.0028, Val Loss: 0.0006, LR: 0.0005588969\n",
      "Epoch [233/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005586158\n",
      "Epoch [233/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0005583348\n",
      "Epoch [233/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005580536\n",
      "Epoch [233/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0005577725\n",
      "Epoch [234/500], Batch [10/110], Train Loss: 0.0039, Val Loss: 0.0006, LR: 0.0005574914\n",
      "Epoch [234/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005572102\n",
      "Epoch [234/500], Batch [30/110], Train Loss: 0.0014, Val Loss: 0.0015, LR: 0.0005569290\n",
      "Epoch [234/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005566479\n",
      "Epoch [234/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005563666\n",
      "Epoch [234/500], Batch [60/110], Train Loss: 0.0104, Val Loss: 0.0007, LR: 0.0005560854\n",
      "Epoch [234/500], Batch [70/110], Train Loss: 0.0040, Val Loss: 0.0007, LR: 0.0005558042\n",
      "Epoch [234/500], Batch [80/110], Train Loss: 0.0067, Val Loss: 0.0008, LR: 0.0005555229\n",
      "Epoch [234/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005552416\n",
      "Epoch [234/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0005549604\n",
      "Epoch [234/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005546790\n",
      "Epoch [235/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005543977\n",
      "Epoch [235/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0005, LR: 0.0005541164\n",
      "Epoch [235/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005538350\n",
      "Epoch [235/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005535537\n",
      "Epoch [235/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005532723\n",
      "Epoch [235/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005529909\n",
      "Epoch [235/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0005527094\n",
      "Epoch [235/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0005524280\n",
      "Epoch [235/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0022, LR: 0.0005521466\n",
      "Epoch [235/500], Batch [100/110], Train Loss: 0.0010, Val Loss: 0.0011, LR: 0.0005518651\n",
      "Epoch [235/500], Batch [110/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0005515836\n",
      "Epoch [236/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005513021\n",
      "Epoch [236/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0006, LR: 0.0005510206\n",
      "Epoch [236/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005507391\n",
      "Epoch [236/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005504575\n",
      "Epoch [236/500], Batch [50/110], Train Loss: 0.0011, Val Loss: 0.0005, LR: 0.0005501760\n",
      "Epoch [236/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005498944\n",
      "Epoch [236/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0005496128\n",
      "Epoch [236/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005493312\n",
      "Epoch [236/500], Batch [90/110], Train Loss: 0.0019, Val Loss: 0.0005, LR: 0.0005490496\n",
      "Epoch [236/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005487680\n",
      "Epoch [236/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0011, LR: 0.0005484863\n",
      "Epoch [237/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005482047\n",
      "Epoch [237/500], Batch [20/110], Train Loss: 0.0040, Val Loss: 0.0007, LR: 0.0005479230\n",
      "Epoch [237/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005476413\n",
      "Epoch [237/500], Batch [40/110], Train Loss: 0.0048, Val Loss: 0.0005, LR: 0.0005473596\n",
      "Epoch [237/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0006, LR: 0.0005470779\n",
      "Epoch [237/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005467962\n",
      "Epoch [237/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005465144\n",
      "Epoch [237/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005462327\n",
      "Epoch [237/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005459509\n",
      "Epoch [237/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005456691\n",
      "Epoch [237/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005453874\n",
      "Epoch [238/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005451055\n",
      "Epoch [238/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005448237\n",
      "Epoch [238/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005445419\n",
      "Epoch [238/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005442600\n",
      "Epoch [238/500], Batch [50/110], Train Loss: 0.0040, Val Loss: 0.0005, LR: 0.0005439782\n",
      "Epoch [238/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005436963\n",
      "Epoch [238/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005434144\n",
      "Epoch [238/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0005431325\n",
      "Epoch [238/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0005428506\n",
      "Epoch [238/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005425687\n",
      "Epoch [238/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005422868\n",
      "Epoch [239/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0006, LR: 0.0005420048\n",
      "Epoch [239/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005417229\n",
      "Epoch [239/500], Batch [30/110], Train Loss: 0.0062, Val Loss: 0.0015, LR: 0.0005414409\n",
      "Epoch [239/500], Batch [40/110], Train Loss: 0.0089, Val Loss: 0.0006, LR: 0.0005411589\n",
      "Epoch [239/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005408769\n",
      "Epoch [239/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005405949\n",
      "Epoch [239/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005403129\n",
      "Epoch [239/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0005400309\n",
      "Epoch [239/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005397488\n",
      "Epoch [239/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005394668\n",
      "Epoch [239/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005391847\n",
      "Epoch [240/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005389026\n",
      "Epoch [240/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005386206\n",
      "Epoch [240/500], Batch [30/110], Train Loss: 0.0034, Val Loss: 0.0006, LR: 0.0005383385\n",
      "Epoch [240/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0005380564\n",
      "Epoch [240/500], Batch [50/110], Train Loss: 0.0025, Val Loss: 0.0008, LR: 0.0005377742\n",
      "Epoch [240/500], Batch [60/110], Train Loss: 0.0215, Val Loss: 0.0011, LR: 0.0005374921\n",
      "Epoch [240/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005372100\n",
      "Epoch [240/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0011, LR: 0.0005369278\n",
      "Epoch [240/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005366457\n",
      "Epoch [240/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0009, LR: 0.0005363635\n",
      "Epoch [240/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005360813\n",
      "Epoch [241/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005357991\n",
      "Epoch [241/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005355169\n",
      "Epoch [241/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0005352347\n",
      "Epoch [241/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005349525\n",
      "Epoch [241/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005346703\n",
      "Epoch [241/500], Batch [60/110], Train Loss: 0.0030, Val Loss: 0.0010, LR: 0.0005343880\n",
      "Epoch [241/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005341058\n",
      "Epoch [241/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005338235\n",
      "Epoch [241/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0005335412\n",
      "Epoch [241/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005332590\n",
      "Epoch [241/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005329767\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 241: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.047 sec , Memory Usage: 305.25 MB\n",
      "Epoch [242/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005326944\n",
      "Epoch [242/500], Batch [20/110], Train Loss: 0.0073, Val Loss: 0.0005, LR: 0.0005324121\n",
      "Epoch [242/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005321298\n",
      "Epoch [242/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0011, LR: 0.0005318474\n",
      "Epoch [242/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005315651\n",
      "Epoch [242/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005312828\n",
      "Epoch [242/500], Batch [70/110], Train Loss: 0.0150, Val Loss: 0.0005, LR: 0.0005310004\n",
      "Epoch [242/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005307181\n",
      "Epoch [242/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005304357\n",
      "Epoch [242/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005301533\n",
      "Epoch [242/500], Batch [110/110], Train Loss: 0.0023, Val Loss: 0.0006, LR: 0.0005298709\n",
      "Epoch [243/500], Batch [10/110], Train Loss: 0.0014, Val Loss: 0.0005, LR: 0.0005295885\n",
      "Epoch [243/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005293061\n",
      "Epoch [243/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005290237\n",
      "Epoch [243/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005287413\n",
      "Epoch [243/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0005284589\n",
      "Epoch [243/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0006, LR: 0.0005281765\n",
      "Epoch [243/500], Batch [70/110], Train Loss: 0.0262, Val Loss: 0.0007, LR: 0.0005278940\n",
      "Epoch [243/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005276116\n",
      "Epoch [243/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005273291\n",
      "Epoch [243/500], Batch [100/110], Train Loss: 0.0100, Val Loss: 0.0005, LR: 0.0005270467\n",
      "Epoch [243/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005267642\n",
      "Epoch [244/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0005264817\n",
      "Epoch [244/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0005261993\n",
      "Epoch [244/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005259168\n",
      "Epoch [244/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005256343\n",
      "Epoch [244/500], Batch [50/110], Train Loss: 0.0089, Val Loss: 0.0007, LR: 0.0005253518\n",
      "Epoch [244/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005250693\n",
      "Epoch [244/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005247868\n",
      "Epoch [244/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005245042\n",
      "Epoch [244/500], Batch [90/110], Train Loss: 0.0163, Val Loss: 0.0005, LR: 0.0005242217\n",
      "Epoch [244/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0005239392\n",
      "Epoch [244/500], Batch [110/110], Train Loss: 0.0077, Val Loss: 0.0006, LR: 0.0005236566\n",
      "Epoch [245/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005233741\n",
      "Epoch [245/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005230915\n",
      "Epoch [245/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005228090\n",
      "Epoch [245/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0005225264\n",
      "Epoch [245/500], Batch [50/110], Train Loss: 0.0041, Val Loss: 0.0005, LR: 0.0005222439\n",
      "Epoch [245/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005219613\n",
      "Epoch [245/500], Batch [70/110], Train Loss: 0.0037, Val Loss: 0.0005, LR: 0.0005216787\n",
      "Epoch [245/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005213961\n",
      "Epoch [245/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005211135\n",
      "Epoch [245/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005208309\n",
      "Epoch [245/500], Batch [110/110], Train Loss: 0.0036, Val Loss: 0.0005, LR: 0.0005205483\n",
      "Epoch [246/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005202657\n",
      "Epoch [246/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005199831\n",
      "Epoch [246/500], Batch [30/110], Train Loss: 0.0018, Val Loss: 0.0006, LR: 0.0005197005\n",
      "Epoch [246/500], Batch [40/110], Train Loss: 0.0198, Val Loss: 0.0007, LR: 0.0005194179\n",
      "Epoch [246/500], Batch [50/110], Train Loss: 0.0040, Val Loss: 0.0005, LR: 0.0005191352\n",
      "Epoch [246/500], Batch [60/110], Train Loss: 0.0036, Val Loss: 0.0012, LR: 0.0005188526\n",
      "Epoch [246/500], Batch [70/110], Train Loss: 0.0024, Val Loss: 0.0010, LR: 0.0005185700\n",
      "Epoch [246/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0005, LR: 0.0005182873\n",
      "Epoch [246/500], Batch [90/110], Train Loss: 0.0190, Val Loss: 0.0006, LR: 0.0005180047\n",
      "Epoch [246/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0005, LR: 0.0005177220\n",
      "Epoch [246/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005174394\n",
      "Epoch [247/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005171567\n",
      "Epoch [247/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0013, LR: 0.0005168741\n",
      "Epoch [247/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005165914\n",
      "Epoch [247/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005163087\n",
      "Epoch [247/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0005160261\n",
      "Epoch [247/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005157434\n",
      "Epoch [247/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0010, LR: 0.0005154607\n",
      "Epoch [247/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0005151780\n",
      "Epoch [247/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0016, LR: 0.0005148954\n",
      "Epoch [247/500], Batch [100/110], Train Loss: 0.0051, Val Loss: 0.0013, LR: 0.0005146127\n",
      "Epoch [247/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005143300\n",
      "Epoch [248/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005140473\n",
      "Epoch [248/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0014, LR: 0.0005137646\n",
      "Epoch [248/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0017, LR: 0.0005134819\n",
      "Epoch [248/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005131992\n",
      "Epoch [248/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0005129165\n",
      "Epoch [248/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005126338\n",
      "Epoch [248/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005123511\n",
      "Epoch [248/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005120683\n",
      "Epoch [248/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005117856\n",
      "Epoch [248/500], Batch [100/110], Train Loss: 0.0204, Val Loss: 0.0010, LR: 0.0005115029\n",
      "Epoch [248/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0005112202\n",
      "Epoch [249/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005109375\n",
      "Epoch [249/500], Batch [20/110], Train Loss: 0.0032, Val Loss: 0.0011, LR: 0.0005106547\n",
      "Epoch [249/500], Batch [30/110], Train Loss: 0.0012, Val Loss: 0.0011, LR: 0.0005103720\n",
      "Epoch [249/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0005100893\n",
      "Epoch [249/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0005098066\n",
      "Epoch [249/500], Batch [60/110], Train Loss: 0.0045, Val Loss: 0.0009, LR: 0.0005095238\n",
      "Epoch [249/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0005092411\n",
      "Epoch [249/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0008, LR: 0.0005089584\n",
      "Epoch [249/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005086756\n",
      "Epoch [249/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005083929\n",
      "Epoch [249/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005081102\n",
      "Epoch [250/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005078274\n",
      "Epoch [250/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0005075447\n",
      "Epoch [250/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005072619\n",
      "Epoch [250/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005069792\n",
      "Epoch [250/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0005, LR: 0.0005066965\n",
      "Epoch [250/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005064137\n",
      "Epoch [250/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0005061310\n",
      "Epoch [250/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005058482\n",
      "Epoch [250/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005055655\n",
      "Epoch [250/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005052827\n",
      "Epoch [250/500], Batch [110/110], Train Loss: 0.0020, Val Loss: 0.0005, LR: 0.0005050000\n",
      "Epoch [251/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005047173\n",
      "Epoch [251/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0005044345\n",
      "Epoch [251/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005041518\n",
      "Epoch [251/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005038690\n",
      "Epoch [251/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005035863\n",
      "Epoch [251/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005033035\n",
      "Epoch [251/500], Batch [70/110], Train Loss: 0.0048, Val Loss: 0.0005, LR: 0.0005030208\n",
      "Epoch [251/500], Batch [80/110], Train Loss: 0.0011, Val Loss: 0.0005, LR: 0.0005027381\n",
      "Epoch [251/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005024553\n",
      "Epoch [251/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0005, LR: 0.0005021726\n",
      "Epoch [251/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0004, LR: 0.0005018898\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 251: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 305.25 MB\n",
      "Epoch [252/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0005, LR: 0.0005016071\n",
      "Epoch [252/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0005013244\n",
      "Epoch [252/500], Batch [30/110], Train Loss: 0.0032, Val Loss: 0.0005, LR: 0.0005010416\n",
      "Epoch [252/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005007589\n",
      "Epoch [252/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0005004762\n",
      "Epoch [252/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0005001934\n",
      "Epoch [252/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004999107\n",
      "Epoch [252/500], Batch [80/110], Train Loss: 0.0014, Val Loss: 0.0007, LR: 0.0004996280\n",
      "Epoch [252/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004993453\n",
      "Epoch [252/500], Batch [100/110], Train Loss: 0.0284, Val Loss: 0.0010, LR: 0.0004990625\n",
      "Epoch [252/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004987798\n",
      "Epoch [253/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004984971\n",
      "Epoch [253/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004982144\n",
      "Epoch [253/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004979317\n",
      "Epoch [253/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004976489\n",
      "Epoch [253/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004973662\n",
      "Epoch [253/500], Batch [60/110], Train Loss: 0.0021, Val Loss: 0.0006, LR: 0.0004970835\n",
      "Epoch [253/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004968008\n",
      "Epoch [253/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0005, LR: 0.0004965181\n",
      "Epoch [253/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004962354\n",
      "Epoch [253/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004959527\n",
      "Epoch [253/500], Batch [110/110], Train Loss: 0.1768, Val Loss: 0.0005, LR: 0.0004956700\n",
      "Epoch [254/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004953873\n",
      "Epoch [254/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0005, LR: 0.0004951046\n",
      "Epoch [254/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004948220\n",
      "Epoch [254/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004945393\n",
      "Epoch [254/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004942566\n",
      "Epoch [254/500], Batch [60/110], Train Loss: 0.0114, Val Loss: 0.0005, LR: 0.0004939739\n",
      "Epoch [254/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004936913\n",
      "Epoch [254/500], Batch [80/110], Train Loss: 0.0050, Val Loss: 0.0005, LR: 0.0004934086\n",
      "Epoch [254/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0005, LR: 0.0004931259\n",
      "Epoch [254/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0004928433\n",
      "Epoch [254/500], Batch [110/110], Train Loss: 0.0055, Val Loss: 0.0004, LR: 0.0004925606\n",
      "Epoch [255/500], Batch [10/110], Train Loss: 0.0018, Val Loss: 0.0006, LR: 0.0004922780\n",
      "Epoch [255/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004919953\n",
      "Epoch [255/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004917127\n",
      "Epoch [255/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004914300\n",
      "Epoch [255/500], Batch [50/110], Train Loss: 0.0027, Val Loss: 0.0005, LR: 0.0004911474\n",
      "Epoch [255/500], Batch [60/110], Train Loss: 0.0019, Val Loss: 0.0005, LR: 0.0004908648\n",
      "Epoch [255/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0005, LR: 0.0004905821\n",
      "Epoch [255/500], Batch [80/110], Train Loss: 0.0062, Val Loss: 0.0005, LR: 0.0004902995\n",
      "Epoch [255/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004900169\n",
      "Epoch [255/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004897343\n",
      "Epoch [255/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004894517\n",
      "Epoch [256/500], Batch [10/110], Train Loss: 0.0026, Val Loss: 0.0005, LR: 0.0004891691\n",
      "Epoch [256/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004888865\n",
      "Epoch [256/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004886039\n",
      "Epoch [256/500], Batch [40/110], Train Loss: 0.0097, Val Loss: 0.0005, LR: 0.0004883213\n",
      "Epoch [256/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0004880387\n",
      "Epoch [256/500], Batch [60/110], Train Loss: 0.0019, Val Loss: 0.0008, LR: 0.0004877561\n",
      "Epoch [256/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0005, LR: 0.0004874736\n",
      "Epoch [256/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0004871910\n",
      "Epoch [256/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004869085\n",
      "Epoch [256/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004866259\n",
      "Epoch [256/500], Batch [110/110], Train Loss: 0.0041, Val Loss: 0.0010, LR: 0.0004863434\n",
      "Epoch [257/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004860608\n",
      "Epoch [257/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004857783\n",
      "Epoch [257/500], Batch [30/110], Train Loss: 0.0021, Val Loss: 0.0012, LR: 0.0004854958\n",
      "Epoch [257/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004852132\n",
      "Epoch [257/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004849307\n",
      "Epoch [257/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004846482\n",
      "Epoch [257/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004843657\n",
      "Epoch [257/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0011, LR: 0.0004840832\n",
      "Epoch [257/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004838007\n",
      "Epoch [257/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004835183\n",
      "Epoch [257/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0004832358\n",
      "Epoch [258/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0014, LR: 0.0004829533\n",
      "Epoch [258/500], Batch [20/110], Train Loss: 0.0014, Val Loss: 0.0006, LR: 0.0004826709\n",
      "Epoch [258/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0004823884\n",
      "Epoch [258/500], Batch [40/110], Train Loss: 0.0475, Val Loss: 0.0013, LR: 0.0004821060\n",
      "Epoch [258/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004818235\n",
      "Epoch [258/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004815411\n",
      "Epoch [258/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004812587\n",
      "Epoch [258/500], Batch [80/110], Train Loss: 0.0247, Val Loss: 0.0007, LR: 0.0004809763\n",
      "Epoch [258/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004806939\n",
      "Epoch [258/500], Batch [100/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0004804115\n",
      "Epoch [258/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004801291\n",
      "Epoch [259/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004798467\n",
      "Epoch [259/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0004795643\n",
      "Epoch [259/500], Batch [30/110], Train Loss: 0.0429, Val Loss: 0.0014, LR: 0.0004792819\n",
      "Epoch [259/500], Batch [40/110], Train Loss: 0.0010, Val Loss: 0.0009, LR: 0.0004789996\n",
      "Epoch [259/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004787172\n",
      "Epoch [259/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0004, LR: 0.0004784349\n",
      "Epoch [259/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0017, LR: 0.0004781526\n",
      "Epoch [259/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0009, LR: 0.0004778702\n",
      "Epoch [259/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004775879\n",
      "Epoch [259/500], Batch [100/110], Train Loss: 0.0031, Val Loss: 0.0008, LR: 0.0004773056\n",
      "Epoch [259/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004770233\n",
      "Epoch [260/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004767410\n",
      "Epoch [260/500], Batch [20/110], Train Loss: 0.0074, Val Loss: 0.0005, LR: 0.0004764588\n",
      "Epoch [260/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004761765\n",
      "Epoch [260/500], Batch [40/110], Train Loss: 0.0052, Val Loss: 0.0004, LR: 0.0004758942\n",
      "Epoch [260/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004756120\n",
      "Epoch [260/500], Batch [60/110], Train Loss: 0.0011, Val Loss: 0.0010, LR: 0.0004753297\n",
      "Epoch [260/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004750475\n",
      "Epoch [260/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004747653\n",
      "Epoch [260/500], Batch [90/110], Train Loss: 0.0066, Val Loss: 0.0005, LR: 0.0004744831\n",
      "Epoch [260/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004742009\n",
      "Epoch [260/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0005, LR: 0.0004739187\n",
      "Epoch [261/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004736365\n",
      "Epoch [261/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0004733543\n",
      "Epoch [261/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0009, LR: 0.0004730722\n",
      "Epoch [261/500], Batch [40/110], Train Loss: 0.0009, Val Loss: 0.0008, LR: 0.0004727900\n",
      "Epoch [261/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004725079\n",
      "Epoch [261/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004722258\n",
      "Epoch [261/500], Batch [70/110], Train Loss: 0.0051, Val Loss: 0.0004, LR: 0.0004719436\n",
      "Epoch [261/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004716615\n",
      "Epoch [261/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004713794\n",
      "Epoch [261/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0004710974\n",
      "Epoch [261/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004708153\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 261: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.060 sec , Memory Usage: 305.25 MB\n",
      "Epoch [262/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004705332\n",
      "Epoch [262/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004702512\n",
      "Epoch [262/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0004699691\n",
      "Epoch [262/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0004696871\n",
      "Epoch [262/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004694051\n",
      "Epoch [262/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0006, LR: 0.0004691231\n",
      "Epoch [262/500], Batch [70/110], Train Loss: 0.0024, Val Loss: 0.0004, LR: 0.0004688411\n",
      "Epoch [262/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0004685591\n",
      "Epoch [262/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004682771\n",
      "Epoch [262/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004679952\n",
      "Epoch [262/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004677132\n",
      "Epoch [263/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004674313\n",
      "Epoch [263/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004671494\n",
      "Epoch [263/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004668675\n",
      "Epoch [263/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004665856\n",
      "Epoch [263/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004663037\n",
      "Epoch [263/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004660218\n",
      "Epoch [263/500], Batch [70/110], Train Loss: 0.0028, Val Loss: 0.0005, LR: 0.0004657400\n",
      "Epoch [263/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004654581\n",
      "Epoch [263/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004651763\n",
      "Epoch [263/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004648945\n",
      "Epoch [263/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004646126\n",
      "Epoch [264/500], Batch [10/110], Train Loss: 0.0347, Val Loss: 0.0011, LR: 0.0004643309\n",
      "Epoch [264/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0004640491\n",
      "Epoch [264/500], Batch [30/110], Train Loss: 0.0088, Val Loss: 0.0005, LR: 0.0004637673\n",
      "Epoch [264/500], Batch [40/110], Train Loss: 0.0034, Val Loss: 0.0005, LR: 0.0004634856\n",
      "Epoch [264/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004632038\n",
      "Epoch [264/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0004629221\n",
      "Epoch [264/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004626404\n",
      "Epoch [264/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004623587\n",
      "Epoch [264/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004620770\n",
      "Epoch [264/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004617953\n",
      "Epoch [264/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004615137\n",
      "Epoch [265/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0004, LR: 0.0004612320\n",
      "Epoch [265/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004609504\n",
      "Epoch [265/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004606688\n",
      "Epoch [265/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004603872\n",
      "Epoch [265/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004601056\n",
      "Epoch [265/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004598240\n",
      "Epoch [265/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004595425\n",
      "Epoch [265/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0007, LR: 0.0004592609\n",
      "Epoch [265/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004589794\n",
      "Epoch [265/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004586979\n",
      "Epoch [265/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004584164\n",
      "Epoch [266/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004581349\n",
      "Epoch [266/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004578534\n",
      "Epoch [266/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0004575720\n",
      "Epoch [266/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0004572906\n",
      "Epoch [266/500], Batch [50/110], Train Loss: 0.0110, Val Loss: 0.0006, LR: 0.0004570091\n",
      "Epoch [266/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0004567277\n",
      "Epoch [266/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004564463\n",
      "Epoch [266/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0004561650\n",
      "Epoch [266/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004558836\n",
      "Epoch [266/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004556023\n",
      "Epoch [266/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004553210\n",
      "Epoch [267/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004550396\n",
      "Epoch [267/500], Batch [20/110], Train Loss: 0.0175, Val Loss: 0.0008, LR: 0.0004547584\n",
      "Epoch [267/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0004544771\n",
      "Epoch [267/500], Batch [40/110], Train Loss: 0.0016, Val Loss: 0.0006, LR: 0.0004541958\n",
      "Epoch [267/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0019, LR: 0.0004539146\n",
      "Epoch [267/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0016, LR: 0.0004536334\n",
      "Epoch [267/500], Batch [70/110], Train Loss: 0.0175, Val Loss: 0.0006, LR: 0.0004533521\n",
      "Epoch [267/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0020, LR: 0.0004530710\n",
      "Epoch [267/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0004527898\n",
      "Epoch [267/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004525086\n",
      "Epoch [267/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0004522275\n",
      "Epoch [268/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0004, LR: 0.0004519464\n",
      "Epoch [268/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004516652\n",
      "Epoch [268/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004513842\n",
      "Epoch [268/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004511031\n",
      "Epoch [268/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0003, LR: 0.0004508220\n",
      "Epoch [268/500], Batch [60/110], Train Loss: 0.0064, Val Loss: 0.0003, LR: 0.0004505410\n",
      "Epoch [268/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004502600\n",
      "Epoch [268/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0004499790\n",
      "Epoch [268/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004496980\n",
      "Epoch [268/500], Batch [100/110], Train Loss: 0.0031, Val Loss: 0.0003, LR: 0.0004494170\n",
      "Epoch [268/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004491361\n",
      "Epoch [269/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0007, LR: 0.0004488552\n",
      "Epoch [269/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0004, LR: 0.0004485743\n",
      "Epoch [269/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0004482934\n",
      "Epoch [269/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004480125\n",
      "Epoch [269/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004477316\n",
      "Epoch [269/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0004474508\n",
      "Epoch [269/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0004471700\n",
      "Epoch [269/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0004468892\n",
      "Epoch [269/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0004466084\n",
      "Epoch [269/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0013, LR: 0.0004463276\n",
      "Epoch [269/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004460469\n",
      "Epoch [270/500], Batch [10/110], Train Loss: 0.0037, Val Loss: 0.0006, LR: 0.0004457662\n",
      "Epoch [270/500], Batch [20/110], Train Loss: 0.0023, Val Loss: 0.0008, LR: 0.0004454855\n",
      "Epoch [270/500], Batch [30/110], Train Loss: 0.0023, Val Loss: 0.0004, LR: 0.0004452048\n",
      "Epoch [270/500], Batch [40/110], Train Loss: 0.0108, Val Loss: 0.0004, LR: 0.0004449241\n",
      "Epoch [270/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004446435\n",
      "Epoch [270/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0003, LR: 0.0004443629\n",
      "Epoch [270/500], Batch [70/110], Train Loss: 0.0018, Val Loss: 0.0003, LR: 0.0004440823\n",
      "Epoch [270/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004438017\n",
      "Epoch [270/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004435211\n",
      "Epoch [270/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004432406\n",
      "Epoch [270/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004429600\n",
      "Epoch [271/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0003, LR: 0.0004426795\n",
      "Epoch [271/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0004423991\n",
      "Epoch [271/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0004, LR: 0.0004421186\n",
      "Epoch [271/500], Batch [40/110], Train Loss: 0.0077, Val Loss: 0.0004, LR: 0.0004418382\n",
      "Epoch [271/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004415577\n",
      "Epoch [271/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0004412773\n",
      "Epoch [271/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004409970\n",
      "Epoch [271/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004407166\n",
      "Epoch [271/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0004404363\n",
      "Epoch [271/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0004401559\n",
      "Epoch [271/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004398756\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 271: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.050 sec , Memory Usage: 305.22 MB\n",
      "Epoch [272/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004395954\n",
      "Epoch [272/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004393151\n",
      "Epoch [272/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004390349\n",
      "Epoch [272/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004387547\n",
      "Epoch [272/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004384745\n",
      "Epoch [272/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004381943\n",
      "Epoch [272/500], Batch [70/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0004379142\n",
      "Epoch [272/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004376340\n",
      "Epoch [272/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004373539\n",
      "Epoch [272/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004370739\n",
      "Epoch [272/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004367938\n",
      "Epoch [273/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004365138\n",
      "Epoch [273/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0004362338\n",
      "Epoch [273/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0004359538\n",
      "Epoch [273/500], Batch [40/110], Train Loss: 0.0039, Val Loss: 0.0004, LR: 0.0004356738\n",
      "Epoch [273/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004353939\n",
      "Epoch [273/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004351139\n",
      "Epoch [273/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004348340\n",
      "Epoch [273/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0004345542\n",
      "Epoch [273/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004342743\n",
      "Epoch [273/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004339945\n",
      "Epoch [273/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004337147\n",
      "Epoch [274/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0004334349\n",
      "Epoch [274/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0004331551\n",
      "Epoch [274/500], Batch [30/110], Train Loss: 0.0021, Val Loss: 0.0003, LR: 0.0004328754\n",
      "Epoch [274/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004325957\n",
      "Epoch [274/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004323160\n",
      "Epoch [274/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004320363\n",
      "Epoch [274/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004317567\n",
      "Epoch [274/500], Batch [80/110], Train Loss: 0.0057, Val Loss: 0.0005, LR: 0.0004314770\n",
      "Epoch [274/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0003, LR: 0.0004311975\n",
      "Epoch [274/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0004309179\n",
      "Epoch [274/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0004306383\n",
      "Epoch [275/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0004303588\n",
      "Epoch [275/500], Batch [20/110], Train Loss: 0.0069, Val Loss: 0.0004, LR: 0.0004300793\n",
      "Epoch [275/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004297998\n",
      "Epoch [275/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004295204\n",
      "Epoch [275/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004292410\n",
      "Epoch [275/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004289616\n",
      "Epoch [275/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0004286822\n",
      "Epoch [275/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004284028\n",
      "Epoch [275/500], Batch [90/110], Train Loss: 0.0183, Val Loss: 0.0007, LR: 0.0004281235\n",
      "Epoch [275/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0004, LR: 0.0004278442\n",
      "Epoch [275/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004275649\n",
      "Epoch [276/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004272857\n",
      "Epoch [276/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004270065\n",
      "Epoch [276/500], Batch [30/110], Train Loss: 0.0044, Val Loss: 0.0003, LR: 0.0004267273\n",
      "Epoch [276/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0004264481\n",
      "Epoch [276/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004261689\n",
      "Epoch [276/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004258898\n",
      "Epoch [276/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004256107\n",
      "Epoch [276/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004253317\n",
      "Epoch [276/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004250526\n",
      "Epoch [276/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004247736\n",
      "Epoch [276/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004244946\n",
      "Epoch [277/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004242156\n",
      "Epoch [277/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0004239367\n",
      "Epoch [277/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0004236578\n",
      "Epoch [277/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004233789\n",
      "Epoch [277/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004231000\n",
      "Epoch [277/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004228212\n",
      "Epoch [277/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0004225424\n",
      "Epoch [277/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004222636\n",
      "Epoch [277/500], Batch [90/110], Train Loss: 0.0048, Val Loss: 0.0003, LR: 0.0004219849\n",
      "Epoch [277/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0004217061\n",
      "Epoch [277/500], Batch [110/110], Train Loss: 0.0037, Val Loss: 0.0004, LR: 0.0004214274\n",
      "Epoch [278/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004211488\n",
      "Epoch [278/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004208701\n",
      "Epoch [278/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004205915\n",
      "Epoch [278/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0004203129\n",
      "Epoch [278/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004200344\n",
      "Epoch [278/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004197558\n",
      "Epoch [278/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0004194773\n",
      "Epoch [278/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004191988\n",
      "Epoch [278/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004189204\n",
      "Epoch [278/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004186420\n",
      "Epoch [278/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004183636\n",
      "Epoch [279/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004180852\n",
      "Epoch [279/500], Batch [20/110], Train Loss: 0.0064, Val Loss: 0.0003, LR: 0.0004178069\n",
      "Epoch [279/500], Batch [30/110], Train Loss: 0.0022, Val Loss: 0.0003, LR: 0.0004175286\n",
      "Epoch [279/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0004172503\n",
      "Epoch [279/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0005, LR: 0.0004169720\n",
      "Epoch [279/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004166938\n",
      "Epoch [279/500], Batch [70/110], Train Loss: 0.0212, Val Loss: 0.0005, LR: 0.0004164156\n",
      "Epoch [279/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0014, LR: 0.0004161375\n",
      "Epoch [279/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0015, LR: 0.0004158593\n",
      "Epoch [279/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0012, LR: 0.0004155812\n",
      "Epoch [279/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0004153031\n",
      "Epoch [280/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004150251\n",
      "Epoch [280/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004147471\n",
      "Epoch [280/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004144691\n",
      "Epoch [280/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004141911\n",
      "Epoch [280/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004139132\n",
      "Epoch [280/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0004136353\n",
      "Epoch [280/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0004133574\n",
      "Epoch [280/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004130796\n",
      "Epoch [280/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0004128018\n",
      "Epoch [280/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004125240\n",
      "Epoch [280/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004122462\n",
      "Epoch [281/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004119685\n",
      "Epoch [281/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0003, LR: 0.0004116908\n",
      "Epoch [281/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004114132\n",
      "Epoch [281/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0003, LR: 0.0004111356\n",
      "Epoch [281/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004108580\n",
      "Epoch [281/500], Batch [60/110], Train Loss: 0.0067, Val Loss: 0.0003, LR: 0.0004105804\n",
      "Epoch [281/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004103028\n",
      "Epoch [281/500], Batch [80/110], Train Loss: 0.0015, Val Loss: 0.0003, LR: 0.0004100253\n",
      "Epoch [281/500], Batch [90/110], Train Loss: 0.0087, Val Loss: 0.0003, LR: 0.0004097479\n",
      "Epoch [281/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0004094704\n",
      "Epoch [281/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004091930\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 281: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.025 sec , Memory Usage: 305.23 MB\n",
      "Epoch [282/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004089156\n",
      "Epoch [282/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004086383\n",
      "Epoch [282/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004083610\n",
      "Epoch [282/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004080837\n",
      "Epoch [282/500], Batch [50/110], Train Loss: 0.0055, Val Loss: 0.0003, LR: 0.0004078064\n",
      "Epoch [282/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004075292\n",
      "Epoch [282/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004072520\n",
      "Epoch [282/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004069748\n",
      "Epoch [282/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004066977\n",
      "Epoch [282/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004064206\n",
      "Epoch [282/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004061436\n",
      "Epoch [283/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004058665\n",
      "Epoch [283/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004055895\n",
      "Epoch [283/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004053126\n",
      "Epoch [283/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004050356\n",
      "Epoch [283/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004047587\n",
      "Epoch [283/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004044819\n",
      "Epoch [283/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0004042050\n",
      "Epoch [283/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004039282\n",
      "Epoch [283/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004036514\n",
      "Epoch [283/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004033747\n",
      "Epoch [283/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004030980\n",
      "Epoch [284/500], Batch [10/110], Train Loss: 0.0049, Val Loss: 0.0002, LR: 0.0004028213\n",
      "Epoch [284/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0004025447\n",
      "Epoch [284/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004022681\n",
      "Epoch [284/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0004019915\n",
      "Epoch [284/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0004017150\n",
      "Epoch [284/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0004, LR: 0.0004014385\n",
      "Epoch [284/500], Batch [70/110], Train Loss: 0.0276, Val Loss: 0.0005, LR: 0.0004011620\n",
      "Epoch [284/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004008856\n",
      "Epoch [284/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0004006092\n",
      "Epoch [284/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0004003328\n",
      "Epoch [284/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0004000565\n",
      "Epoch [285/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003997802\n",
      "Epoch [285/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0008, LR: 0.0003995039\n",
      "Epoch [285/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0003992277\n",
      "Epoch [285/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003989515\n",
      "Epoch [285/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003986753\n",
      "Epoch [285/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003983992\n",
      "Epoch [285/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003981231\n",
      "Epoch [285/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003978471\n",
      "Epoch [285/500], Batch [90/110], Train Loss: 0.0022, Val Loss: 0.0002, LR: 0.0003975710\n",
      "Epoch [285/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003972950\n",
      "Epoch [285/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003970191\n",
      "Epoch [286/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0003967432\n",
      "Epoch [286/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003964673\n",
      "Epoch [286/500], Batch [30/110], Train Loss: 0.0017, Val Loss: 0.0003, LR: 0.0003961915\n",
      "Epoch [286/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003959156\n",
      "Epoch [286/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003956399\n",
      "Epoch [286/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003953641\n",
      "Epoch [286/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003950884\n",
      "Epoch [286/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003948128\n",
      "Epoch [286/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003945371\n",
      "Epoch [286/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003942615\n",
      "Epoch [286/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003939860\n",
      "Epoch [287/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003937105\n",
      "Epoch [287/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003934350\n",
      "Epoch [287/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003931595\n",
      "Epoch [287/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003928841\n",
      "Epoch [287/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003926087\n",
      "Epoch [287/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003923334\n",
      "Epoch [287/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003920581\n",
      "Epoch [287/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003917828\n",
      "Epoch [287/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003915076\n",
      "Epoch [287/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003912324\n",
      "Epoch [287/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003909572\n",
      "Epoch [288/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003906821\n",
      "Epoch [288/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0003904070\n",
      "Epoch [288/500], Batch [30/110], Train Loss: 0.0017, Val Loss: 0.0007, LR: 0.0003901320\n",
      "Epoch [288/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003898570\n",
      "Epoch [288/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003895820\n",
      "Epoch [288/500], Batch [60/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0003893071\n",
      "Epoch [288/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003890322\n",
      "Epoch [288/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003887573\n",
      "Epoch [288/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003884825\n",
      "Epoch [288/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003882077\n",
      "Epoch [288/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003879330\n",
      "Epoch [289/500], Batch [10/110], Train Loss: 0.0016, Val Loss: 0.0002, LR: 0.0003876583\n",
      "Epoch [289/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003873836\n",
      "Epoch [289/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003871090\n",
      "Epoch [289/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0003868344\n",
      "Epoch [289/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003865599\n",
      "Epoch [289/500], Batch [60/110], Train Loss: 0.0421, Val Loss: 0.0009, LR: 0.0003862854\n",
      "Epoch [289/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003860109\n",
      "Epoch [289/500], Batch [80/110], Train Loss: 0.0027, Val Loss: 0.0003, LR: 0.0003857364\n",
      "Epoch [289/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003854621\n",
      "Epoch [289/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003851877\n",
      "Epoch [289/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003849134\n",
      "Epoch [290/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003846391\n",
      "Epoch [290/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003843649\n",
      "Epoch [290/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003840907\n",
      "Epoch [290/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003838165\n",
      "Epoch [290/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003835424\n",
      "Epoch [290/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0003832683\n",
      "Epoch [290/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003829943\n",
      "Epoch [290/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003827203\n",
      "Epoch [290/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003824463\n",
      "Epoch [290/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003821724\n",
      "Epoch [290/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003818985\n",
      "Epoch [291/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003816247\n",
      "Epoch [291/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003813509\n",
      "Epoch [291/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003810771\n",
      "Epoch [291/500], Batch [40/110], Train Loss: 0.0009, Val Loss: 0.0003, LR: 0.0003808034\n",
      "Epoch [291/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003805297\n",
      "Epoch [291/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003802561\n",
      "Epoch [291/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003799825\n",
      "Epoch [291/500], Batch [80/110], Train Loss: 0.0025, Val Loss: 0.0003, LR: 0.0003797089\n",
      "Epoch [291/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003794354\n",
      "Epoch [291/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0003791619\n",
      "Epoch [291/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003788885\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 291: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 305.23 MB\n",
      "Epoch [292/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003786151\n",
      "Epoch [292/500], Batch [20/110], Train Loss: 0.0023, Val Loss: 0.0003, LR: 0.0003783417\n",
      "Epoch [292/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003780684\n",
      "Epoch [292/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003777952\n",
      "Epoch [292/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003775219\n",
      "Epoch [292/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003772488\n",
      "Epoch [292/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003769756\n",
      "Epoch [292/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003767025\n",
      "Epoch [292/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0006, LR: 0.0003764294\n",
      "Epoch [292/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003761564\n",
      "Epoch [292/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003758835\n",
      "Epoch [293/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003756105\n",
      "Epoch [293/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003753376\n",
      "Epoch [293/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003750648\n",
      "Epoch [293/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003747920\n",
      "Epoch [293/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0003745192\n",
      "Epoch [293/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0007, LR: 0.0003742465\n",
      "Epoch [293/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0003739738\n",
      "Epoch [293/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0003737012\n",
      "Epoch [293/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0007, LR: 0.0003734286\n",
      "Epoch [293/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003731560\n",
      "Epoch [293/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0003728835\n",
      "Epoch [294/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0010, LR: 0.0003726111\n",
      "Epoch [294/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003723386\n",
      "Epoch [294/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003720663\n",
      "Epoch [294/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003717939\n",
      "Epoch [294/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003715216\n",
      "Epoch [294/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003712494\n",
      "Epoch [294/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003709772\n",
      "Epoch [294/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003707050\n",
      "Epoch [294/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0004, LR: 0.0003704329\n",
      "Epoch [294/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003701608\n",
      "Epoch [294/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0003698888\n",
      "Epoch [295/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003696168\n",
      "Epoch [295/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003693449\n",
      "Epoch [295/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003690730\n",
      "Epoch [295/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003688011\n",
      "Epoch [295/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003685293\n",
      "Epoch [295/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003682575\n",
      "Epoch [295/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003679858\n",
      "Epoch [295/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003677142\n",
      "Epoch [295/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003674425\n",
      "Epoch [295/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003671709\n",
      "Epoch [295/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003668994\n",
      "Epoch [296/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003666279\n",
      "Epoch [296/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003663565\n",
      "Epoch [296/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003660851\n",
      "Epoch [296/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003658137\n",
      "Epoch [296/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003655424\n",
      "Epoch [296/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0003652711\n",
      "Epoch [296/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003649999\n",
      "Epoch [296/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0010, LR: 0.0003647287\n",
      "Epoch [296/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003644576\n",
      "Epoch [296/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0003, LR: 0.0003641865\n",
      "Epoch [296/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003639155\n",
      "Epoch [297/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003636445\n",
      "Epoch [297/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003633735\n",
      "Epoch [297/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003631026\n",
      "Epoch [297/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003628318\n",
      "Epoch [297/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0004, LR: 0.0003625610\n",
      "Epoch [297/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003622902\n",
      "Epoch [297/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003620195\n",
      "Epoch [297/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003617488\n",
      "Epoch [297/500], Batch [90/110], Train Loss: 0.0028, Val Loss: 0.0004, LR: 0.0003614782\n",
      "Epoch [297/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003612076\n",
      "Epoch [297/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003609371\n",
      "Epoch [298/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003606666\n",
      "Epoch [298/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003603962\n",
      "Epoch [298/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003601258\n",
      "Epoch [298/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003598555\n",
      "Epoch [298/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003595852\n",
      "Epoch [298/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003593149\n",
      "Epoch [298/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003590447\n",
      "Epoch [298/500], Batch [80/110], Train Loss: 0.0017, Val Loss: 0.0004, LR: 0.0003587746\n",
      "Epoch [298/500], Batch [90/110], Train Loss: 0.0029, Val Loss: 0.0003, LR: 0.0003585045\n",
      "Epoch [298/500], Batch [100/110], Train Loss: 0.0019, Val Loss: 0.0003, LR: 0.0003582344\n",
      "Epoch [298/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003579644\n",
      "Epoch [299/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003576945\n",
      "Epoch [299/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003574246\n",
      "Epoch [299/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0003571547\n",
      "Epoch [299/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003568849\n",
      "Epoch [299/500], Batch [50/110], Train Loss: 0.0137, Val Loss: 0.0006, LR: 0.0003566151\n",
      "Epoch [299/500], Batch [60/110], Train Loss: 0.0055, Val Loss: 0.0004, LR: 0.0003563454\n",
      "Epoch [299/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003560757\n",
      "Epoch [299/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003558061\n",
      "Epoch [299/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003555365\n",
      "Epoch [299/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003552670\n",
      "Epoch [299/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003549975\n",
      "Epoch [300/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003547281\n",
      "Epoch [300/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003544587\n",
      "Epoch [300/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003541894\n",
      "Epoch [300/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003539201\n",
      "Epoch [300/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003536509\n",
      "Epoch [300/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003533817\n",
      "Epoch [300/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003531126\n",
      "Epoch [300/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003528435\n",
      "Epoch [300/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003525745\n",
      "Epoch [300/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003523055\n",
      "Epoch [300/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003520366\n",
      "Epoch [301/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0003517677\n",
      "Epoch [301/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0003514989\n",
      "Epoch [301/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0003, LR: 0.0003512301\n",
      "Epoch [301/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003509614\n",
      "Epoch [301/500], Batch [50/110], Train Loss: 0.0014, Val Loss: 0.0003, LR: 0.0003506927\n",
      "Epoch [301/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003504241\n",
      "Epoch [301/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003501555\n",
      "Epoch [301/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003498870\n",
      "Epoch [301/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003496185\n",
      "Epoch [301/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003493500\n",
      "Epoch [301/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003490817\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 301: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.033 sec , Memory Usage: 305.23 MB\n",
      "Epoch [302/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003488133\n",
      "Epoch [302/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003485451\n",
      "Epoch [302/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003482769\n",
      "Epoch [302/500], Batch [40/110], Train Loss: 0.0185, Val Loss: 0.0004, LR: 0.0003480087\n",
      "Epoch [302/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003477406\n",
      "Epoch [302/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0004, LR: 0.0003474725\n",
      "Epoch [302/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003472045\n",
      "Epoch [302/500], Batch [80/110], Train Loss: 0.0011, Val Loss: 0.0005, LR: 0.0003469365\n",
      "Epoch [302/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003466686\n",
      "Epoch [302/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0003464007\n",
      "Epoch [302/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003461329\n",
      "Epoch [303/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003458652\n",
      "Epoch [303/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0003455974\n",
      "Epoch [303/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003453298\n",
      "Epoch [303/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003450622\n",
      "Epoch [303/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0003, LR: 0.0003447946\n",
      "Epoch [303/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003445271\n",
      "Epoch [303/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003442597\n",
      "Epoch [303/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003439923\n",
      "Epoch [303/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003437250\n",
      "Epoch [303/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003434577\n",
      "Epoch [303/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003431904\n",
      "Epoch [304/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0004, LR: 0.0003429232\n",
      "Epoch [304/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003426561\n",
      "Epoch [304/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003423890\n",
      "Epoch [304/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003421220\n",
      "Epoch [304/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003418550\n",
      "Epoch [304/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003415881\n",
      "Epoch [304/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003413213\n",
      "Epoch [304/500], Batch [80/110], Train Loss: 0.0050, Val Loss: 0.0003, LR: 0.0003410544\n",
      "Epoch [304/500], Batch [90/110], Train Loss: 0.0129, Val Loss: 0.0005, LR: 0.0003407877\n",
      "Epoch [304/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0003405210\n",
      "Epoch [304/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003402543\n",
      "Epoch [305/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003399877\n",
      "Epoch [305/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003397212\n",
      "Epoch [305/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003394547\n",
      "Epoch [305/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003391883\n",
      "Epoch [305/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0003389219\n",
      "Epoch [305/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003386556\n",
      "Epoch [305/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003383893\n",
      "Epoch [305/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003381231\n",
      "Epoch [305/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003378569\n",
      "Epoch [305/500], Batch [100/110], Train Loss: 0.0083, Val Loss: 0.0003, LR: 0.0003375908\n",
      "Epoch [305/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003373247\n",
      "Epoch [306/500], Batch [10/110], Train Loss: 0.0021, Val Loss: 0.0002, LR: 0.0003370587\n",
      "Epoch [306/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003367928\n",
      "Epoch [306/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003365269\n",
      "Epoch [306/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0006, LR: 0.0003362611\n",
      "Epoch [306/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003359953\n",
      "Epoch [306/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003357296\n",
      "Epoch [306/500], Batch [70/110], Train Loss: 0.0020, Val Loss: 0.0003, LR: 0.0003354639\n",
      "Epoch [306/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0003351983\n",
      "Epoch [306/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003349327\n",
      "Epoch [306/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003346672\n",
      "Epoch [306/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003344018\n",
      "Epoch [307/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003341364\n",
      "Epoch [307/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003338710\n",
      "Epoch [307/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003336057\n",
      "Epoch [307/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003333405\n",
      "Epoch [307/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003330753\n",
      "Epoch [307/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003328102\n",
      "Epoch [307/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003325452\n",
      "Epoch [307/500], Batch [80/110], Train Loss: 0.0024, Val Loss: 0.0002, LR: 0.0003322802\n",
      "Epoch [307/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0003320152\n",
      "Epoch [307/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003317503\n",
      "Epoch [307/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003314855\n",
      "Epoch [308/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003312207\n",
      "Epoch [308/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003309560\n",
      "Epoch [308/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003306914\n",
      "Epoch [308/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003304268\n",
      "Epoch [308/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003301622\n",
      "Epoch [308/500], Batch [60/110], Train Loss: 0.0018, Val Loss: 0.0002, LR: 0.0003298977\n",
      "Epoch [308/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003296333\n",
      "Epoch [308/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003293689\n",
      "Epoch [308/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003291046\n",
      "Epoch [308/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003288403\n",
      "Epoch [308/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003285761\n",
      "Epoch [309/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003283120\n",
      "Epoch [309/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0003280479\n",
      "Epoch [309/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003277839\n",
      "Epoch [309/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003275199\n",
      "Epoch [309/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003272560\n",
      "Epoch [309/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003269921\n",
      "Epoch [309/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003267283\n",
      "Epoch [309/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003264646\n",
      "Epoch [309/500], Batch [90/110], Train Loss: 0.0013, Val Loss: 0.0003, LR: 0.0003262009\n",
      "Epoch [309/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0006, LR: 0.0003259373\n",
      "Epoch [309/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0003, LR: 0.0003256737\n",
      "Epoch [310/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003254102\n",
      "Epoch [310/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003251467\n",
      "Epoch [310/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0003, LR: 0.0003248833\n",
      "Epoch [310/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003246200\n",
      "Epoch [310/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003243567\n",
      "Epoch [310/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003240935\n",
      "Epoch [310/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0003238304\n",
      "Epoch [310/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003235673\n",
      "Epoch [310/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0003233042\n",
      "Epoch [310/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003230413\n",
      "Epoch [310/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003227783\n",
      "Epoch [311/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0003225155\n",
      "Epoch [311/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0003222527\n",
      "Epoch [311/500], Batch [30/110], Train Loss: 0.0049, Val Loss: 0.0003, LR: 0.0003219900\n",
      "Epoch [311/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003217273\n",
      "Epoch [311/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0003214647\n",
      "Epoch [311/500], Batch [60/110], Train Loss: 0.0111, Val Loss: 0.0003, LR: 0.0003212021\n",
      "Epoch [311/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0003209396\n",
      "Epoch [311/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003206772\n",
      "Epoch [311/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003204148\n",
      "Epoch [311/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003201525\n",
      "Epoch [311/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003198902\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 311: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.039 sec , Memory Usage: 305.23 MB\n",
      "Epoch [312/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003196280\n",
      "Epoch [312/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003193659\n",
      "Epoch [312/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003191038\n",
      "Epoch [312/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003188418\n",
      "Epoch [312/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003185798\n",
      "Epoch [312/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0008, LR: 0.0003183179\n",
      "Epoch [312/500], Batch [70/110], Train Loss: 0.0014, Val Loss: 0.0003, LR: 0.0003180561\n",
      "Epoch [312/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003177943\n",
      "Epoch [312/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003175326\n",
      "Epoch [312/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003172709\n",
      "Epoch [312/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003170093\n",
      "Epoch [313/500], Batch [10/110], Train Loss: 0.0016, Val Loss: 0.0004, LR: 0.0003167478\n",
      "Epoch [313/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003164864\n",
      "Epoch [313/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003162249\n",
      "Epoch [313/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0003159636\n",
      "Epoch [313/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003157023\n",
      "Epoch [313/500], Batch [60/110], Train Loss: 0.0025, Val Loss: 0.0003, LR: 0.0003154411\n",
      "Epoch [313/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003151799\n",
      "Epoch [313/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003149188\n",
      "Epoch [313/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003146578\n",
      "Epoch [313/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0003143968\n",
      "Epoch [313/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003141359\n",
      "Epoch [314/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0003138751\n",
      "Epoch [314/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003136143\n",
      "Epoch [314/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0003133536\n",
      "Epoch [314/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003130929\n",
      "Epoch [314/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003128323\n",
      "Epoch [314/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003125718\n",
      "Epoch [314/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0003123113\n",
      "Epoch [314/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003120509\n",
      "Epoch [314/500], Batch [90/110], Train Loss: 0.0012, Val Loss: 0.0003, LR: 0.0003117905\n",
      "Epoch [314/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003115303\n",
      "Epoch [314/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003112700\n",
      "Epoch [315/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003110099\n",
      "Epoch [315/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0003107498\n",
      "Epoch [315/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003104898\n",
      "Epoch [315/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003102298\n",
      "Epoch [315/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003099699\n",
      "Epoch [315/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003097100\n",
      "Epoch [315/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003094503\n",
      "Epoch [315/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0003091905\n",
      "Epoch [315/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003089309\n",
      "Epoch [315/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0003086713\n",
      "Epoch [315/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003084118\n",
      "Epoch [316/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003081523\n",
      "Epoch [316/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003078929\n",
      "Epoch [316/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003076336\n",
      "Epoch [316/500], Batch [40/110], Train Loss: 0.0019, Val Loss: 0.0003, LR: 0.0003073744\n",
      "Epoch [316/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003071152\n",
      "Epoch [316/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003068560\n",
      "Epoch [316/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003065969\n",
      "Epoch [316/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0003063379\n",
      "Epoch [316/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003060790\n",
      "Epoch [316/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003058201\n",
      "Epoch [316/500], Batch [110/110], Train Loss: 0.0007, Val Loss: 0.0002, LR: 0.0003055613\n",
      "Epoch [317/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0003053026\n",
      "Epoch [317/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003050439\n",
      "Epoch [317/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0003047853\n",
      "Epoch [317/500], Batch [40/110], Train Loss: 0.0079, Val Loss: 0.0003, LR: 0.0003045267\n",
      "Epoch [317/500], Batch [50/110], Train Loss: 0.0033, Val Loss: 0.0008, LR: 0.0003042682\n",
      "Epoch [317/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0003040098\n",
      "Epoch [317/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003037515\n",
      "Epoch [317/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003034932\n",
      "Epoch [317/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0003032350\n",
      "Epoch [317/500], Batch [100/110], Train Loss: 0.0065, Val Loss: 0.0002, LR: 0.0003029768\n",
      "Epoch [317/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003027187\n",
      "Epoch [318/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003024607\n",
      "Epoch [318/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003022027\n",
      "Epoch [318/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003019448\n",
      "Epoch [318/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003016870\n",
      "Epoch [318/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003014292\n",
      "Epoch [318/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003011716\n",
      "Epoch [318/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0003009139\n",
      "Epoch [318/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0003006564\n",
      "Epoch [318/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0003003989\n",
      "Epoch [318/500], Batch [100/110], Train Loss: 0.0035, Val Loss: 0.0002, LR: 0.0003001414\n",
      "Epoch [318/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002998841\n",
      "Epoch [319/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002996268\n",
      "Epoch [319/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002993696\n",
      "Epoch [319/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002991124\n",
      "Epoch [319/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002988553\n",
      "Epoch [319/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002985983\n",
      "Epoch [319/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002983413\n",
      "Epoch [319/500], Batch [70/110], Train Loss: 0.0025, Val Loss: 0.0002, LR: 0.0002980844\n",
      "Epoch [319/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002978276\n",
      "Epoch [319/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002975709\n",
      "Epoch [319/500], Batch [100/110], Train Loss: 0.0016, Val Loss: 0.0003, LR: 0.0002973142\n",
      "Epoch [319/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002970576\n",
      "Epoch [320/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002968010\n",
      "Epoch [320/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002965445\n",
      "Epoch [320/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002962881\n",
      "Epoch [320/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002960318\n",
      "Epoch [320/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002957755\n",
      "Epoch [320/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002955193\n",
      "Epoch [320/500], Batch [70/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0002952631\n",
      "Epoch [320/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002950071\n",
      "Epoch [320/500], Batch [90/110], Train Loss: 0.0009, Val Loss: 0.0003, LR: 0.0002947511\n",
      "Epoch [320/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002944951\n",
      "Epoch [320/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002942393\n",
      "Epoch [321/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002939835\n",
      "Epoch [321/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002937277\n",
      "Epoch [321/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002934721\n",
      "Epoch [321/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002932165\n",
      "Epoch [321/500], Batch [50/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0002929609\n",
      "Epoch [321/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002927055\n",
      "Epoch [321/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002924501\n",
      "Epoch [321/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002921948\n",
      "Epoch [321/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002919395\n",
      "Epoch [321/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002916844\n",
      "Epoch [321/500], Batch [110/110], Train Loss: 0.0023, Val Loss: 0.0002, LR: 0.0002914293\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 321: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.036 sec , Memory Usage: 305.23 MB\n",
      "Epoch [322/500], Batch [10/110], Train Loss: 0.0040, Val Loss: 0.0002, LR: 0.0002911742\n",
      "Epoch [322/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002909193\n",
      "Epoch [322/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002906644\n",
      "Epoch [322/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0002904095\n",
      "Epoch [322/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002901548\n",
      "Epoch [322/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0002899001\n",
      "Epoch [322/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002896455\n",
      "Epoch [322/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002893909\n",
      "Epoch [322/500], Batch [90/110], Train Loss: 0.0118, Val Loss: 0.0003, LR: 0.0002891364\n",
      "Epoch [322/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002888820\n",
      "Epoch [322/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0002886277\n",
      "Epoch [323/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002883734\n",
      "Epoch [323/500], Batch [20/110], Train Loss: 0.0024, Val Loss: 0.0002, LR: 0.0002881192\n",
      "Epoch [323/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002878651\n",
      "Epoch [323/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002876111\n",
      "Epoch [323/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002873571\n",
      "Epoch [323/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002871032\n",
      "Epoch [323/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002868493\n",
      "Epoch [323/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002865956\n",
      "Epoch [323/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002863419\n",
      "Epoch [323/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002860882\n",
      "Epoch [323/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002858347\n",
      "Epoch [324/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002855812\n",
      "Epoch [324/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002853278\n",
      "Epoch [324/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002850744\n",
      "Epoch [324/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002848212\n",
      "Epoch [324/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002845680\n",
      "Epoch [324/500], Batch [60/110], Train Loss: 0.0015, Val Loss: 0.0002, LR: 0.0002843149\n",
      "Epoch [324/500], Batch [70/110], Train Loss: 0.0048, Val Loss: 0.0002, LR: 0.0002840618\n",
      "Epoch [324/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002838088\n",
      "Epoch [324/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002835559\n",
      "Epoch [324/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002833031\n",
      "Epoch [324/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002830503\n",
      "Epoch [325/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002827976\n",
      "Epoch [325/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002825450\n",
      "Epoch [325/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002822925\n",
      "Epoch [325/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002820400\n",
      "Epoch [325/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002817876\n",
      "Epoch [325/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002815352\n",
      "Epoch [325/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002812830\n",
      "Epoch [325/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002810308\n",
      "Epoch [325/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002807787\n",
      "Epoch [325/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002805267\n",
      "Epoch [325/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002802747\n",
      "Epoch [326/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002800228\n",
      "Epoch [326/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002797710\n",
      "Epoch [326/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0003, LR: 0.0002795193\n",
      "Epoch [326/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002792676\n",
      "Epoch [326/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002790160\n",
      "Epoch [326/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002787645\n",
      "Epoch [326/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002785130\n",
      "Epoch [326/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002782616\n",
      "Epoch [326/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002780103\n",
      "Epoch [326/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002777591\n",
      "Epoch [326/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002775080\n",
      "Epoch [327/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002772569\n",
      "Epoch [327/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002770059\n",
      "Epoch [327/500], Batch [30/110], Train Loss: 0.0026, Val Loss: 0.0002, LR: 0.0002767550\n",
      "Epoch [327/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002765041\n",
      "Epoch [327/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002762533\n",
      "Epoch [327/500], Batch [60/110], Train Loss: 0.0072, Val Loss: 0.0003, LR: 0.0002760026\n",
      "Epoch [327/500], Batch [70/110], Train Loss: 0.0020, Val Loss: 0.0002, LR: 0.0002757520\n",
      "Epoch [327/500], Batch [80/110], Train Loss: 0.0052, Val Loss: 0.0003, LR: 0.0002755014\n",
      "Epoch [327/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002752510\n",
      "Epoch [327/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002750005\n",
      "Epoch [327/500], Batch [110/110], Train Loss: 0.0064, Val Loss: 0.0003, LR: 0.0002747502\n",
      "Epoch [328/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002745000\n",
      "Epoch [328/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002742498\n",
      "Epoch [328/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002739997\n",
      "Epoch [328/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002737496\n",
      "Epoch [328/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002734997\n",
      "Epoch [328/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002732498\n",
      "Epoch [328/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002730000\n",
      "Epoch [328/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002727503\n",
      "Epoch [328/500], Batch [90/110], Train Loss: 0.0010, Val Loss: 0.0003, LR: 0.0002725006\n",
      "Epoch [328/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002722511\n",
      "Epoch [328/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002720016\n",
      "Epoch [329/500], Batch [10/110], Train Loss: 0.0062, Val Loss: 0.0002, LR: 0.0002717521\n",
      "Epoch [329/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002715028\n",
      "Epoch [329/500], Batch [30/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0002712535\n",
      "Epoch [329/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002710043\n",
      "Epoch [329/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002707552\n",
      "Epoch [329/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002705062\n",
      "Epoch [329/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002702572\n",
      "Epoch [329/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002700083\n",
      "Epoch [329/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002697595\n",
      "Epoch [329/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002695107\n",
      "Epoch [329/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002692621\n",
      "Epoch [330/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002690135\n",
      "Epoch [330/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002687650\n",
      "Epoch [330/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0002, LR: 0.0002685166\n",
      "Epoch [330/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002682682\n",
      "Epoch [330/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0002680199\n",
      "Epoch [330/500], Batch [60/110], Train Loss: 0.0023, Val Loss: 0.0003, LR: 0.0002677718\n",
      "Epoch [330/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002675236\n",
      "Epoch [330/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002672756\n",
      "Epoch [330/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002670276\n",
      "Epoch [330/500], Batch [100/110], Train Loss: 0.0028, Val Loss: 0.0002, LR: 0.0002667797\n",
      "Epoch [330/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002665319\n",
      "Epoch [331/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002662842\n",
      "Epoch [331/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002660365\n",
      "Epoch [331/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002657890\n",
      "Epoch [331/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0006, LR: 0.0002655415\n",
      "Epoch [331/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002652941\n",
      "Epoch [331/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002650467\n",
      "Epoch [331/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002647995\n",
      "Epoch [331/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002645523\n",
      "Epoch [331/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002643052\n",
      "Epoch [331/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0005, LR: 0.0002640581\n",
      "Epoch [331/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002638112\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 331: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.047 sec , Memory Usage: 305.23 MB\n",
      "Epoch [332/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002635643\n",
      "Epoch [332/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002633175\n",
      "Epoch [332/500], Batch [30/110], Train Loss: 0.0039, Val Loss: 0.0005, LR: 0.0002630708\n",
      "Epoch [332/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002628242\n",
      "Epoch [332/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002625776\n",
      "Epoch [332/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002623312\n",
      "Epoch [332/500], Batch [70/110], Train Loss: 0.0018, Val Loss: 0.0002, LR: 0.0002620848\n",
      "Epoch [332/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002618384\n",
      "Epoch [332/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002615922\n",
      "Epoch [332/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002613460\n",
      "Epoch [332/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002611000\n",
      "Epoch [333/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002608540\n",
      "Epoch [333/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002606080\n",
      "Epoch [333/500], Batch [30/110], Train Loss: 0.0023, Val Loss: 0.0002, LR: 0.0002603622\n",
      "Epoch [333/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002601164\n",
      "Epoch [333/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002598708\n",
      "Epoch [333/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002596252\n",
      "Epoch [333/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0002593796\n",
      "Epoch [333/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0002591342\n",
      "Epoch [333/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002588889\n",
      "Epoch [333/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002586436\n",
      "Epoch [333/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002583984\n",
      "Epoch [334/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002581533\n",
      "Epoch [334/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002579082\n",
      "Epoch [334/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0002576633\n",
      "Epoch [334/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002574184\n",
      "Epoch [334/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002571736\n",
      "Epoch [334/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002569289\n",
      "Epoch [334/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0002566842\n",
      "Epoch [334/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002564397\n",
      "Epoch [334/500], Batch [90/110], Train Loss: 0.0027, Val Loss: 0.0003, LR: 0.0002561952\n",
      "Epoch [334/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002559508\n",
      "Epoch [334/500], Batch [110/110], Train Loss: 0.0015, Val Loss: 0.0002, LR: 0.0002557065\n",
      "Epoch [335/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0002, LR: 0.0002554623\n",
      "Epoch [335/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002552181\n",
      "Epoch [335/500], Batch [30/110], Train Loss: 0.0020, Val Loss: 0.0002, LR: 0.0002549741\n",
      "Epoch [335/500], Batch [40/110], Train Loss: 0.0027, Val Loss: 0.0002, LR: 0.0002547301\n",
      "Epoch [335/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002544862\n",
      "Epoch [335/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002542424\n",
      "Epoch [335/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0002539986\n",
      "Epoch [335/500], Batch [80/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0002537550\n",
      "Epoch [335/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002535114\n",
      "Epoch [335/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002532679\n",
      "Epoch [335/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002530245\n",
      "Epoch [336/500], Batch [10/110], Train Loss: 0.0020, Val Loss: 0.0002, LR: 0.0002527812\n",
      "Epoch [336/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002525379\n",
      "Epoch [336/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002522948\n",
      "Epoch [336/500], Batch [40/110], Train Loss: 0.0046, Val Loss: 0.0002, LR: 0.0002520517\n",
      "Epoch [336/500], Batch [50/110], Train Loss: 0.0013, Val Loss: 0.0002, LR: 0.0002518087\n",
      "Epoch [336/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002515658\n",
      "Epoch [336/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0002513229\n",
      "Epoch [336/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002510802\n",
      "Epoch [336/500], Batch [90/110], Train Loss: 0.0221, Val Loss: 0.0002, LR: 0.0002508375\n",
      "Epoch [336/500], Batch [100/110], Train Loss: 0.0005, Val Loss: 0.0002, LR: 0.0002505949\n",
      "Epoch [336/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002503524\n",
      "Epoch [337/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002501100\n",
      "Epoch [337/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002498677\n",
      "Epoch [337/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002496254\n",
      "Epoch [337/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0002493833\n",
      "Epoch [337/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002491412\n",
      "Epoch [337/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002488992\n",
      "Epoch [337/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002486573\n",
      "Epoch [337/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002484154\n",
      "Epoch [337/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002481737\n",
      "Epoch [337/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002479320\n",
      "Epoch [337/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002476904\n",
      "Epoch [338/500], Batch [10/110], Train Loss: 0.0018, Val Loss: 0.0002, LR: 0.0002474489\n",
      "Epoch [338/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002472075\n",
      "Epoch [338/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002469662\n",
      "Epoch [338/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002467249\n",
      "Epoch [338/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002464838\n",
      "Epoch [338/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002462427\n",
      "Epoch [338/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002460017\n",
      "Epoch [338/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002457608\n",
      "Epoch [338/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002455200\n",
      "Epoch [338/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002452792\n",
      "Epoch [338/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002450386\n",
      "Epoch [339/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002447980\n",
      "Epoch [339/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002445575\n",
      "Epoch [339/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002443171\n",
      "Epoch [339/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002440768\n",
      "Epoch [339/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002438366\n",
      "Epoch [339/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002435964\n",
      "Epoch [339/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002433563\n",
      "Epoch [339/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002431164\n",
      "Epoch [339/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0005, LR: 0.0002428765\n",
      "Epoch [339/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002426367\n",
      "Epoch [339/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002423970\n",
      "Epoch [340/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002421573\n",
      "Epoch [340/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002419178\n",
      "Epoch [340/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002416783\n",
      "Epoch [340/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002414389\n",
      "Epoch [340/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002411997\n",
      "Epoch [340/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002409605\n",
      "Epoch [340/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002407213\n",
      "Epoch [340/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002404823\n",
      "Epoch [340/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002402434\n",
      "Epoch [340/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002400045\n",
      "Epoch [340/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002397657\n",
      "Epoch [341/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002395271\n",
      "Epoch [341/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002392885\n",
      "Epoch [341/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002390499\n",
      "Epoch [341/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002388115\n",
      "Epoch [341/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002385732\n",
      "Epoch [341/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002383349\n",
      "Epoch [341/500], Batch [70/110], Train Loss: 0.0015, Val Loss: 0.0002, LR: 0.0002380968\n",
      "Epoch [341/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002378587\n",
      "Epoch [341/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002376207\n",
      "Epoch [341/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002373828\n",
      "Epoch [341/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002371450\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 341: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.027 sec , Memory Usage: 305.23 MB\n",
      "Epoch [342/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0002369073\n",
      "Epoch [342/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002366696\n",
      "Epoch [342/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002364321\n",
      "Epoch [342/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002361946\n",
      "Epoch [342/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0002359572\n",
      "Epoch [342/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002357199\n",
      "Epoch [342/500], Batch [70/110], Train Loss: 0.0021, Val Loss: 0.0002, LR: 0.0002354827\n",
      "Epoch [342/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002352456\n",
      "Epoch [342/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002350086\n",
      "Epoch [342/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002347716\n",
      "Epoch [342/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002345348\n",
      "Epoch [343/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002342980\n",
      "Epoch [343/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002340614\n",
      "Epoch [343/500], Batch [30/110], Train Loss: 0.0039, Val Loss: 0.0002, LR: 0.0002338248\n",
      "Epoch [343/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002335883\n",
      "Epoch [343/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002333519\n",
      "Epoch [343/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002331156\n",
      "Epoch [343/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0002328793\n",
      "Epoch [343/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002326432\n",
      "Epoch [343/500], Batch [90/110], Train Loss: 0.0023, Val Loss: 0.0003, LR: 0.0002324071\n",
      "Epoch [343/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002321712\n",
      "Epoch [343/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002319353\n",
      "Epoch [344/500], Batch [10/110], Train Loss: 0.0037, Val Loss: 0.0003, LR: 0.0002316995\n",
      "Epoch [344/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002314638\n",
      "Epoch [344/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0002312282\n",
      "Epoch [344/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002309927\n",
      "Epoch [344/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002307573\n",
      "Epoch [344/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002305219\n",
      "Epoch [344/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002302867\n",
      "Epoch [344/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0005, LR: 0.0002300515\n",
      "Epoch [344/500], Batch [90/110], Train Loss: 0.0016, Val Loss: 0.0004, LR: 0.0002298164\n",
      "Epoch [344/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002295815\n",
      "Epoch [344/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002293466\n",
      "Epoch [345/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0002291118\n",
      "Epoch [345/500], Batch [20/110], Train Loss: 0.0043, Val Loss: 0.0003, LR: 0.0002288771\n",
      "Epoch [345/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002286424\n",
      "Epoch [345/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002284079\n",
      "Epoch [345/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002281735\n",
      "Epoch [345/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002279391\n",
      "Epoch [345/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0002277049\n",
      "Epoch [345/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002274707\n",
      "Epoch [345/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002272366\n",
      "Epoch [345/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002270026\n",
      "Epoch [345/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0002267687\n",
      "Epoch [346/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002265349\n",
      "Epoch [346/500], Batch [20/110], Train Loss: 0.0023, Val Loss: 0.0003, LR: 0.0002263012\n",
      "Epoch [346/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0003, LR: 0.0002260676\n",
      "Epoch [346/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002258340\n",
      "Epoch [346/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002256006\n",
      "Epoch [346/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002253673\n",
      "Epoch [346/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002251340\n",
      "Epoch [346/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002249008\n",
      "Epoch [346/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002246678\n",
      "Epoch [346/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002244348\n",
      "Epoch [346/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0003, LR: 0.0002242019\n",
      "Epoch [347/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002239691\n",
      "Epoch [347/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002237364\n",
      "Epoch [347/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0002235037\n",
      "Epoch [347/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002232712\n",
      "Epoch [347/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002230388\n",
      "Epoch [347/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002228064\n",
      "Epoch [347/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002225742\n",
      "Epoch [347/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002223420\n",
      "Epoch [347/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002221100\n",
      "Epoch [347/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002218780\n",
      "Epoch [347/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002216461\n",
      "Epoch [348/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002214143\n",
      "Epoch [348/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002211826\n",
      "Epoch [348/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002209510\n",
      "Epoch [348/500], Batch [40/110], Train Loss: 0.0009, Val Loss: 0.0003, LR: 0.0002207195\n",
      "Epoch [348/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002204881\n",
      "Epoch [348/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002202567\n",
      "Epoch [348/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002200255\n",
      "Epoch [348/500], Batch [80/110], Train Loss: 0.0021, Val Loss: 0.0002, LR: 0.0002197944\n",
      "Epoch [348/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0002195633\n",
      "Epoch [348/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002193324\n",
      "Epoch [348/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002191015\n",
      "Epoch [349/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0002188707\n",
      "Epoch [349/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002186401\n",
      "Epoch [349/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0005, LR: 0.0002184095\n",
      "Epoch [349/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002181790\n",
      "Epoch [349/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0002179486\n",
      "Epoch [349/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002177183\n",
      "Epoch [349/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002174881\n",
      "Epoch [349/500], Batch [80/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0002172580\n",
      "Epoch [349/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002170280\n",
      "Epoch [349/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002167980\n",
      "Epoch [349/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002165682\n",
      "Epoch [350/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002163385\n",
      "Epoch [350/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002161088\n",
      "Epoch [350/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002158793\n",
      "Epoch [350/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002156498\n",
      "Epoch [350/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002154205\n",
      "Epoch [350/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002151912\n",
      "Epoch [350/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002149620\n",
      "Epoch [350/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002147330\n",
      "Epoch [350/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002145040\n",
      "Epoch [350/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0002, LR: 0.0002142751\n",
      "Epoch [350/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002140463\n",
      "Epoch [351/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0002138176\n",
      "Epoch [351/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002135890\n",
      "Epoch [351/500], Batch [30/110], Train Loss: 0.0033, Val Loss: 0.0002, LR: 0.0002133605\n",
      "Epoch [351/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002131321\n",
      "Epoch [351/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002129038\n",
      "Epoch [351/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002126755\n",
      "Epoch [351/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002124474\n",
      "Epoch [351/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002122194\n",
      "Epoch [351/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002119915\n",
      "Epoch [351/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002117636\n",
      "Epoch [351/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002115359\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 351: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.050 sec , Memory Usage: 305.23 MB\n",
      "Epoch [352/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002113082\n",
      "Epoch [352/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002110807\n",
      "Epoch [352/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002108532\n",
      "Epoch [352/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002106259\n",
      "Epoch [352/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002103986\n",
      "Epoch [352/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002101714\n",
      "Epoch [352/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0002099444\n",
      "Epoch [352/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002097174\n",
      "Epoch [352/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002094905\n",
      "Epoch [352/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002092637\n",
      "Epoch [352/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0002090370\n",
      "Epoch [353/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002088104\n",
      "Epoch [353/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002085840\n",
      "Epoch [353/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0002, LR: 0.0002083576\n",
      "Epoch [353/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002081313\n",
      "Epoch [353/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002079051\n",
      "Epoch [353/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0003, LR: 0.0002076789\n",
      "Epoch [353/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002074529\n",
      "Epoch [353/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002072270\n",
      "Epoch [353/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002070012\n",
      "Epoch [353/500], Batch [100/110], Train Loss: 0.0052, Val Loss: 0.0002, LR: 0.0002067755\n",
      "Epoch [353/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002065499\n",
      "Epoch [354/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002063244\n",
      "Epoch [354/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002060989\n",
      "Epoch [354/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002058736\n",
      "Epoch [354/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0003, LR: 0.0002056484\n",
      "Epoch [354/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002054232\n",
      "Epoch [354/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002051982\n",
      "Epoch [354/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0002049733\n",
      "Epoch [354/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002047484\n",
      "Epoch [354/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002045237\n",
      "Epoch [354/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002042990\n",
      "Epoch [354/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002040745\n",
      "Epoch [355/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0002038501\n",
      "Epoch [355/500], Batch [20/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0002036257\n",
      "Epoch [355/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002034015\n",
      "Epoch [355/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002031773\n",
      "Epoch [355/500], Batch [50/110], Train Loss: 0.0025, Val Loss: 0.0002, LR: 0.0002029533\n",
      "Epoch [355/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0002027293\n",
      "Epoch [355/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002025054\n",
      "Epoch [355/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0002022817\n",
      "Epoch [355/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0002020580\n",
      "Epoch [355/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002018345\n",
      "Epoch [355/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0003, LR: 0.0002016110\n",
      "Epoch [356/500], Batch [10/110], Train Loss: 0.0032, Val Loss: 0.0003, LR: 0.0002013876\n",
      "Epoch [356/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002011644\n",
      "Epoch [356/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0002009412\n",
      "Epoch [356/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002007182\n",
      "Epoch [356/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002004952\n",
      "Epoch [356/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002002723\n",
      "Epoch [356/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0002000496\n",
      "Epoch [356/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001998269\n",
      "Epoch [356/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001996043\n",
      "Epoch [356/500], Batch [100/110], Train Loss: 0.0027, Val Loss: 0.0003, LR: 0.0001993819\n",
      "Epoch [356/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0003, LR: 0.0001991595\n",
      "Epoch [357/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001989372\n",
      "Epoch [357/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0001987151\n",
      "Epoch [357/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0002, LR: 0.0001984930\n",
      "Epoch [357/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001982710\n",
      "Epoch [357/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001980492\n",
      "Epoch [357/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001978274\n",
      "Epoch [357/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001976057\n",
      "Epoch [357/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001973841\n",
      "Epoch [357/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001971627\n",
      "Epoch [357/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001969413\n",
      "Epoch [357/500], Batch [110/110], Train Loss: 0.0014, Val Loss: 0.0003, LR: 0.0001967200\n",
      "Epoch [358/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001964989\n",
      "Epoch [358/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001962778\n",
      "Epoch [358/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001960569\n",
      "Epoch [358/500], Batch [40/110], Train Loss: 0.0013, Val Loss: 0.0003, LR: 0.0001958360\n",
      "Epoch [358/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001956152\n",
      "Epoch [358/500], Batch [60/110], Train Loss: 0.0010, Val Loss: 0.0004, LR: 0.0001953946\n",
      "Epoch [358/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001951740\n",
      "Epoch [358/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001949535\n",
      "Epoch [358/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001947332\n",
      "Epoch [358/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001945129\n",
      "Epoch [358/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0001942928\n",
      "Epoch [359/500], Batch [10/110], Train Loss: 0.0016, Val Loss: 0.0003, LR: 0.0001940727\n",
      "Epoch [359/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001938528\n",
      "Epoch [359/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0004, LR: 0.0001936329\n",
      "Epoch [359/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0003, LR: 0.0001934132\n",
      "Epoch [359/500], Batch [50/110], Train Loss: 0.0023, Val Loss: 0.0005, LR: 0.0001931935\n",
      "Epoch [359/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001929740\n",
      "Epoch [359/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001927545\n",
      "Epoch [359/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001925352\n",
      "Epoch [359/500], Batch [90/110], Train Loss: 0.0011, Val Loss: 0.0003, LR: 0.0001923159\n",
      "Epoch [359/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001920968\n",
      "Epoch [359/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001918778\n",
      "Epoch [360/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001916588\n",
      "Epoch [360/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001914400\n",
      "Epoch [360/500], Batch [30/110], Train Loss: 0.0007, Val Loss: 0.0004, LR: 0.0001912213\n",
      "Epoch [360/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001910026\n",
      "Epoch [360/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001907841\n",
      "Epoch [360/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001905657\n",
      "Epoch [360/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0001903474\n",
      "Epoch [360/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001901292\n",
      "Epoch [360/500], Batch [90/110], Train Loss: 0.0031, Val Loss: 0.0002, LR: 0.0001899110\n",
      "Epoch [360/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001896930\n",
      "Epoch [360/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0002, LR: 0.0001894751\n",
      "Epoch [361/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001892573\n",
      "Epoch [361/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001890396\n",
      "Epoch [361/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001888220\n",
      "Epoch [361/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001886045\n",
      "Epoch [361/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001883871\n",
      "Epoch [361/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001881698\n",
      "Epoch [361/500], Batch [70/110], Train Loss: 0.0103, Val Loss: 0.0003, LR: 0.0001879526\n",
      "Epoch [361/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001877356\n",
      "Epoch [361/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001875186\n",
      "Epoch [361/500], Batch [100/110], Train Loss: 0.0391, Val Loss: 0.0002, LR: 0.0001873017\n",
      "Epoch [361/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001870849\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 361: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 305.24 MB\n",
      "Epoch [362/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001868683\n",
      "Epoch [362/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001866517\n",
      "Epoch [362/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001864352\n",
      "Epoch [362/500], Batch [40/110], Train Loss: 0.0024, Val Loss: 0.0002, LR: 0.0001862189\n",
      "Epoch [362/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001860026\n",
      "Epoch [362/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001857865\n",
      "Epoch [362/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001855704\n",
      "Epoch [362/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001853545\n",
      "Epoch [362/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001851387\n",
      "Epoch [362/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001849229\n",
      "Epoch [362/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001847073\n",
      "Epoch [363/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001844918\n",
      "Epoch [363/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001842764\n",
      "Epoch [363/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001840610\n",
      "Epoch [363/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0001838458\n",
      "Epoch [363/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001836307\n",
      "Epoch [363/500], Batch [60/110], Train Loss: 0.0029, Val Loss: 0.0002, LR: 0.0001834157\n",
      "Epoch [363/500], Batch [70/110], Train Loss: 0.0025, Val Loss: 0.0002, LR: 0.0001832008\n",
      "Epoch [363/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001829860\n",
      "Epoch [363/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001827714\n",
      "Epoch [363/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001825568\n",
      "Epoch [363/500], Batch [110/110], Train Loss: 0.0047, Val Loss: 0.0002, LR: 0.0001823423\n",
      "Epoch [364/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001821279\n",
      "Epoch [364/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001819137\n",
      "Epoch [364/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001816995\n",
      "Epoch [364/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001814855\n",
      "Epoch [364/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001812715\n",
      "Epoch [364/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001810577\n",
      "Epoch [364/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001808439\n",
      "Epoch [364/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001806303\n",
      "Epoch [364/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0008, LR: 0.0001804168\n",
      "Epoch [364/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0009, LR: 0.0001802034\n",
      "Epoch [364/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0008, LR: 0.0001799901\n",
      "Epoch [365/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001797768\n",
      "Epoch [365/500], Batch [20/110], Train Loss: 0.0029, Val Loss: 0.0002, LR: 0.0001795637\n",
      "Epoch [365/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001793507\n",
      "Epoch [365/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0002, LR: 0.0001791379\n",
      "Epoch [365/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001789251\n",
      "Epoch [365/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001787124\n",
      "Epoch [365/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0003, LR: 0.0001784998\n",
      "Epoch [365/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001782874\n",
      "Epoch [365/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001780750\n",
      "Epoch [365/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001778628\n",
      "Epoch [365/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001776506\n",
      "Epoch [366/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001774386\n",
      "Epoch [366/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0001772267\n",
      "Epoch [366/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001770148\n",
      "Epoch [366/500], Batch [40/110], Train Loss: 0.0024, Val Loss: 0.0002, LR: 0.0001768031\n",
      "Epoch [366/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001765915\n",
      "Epoch [366/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0005, LR: 0.0001763800\n",
      "Epoch [366/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001761686\n",
      "Epoch [366/500], Batch [80/110], Train Loss: 0.0007, Val Loss: 0.0002, LR: 0.0001759573\n",
      "Epoch [366/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001757462\n",
      "Epoch [366/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001755351\n",
      "Epoch [366/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001753241\n",
      "Epoch [367/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0001751133\n",
      "Epoch [367/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001749025\n",
      "Epoch [367/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001746919\n",
      "Epoch [367/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001744813\n",
      "Epoch [367/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001742709\n",
      "Epoch [367/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001740606\n",
      "Epoch [367/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001738504\n",
      "Epoch [367/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001736403\n",
      "Epoch [367/500], Batch [90/110], Train Loss: 0.0021, Val Loss: 0.0002, LR: 0.0001734303\n",
      "Epoch [367/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001732204\n",
      "Epoch [367/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0004, LR: 0.0001730106\n",
      "Epoch [368/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0006, LR: 0.0001728010\n",
      "Epoch [368/500], Batch [20/110], Train Loss: 0.0127, Val Loss: 0.0007, LR: 0.0001725914\n",
      "Epoch [368/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0007, LR: 0.0001723820\n",
      "Epoch [368/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001721726\n",
      "Epoch [368/500], Batch [50/110], Train Loss: 0.0008, Val Loss: 0.0004, LR: 0.0001719634\n",
      "Epoch [368/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001717543\n",
      "Epoch [368/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001715452\n",
      "Epoch [368/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001713363\n",
      "Epoch [368/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001711275\n",
      "Epoch [368/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001709188\n",
      "Epoch [368/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0001707103\n",
      "Epoch [369/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001705018\n",
      "Epoch [369/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001702934\n",
      "Epoch [369/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001700852\n",
      "Epoch [369/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001698770\n",
      "Epoch [369/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001696690\n",
      "Epoch [369/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001694611\n",
      "Epoch [369/500], Batch [70/110], Train Loss: 0.0023, Val Loss: 0.0002, LR: 0.0001692532\n",
      "Epoch [369/500], Batch [80/110], Train Loss: 0.0038, Val Loss: 0.0003, LR: 0.0001690455\n",
      "Epoch [369/500], Batch [90/110], Train Loss: 0.0019, Val Loss: 0.0002, LR: 0.0001688379\n",
      "Epoch [369/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001686305\n",
      "Epoch [369/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001684231\n",
      "Epoch [370/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0001682158\n",
      "Epoch [370/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001680087\n",
      "Epoch [370/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001678016\n",
      "Epoch [370/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001675947\n",
      "Epoch [370/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001673878\n",
      "Epoch [370/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001671811\n",
      "Epoch [370/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001669745\n",
      "Epoch [370/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001667680\n",
      "Epoch [370/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001665616\n",
      "Epoch [370/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001663553\n",
      "Epoch [370/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001661492\n",
      "Epoch [371/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001659431\n",
      "Epoch [371/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001657372\n",
      "Epoch [371/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0003, LR: 0.0001655313\n",
      "Epoch [371/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001653256\n",
      "Epoch [371/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001651200\n",
      "Epoch [371/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001649145\n",
      "Epoch [371/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001647091\n",
      "Epoch [371/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001645038\n",
      "Epoch [371/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001642987\n",
      "Epoch [371/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001640936\n",
      "Epoch [371/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0001638887\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 371: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.047 sec , Memory Usage: 305.24 MB\n",
      "Epoch [372/500], Batch [10/110], Train Loss: 0.0013, Val Loss: 0.0002, LR: 0.0001636838\n",
      "Epoch [372/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001634791\n",
      "Epoch [372/500], Batch [30/110], Train Loss: 0.0011, Val Loss: 0.0002, LR: 0.0001632745\n",
      "Epoch [372/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001630700\n",
      "Epoch [372/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001628656\n",
      "Epoch [372/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001626613\n",
      "Epoch [372/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001624572\n",
      "Epoch [372/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0002, LR: 0.0001622531\n",
      "Epoch [372/500], Batch [90/110], Train Loss: 0.0026, Val Loss: 0.0002, LR: 0.0001620492\n",
      "Epoch [372/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001618453\n",
      "Epoch [372/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001616416\n",
      "Epoch [373/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001614380\n",
      "Epoch [373/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001612345\n",
      "Epoch [373/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001610311\n",
      "Epoch [373/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0001608279\n",
      "Epoch [373/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001606247\n",
      "Epoch [373/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001604217\n",
      "Epoch [373/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001602187\n",
      "Epoch [373/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001600159\n",
      "Epoch [373/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001598132\n",
      "Epoch [373/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001596106\n",
      "Epoch [373/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001594081\n",
      "Epoch [374/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001592057\n",
      "Epoch [374/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001590035\n",
      "Epoch [374/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001588013\n",
      "Epoch [374/500], Batch [40/110], Train Loss: 0.0518, Val Loss: 0.0003, LR: 0.0001585993\n",
      "Epoch [374/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001583974\n",
      "Epoch [374/500], Batch [60/110], Train Loss: 0.0031, Val Loss: 0.0003, LR: 0.0001581956\n",
      "Epoch [374/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001579939\n",
      "Epoch [374/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001577923\n",
      "Epoch [374/500], Batch [90/110], Train Loss: 0.0015, Val Loss: 0.0002, LR: 0.0001575909\n",
      "Epoch [374/500], Batch [100/110], Train Loss: 0.0016, Val Loss: 0.0002, LR: 0.0001573895\n",
      "Epoch [374/500], Batch [110/110], Train Loss: 0.0030, Val Loss: 0.0002, LR: 0.0001571883\n",
      "Epoch [375/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001569871\n",
      "Epoch [375/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001567861\n",
      "Epoch [375/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001565852\n",
      "Epoch [375/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001563844\n",
      "Epoch [375/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001561838\n",
      "Epoch [375/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001559832\n",
      "Epoch [375/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001557828\n",
      "Epoch [375/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001555824\n",
      "Epoch [375/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001553822\n",
      "Epoch [375/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001551821\n",
      "Epoch [375/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0004, LR: 0.0001549821\n",
      "Epoch [376/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001547823\n",
      "Epoch [376/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001545825\n",
      "Epoch [376/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001543829\n",
      "Epoch [376/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001541833\n",
      "Epoch [376/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001539839\n",
      "Epoch [376/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001537846\n",
      "Epoch [376/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001535854\n",
      "Epoch [376/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001533864\n",
      "Epoch [376/500], Batch [90/110], Train Loss: 0.0022, Val Loss: 0.0002, LR: 0.0001531874\n",
      "Epoch [376/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001529886\n",
      "Epoch [376/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001527898\n",
      "Epoch [377/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001525912\n",
      "Epoch [377/500], Batch [20/110], Train Loss: 0.0087, Val Loss: 0.0004, LR: 0.0001523927\n",
      "Epoch [377/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001521943\n",
      "Epoch [377/500], Batch [40/110], Train Loss: 0.0007, Val Loss: 0.0002, LR: 0.0001519961\n",
      "Epoch [377/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001517979\n",
      "Epoch [377/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001515999\n",
      "Epoch [377/500], Batch [70/110], Train Loss: 0.0025, Val Loss: 0.0002, LR: 0.0001514020\n",
      "Epoch [377/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001512042\n",
      "Epoch [377/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001510065\n",
      "Epoch [377/500], Batch [100/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001508089\n",
      "Epoch [377/500], Batch [110/110], Train Loss: 0.0031, Val Loss: 0.0002, LR: 0.0001506114\n",
      "Epoch [378/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001504141\n",
      "Epoch [378/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001502169\n",
      "Epoch [378/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001500198\n",
      "Epoch [378/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001498228\n",
      "Epoch [378/500], Batch [50/110], Train Loss: 0.0039, Val Loss: 0.0002, LR: 0.0001496259\n",
      "Epoch [378/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001494291\n",
      "Epoch [378/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0001492325\n",
      "Epoch [378/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001490359\n",
      "Epoch [378/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001488395\n",
      "Epoch [378/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001486432\n",
      "Epoch [378/500], Batch [110/110], Train Loss: 0.0026, Val Loss: 0.0002, LR: 0.0001484470\n",
      "Epoch [379/500], Batch [10/110], Train Loss: 0.0023, Val Loss: 0.0002, LR: 0.0001482510\n",
      "Epoch [379/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001480550\n",
      "Epoch [379/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001478592\n",
      "Epoch [379/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001476635\n",
      "Epoch [379/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001474679\n",
      "Epoch [379/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001472724\n",
      "Epoch [379/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001470770\n",
      "Epoch [379/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001468818\n",
      "Epoch [379/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0002, LR: 0.0001466866\n",
      "Epoch [379/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001464916\n",
      "Epoch [379/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001462967\n",
      "Epoch [380/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001461019\n",
      "Epoch [380/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001459073\n",
      "Epoch [380/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001457127\n",
      "Epoch [380/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001455183\n",
      "Epoch [380/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001453240\n",
      "Epoch [380/500], Batch [60/110], Train Loss: 0.0009, Val Loss: 0.0002, LR: 0.0001451298\n",
      "Epoch [380/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001449357\n",
      "Epoch [380/500], Batch [80/110], Train Loss: 0.0020, Val Loss: 0.0002, LR: 0.0001447417\n",
      "Epoch [380/500], Batch [90/110], Train Loss: 0.0038, Val Loss: 0.0002, LR: 0.0001445479\n",
      "Epoch [380/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001443541\n",
      "Epoch [380/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001441605\n",
      "Epoch [381/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001439670\n",
      "Epoch [381/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001437737\n",
      "Epoch [381/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001435804\n",
      "Epoch [381/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001433873\n",
      "Epoch [381/500], Batch [50/110], Train Loss: 0.0016, Val Loss: 0.0002, LR: 0.0001431942\n",
      "Epoch [381/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001430013\n",
      "Epoch [381/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001428086\n",
      "Epoch [381/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001426159\n",
      "Epoch [381/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001424233\n",
      "Epoch [381/500], Batch [100/110], Train Loss: 0.0039, Val Loss: 0.0002, LR: 0.0001422309\n",
      "Epoch [381/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001420386\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 381: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 305.24 MB\n",
      "Epoch [382/500], Batch [10/110], Train Loss: 0.0009, Val Loss: 0.0002, LR: 0.0001418464\n",
      "Epoch [382/500], Batch [20/110], Train Loss: 0.0142, Val Loss: 0.0002, LR: 0.0001416543\n",
      "Epoch [382/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001414624\n",
      "Epoch [382/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001412705\n",
      "Epoch [382/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001410788\n",
      "Epoch [382/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001408872\n",
      "Epoch [382/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001406957\n",
      "Epoch [382/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001405044\n",
      "Epoch [382/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001403131\n",
      "Epoch [382/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001401220\n",
      "Epoch [382/500], Batch [110/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0001399310\n",
      "Epoch [383/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001397401\n",
      "Epoch [383/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001395494\n",
      "Epoch [383/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001393587\n",
      "Epoch [383/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001391682\n",
      "Epoch [383/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001389778\n",
      "Epoch [383/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001387875\n",
      "Epoch [383/500], Batch [70/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0001385973\n",
      "Epoch [383/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001384073\n",
      "Epoch [383/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001382173\n",
      "Epoch [383/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001380275\n",
      "Epoch [383/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001378378\n",
      "Epoch [384/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001376483\n",
      "Epoch [384/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001374588\n",
      "Epoch [384/500], Batch [30/110], Train Loss: 0.0072, Val Loss: 0.0002, LR: 0.0001372695\n",
      "Epoch [384/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001370803\n",
      "Epoch [384/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001368912\n",
      "Epoch [384/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001367022\n",
      "Epoch [384/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001365133\n",
      "Epoch [384/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001363246\n",
      "Epoch [384/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0001361360\n",
      "Epoch [384/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001359475\n",
      "Epoch [384/500], Batch [110/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0001357591\n",
      "Epoch [385/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001355709\n",
      "Epoch [385/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001353828\n",
      "Epoch [385/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001351947\n",
      "Epoch [385/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001350069\n",
      "Epoch [385/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0003, LR: 0.0001348191\n",
      "Epoch [385/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001346314\n",
      "Epoch [385/500], Batch [70/110], Train Loss: 0.0029, Val Loss: 0.0002, LR: 0.0001344439\n",
      "Epoch [385/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001342565\n",
      "Epoch [385/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001340692\n",
      "Epoch [385/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001338821\n",
      "Epoch [385/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001336950\n",
      "Epoch [386/500], Batch [10/110], Train Loss: 0.0039, Val Loss: 0.0002, LR: 0.0001335081\n",
      "Epoch [386/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0001333213\n",
      "Epoch [386/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001331346\n",
      "Epoch [386/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001329481\n",
      "Epoch [386/500], Batch [50/110], Train Loss: 0.0025, Val Loss: 0.0003, LR: 0.0001327616\n",
      "Epoch [386/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001325753\n",
      "Epoch [386/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001323891\n",
      "Epoch [386/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001322031\n",
      "Epoch [386/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0003, LR: 0.0001320171\n",
      "Epoch [386/500], Batch [100/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0001318313\n",
      "Epoch [386/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001316456\n",
      "Epoch [387/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001314600\n",
      "Epoch [387/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001312745\n",
      "Epoch [387/500], Batch [30/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0001310892\n",
      "Epoch [387/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001309040\n",
      "Epoch [387/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001307189\n",
      "Epoch [387/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001305339\n",
      "Epoch [387/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001303490\n",
      "Epoch [387/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001301643\n",
      "Epoch [387/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001299797\n",
      "Epoch [387/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001297952\n",
      "Epoch [387/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001296109\n",
      "Epoch [388/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001294266\n",
      "Epoch [388/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0004, LR: 0.0001292425\n",
      "Epoch [388/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001290585\n",
      "Epoch [388/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001288746\n",
      "Epoch [388/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001286909\n",
      "Epoch [388/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001285072\n",
      "Epoch [388/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001283237\n",
      "Epoch [388/500], Batch [80/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0001281404\n",
      "Epoch [388/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001279571\n",
      "Epoch [388/500], Batch [100/110], Train Loss: 0.0019, Val Loss: 0.0002, LR: 0.0001277740\n",
      "Epoch [388/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001275910\n",
      "Epoch [389/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001274081\n",
      "Epoch [389/500], Batch [20/110], Train Loss: 0.0006, Val Loss: 0.0003, LR: 0.0001272253\n",
      "Epoch [389/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001270427\n",
      "Epoch [389/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001268601\n",
      "Epoch [389/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001266777\n",
      "Epoch [389/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001264955\n",
      "Epoch [389/500], Batch [70/110], Train Loss: 0.0023, Val Loss: 0.0002, LR: 0.0001263133\n",
      "Epoch [389/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001261313\n",
      "Epoch [389/500], Batch [90/110], Train Loss: 0.0019, Val Loss: 0.0002, LR: 0.0001259494\n",
      "Epoch [389/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001257676\n",
      "Epoch [389/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001255860\n",
      "Epoch [390/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0001254044\n",
      "Epoch [390/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001252230\n",
      "Epoch [390/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001250417\n",
      "Epoch [390/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001248606\n",
      "Epoch [390/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001246795\n",
      "Epoch [390/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001244986\n",
      "Epoch [390/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001243178\n",
      "Epoch [390/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001241372\n",
      "Epoch [390/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001239566\n",
      "Epoch [390/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001237762\n",
      "Epoch [390/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001235959\n",
      "Epoch [391/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0003, LR: 0.0001234158\n",
      "Epoch [391/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001232357\n",
      "Epoch [391/500], Batch [30/110], Train Loss: 0.0043, Val Loss: 0.0002, LR: 0.0001230558\n",
      "Epoch [391/500], Batch [40/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0001228760\n",
      "Epoch [391/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001226964\n",
      "Epoch [391/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001225168\n",
      "Epoch [391/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001223374\n",
      "Epoch [391/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001221581\n",
      "Epoch [391/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001219789\n",
      "Epoch [391/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001217999\n",
      "Epoch [391/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001216210\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 391: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.031 sec , Memory Usage: 305.17 MB\n",
      "Epoch [392/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001214422\n",
      "Epoch [392/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001212635\n",
      "Epoch [392/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001210850\n",
      "Epoch [392/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001209066\n",
      "Epoch [392/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001207283\n",
      "Epoch [392/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0001205501\n",
      "Epoch [392/500], Batch [70/110], Train Loss: 0.0033, Val Loss: 0.0001, LR: 0.0001203721\n",
      "Epoch [392/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001201942\n",
      "Epoch [392/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001200164\n",
      "Epoch [392/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001198387\n",
      "Epoch [392/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001196612\n",
      "Epoch [393/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001194837\n",
      "Epoch [393/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001193065\n",
      "Epoch [393/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001191293\n",
      "Epoch [393/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001189523\n",
      "Epoch [393/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001187754\n",
      "Epoch [393/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001185986\n",
      "Epoch [393/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001184219\n",
      "Epoch [393/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001182454\n",
      "Epoch [393/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001180690\n",
      "Epoch [393/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001178927\n",
      "Epoch [393/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001177165\n",
      "Epoch [394/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001175405\n",
      "Epoch [394/500], Batch [20/110], Train Loss: 0.0013, Val Loss: 0.0002, LR: 0.0001173646\n",
      "Epoch [394/500], Batch [30/110], Train Loss: 0.0023, Val Loss: 0.0002, LR: 0.0001171889\n",
      "Epoch [394/500], Batch [40/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0001170132\n",
      "Epoch [394/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001168377\n",
      "Epoch [394/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0001166623\n",
      "Epoch [394/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001164870\n",
      "Epoch [394/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001163119\n",
      "Epoch [394/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001161369\n",
      "Epoch [394/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001159620\n",
      "Epoch [394/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001157872\n",
      "Epoch [395/500], Batch [10/110], Train Loss: 0.0025, Val Loss: 0.0002, LR: 0.0001156126\n",
      "Epoch [395/500], Batch [20/110], Train Loss: 0.0023, Val Loss: 0.0002, LR: 0.0001154381\n",
      "Epoch [395/500], Batch [30/110], Train Loss: 0.0018, Val Loss: 0.0002, LR: 0.0001152637\n",
      "Epoch [395/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001150895\n",
      "Epoch [395/500], Batch [50/110], Train Loss: 0.0020, Val Loss: 0.0002, LR: 0.0001149153\n",
      "Epoch [395/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001147413\n",
      "Epoch [395/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0002, LR: 0.0001145675\n",
      "Epoch [395/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001143937\n",
      "Epoch [395/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001142201\n",
      "Epoch [395/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001140466\n",
      "Epoch [395/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001138733\n",
      "Epoch [396/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001137000\n",
      "Epoch [396/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001135269\n",
      "Epoch [396/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0001133540\n",
      "Epoch [396/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001131811\n",
      "Epoch [396/500], Batch [50/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0001130084\n",
      "Epoch [396/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001128358\n",
      "Epoch [396/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001126633\n",
      "Epoch [396/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001124910\n",
      "Epoch [396/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001123188\n",
      "Epoch [396/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001121467\n",
      "Epoch [396/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001119748\n",
      "Epoch [397/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001118029\n",
      "Epoch [397/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001116312\n",
      "Epoch [397/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001114597\n",
      "Epoch [397/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001112882\n",
      "Epoch [397/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0003, LR: 0.0001111169\n",
      "Epoch [397/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001109457\n",
      "Epoch [397/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001107747\n",
      "Epoch [397/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001106038\n",
      "Epoch [397/500], Batch [90/110], Train Loss: 0.0019, Val Loss: 0.0002, LR: 0.0001104330\n",
      "Epoch [397/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001102623\n",
      "Epoch [397/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001100918\n",
      "Epoch [398/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001099213\n",
      "Epoch [398/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001097511\n",
      "Epoch [398/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001095809\n",
      "Epoch [398/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0002, LR: 0.0001094109\n",
      "Epoch [398/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001092410\n",
      "Epoch [398/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001090712\n",
      "Epoch [398/500], Batch [70/110], Train Loss: 0.0021, Val Loss: 0.0002, LR: 0.0001089016\n",
      "Epoch [398/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001087321\n",
      "Epoch [398/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001085627\n",
      "Epoch [398/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001083935\n",
      "Epoch [398/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001082243\n",
      "Epoch [399/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001080554\n",
      "Epoch [399/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0001078865\n",
      "Epoch [399/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001077178\n",
      "Epoch [399/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001075492\n",
      "Epoch [399/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001073807\n",
      "Epoch [399/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001072124\n",
      "Epoch [399/500], Batch [70/110], Train Loss: 0.0027, Val Loss: 0.0002, LR: 0.0001070441\n",
      "Epoch [399/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001068761\n",
      "Epoch [399/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0002, LR: 0.0001067081\n",
      "Epoch [399/500], Batch [100/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0001065403\n",
      "Epoch [399/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001063726\n",
      "Epoch [400/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001062050\n",
      "Epoch [400/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001060376\n",
      "Epoch [400/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001058703\n",
      "Epoch [400/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001057031\n",
      "Epoch [400/500], Batch [50/110], Train Loss: 0.0087, Val Loss: 0.0003, LR: 0.0001055361\n",
      "Epoch [400/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001053692\n",
      "Epoch [400/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001052024\n",
      "Epoch [400/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001050358\n",
      "Epoch [400/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001048692\n",
      "Epoch [400/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001047028\n",
      "Epoch [400/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001045366\n",
      "Epoch [401/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001043705\n",
      "Epoch [401/500], Batch [20/110], Train Loss: 0.0043, Val Loss: 0.0002, LR: 0.0001042045\n",
      "Epoch [401/500], Batch [30/110], Train Loss: 0.0023, Val Loss: 0.0002, LR: 0.0001040386\n",
      "Epoch [401/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001038729\n",
      "Epoch [401/500], Batch [50/110], Train Loss: 0.0016, Val Loss: 0.0002, LR: 0.0001037073\n",
      "Epoch [401/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0001035418\n",
      "Epoch [401/500], Batch [70/110], Train Loss: 0.0020, Val Loss: 0.0002, LR: 0.0001033764\n",
      "Epoch [401/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001032112\n",
      "Epoch [401/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001030462\n",
      "Epoch [401/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001028812\n",
      "Epoch [401/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0002, LR: 0.0001027164\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 401: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.047 sec , Memory Usage: 305.17 MB\n",
      "Epoch [402/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001025517\n",
      "Epoch [402/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001023871\n",
      "Epoch [402/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001022227\n",
      "Epoch [402/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001020584\n",
      "Epoch [402/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001018943\n",
      "Epoch [402/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001017302\n",
      "Epoch [402/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001015663\n",
      "Epoch [402/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001014026\n",
      "Epoch [402/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0003, LR: 0.0001012389\n",
      "Epoch [402/500], Batch [100/110], Train Loss: 0.0030, Val Loss: 0.0002, LR: 0.0001010754\n",
      "Epoch [402/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001009121\n",
      "Epoch [403/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001007488\n",
      "Epoch [403/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001005857\n",
      "Epoch [403/500], Batch [30/110], Train Loss: 0.0009, Val Loss: 0.0002, LR: 0.0001004227\n",
      "Epoch [403/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001002599\n",
      "Epoch [403/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0001000972\n",
      "Epoch [403/500], Batch [60/110], Train Loss: 0.0018, Val Loss: 0.0002, LR: 0.0000999346\n",
      "Epoch [403/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000997722\n",
      "Epoch [403/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000996099\n",
      "Epoch [403/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000994477\n",
      "Epoch [403/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000992856\n",
      "Epoch [403/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000991237\n",
      "Epoch [404/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000989619\n",
      "Epoch [404/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000988003\n",
      "Epoch [404/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000986387\n",
      "Epoch [404/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000984774\n",
      "Epoch [404/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000983161\n",
      "Epoch [404/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000981550\n",
      "Epoch [404/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000979940\n",
      "Epoch [404/500], Batch [80/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0000978331\n",
      "Epoch [404/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000976724\n",
      "Epoch [404/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000975118\n",
      "Epoch [404/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000973514\n",
      "Epoch [405/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000971910\n",
      "Epoch [405/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000970308\n",
      "Epoch [405/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000968708\n",
      "Epoch [405/500], Batch [40/110], Train Loss: 0.0017, Val Loss: 0.0002, LR: 0.0000967109\n",
      "Epoch [405/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000965511\n",
      "Epoch [405/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000963914\n",
      "Epoch [405/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000962319\n",
      "Epoch [405/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000960725\n",
      "Epoch [405/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000959132\n",
      "Epoch [405/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000957541\n",
      "Epoch [405/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000955951\n",
      "Epoch [406/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000954363\n",
      "Epoch [406/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000952775\n",
      "Epoch [406/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000951189\n",
      "Epoch [406/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000949605\n",
      "Epoch [406/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000948022\n",
      "Epoch [406/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000946440\n",
      "Epoch [406/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000944859\n",
      "Epoch [406/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000943280\n",
      "Epoch [406/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000941702\n",
      "Epoch [406/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000940125\n",
      "Epoch [406/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000938550\n",
      "Epoch [407/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000936976\n",
      "Epoch [407/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000935404\n",
      "Epoch [407/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000933833\n",
      "Epoch [407/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000932263\n",
      "Epoch [407/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000930694\n",
      "Epoch [407/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000929127\n",
      "Epoch [407/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000927561\n",
      "Epoch [407/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000925997\n",
      "Epoch [407/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000924434\n",
      "Epoch [407/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000922872\n",
      "Epoch [407/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000921312\n",
      "Epoch [408/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000919753\n",
      "Epoch [408/500], Batch [20/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000918195\n",
      "Epoch [408/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000916639\n",
      "Epoch [408/500], Batch [40/110], Train Loss: 0.0025, Val Loss: 0.0002, LR: 0.0000915084\n",
      "Epoch [408/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000913530\n",
      "Epoch [408/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000911978\n",
      "Epoch [408/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000910427\n",
      "Epoch [408/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000908877\n",
      "Epoch [408/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000907329\n",
      "Epoch [408/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000905782\n",
      "Epoch [408/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000904236\n",
      "Epoch [409/500], Batch [10/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000902692\n",
      "Epoch [409/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000901149\n",
      "Epoch [409/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000899608\n",
      "Epoch [409/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000898067\n",
      "Epoch [409/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000896529\n",
      "Epoch [409/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000894991\n",
      "Epoch [409/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000893455\n",
      "Epoch [409/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000891920\n",
      "Epoch [409/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000890387\n",
      "Epoch [409/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000888855\n",
      "Epoch [409/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000887324\n",
      "Epoch [410/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000885795\n",
      "Epoch [410/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000884267\n",
      "Epoch [410/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000882740\n",
      "Epoch [410/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000881215\n",
      "Epoch [410/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000879691\n",
      "Epoch [410/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000878169\n",
      "Epoch [410/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0000876648\n",
      "Epoch [410/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000875128\n",
      "Epoch [410/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0000873610\n",
      "Epoch [410/500], Batch [100/110], Train Loss: 0.0026, Val Loss: 0.0002, LR: 0.0000872092\n",
      "Epoch [410/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000870577\n",
      "Epoch [411/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000869062\n",
      "Epoch [411/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000867549\n",
      "Epoch [411/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000866038\n",
      "Epoch [411/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000864528\n",
      "Epoch [411/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000863019\n",
      "Epoch [411/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000861511\n",
      "Epoch [411/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000860005\n",
      "Epoch [411/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000858500\n",
      "Epoch [411/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000856997\n",
      "Epoch [411/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000855495\n",
      "Epoch [411/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0000853994\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 411: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.047 sec , Memory Usage: 305.17 MB\n",
      "Epoch [412/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000852495\n",
      "Epoch [412/500], Batch [20/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0000850997\n",
      "Epoch [412/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000849500\n",
      "Epoch [412/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000848005\n",
      "Epoch [412/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000846511\n",
      "Epoch [412/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000845019\n",
      "Epoch [412/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000843528\n",
      "Epoch [412/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000842038\n",
      "Epoch [412/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000840550\n",
      "Epoch [412/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000839063\n",
      "Epoch [412/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000837577\n",
      "Epoch [413/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000836093\n",
      "Epoch [413/500], Batch [20/110], Train Loss: 0.0290, Val Loss: 0.0001, LR: 0.0000834610\n",
      "Epoch [413/500], Batch [30/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0000833129\n",
      "Epoch [413/500], Batch [40/110], Train Loss: 0.0012, Val Loss: 0.0001, LR: 0.0000831649\n",
      "Epoch [413/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000830170\n",
      "Epoch [413/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000828693\n",
      "Epoch [413/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000827217\n",
      "Epoch [413/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000825742\n",
      "Epoch [413/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000824269\n",
      "Epoch [413/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000822797\n",
      "Epoch [413/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000821327\n",
      "Epoch [414/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000819858\n",
      "Epoch [414/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000818390\n",
      "Epoch [414/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000816924\n",
      "Epoch [414/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000815459\n",
      "Epoch [414/500], Batch [50/110], Train Loss: 0.0012, Val Loss: 0.0001, LR: 0.0000813995\n",
      "Epoch [414/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000812533\n",
      "Epoch [414/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000811072\n",
      "Epoch [414/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000809613\n",
      "Epoch [414/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000808155\n",
      "Epoch [414/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000806698\n",
      "Epoch [414/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000805243\n",
      "Epoch [415/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000803789\n",
      "Epoch [415/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000802337\n",
      "Epoch [415/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000800886\n",
      "Epoch [415/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000799436\n",
      "Epoch [415/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000797988\n",
      "Epoch [415/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000796541\n",
      "Epoch [415/500], Batch [70/110], Train Loss: 0.0017, Val Loss: 0.0001, LR: 0.0000795095\n",
      "Epoch [415/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000793651\n",
      "Epoch [415/500], Batch [90/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0000792208\n",
      "Epoch [415/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000790767\n",
      "Epoch [415/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000789327\n",
      "Epoch [416/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000787888\n",
      "Epoch [416/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000786451\n",
      "Epoch [416/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000785015\n",
      "Epoch [416/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000783581\n",
      "Epoch [416/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000782148\n",
      "Epoch [416/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000780716\n",
      "Epoch [416/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000779286\n",
      "Epoch [416/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000777857\n",
      "Epoch [416/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000776430\n",
      "Epoch [416/500], Batch [100/110], Train Loss: 0.0012, Val Loss: 0.0001, LR: 0.0000775004\n",
      "Epoch [416/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000773579\n",
      "Epoch [417/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000772156\n",
      "Epoch [417/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000770734\n",
      "Epoch [417/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000769313\n",
      "Epoch [417/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000767894\n",
      "Epoch [417/500], Batch [50/110], Train Loss: 0.0018, Val Loss: 0.0001, LR: 0.0000766477\n",
      "Epoch [417/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000765060\n",
      "Epoch [417/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000763646\n",
      "Epoch [417/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000762232\n",
      "Epoch [417/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000760820\n",
      "Epoch [417/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000759409\n",
      "Epoch [417/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000758000\n",
      "Epoch [418/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0002, LR: 0.0000756592\n",
      "Epoch [418/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000755186\n",
      "Epoch [418/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000753781\n",
      "Epoch [418/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000752377\n",
      "Epoch [418/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000750975\n",
      "Epoch [418/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000749574\n",
      "Epoch [418/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000748174\n",
      "Epoch [418/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000746776\n",
      "Epoch [418/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000745380\n",
      "Epoch [418/500], Batch [100/110], Train Loss: 0.0018, Val Loss: 0.0001, LR: 0.0000743984\n",
      "Epoch [418/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000742590\n",
      "Epoch [419/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000741198\n",
      "Epoch [419/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000739807\n",
      "Epoch [419/500], Batch [30/110], Train Loss: 0.0031, Val Loss: 0.0002, LR: 0.0000738417\n",
      "Epoch [419/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000737029\n",
      "Epoch [419/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000735642\n",
      "Epoch [419/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000734257\n",
      "Epoch [419/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000732873\n",
      "Epoch [419/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000731490\n",
      "Epoch [419/500], Batch [90/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000730109\n",
      "Epoch [419/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000728729\n",
      "Epoch [419/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0001, LR: 0.0000727351\n",
      "Epoch [420/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000725974\n",
      "Epoch [420/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000724598\n",
      "Epoch [420/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000723224\n",
      "Epoch [420/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000721851\n",
      "Epoch [420/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000720480\n",
      "Epoch [420/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000719110\n",
      "Epoch [420/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000717742\n",
      "Epoch [420/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0001, LR: 0.0000716375\n",
      "Epoch [420/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000715009\n",
      "Epoch [420/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000713645\n",
      "Epoch [420/500], Batch [110/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000712282\n",
      "Epoch [421/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000710921\n",
      "Epoch [421/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000709561\n",
      "Epoch [421/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000708202\n",
      "Epoch [421/500], Batch [40/110], Train Loss: 0.0044, Val Loss: 0.0002, LR: 0.0000706845\n",
      "Epoch [421/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000705489\n",
      "Epoch [421/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000704135\n",
      "Epoch [421/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0002, LR: 0.0000702782\n",
      "Epoch [421/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000701430\n",
      "Epoch [421/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000700080\n",
      "Epoch [421/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000698732\n",
      "Epoch [421/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000697384\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 421: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.039 sec , Memory Usage: 305.20 MB\n",
      "Epoch [422/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000696038\n",
      "Epoch [422/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000694694\n",
      "Epoch [422/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000693351\n",
      "Epoch [422/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000692010\n",
      "Epoch [422/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000690669\n",
      "Epoch [422/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000689331\n",
      "Epoch [422/500], Batch [70/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0000687993\n",
      "Epoch [422/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000686657\n",
      "Epoch [422/500], Batch [90/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000685323\n",
      "Epoch [422/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000683990\n",
      "Epoch [422/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000682658\n",
      "Epoch [423/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000681328\n",
      "Epoch [423/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000680000\n",
      "Epoch [423/500], Batch [30/110], Train Loss: 0.0013, Val Loss: 0.0002, LR: 0.0000678672\n",
      "Epoch [423/500], Batch [40/110], Train Loss: 0.0063, Val Loss: 0.0002, LR: 0.0000677346\n",
      "Epoch [423/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000676022\n",
      "Epoch [423/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000674699\n",
      "Epoch [423/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000673377\n",
      "Epoch [423/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000672057\n",
      "Epoch [423/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000670738\n",
      "Epoch [423/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000669421\n",
      "Epoch [423/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000668105\n",
      "Epoch [424/500], Batch [10/110], Train Loss: 0.0017, Val Loss: 0.0002, LR: 0.0000666791\n",
      "Epoch [424/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000665478\n",
      "Epoch [424/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000664166\n",
      "Epoch [424/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000662856\n",
      "Epoch [424/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000661547\n",
      "Epoch [424/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000660240\n",
      "Epoch [424/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000658934\n",
      "Epoch [424/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000657629\n",
      "Epoch [424/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000656326\n",
      "Epoch [424/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000655025\n",
      "Epoch [424/500], Batch [110/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000653725\n",
      "Epoch [425/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000652426\n",
      "Epoch [425/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000651129\n",
      "Epoch [425/500], Batch [30/110], Train Loss: 0.0003, Val Loss: 0.0002, LR: 0.0000649833\n",
      "Epoch [425/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000648538\n",
      "Epoch [425/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000647245\n",
      "Epoch [425/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000645954\n",
      "Epoch [425/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000644664\n",
      "Epoch [425/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000643375\n",
      "Epoch [425/500], Batch [90/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0000642088\n",
      "Epoch [425/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0000640802\n",
      "Epoch [425/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000639518\n",
      "Epoch [426/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000638235\n",
      "Epoch [426/500], Batch [20/110], Train Loss: 0.0036, Val Loss: 0.0002, LR: 0.0000636953\n",
      "Epoch [426/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000635673\n",
      "Epoch [426/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000634395\n",
      "Epoch [426/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000633118\n",
      "Epoch [426/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000631842\n",
      "Epoch [426/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000630568\n",
      "Epoch [426/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000629295\n",
      "Epoch [426/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000628023\n",
      "Epoch [426/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000626753\n",
      "Epoch [426/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000625485\n",
      "Epoch [427/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000624218\n",
      "Epoch [427/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000622952\n",
      "Epoch [427/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000621688\n",
      "Epoch [427/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000620425\n",
      "Epoch [427/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000619164\n",
      "Epoch [427/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000617904\n",
      "Epoch [427/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000616646\n",
      "Epoch [427/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000615389\n",
      "Epoch [427/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000614134\n",
      "Epoch [427/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000612879\n",
      "Epoch [427/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000611627\n",
      "Epoch [428/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000610376\n",
      "Epoch [428/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000609126\n",
      "Epoch [428/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000607878\n",
      "Epoch [428/500], Batch [40/110], Train Loss: 0.0025, Val Loss: 0.0002, LR: 0.0000606631\n",
      "Epoch [428/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000605386\n",
      "Epoch [428/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000604142\n",
      "Epoch [428/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000602899\n",
      "Epoch [428/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000601658\n",
      "Epoch [428/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000600419\n",
      "Epoch [428/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000599181\n",
      "Epoch [428/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000597944\n",
      "Epoch [429/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000596709\n",
      "Epoch [429/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000595475\n",
      "Epoch [429/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000594243\n",
      "Epoch [429/500], Batch [40/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0000593012\n",
      "Epoch [429/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000591783\n",
      "Epoch [429/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000590555\n",
      "Epoch [429/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000589328\n",
      "Epoch [429/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000588103\n",
      "Epoch [429/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000586880\n",
      "Epoch [429/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000585658\n",
      "Epoch [429/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000584437\n",
      "Epoch [430/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000583218\n",
      "Epoch [430/500], Batch [20/110], Train Loss: 0.0017, Val Loss: 0.0001, LR: 0.0000582000\n",
      "Epoch [430/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000580784\n",
      "Epoch [430/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000579569\n",
      "Epoch [430/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000578356\n",
      "Epoch [430/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000577144\n",
      "Epoch [430/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000575933\n",
      "Epoch [430/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000574724\n",
      "Epoch [430/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000573517\n",
      "Epoch [430/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000572311\n",
      "Epoch [430/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000571106\n",
      "Epoch [431/500], Batch [10/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0000569903\n",
      "Epoch [431/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000568701\n",
      "Epoch [431/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000567501\n",
      "Epoch [431/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000566302\n",
      "Epoch [431/500], Batch [50/110], Train Loss: 0.0015, Val Loss: 0.0001, LR: 0.0000565105\n",
      "Epoch [431/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000563909\n",
      "Epoch [431/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000562715\n",
      "Epoch [431/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000561522\n",
      "Epoch [431/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000560331\n",
      "Epoch [431/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000559141\n",
      "Epoch [431/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000557952\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 431: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.037 sec , Memory Usage: 305.20 MB\n",
      "Epoch [432/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000556765\n",
      "Epoch [432/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000555580\n",
      "Epoch [432/500], Batch [30/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000554395\n",
      "Epoch [432/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000553213\n",
      "Epoch [432/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000552032\n",
      "Epoch [432/500], Batch [60/110], Train Loss: 0.0264, Val Loss: 0.0001, LR: 0.0000550852\n",
      "Epoch [432/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000549674\n",
      "Epoch [432/500], Batch [80/110], Train Loss: 0.0027, Val Loss: 0.0001, LR: 0.0000548497\n",
      "Epoch [432/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000547322\n",
      "Epoch [432/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000546148\n",
      "Epoch [432/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000544975\n",
      "Epoch [433/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000543805\n",
      "Epoch [433/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000542635\n",
      "Epoch [433/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000541467\n",
      "Epoch [433/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000540301\n",
      "Epoch [433/500], Batch [50/110], Train Loss: 0.0019, Val Loss: 0.0001, LR: 0.0000539136\n",
      "Epoch [433/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0001, LR: 0.0000537972\n",
      "Epoch [433/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000536810\n",
      "Epoch [433/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000535650\n",
      "Epoch [433/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000534490\n",
      "Epoch [433/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000533333\n",
      "Epoch [433/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000532177\n",
      "Epoch [434/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0000531022\n",
      "Epoch [434/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000529869\n",
      "Epoch [434/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000528717\n",
      "Epoch [434/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000527567\n",
      "Epoch [434/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000526418\n",
      "Epoch [434/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000525271\n",
      "Epoch [434/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000524125\n",
      "Epoch [434/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000522980\n",
      "Epoch [434/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000521838\n",
      "Epoch [434/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000520696\n",
      "Epoch [434/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000519556\n",
      "Epoch [435/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000518418\n",
      "Epoch [435/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000517281\n",
      "Epoch [435/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000516145\n",
      "Epoch [435/500], Batch [40/110], Train Loss: 0.0024, Val Loss: 0.0001, LR: 0.0000515011\n",
      "Epoch [435/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000513879\n",
      "Epoch [435/500], Batch [60/110], Train Loss: 0.0007, Val Loss: 0.0002, LR: 0.0000512748\n",
      "Epoch [435/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0002, LR: 0.0000511618\n",
      "Epoch [435/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000510490\n",
      "Epoch [435/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000509363\n",
      "Epoch [435/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000508238\n",
      "Epoch [435/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000507115\n",
      "Epoch [436/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000505992\n",
      "Epoch [436/500], Batch [20/110], Train Loss: 0.0027, Val Loss: 0.0001, LR: 0.0000504872\n",
      "Epoch [436/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000503753\n",
      "Epoch [436/500], Batch [40/110], Train Loss: 0.0031, Val Loss: 0.0001, LR: 0.0000502635\n",
      "Epoch [436/500], Batch [50/110], Train Loss: 0.0040, Val Loss: 0.0001, LR: 0.0000501519\n",
      "Epoch [436/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000500404\n",
      "Epoch [436/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000499291\n",
      "Epoch [436/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000498179\n",
      "Epoch [436/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000497068\n",
      "Epoch [436/500], Batch [100/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0000495960\n",
      "Epoch [436/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000494852\n",
      "Epoch [437/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000493747\n",
      "Epoch [437/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000492642\n",
      "Epoch [437/500], Batch [30/110], Train Loss: 0.0264, Val Loss: 0.0001, LR: 0.0000491539\n",
      "Epoch [437/500], Batch [40/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000490438\n",
      "Epoch [437/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000489338\n",
      "Epoch [437/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000488240\n",
      "Epoch [437/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000487143\n",
      "Epoch [437/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000486047\n",
      "Epoch [437/500], Batch [90/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000484953\n",
      "Epoch [437/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000483861\n",
      "Epoch [437/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000482770\n",
      "Epoch [438/500], Batch [10/110], Train Loss: 0.0323, Val Loss: 0.0001, LR: 0.0000481680\n",
      "Epoch [438/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000480592\n",
      "Epoch [438/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000479506\n",
      "Epoch [438/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000478421\n",
      "Epoch [438/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000477338\n",
      "Epoch [438/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000476255\n",
      "Epoch [438/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000475175\n",
      "Epoch [438/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000474096\n",
      "Epoch [438/500], Batch [90/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000473018\n",
      "Epoch [438/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000471942\n",
      "Epoch [438/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000470868\n",
      "Epoch [439/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000469795\n",
      "Epoch [439/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000468723\n",
      "Epoch [439/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000467653\n",
      "Epoch [439/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000466585\n",
      "Epoch [439/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000465518\n",
      "Epoch [439/500], Batch [60/110], Train Loss: 0.0019, Val Loss: 0.0001, LR: 0.0000464452\n",
      "Epoch [439/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000463388\n",
      "Epoch [439/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000462325\n",
      "Epoch [439/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000461264\n",
      "Epoch [439/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000460205\n",
      "Epoch [439/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000459146\n",
      "Epoch [440/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000458090\n",
      "Epoch [440/500], Batch [20/110], Train Loss: 0.0021, Val Loss: 0.0001, LR: 0.0000457035\n",
      "Epoch [440/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000455981\n",
      "Epoch [440/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000454929\n",
      "Epoch [440/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000453878\n",
      "Epoch [440/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000452829\n",
      "Epoch [440/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0001, LR: 0.0000451782\n",
      "Epoch [440/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000450736\n",
      "Epoch [440/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000449691\n",
      "Epoch [440/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000448648\n",
      "Epoch [440/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0000447606\n",
      "Epoch [441/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000446566\n",
      "Epoch [441/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000445528\n",
      "Epoch [441/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000444491\n",
      "Epoch [441/500], Batch [40/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000443455\n",
      "Epoch [441/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000442421\n",
      "Epoch [441/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000441388\n",
      "Epoch [441/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000440357\n",
      "Epoch [441/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000439328\n",
      "Epoch [441/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000438300\n",
      "Epoch [441/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000437273\n",
      "Epoch [441/500], Batch [110/110], Train Loss: 0.0009, Val Loss: 0.0001, LR: 0.0000436248\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 441: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.028 sec , Memory Usage: 305.20 MB\n",
      "Epoch [442/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0000435224\n",
      "Epoch [442/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000434202\n",
      "Epoch [442/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000433182\n",
      "Epoch [442/500], Batch [40/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0000432163\n",
      "Epoch [442/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000431145\n",
      "Epoch [442/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000430129\n",
      "Epoch [442/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000429115\n",
      "Epoch [442/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000428102\n",
      "Epoch [442/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000427090\n",
      "Epoch [442/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000426080\n",
      "Epoch [442/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000425072\n",
      "Epoch [443/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000424065\n",
      "Epoch [443/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000423059\n",
      "Epoch [443/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000422055\n",
      "Epoch [443/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000421053\n",
      "Epoch [443/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000420052\n",
      "Epoch [443/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000419053\n",
      "Epoch [443/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000418055\n",
      "Epoch [443/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000417058\n",
      "Epoch [443/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000416063\n",
      "Epoch [443/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000415070\n",
      "Epoch [443/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000414078\n",
      "Epoch [444/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000413088\n",
      "Epoch [444/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000412099\n",
      "Epoch [444/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000411112\n",
      "Epoch [444/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000410126\n",
      "Epoch [444/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000409141\n",
      "Epoch [444/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000408159\n",
      "Epoch [444/500], Batch [70/110], Train Loss: 0.0009, Val Loss: 0.0001, LR: 0.0000407177\n",
      "Epoch [444/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000406198\n",
      "Epoch [444/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000405219\n",
      "Epoch [444/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000404243\n",
      "Epoch [444/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000403267\n",
      "Epoch [445/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000402294\n",
      "Epoch [445/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000401322\n",
      "Epoch [445/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000400351\n",
      "Epoch [445/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000399382\n",
      "Epoch [445/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000398414\n",
      "Epoch [445/500], Batch [60/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000397448\n",
      "Epoch [445/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000396483\n",
      "Epoch [445/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000395520\n",
      "Epoch [445/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000394559\n",
      "Epoch [445/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000393599\n",
      "Epoch [445/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000392640\n",
      "Epoch [446/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000391683\n",
      "Epoch [446/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000390728\n",
      "Epoch [446/500], Batch [30/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000389774\n",
      "Epoch [446/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000388821\n",
      "Epoch [446/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000387870\n",
      "Epoch [446/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000386921\n",
      "Epoch [446/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000385973\n",
      "Epoch [446/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000385027\n",
      "Epoch [446/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000384082\n",
      "Epoch [446/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000383139\n",
      "Epoch [446/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000382197\n",
      "Epoch [447/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000381257\n",
      "Epoch [447/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000380318\n",
      "Epoch [447/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000379381\n",
      "Epoch [447/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000378445\n",
      "Epoch [447/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000377511\n",
      "Epoch [447/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000376578\n",
      "Epoch [447/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000375647\n",
      "Epoch [447/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000374717\n",
      "Epoch [447/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000373789\n",
      "Epoch [447/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000372863\n",
      "Epoch [447/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000371938\n",
      "Epoch [448/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000371014\n",
      "Epoch [448/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000370092\n",
      "Epoch [448/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000369172\n",
      "Epoch [448/500], Batch [40/110], Train Loss: 0.0024, Val Loss: 0.0001, LR: 0.0000368253\n",
      "Epoch [448/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000367336\n",
      "Epoch [448/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000366420\n",
      "Epoch [448/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000365505\n",
      "Epoch [448/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000364593\n",
      "Epoch [448/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000363681\n",
      "Epoch [448/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000362772\n",
      "Epoch [448/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0000361863\n",
      "Epoch [449/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000360957\n",
      "Epoch [449/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000360052\n",
      "Epoch [449/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000359148\n",
      "Epoch [449/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000358246\n",
      "Epoch [449/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000357345\n",
      "Epoch [449/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000356446\n",
      "Epoch [449/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000355549\n",
      "Epoch [449/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000354653\n",
      "Epoch [449/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000353758\n",
      "Epoch [449/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000352865\n",
      "Epoch [449/500], Batch [110/110], Train Loss: 0.0022, Val Loss: 0.0001, LR: 0.0000351974\n",
      "Epoch [450/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000351084\n",
      "Epoch [450/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000350196\n",
      "Epoch [450/500], Batch [30/110], Train Loss: 0.0022, Val Loss: 0.0001, LR: 0.0000349309\n",
      "Epoch [450/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000348424\n",
      "Epoch [450/500], Batch [50/110], Train Loss: 0.0009, Val Loss: 0.0001, LR: 0.0000347540\n",
      "Epoch [450/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000346658\n",
      "Epoch [450/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000345777\n",
      "Epoch [450/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000344898\n",
      "Epoch [450/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000344021\n",
      "Epoch [450/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000343145\n",
      "Epoch [450/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000342270\n",
      "Epoch [451/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000341397\n",
      "Epoch [451/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000340526\n",
      "Epoch [451/500], Batch [30/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000339656\n",
      "Epoch [451/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000338788\n",
      "Epoch [451/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000337921\n",
      "Epoch [451/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000337056\n",
      "Epoch [451/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000336192\n",
      "Epoch [451/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000335330\n",
      "Epoch [451/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000334469\n",
      "Epoch [451/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000333610\n",
      "Epoch [451/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000332752\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 451: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.034 sec , Memory Usage: 305.21 MB\n",
      "Epoch [452/500], Batch [10/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000331896\n",
      "Epoch [452/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000331042\n",
      "Epoch [452/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000330189\n",
      "Epoch [452/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000329337\n",
      "Epoch [452/500], Batch [50/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0000328487\n",
      "Epoch [452/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000327639\n",
      "Epoch [452/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000326792\n",
      "Epoch [452/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000325947\n",
      "Epoch [452/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000325103\n",
      "Epoch [452/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000324261\n",
      "Epoch [452/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000323421\n",
      "Epoch [453/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000322581\n",
      "Epoch [453/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000321744\n",
      "Epoch [453/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000320908\n",
      "Epoch [453/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000320073\n",
      "Epoch [453/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000319240\n",
      "Epoch [453/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000318409\n",
      "Epoch [453/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000317579\n",
      "Epoch [453/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000316751\n",
      "Epoch [453/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000315924\n",
      "Epoch [453/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000315099\n",
      "Epoch [453/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000314275\n",
      "Epoch [454/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000313453\n",
      "Epoch [454/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000312633\n",
      "Epoch [454/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000311814\n",
      "Epoch [454/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000310996\n",
      "Epoch [454/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000310180\n",
      "Epoch [454/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000309366\n",
      "Epoch [454/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000308553\n",
      "Epoch [454/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000307742\n",
      "Epoch [454/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000306932\n",
      "Epoch [454/500], Batch [100/110], Train Loss: 0.0009, Val Loss: 0.0001, LR: 0.0000306124\n",
      "Epoch [454/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000305317\n",
      "Epoch [455/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000304512\n",
      "Epoch [455/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000303708\n",
      "Epoch [455/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000302906\n",
      "Epoch [455/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000302106\n",
      "Epoch [455/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000301307\n",
      "Epoch [455/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000300510\n",
      "Epoch [455/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000299714\n",
      "Epoch [455/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000298920\n",
      "Epoch [455/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000298127\n",
      "Epoch [455/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000297336\n",
      "Epoch [455/500], Batch [110/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0000296546\n",
      "Epoch [456/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000295758\n",
      "Epoch [456/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000294972\n",
      "Epoch [456/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000294187\n",
      "Epoch [456/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000293403\n",
      "Epoch [456/500], Batch [50/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0000292622\n",
      "Epoch [456/500], Batch [60/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0000291841\n",
      "Epoch [456/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000291062\n",
      "Epoch [456/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000290285\n",
      "Epoch [456/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000289510\n",
      "Epoch [456/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000288736\n",
      "Epoch [456/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000287963\n",
      "Epoch [457/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000287192\n",
      "Epoch [457/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000286423\n",
      "Epoch [457/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000285655\n",
      "Epoch [457/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000284888\n",
      "Epoch [457/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000284124\n",
      "Epoch [457/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000283360\n",
      "Epoch [457/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000282599\n",
      "Epoch [457/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000281839\n",
      "Epoch [457/500], Batch [90/110], Train Loss: 0.0021, Val Loss: 0.0001, LR: 0.0000281080\n",
      "Epoch [457/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000280323\n",
      "Epoch [457/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000279568\n",
      "Epoch [458/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0000278814\n",
      "Epoch [458/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000278062\n",
      "Epoch [458/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000277311\n",
      "Epoch [458/500], Batch [40/110], Train Loss: 0.0021, Val Loss: 0.0002, LR: 0.0000276562\n",
      "Epoch [458/500], Batch [50/110], Train Loss: 0.0039, Val Loss: 0.0002, LR: 0.0000275814\n",
      "Epoch [458/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000275068\n",
      "Epoch [458/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000274323\n",
      "Epoch [458/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000273580\n",
      "Epoch [458/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000272839\n",
      "Epoch [458/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000272099\n",
      "Epoch [458/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000271361\n",
      "Epoch [459/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000270624\n",
      "Epoch [459/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000269889\n",
      "Epoch [459/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000269155\n",
      "Epoch [459/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000268423\n",
      "Epoch [459/500], Batch [50/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0000267693\n",
      "Epoch [459/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000266964\n",
      "Epoch [459/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000266237\n",
      "Epoch [459/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000265511\n",
      "Epoch [459/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000264786\n",
      "Epoch [459/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000264064\n",
      "Epoch [459/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000263343\n",
      "Epoch [460/500], Batch [10/110], Train Loss: 0.0012, Val Loss: 0.0002, LR: 0.0000262623\n",
      "Epoch [460/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000261905\n",
      "Epoch [460/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000261189\n",
      "Epoch [460/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000260474\n",
      "Epoch [460/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000259760\n",
      "Epoch [460/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000259049\n",
      "Epoch [460/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000258338\n",
      "Epoch [460/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000257630\n",
      "Epoch [460/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000256923\n",
      "Epoch [460/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000256217\n",
      "Epoch [460/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000255513\n",
      "Epoch [461/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000254811\n",
      "Epoch [461/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000254110\n",
      "Epoch [461/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000253411\n",
      "Epoch [461/500], Batch [40/110], Train Loss: 0.0019, Val Loss: 0.0002, LR: 0.0000252713\n",
      "Epoch [461/500], Batch [50/110], Train Loss: 0.0016, Val Loss: 0.0002, LR: 0.0000252017\n",
      "Epoch [461/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000251323\n",
      "Epoch [461/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000250630\n",
      "Epoch [461/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000249938\n",
      "Epoch [461/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000249248\n",
      "Epoch [461/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000248560\n",
      "Epoch [461/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000247873\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 461: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.032 sec , Memory Usage: 305.22 MB\n",
      "Epoch [462/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000247188\n",
      "Epoch [462/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000246505\n",
      "Epoch [462/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000245823\n",
      "Epoch [462/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000245142\n",
      "Epoch [462/500], Batch [50/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000244463\n",
      "Epoch [462/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000243786\n",
      "Epoch [462/500], Batch [70/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0000243110\n",
      "Epoch [462/500], Batch [80/110], Train Loss: 0.0002, Val Loss: 0.0002, LR: 0.0000242436\n",
      "Epoch [462/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000241763\n",
      "Epoch [462/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000241092\n",
      "Epoch [462/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000240423\n",
      "Epoch [463/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000239755\n",
      "Epoch [463/500], Batch [20/110], Train Loss: 0.0005, Val Loss: 0.0002, LR: 0.0000239089\n",
      "Epoch [463/500], Batch [30/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0000238424\n",
      "Epoch [463/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000237761\n",
      "Epoch [463/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000237099\n",
      "Epoch [463/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000236439\n",
      "Epoch [463/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000235781\n",
      "Epoch [463/500], Batch [80/110], Train Loss: 0.0014, Val Loss: 0.0002, LR: 0.0000235124\n",
      "Epoch [463/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000234468\n",
      "Epoch [463/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000233815\n",
      "Epoch [463/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000233162\n",
      "Epoch [464/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000232512\n",
      "Epoch [464/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000231863\n",
      "Epoch [464/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000231215\n",
      "Epoch [464/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000230569\n",
      "Epoch [464/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000229925\n",
      "Epoch [464/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000229282\n",
      "Epoch [464/500], Batch [70/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0000228641\n",
      "Epoch [464/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000228001\n",
      "Epoch [464/500], Batch [90/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0000227363\n",
      "Epoch [464/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000226727\n",
      "Epoch [464/500], Batch [110/110], Train Loss: 0.0019, Val Loss: 0.0002, LR: 0.0000226092\n",
      "Epoch [465/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000225459\n",
      "Epoch [465/500], Batch [20/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000224827\n",
      "Epoch [465/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000224197\n",
      "Epoch [465/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000223568\n",
      "Epoch [465/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000222941\n",
      "Epoch [465/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000222316\n",
      "Epoch [465/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000221692\n",
      "Epoch [465/500], Batch [80/110], Train Loss: 0.0016, Val Loss: 0.0001, LR: 0.0000221069\n",
      "Epoch [465/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000220449\n",
      "Epoch [465/500], Batch [100/110], Train Loss: 0.0011, Val Loss: 0.0002, LR: 0.0000219830\n",
      "Epoch [465/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0000219212\n",
      "Epoch [466/500], Batch [10/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000218596\n",
      "Epoch [466/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000217982\n",
      "Epoch [466/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000217369\n",
      "Epoch [466/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000216757\n",
      "Epoch [466/500], Batch [50/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000216148\n",
      "Epoch [466/500], Batch [60/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0000215540\n",
      "Epoch [466/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000214933\n",
      "Epoch [466/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000214328\n",
      "Epoch [466/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000213725\n",
      "Epoch [466/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000213123\n",
      "Epoch [466/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000212523\n",
      "Epoch [467/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000211924\n",
      "Epoch [467/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000211327\n",
      "Epoch [467/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000210732\n",
      "Epoch [467/500], Batch [40/110], Train Loss: 0.0019, Val Loss: 0.0001, LR: 0.0000210138\n",
      "Epoch [467/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000209545\n",
      "Epoch [467/500], Batch [60/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000208955\n",
      "Epoch [467/500], Batch [70/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000208365\n",
      "Epoch [467/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000207778\n",
      "Epoch [467/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000207192\n",
      "Epoch [467/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0002, LR: 0.0000206607\n",
      "Epoch [467/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000206025\n",
      "Epoch [468/500], Batch [10/110], Train Loss: 0.0040, Val Loss: 0.0002, LR: 0.0000205443\n",
      "Epoch [468/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000204864\n",
      "Epoch [468/500], Batch [30/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000204285\n",
      "Epoch [468/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000203709\n",
      "Epoch [468/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000203134\n",
      "Epoch [468/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000202561\n",
      "Epoch [468/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000201989\n",
      "Epoch [468/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000201419\n",
      "Epoch [468/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000200850\n",
      "Epoch [468/500], Batch [100/110], Train Loss: 0.0015, Val Loss: 0.0001, LR: 0.0000200283\n",
      "Epoch [468/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000199717\n",
      "Epoch [469/500], Batch [10/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0000199154\n",
      "Epoch [469/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000198591\n",
      "Epoch [469/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000198031\n",
      "Epoch [469/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000197471\n",
      "Epoch [469/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000196914\n",
      "Epoch [469/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000196358\n",
      "Epoch [469/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000195804\n",
      "Epoch [469/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000195251\n",
      "Epoch [469/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000194700\n",
      "Epoch [469/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000194150\n",
      "Epoch [469/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000193602\n",
      "Epoch [470/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000193055\n",
      "Epoch [470/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000192511\n",
      "Epoch [470/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000191967\n",
      "Epoch [470/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000191426\n",
      "Epoch [470/500], Batch [50/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000190886\n",
      "Epoch [470/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000190347\n",
      "Epoch [470/500], Batch [70/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000189810\n",
      "Epoch [470/500], Batch [80/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000189275\n",
      "Epoch [470/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000188741\n",
      "Epoch [470/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000188209\n",
      "Epoch [470/500], Batch [110/110], Train Loss: 0.0010, Val Loss: 0.0002, LR: 0.0000187678\n",
      "Epoch [471/500], Batch [10/110], Train Loss: 0.0010, Val Loss: 0.0001, LR: 0.0000187149\n",
      "Epoch [471/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000186622\n",
      "Epoch [471/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000186096\n",
      "Epoch [471/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000185572\n",
      "Epoch [471/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000185049\n",
      "Epoch [471/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000184528\n",
      "Epoch [471/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000184008\n",
      "Epoch [471/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000183490\n",
      "Epoch [471/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000182974\n",
      "Epoch [471/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000182459\n",
      "Epoch [471/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000181946\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 471: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.035 sec , Memory Usage: 305.23 MB\n",
      "Epoch [472/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000181435\n",
      "Epoch [472/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000180925\n",
      "Epoch [472/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000180416\n",
      "Epoch [472/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000179910\n",
      "Epoch [472/500], Batch [50/110], Train Loss: 0.0026, Val Loss: 0.0001, LR: 0.0000179404\n",
      "Epoch [472/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000178901\n",
      "Epoch [472/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000178399\n",
      "Epoch [472/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000177898\n",
      "Epoch [472/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000177399\n",
      "Epoch [472/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000176902\n",
      "Epoch [472/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000176407\n",
      "Epoch [473/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000175912\n",
      "Epoch [473/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000175420\n",
      "Epoch [473/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000174929\n",
      "Epoch [473/500], Batch [40/110], Train Loss: 0.0013, Val Loss: 0.0002, LR: 0.0000174440\n",
      "Epoch [473/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000173952\n",
      "Epoch [473/500], Batch [60/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000173466\n",
      "Epoch [473/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000172981\n",
      "Epoch [473/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000172499\n",
      "Epoch [473/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000172017\n",
      "Epoch [473/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000171537\n",
      "Epoch [473/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000171059\n",
      "Epoch [474/500], Batch [10/110], Train Loss: 0.0024, Val Loss: 0.0001, LR: 0.0000170583\n",
      "Epoch [474/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000170108\n",
      "Epoch [474/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000169634\n",
      "Epoch [474/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000169163\n",
      "Epoch [474/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000168692\n",
      "Epoch [474/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000168224\n",
      "Epoch [474/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000167757\n",
      "Epoch [474/500], Batch [80/110], Train Loss: 0.0046, Val Loss: 0.0001, LR: 0.0000167291\n",
      "Epoch [474/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000166827\n",
      "Epoch [474/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000166365\n",
      "Epoch [474/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000165905\n",
      "Epoch [475/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000165446\n",
      "Epoch [475/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000164988\n",
      "Epoch [475/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000164532\n",
      "Epoch [475/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000164078\n",
      "Epoch [475/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000163625\n",
      "Epoch [475/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000163174\n",
      "Epoch [475/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000162725\n",
      "Epoch [475/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000162277\n",
      "Epoch [475/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000161831\n",
      "Epoch [475/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000161386\n",
      "Epoch [475/500], Batch [110/110], Train Loss: 0.0016, Val Loss: 0.0001, LR: 0.0000160943\n",
      "Epoch [476/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000160501\n",
      "Epoch [476/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000160061\n",
      "Epoch [476/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000159623\n",
      "Epoch [476/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000159186\n",
      "Epoch [476/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000158751\n",
      "Epoch [476/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000158318\n",
      "Epoch [476/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000157886\n",
      "Epoch [476/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000157455\n",
      "Epoch [476/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000157027\n",
      "Epoch [476/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000156599\n",
      "Epoch [476/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000156174\n",
      "Epoch [477/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000155750\n",
      "Epoch [477/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000155328\n",
      "Epoch [477/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000154907\n",
      "Epoch [477/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000154488\n",
      "Epoch [477/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000154070\n",
      "Epoch [477/500], Batch [60/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000153654\n",
      "Epoch [477/500], Batch [70/110], Train Loss: 0.0008, Val Loss: 0.0001, LR: 0.0000153240\n",
      "Epoch [477/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000152827\n",
      "Epoch [477/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000152416\n",
      "Epoch [477/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000152006\n",
      "Epoch [477/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000151598\n",
      "Epoch [478/500], Batch [10/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0000151192\n",
      "Epoch [478/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000150787\n",
      "Epoch [478/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000150384\n",
      "Epoch [478/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000149982\n",
      "Epoch [478/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000149582\n",
      "Epoch [478/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000149184\n",
      "Epoch [478/500], Batch [70/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0000148787\n",
      "Epoch [478/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000148392\n",
      "Epoch [478/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000147998\n",
      "Epoch [478/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000147606\n",
      "Epoch [478/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000147216\n",
      "Epoch [479/500], Batch [10/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000146827\n",
      "Epoch [479/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000146440\n",
      "Epoch [479/500], Batch [30/110], Train Loss: 0.0015, Val Loss: 0.0001, LR: 0.0000146054\n",
      "Epoch [479/500], Batch [40/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000145670\n",
      "Epoch [479/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000145288\n",
      "Epoch [479/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000144907\n",
      "Epoch [479/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000144528\n",
      "Epoch [479/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000144150\n",
      "Epoch [479/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000143774\n",
      "Epoch [479/500], Batch [100/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0000143400\n",
      "Epoch [479/500], Batch [110/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0000143027\n",
      "Epoch [480/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000142656\n",
      "Epoch [480/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000142286\n",
      "Epoch [480/500], Batch [30/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0000141918\n",
      "Epoch [480/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000141552\n",
      "Epoch [480/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000141187\n",
      "Epoch [480/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000140824\n",
      "Epoch [480/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000140463\n",
      "Epoch [480/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000140103\n",
      "Epoch [480/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000139744\n",
      "Epoch [480/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000139387\n",
      "Epoch [480/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000139032\n",
      "Epoch [481/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000138679\n",
      "Epoch [481/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000138327\n",
      "Epoch [481/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000137976\n",
      "Epoch [481/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000137628\n",
      "Epoch [481/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000137280\n",
      "Epoch [481/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000136935\n",
      "Epoch [481/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000136591\n",
      "Epoch [481/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000136249\n",
      "Epoch [481/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000135908\n",
      "Epoch [481/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000135569\n",
      "Epoch [481/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000135231\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 481: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.037 sec , Memory Usage: 305.23 MB\n",
      "Epoch [482/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000134895\n",
      "Epoch [482/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000134561\n",
      "Epoch [482/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000134228\n",
      "Epoch [482/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000133897\n",
      "Epoch [482/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000133567\n",
      "Epoch [482/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000133240\n",
      "Epoch [482/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000132913\n",
      "Epoch [482/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000132589\n",
      "Epoch [482/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000132265\n",
      "Epoch [482/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000131944\n",
      "Epoch [482/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000131624\n",
      "Epoch [483/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000131306\n",
      "Epoch [483/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000130989\n",
      "Epoch [483/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000130674\n",
      "Epoch [483/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000130360\n",
      "Epoch [483/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000130049\n",
      "Epoch [483/500], Batch [60/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000129738\n",
      "Epoch [483/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000129430\n",
      "Epoch [483/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000129123\n",
      "Epoch [483/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000128817\n",
      "Epoch [483/500], Batch [100/110], Train Loss: 0.0017, Val Loss: 0.0001, LR: 0.0000128513\n",
      "Epoch [483/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000128211\n",
      "Epoch [484/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000127910\n",
      "Epoch [484/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000127611\n",
      "Epoch [484/500], Batch [30/110], Train Loss: 0.0016, Val Loss: 0.0001, LR: 0.0000127314\n",
      "Epoch [484/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000127018\n",
      "Epoch [484/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000126724\n",
      "Epoch [484/500], Batch [60/110], Train Loss: 0.0015, Val Loss: 0.0001, LR: 0.0000126431\n",
      "Epoch [484/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000126140\n",
      "Epoch [484/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000125851\n",
      "Epoch [484/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000125563\n",
      "Epoch [484/500], Batch [100/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000125277\n",
      "Epoch [484/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000124992\n",
      "Epoch [485/500], Batch [10/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000124710\n",
      "Epoch [485/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000124428\n",
      "Epoch [485/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000124148\n",
      "Epoch [485/500], Batch [40/110], Train Loss: 0.0018, Val Loss: 0.0001, LR: 0.0000123870\n",
      "Epoch [485/500], Batch [50/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000123594\n",
      "Epoch [485/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000123319\n",
      "Epoch [485/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000123045\n",
      "Epoch [485/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000122774\n",
      "Epoch [485/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000122504\n",
      "Epoch [485/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000122235\n",
      "Epoch [485/500], Batch [110/110], Train Loss: 0.0004, Val Loss: 0.0001, LR: 0.0000121968\n",
      "Epoch [486/500], Batch [10/110], Train Loss: 0.0009, Val Loss: 0.0001, LR: 0.0000121703\n",
      "Epoch [486/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000121439\n",
      "Epoch [486/500], Batch [30/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0000121177\n",
      "Epoch [486/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000120917\n",
      "Epoch [486/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000120658\n",
      "Epoch [486/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000120401\n",
      "Epoch [486/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000120145\n",
      "Epoch [486/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000119891\n",
      "Epoch [486/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000119639\n",
      "Epoch [486/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000119388\n",
      "Epoch [486/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000119139\n",
      "Epoch [487/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000118891\n",
      "Epoch [487/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000118645\n",
      "Epoch [487/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000118401\n",
      "Epoch [487/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000118158\n",
      "Epoch [487/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000117917\n",
      "Epoch [487/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000117677\n",
      "Epoch [487/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000117439\n",
      "Epoch [487/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000117203\n",
      "Epoch [487/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000116968\n",
      "Epoch [487/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000116735\n",
      "Epoch [487/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000116504\n",
      "Epoch [488/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000116274\n",
      "Epoch [488/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000116045\n",
      "Epoch [488/500], Batch [30/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000115819\n",
      "Epoch [488/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000115594\n",
      "Epoch [488/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000115370\n",
      "Epoch [488/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000115148\n",
      "Epoch [488/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000114928\n",
      "Epoch [488/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000114710\n",
      "Epoch [488/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000114493\n",
      "Epoch [488/500], Batch [100/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000114277\n",
      "Epoch [488/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000114063\n",
      "Epoch [489/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000113851\n",
      "Epoch [489/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000113641\n",
      "Epoch [489/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000113432\n",
      "Epoch [489/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000113224\n",
      "Epoch [489/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000113019\n",
      "Epoch [489/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000112815\n",
      "Epoch [489/500], Batch [70/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000112612\n",
      "Epoch [489/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000112411\n",
      "Epoch [489/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0002, LR: 0.0000112212\n",
      "Epoch [489/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000112014\n",
      "Epoch [489/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000111818\n",
      "Epoch [490/500], Batch [10/110], Train Loss: 0.0004, Val Loss: 0.0002, LR: 0.0000111624\n",
      "Epoch [490/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000111431\n",
      "Epoch [490/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000111240\n",
      "Epoch [490/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000111050\n",
      "Epoch [490/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000110862\n",
      "Epoch [490/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000110676\n",
      "Epoch [490/500], Batch [70/110], Train Loss: 0.0024, Val Loss: 0.0001, LR: 0.0000110491\n",
      "Epoch [490/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000110308\n",
      "Epoch [490/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000110126\n",
      "Epoch [490/500], Batch [100/110], Train Loss: 0.0012, Val Loss: 0.0001, LR: 0.0000109946\n",
      "Epoch [490/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000109768\n",
      "Epoch [491/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000109591\n",
      "Epoch [491/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000109416\n",
      "Epoch [491/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000109242\n",
      "Epoch [491/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000109070\n",
      "Epoch [491/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000108900\n",
      "Epoch [491/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000108731\n",
      "Epoch [491/500], Batch [70/110], Train Loss: 0.0016, Val Loss: 0.0001, LR: 0.0000108564\n",
      "Epoch [491/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000108399\n",
      "Epoch [491/500], Batch [90/110], Train Loss: 0.0008, Val Loss: 0.0002, LR: 0.0000108235\n",
      "Epoch [491/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000108073\n",
      "Epoch [491/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000107912\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "Epoch 491: OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.051 sec , Memory Usage: 305.25 MB\n",
      "Epoch [492/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000107753\n",
      "Epoch [492/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000107596\n",
      "Epoch [492/500], Batch [30/110], Train Loss: 0.0010, Val Loss: 0.0001, LR: 0.0000107440\n",
      "Epoch [492/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000107286\n",
      "Epoch [492/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000107133\n",
      "Epoch [492/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000106983\n",
      "Epoch [492/500], Batch [70/110], Train Loss: 0.0002, Val Loss: 0.0001, LR: 0.0000106833\n",
      "Epoch [492/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000106686\n",
      "Epoch [492/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000106539\n",
      "Epoch [492/500], Batch [100/110], Train Loss: 0.0050, Val Loss: 0.0001, LR: 0.0000106395\n",
      "Epoch [492/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000106252\n",
      "Epoch [493/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000106111\n",
      "Epoch [493/500], Batch [20/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000105971\n",
      "Epoch [493/500], Batch [30/110], Train Loss: 0.0024, Val Loss: 0.0001, LR: 0.0000105833\n",
      "Epoch [493/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105697\n",
      "Epoch [493/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105562\n",
      "Epoch [493/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105429\n",
      "Epoch [493/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105297\n",
      "Epoch [493/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105167\n",
      "Epoch [493/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000105039\n",
      "Epoch [493/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000104912\n",
      "Epoch [493/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000104787\n",
      "Epoch [494/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000104663\n",
      "Epoch [494/500], Batch [20/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000104542\n",
      "Epoch [494/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000104421\n",
      "Epoch [494/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000104303\n",
      "Epoch [494/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000104186\n",
      "Epoch [494/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000104070\n",
      "Epoch [494/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103956\n",
      "Epoch [494/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103844\n",
      "Epoch [494/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103733\n",
      "Epoch [494/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103624\n",
      "Epoch [494/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103517\n",
      "Epoch [495/500], Batch [10/110], Train Loss: 0.0017, Val Loss: 0.0001, LR: 0.0000103411\n",
      "Epoch [495/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103307\n",
      "Epoch [495/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103205\n",
      "Epoch [495/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103104\n",
      "Epoch [495/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000103004\n",
      "Epoch [495/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102907\n",
      "Epoch [495/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102811\n",
      "Epoch [495/500], Batch [80/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000102716\n",
      "Epoch [495/500], Batch [90/110], Train Loss: 0.0005, Val Loss: 0.0001, LR: 0.0000102623\n",
      "Epoch [495/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102532\n",
      "Epoch [495/500], Batch [110/110], Train Loss: 0.0011, Val Loss: 0.0001, LR: 0.0000102443\n",
      "Epoch [496/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102355\n",
      "Epoch [496/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102268\n",
      "Epoch [496/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102183\n",
      "Epoch [496/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102100\n",
      "Epoch [496/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000102019\n",
      "Epoch [496/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000101939\n",
      "Epoch [496/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000101860\n",
      "Epoch [496/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000101784\n",
      "Epoch [496/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000101709\n",
      "Epoch [496/500], Batch [100/110], Train Loss: 0.0003, Val Loss: 0.0001, LR: 0.0000101635\n",
      "Epoch [496/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000101563\n",
      "Epoch [497/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000101493\n",
      "Epoch [497/500], Batch [20/110], Train Loss: 0.0035, Val Loss: 0.0001, LR: 0.0000101424\n",
      "Epoch [497/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000101357\n",
      "Epoch [497/500], Batch [40/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000101292\n",
      "Epoch [497/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000101228\n",
      "Epoch [497/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000101166\n",
      "Epoch [497/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000101105\n",
      "Epoch [497/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000101047\n",
      "Epoch [497/500], Batch [90/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000100989\n",
      "Epoch [497/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100933\n",
      "Epoch [497/500], Batch [110/110], Train Loss: 0.0008, Val Loss: 0.0002, LR: 0.0000100879\n",
      "Epoch [498/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100827\n",
      "Epoch [498/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100776\n",
      "Epoch [498/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100727\n",
      "Epoch [498/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100679\n",
      "Epoch [498/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100633\n",
      "Epoch [498/500], Batch [60/110], Train Loss: 0.0008, Val Loss: 0.0002, LR: 0.0000100589\n",
      "Epoch [498/500], Batch [70/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100546\n",
      "Epoch [498/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0002, LR: 0.0000100505\n",
      "Epoch [498/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100465\n",
      "Epoch [498/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100427\n",
      "Epoch [498/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100391\n",
      "Epoch [499/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100356\n",
      "Epoch [499/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100323\n",
      "Epoch [499/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100292\n",
      "Epoch [499/500], Batch [40/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100262\n",
      "Epoch [499/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100233\n",
      "Epoch [499/500], Batch [60/110], Train Loss: 0.0014, Val Loss: 0.0001, LR: 0.0000100207\n",
      "Epoch [499/500], Batch [70/110], Train Loss: 0.0006, Val Loss: 0.0001, LR: 0.0000100182\n",
      "Epoch [499/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100158\n",
      "Epoch [499/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100136\n",
      "Epoch [499/500], Batch [100/110], Train Loss: 0.0013, Val Loss: 0.0001, LR: 0.0000100116\n",
      "Epoch [499/500], Batch [110/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000100098\n",
      "Epoch [500/500], Batch [10/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100081\n",
      "Epoch [500/500], Batch [20/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100065\n",
      "Epoch [500/500], Batch [30/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100052\n",
      "Epoch [500/500], Batch [40/110], Train Loss: 0.0001, Val Loss: 0.0001, LR: 0.0000100040\n",
      "Epoch [500/500], Batch [50/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100029\n",
      "Epoch [500/500], Batch [60/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100020\n",
      "Epoch [500/500], Batch [70/110], Train Loss: 0.0275, Val Loss: 0.0001, LR: 0.0000100013\n",
      "Epoch [500/500], Batch [80/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100007\n",
      "Epoch [500/500], Batch [90/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100003\n",
      "Epoch [500/500], Batch [100/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100001\n",
      "Epoch [500/500], Batch [110/110], Train Loss: 0.0000, Val Loss: 0.0001, LR: 0.0000100000\n",
      "Confusion Matrix:\n",
      "[[631   0]\n",
      " [  0 869]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   1.00000   1.00000       631\n",
      "           1    1.00000   1.00000   1.00000       869\n",
      "\n",
      "    accuracy                        1.00000      1500\n",
      "   macro avg    1.00000   1.00000   1.00000      1500\n",
      "weighted avg    1.00000   1.00000   1.00000      1500\n",
      "\n",
      "Total Errors: 0\n",
      "OK- Accuracy: 1.00000, Precision: 1.00000, Recall: 1.00000, F1: 1.00000, ROC AUC: 1.00000, AUPR (PR-AUC): 1.00000, Sensitivity: 1.00000, Specificity: 1.00000, Far: 0.0, False Positive Rate (FPR): 0.00000, False Negative Rate (FNR): 0.00000, Runtime: 0.051 sec , Memory Usage: 564.12 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEEDlist = [28, 7, 1234, 2025]\n",
    "for i, SEED in enumerate(SEEDlist):\n",
    "    train_loader, val_loader, test_loader = None, None, None\n",
    "    print(f\"=========== SEED: {SEED} , FOLD: {i+1}/{len(SEEDlist)}, D: {device} ===========\")\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    current_dir = \"Data/\"\n",
    "    df = pd.read_csv(os.path.join(current_dir, 'DDos.csv'))\n",
    "    encoder = LabelEncoder()\n",
    "    df[' Label'] = encoder.fit_transform(df[' Label'])\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna()\n",
    "    df = df.astype(int)\n",
    "    df = df.sample(n=10000, random_state=SEED, replace=False)\n",
    "    X = df.drop(' Label', axis=1)\n",
    "    y = df[' Label']\n",
    "    X.columns = X.columns.str.strip()\n",
    "    important_features = ['Bwd Packet Length Std', 'Average Packet Size', 'Flow Duration', 'Flow IAT Std']\n",
    "    important_df = X[important_features] * 2.0\n",
    "    remaining_df = X.drop(columns=important_features)\n",
    "    X = pd.concat([remaining_df, important_df], axis=1)\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    print(y.value_counts())\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    batch_size = 64\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    epochs = 500\n",
    "    SL = len(train_loader) * epochs\n",
    "    model = MODEL().to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=SL, eta_min=1e-5)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"=========== TP: {total_params:,} ===========\")\n",
    "    train(model, criterion, optimizer, scheduler, epochs, train_loader, val_loader, test_loader)\n",
    "    RES = test(model, test_loader)\n",
    "    print(RES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
